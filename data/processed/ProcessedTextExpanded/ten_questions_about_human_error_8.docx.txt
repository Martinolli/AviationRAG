Title: Ten Questions About Human Error: A New View of Human Factors And System Safety Chapters 6 – 7 – 8 Author(s): Sidney W. A. Dekker Category: Analysis, Human, Factors, Errors Tags: human, error, factors, accident Chapter 6 Why Do Operators Become Complacent? The introduction of powerful automation in a variety of transport applications has increased the emphasis on human cognitive work. Human opera­ tors on, for example, ship bridges or aircraft flight decks, spend much time integrating data, planning activities, and managing a suite of machine resources in the conduct of their tasks. This shift has contributed to the utility of a concept such as situation awareness. One large term can capture the extent to which operators are in tune with relevant process data and can form a picture of the system and its progress in space and time. As the Royal Majesty example in Chapter 5 showed, most high-tech settings are actually not characterized by a single human interacting with a machine. In almost all cases, multiple people—crews or teams of operators—-jointly interact with the automated system in pursuit of operational objectives. These crews or teams have to coordinate their activities with those of the system in order to achieve common goals. Despite the weight that crews (and human factors researchers) repeat­ only attribute to having a shared understanding of their system state and problems to be solved, the consensus in transportation human factors on a concept of crew situation awareness seems far off. It appears that various labels are used interchangeably to refer to the same basic phenomenon, for example, group situation awareness, shared problem models, team situation awareness, mutual knowledge, shared mental models, joint situation aware­ ness, and shared understanding. At the same time, results about what con­ constitutes the phenomenon are fragmented, and ideas on how to measure it remain divided. Methods to gain empirical access range from modified measures of practitioner expertise to questionnaires interjected into suddenly frozen simulation scenarios to implicit probes embedded in unfolding simulations of natural task behavior. Most critically, however, a common definition or model of crew situation awareness remains elusive. There is human factors research, for example, that claims to identify links between crew situation awareness and other parameters (such as planning or crew-member roles). However, such research often does not mention a definition of the phenomenon. This renders empirical demonstrations of the phenomenon unverifiable and inconclusive. After all, how can a researcher claim that he or she saw something if that some­ thing was not defined? Perhaps there is no need to define the phenomenon because everybody knows what it means. Indeed, situation awareness is what we call a folk model. It has come up from the practitioner community (fighter pilots in this case) to indicate the degree of coupling between hu­ man and environment. Folk models are highly useful because they can collapse complex, multidimensional problems into simple labels that every­ body can relate to. But this is also where the risks lie, certainly when researchers pick up on a folk label and attempt to investigate and model it scientifically. Situation awareness is not alone in this. Human factors today have more concepts that aim to provide insight into the human performance issues that underlie complex behavioral sequences. It is often tempting to mistake the labels themselves for deeper insight—something that is becoming in­ increasingly common in, for example, accident analyses. Thus, loss of situation awareness, automation complacency, and loss of effective crew resource management can now be found among the causal factors and conclusions in accident reports. This happens without further specification of the psychological mechanisms responsible for the observed behavior—much less how such mechanisms or behavior could have forced the sequence of events toward its eventual outcome. The labels (modernist replacements of the old pilot error) are used to refer to concepts that are intuitively meaning ­. Everyone is assumed to understand or implicitly agree on them, yet no effort is usually made to explicate or reach an agreement on the underlying mechanisms or precise definitions. People may no longer dare to ask what these labels mean, lest others suspect they are not really initiated in the particulars of their business. Indeed, large labels that correspond roughly to mental phenomena we know from daily life are deemed sufficient—they need no further explanation. This is often accepted practice for psychological phenomena because, as humans, we all have privileged knowledge about how the mind works (be­ because we all have one). However, a verifiable and detailed mapping be­ between the context-specific (and measurable) particulars of behavior on the one hand and a concept-dependent model on the other is not achieved—the jump from context specifics (somebody flying into a mountainside) to concept dependence (the operator must have lost SA) is immune to critique or verification. Folk models are not necessarily incorrect, but compared to articulated models, they focus on descriptions rather than explanations, and they are very hard to prove wrong. Folk models are pervasive in the history of science. One well-known example of a folk model from modern times is Freud’s psychodynamic model, which links observable behavior and emotions to nonobservable structures (id, ego, superego) and their interactions. One feature of folk models is that nonobservable constructs are endowed with the necessary causal power without much specification of the mechanism responsible for such causation. According to Kern (1998), for example, complacency can cause a loss of situation awareness. In other words, one folk problem causes another folk problem. Such assertions leave few people any wiser. Because both folk problems are constructs postulated by outside observers (and mostly post hoc), they cannot logically cause anything in the empirical world. Yet this is precisely what they are as­ summed to be capable of. In wrapping up a conference on situation aware­ ness, Charles Billings warned against this danger in 1996: The most serious shortcoming of the situation awareness construct as we have thought about it to date, however, is that it's too neat, too holistic, and too seductive. We heard here that deficient SA was a causal factor in many airline accidents associated with human error. We must avoid this trap: deficient situation awareness doesn't "cause" anything. Faulty spatial perception, diverted attention, inability to acquire data in the time available, deficient decision-making, perhaps, but not a deficient abstraction! (p. 3) What Billings did not mention is that "diverted attention" and "deficient decision-making" themselves are abstractions at some level (and post hoc ones at that). They are nevertheless less contentious because they provide a reasonable level of detail in their description of the psychological mechanisms that account for their causation. Situation awareness is too "neat" and "holistic" in the sense that it lacks such a level of detail and thus fails to account for a psychological mechanism that would connect features of the se­ sequence of events to the outcome. The folk model, however, was coined precisely because practitioners (pilots) wanted something "neat" and "holistic" that could capture critical but inexplicit aspects of their performance in complex, dynamic situations. We have to see their use of a folk model as legitimate. It can fulfill a useful function with respect to the concerns and goals of a user community. This does not mean that the concepts coined by users can be taken up and causally manipulated by scientists without serious foundational analysis and explication of their meaning. Resisting the temptation, however, can be difficult. After all, human factors is a discipline that lives by its applied usefulness. If the discipline does not generate anything of interest to ap­ plied communities, then why would they bother funding the work? In this sense, folk models can seem like a wonderfully convenient bridge between basic and applied worlds, between scientific and practitioner communities. Terms like situation awareness allow both camps to speak the same language. However, such conceptual sharing risks selling out to superficial validity. It may not do human factors a lot of good in the long run, nor may it really benefit the practitioner consumers of research results. Another folk concept is complacency. Why does people's vigilance de­ decline over time, especially when confronted with repetitive stimuli? Vigilance decrements have formed an interesting research problem ever since the birth of human factors during and just after the Second World War. The idea of complacency has always been related to vigilance problems. Al­ though complacency connotes something motivational (people must ensure that they watch the process carefully), the human factors literature actually has little in the way of explanation or definition. What is complacency? Why does it occur? If you want answers to these questions, do not turn to the human factors literature. You will not find answers there. Complacency is one of those constructs whose meaning is assumed to be known by everyone. This justifies taking it up in scientific discourse as something that can be manipulated or studied as an independent or dependent variable without having to go through the bother of defining what it actually is or how it works. In other words, complacency makes a "neat" and "holistic" case for studying folk models. DEFINITION BY SUBSTITUTION The most evident characteristic of folk models is that they define their central constructs by substitution rather than decomposition. A folk concept is explained simply by referring to another phenomenon or construct that it­ self is in equal need of explanation. Substitution is not the same as decom­ position: Substituting replaces one high-level label with another, whereas decomposition takes the analysis down into subsequent levels of greater de­ tail, which transform the high-level concept into increasingly measurable context specifics. A good example of definition by substitution is the label complacency in relation to the problems observed on automated flight decks. Most textbooks on aviation human factors talk about complacency and even endow it with causal power, but none really define (i.e., decom­ pose) it: • According to Wiener (1988, p. 452), "boredom and complacency are often mentioned" in connection with the out-of-the-loop issue in automated cockpits. However, whether complacency causes an out-of-the-loop condition or whether it is the other way around is left unanswered. • O'Hare and Roscoe (1990, p. 117) stated that "because autopilots have proved extremely reliable, pilots tend to become complacent and fail to monitor them." Complacency, in other words, is invoked to explain monitor failures. • Kern (1998, p. 240) maintained that "as pilots perform duties as system monitors, they will be lulled into complacency, lose situational awareness, and not be prepared to react in a timely manner when the system fails." Thus, complacency can cause a loss of situational awareness. However, how this occurs is left to the imagination. • On the same page in their textbook, Campbell and Bagshaw (1991, p. 126) said that complacency is both a "trait that can lead to a reduced aware­ ness of danger" and a "state of confidence plus contentment" (emphasis added). In other words, complacency is, at the same time, a long-lasting, enduring feature of personality (a trait) and a shorter-lived, transient phase in performance (a state). • For the purpose of categorizing incident reports, Parasuraman, Molly, and Singh (1993, p. 3) defined complacency as: "self-satisfaction which may result in non-vigilance based on an unjustified assumption of satisfactory system state." This is part definition but also part substitution: Self-satisfaction takes the place of complacency and is assumed to speak for itself. There is no need to make explicit which psychological mechanism of self-satisfaction arises or how it produces nonvigilance. It is, in fact, difficult to find real content on complacency in the human factors literature. The phenomenon is often described or mentioned in relation to some deviation or diversion from official guidance (people should coordinate, double-check, look—but they do not), which is both normativist and judgmental. The definition of "unjustified assumption of satisfactory system state" by Parasuraman et al. (1993) is emblematic of human factors' understanding of work by reference to externally dictated norms. If we want to understand complacency, the whole point is to analyze why the assumption of a satisfactory system state is justified (not unjustified) by those who are making that assumption. If it were unjustified, and they knew that, they would not make the assumption and would consequently not become complacent. Saying that an assumption of a satisfactory system state is unjustified (but people still keep making it—they must be motivationally deficient) does not explain much at all. None of the above examples really provide a definition of complacency. Instead, complacency is treated as self-evident (everybody knows what it means, right?), and thus, it can be defined by substituting one label for an­ another. The human factors literature equates complacency with many different labels, including boredom, overconfidence, contentment, unwarranted faith, overreliance, self-satisfaction, and even a low index of suspicion. So if we ask, "What do you mean by 'complacency'?" and the reply is, "Well, it is self-satisfaction," we can be expected to say, "Oh, of course, now I understand what you mean." But do we really? Explanation by substitution actually raises more questions than it answers. By failing to propose an articulated psychological mechanism responsible for the behavior observed, we are left to wonder. How is it that complacency produces vigilance decrements, or how is it that complacency leads to a loss of situation awareness? The explanation could be a decay of neurological connections, fluctuations in learning and motivation, or a conscious trade-off between competing goals in a changing environment. Such definitions, which begin to operationalize the large concept of complacency, suggest possible probes that a researcher could use to monitor for the target effect. However, because none of the descriptions of complacency available today offer any such roads to in­ sight, claims that complacency was at the heart of a sequence of events are immune to critique and falsification. IMMUNITY AGAINST FALSIFICATION Most philosophies of science rely on the empirical world as the touchstone or ultimate arbiter (a reality check) for postulated theories. Following Popper's rejection of the inductive method in the empirical sciences, theories, and hypotheses can only be deductively validated by means of falsifiability. This usually involves some form of empirical testing to look for exceptions to the postulated hypothesis, where the absence of contradictory evidence becomes corroboration of the theory. Falsification deals with the central weakness of the inductive method of verification, which, as pointed out by David Hume, requires an infinite number of confirming empirical demon­ stations. Falsification, on the other hand, can work on the basis of only one empirical instance, which proves the theory wrong. As seen in Chapter 3, this is, of course, a highly idealized, almost clinical conceptualization of the scientific enterprise. Yet, regardless, theories that do not permit falsification at all are highly suspect. The resistance of folk models against falsification is known as immunization. Folk models leave assertions about empirical reality underspecified, without a trace for others to follow or critique. For example, a senior training captain once asserted that cockpit discipline is compromised when any of the following attitudes are prevalent: arrogance, complacency, and over­ confidence. Nobody can disagree because the assertion is underspecified and, therefore, immune against falsification. This is similar to psychoanalysts claiming that obsessive-compulsive disorders are the result of overly harsh toilet training that fixates the individual in the anal stage. In the same vein, if the question of "Where are we headed?" from one pilot to the other is interpreted as a loss of situation awareness (Aeronautica Civil, 1996), this claim is immune against falsification. The journey from context-specific behavior (people asking questions) to the postulated psychological mechanism (loss of situation awareness) is made in one big leap, leaving no trace for others to follow or critique. Current theories of situation awareness are not sufficiently articulated to be able to explain why asking questions about direction represents a loss of situation awareness. Some theories may superficially appear to have the characteristics of good scientific models, yet just below the surface, they lack an articulated mechanism that is amenable to falsification. Although falsifiability may at first seem like a self-defeating criterion for scientific progress, the opposite is true: The most falsifiable models are usually also the most informative ones in the sense that they make stronger and more demonstrable claims about reality. In other words, falsifiability and informativeness are two sides of the same coin. Folk Models Versus Young and Promising Models One risk in rejecting folk models is that the baby is thrown out with the bath water. In other words, there is the risk of rejecting even those models that may be able to generate useful empirical results if only given the time and opportunity to do so. Indeed, the more articulated human factors constructs (such as decision-making and diagnosis) are distinguished from the less articulated ones (situation awareness, complacency) in part by their maturity by how long they have been around in the discipline. What opportunity should the younger ones receive before being rejected as unproductive? The answer to this question hinges, once again, on falsifiability. Ideal progress in science is described as the succession of theories, each of which is more falsifiable (and thus more informative) than the one before it. Yet, when we assess loss of situation awareness or complacency as more novel explanations of phenomena that were previously covered by other explanations, it is easy to see that falsifiability has actually decreased rather than increased. Take, for example, an automation-related accident that occurred when situation awareness or automation-induced complacency did not yet exist in 1973. The aircraft in question was on approach in rapidly changing weather conditions. It was equipped with a slightly deficient flight director (a device on the central instrument panel showing the pilot where to go based on an unseen variety of sensory inputs), which the captain of the air­ plane distrusted. The airplane struck a seawall bounding Boston's Logan Airport about 1 kilometer short of the runway and slightly to the side of it, killing all 89 people onboard. In its comment on the crash, the National Transportation Safety Board explained how an accumulation of discrepancies, none critical in themselves, can rapidly deteriorate into a high-risk situation without positive flight management. The first officer, who was flying, was preoccupied with the information presented by his flight-director systems, to the detriment of his attention to altitude, heading, and airspeed control (NTSB, 1974). Today, both automation-induced complacency of the first officer and a loss of situation awareness of the entire crew could likely be cited as the causes of this crash. (Actually, the fact that the same set of empirical phenomena can comfortably be grouped under either label—complacency or loss of situation awareness—is additional testimony to the undifferentiated and underspecified nature of these concepts.) These supposed explanations (complacency, loss of situation awareness) were obviously not needed in 1974 to deal with this accident. The analysis left us instead with more de­ tailed, more falsifiable, and more traceable assertions that linked features of the situation (e.g., an accumulation of discrepancies) with measurable or demonstrable aspects of human performance (diversion of attention to the flight director vs. other sources of data). The decrease of falsifiability represented by complacency and situation awareness as hypothetical contenders in explaining this crash represents the inverse of scientific progress and, therefore, argues for the rejection of such novel concepts. OVERGENERALIZATION The lack of specificity of folk models and the inability to falsify them con­ tribute to their overgeneralization. One famous example of overgeneralization in psychology is the inverted U curve, also known as the Yerkes-Dodson law. Ubiquitous in human factors textbooks, the inverted-U curve couples arousal with performance (without clearly stating any units of either arousal or performance), where a person's best performance is claimed to occur between too much arousal (or stress) and too little, tracing a sort of hyperbole. The original experiments were, however, neither about performance nor about arousal (Yerkes & Dodson, 1908). They were not even about humans. Examining "the relation between stimulus strength and habit formation," the researchers subjected laboratory rats to electrical shocks to see how quickly they decided to take a particular pathway versus another. The conclusion was that rats learn best (that is, they form habits most rapidly) at any but the highest or lowest shock. The results approximated an inverted U only with a most generous curve fitting; the x-axis was never defined in psychological terms but in terms of shock strength, and even this was confounded: Yerkes and Dodson used different levels of shock which were too poorly calibrated to know how different they really were. The subsequent overgeneralization of the Yerkes-Dodson results (to no fault of their own, incidentally) has confounded stress and arousal, and after a century, there is still little evidence that any kind of inverted-U relationship holds for stress (or arousal) and human performance. Overgeneralizations take narrow laboratory findings and apply them uncritically to any broad situation where behavioral particulars bear some prima facie resemblance to the phenomenon that was investigated under controlled circumstances. Other examples of overgeneralization and overapplication include per­ perceptual tunneling (putatively championed by the crew of an airliner that descended into the Everglades after its autopilot was inadvertently switched off) and the loss of effective Crew Resource Management (CRM) as major explanations of accidents (e.g., Aeronautica Civil, 1996). A most frequently quoted sequence of events with respect to cockpit resource management is the flight of an iced-up airliner from Washington National Airport in the winter of 1982 that ended shortly after takeoff on the 14th Street bridge and in the Potomac River. The basic cause of the accident is said to be the copilot's unassertive remarks about an irregular engine instrument reading (despite the fact that the copilot was known for his assertiveness). This supposed explanation hides many other factors that might be more relevant, including air- traffic control pressures, the controversy surrounding rejected takeoffs close to decision speed, the sensitivity of the aircraft type to icing, and its pitch-up tendency with even little ice on the slats (devices on the wing's leading edge that help it fly at slow speeds), and ambiguous engineering language in the airplane manual to describe the conditions for the use of engine anti-ice. In an effort to explain complex behavior and still make a connection to the applied worlds from which it owes its existence, human transportation factors may be doing themselves a disservice by inventing and uncritically using folk models. If we use models that do not articulate the performance measures that can be used in the particular contexts that we want to speak about, we can make no progress in better understanding the sources of success and failure in our operational environments. Chapter 7 Why Don't They Follow the Procedures? People do not always follow procedures. We can easily observe this when watching people at work, and managers, supervisors, and regulators (or anybody else responsible for safe outcomes of work) often consider it to be a large practical problem. In Chapter 6, we saw how complacency would be a very unsatisfactory label for explaining practical drift away from written guidance. But what lies behind it, then? In hindsight, after a mishap, rule violations seem to play such a dominant causal role. If only they had followed the procedure! Studies keep re­ turning the basic finding that procedure violations precede accidents. For example, an analysis carried out for an aircraft manufacturer identified "pilot deviation from basic operational procedure" as a primary factor in almost 100 accidents (Lautman & Gallimore, 1987, p. 2). One methodological problem with such work is that it selects its cases on the dependent variable (the accident), thereby generating tautologies rather than findings. But performance variations, especially those at odds with written guidance, easily get overestimated for their role in the sequence of events: The interpretation of what happened may then be distorted by naturalistic biases to overestimate the possible causal role of unofficial action or a procedural violation. . . . While it is possible to show that violations of procedures are involved in many safety events, many violations of procedures are not, and in­ deed some violations (strictly interpreted) appear to represent more effective ways of working. (McDonald, Corrigan, & Ward, 2002, pp. 3-5) As seen in Chapter 4, hindsight turns complex, tangled histories laced with uncertainty and pressure into neat, linear anecdotes with obvious choices. What look like violations from the outside, in hindsight, are often actions that make sense given the pressures and trade-offs that exist on the inside of real work. Finding procedure violations as causes or contributors to mis­ haps, in other words, says more about us and the biases we introduce when looking back on a sequence of events than it does about people who were doing actual work at the time. Yet if procedure violations are judged to be such a large ingredient of mishaps, then it can be tempting, in the wake of failure, to introduce even more procedures, to change existing ones, or to enforce stricter compliance. For example, shortly after a fatal shootdown of two U.S. Black Hawk helicopters over Northern Iraq by U.S. fighter jets, "higher headquarters in Europe dispatched a sweeping set of rules in documents several inches thick to 'absolutely guarantee' that whatever caused this tragedy would never happen again" (Snook, 2000, p. 201). It is a common, but not typically satisfactory, reaction. Introducing more procedures does not necessarily avoid the next incident, nor do exhortations to follow the rules more care­ fully necessarily increase compliance or enhance safety. In the end, a mismatch between procedures and practice is not unique to accident sequences. Not following procedures does not necessarily lead to trouble, and safe outcomes may be preceded by just as many procedural deviations as accidents are. PROCEDURE APPLICATION AS RULE-FOLLOWING When rules are violated, are these bad people ignoring the rules? Or are these bad rules ill-matched to the demands of real work? To be sure, procedures with the aim of standardization can play an important role in shaping safe practice. Commercial aviation is often held up as a prime example of the powerful effect of standardization on safety. But there is a deeper, more complex dynamic where the real practice is continually adrift from official written guidance, settling at times, unsettled, and shifting at others. There is a deeper, more complex interplay whereby practice some­ times precedes and defines the rules rather than being defined by them. In those cases, is a violation an expression of defiance or an expression of compliance—people following practical rules rather than official, impractical ones? These possibilities lie between two opposing models of what procedures mean and what they, in turn, mean for safety. These models of procedures guide how organizations think about making progress on safety. The first model is based on the notion that not following procedures can lead to un­ safe situations. These are its premises: • Procedures represent the best thought-out, and thus the safest, way to carry out a job. • The following procedure is mostly simple IF-THEN rule-based mental activity: IF this situation occurs, then this algorithm (e.g., checklist) ap­ plies. • Safety results from people following procedures. • For progress on safety, organizations must invest in people's knowledge of procedures and ensure that procedures are followed. In this idea of procedures, those who violate them are often depicted as putting themselves above the law. These people may think that rules and procedures are made for others but not for them, as they know how to really do the job. This idea of rules and procedures suggests that there is something exceptional or misguidedly elitist about those who choose not to follow the rules. After a maintenance-related mishap, for example, investigators found that "the engineers who carried out the flap change demonstrated a willing­ ness to work around difficulties without reference to the design authority, in­ including situations where compliance with the maintenance manual could not be achieved" (Joint Aviation Authorities, 2001). The engineers demonstrated a "willingness." Such terminology embodies notions of volition (the engineers had a free choice either to comply or not) and full rationality (they knew what they were doing). They violated willingly. Violators are wrong, be­ because rules and procedures prescribe the best, safest way to do a job, independent of who does that job. Rules and procedures are for everyone. Such characterizations are naive at best and always misleading. If you know where to look, daily practice is testimony to the ambiguity of procedures and evidence that procedures are a rather problematic category of human work. First, real work takes place in a context of limited resources and multiple goals and pressures. Procedures assume that there is time to do them in certainty (of what the situation is) and sufficient information available (e.g., about whether tasks are accomplished according to the procedure). This already keeps rules at a distance from actual tasks because real work seldom meets those criteria. Work-to-rule strikes show how it can be impossible to follow the rules and get the job done at the same time. Aviation line maintenance is emblematic: A job-perception gap exists where supervisors are convinced that safety and success result from mechanics following procedures—a sign-off means that applicable procedures were followed. However, mechanics may encounter problems that require the right tools or parts; the aircraft may be parked far away from the base. Or there may be too little time: Aircraft with a considerable number of problems may have to be turned around for the next flight within half an hour. Mechanics, consequently, see success as the result of their evolved skills at adapting, inventing, compromising, and improvising in the face of local pressures and challenges on the line—a sign-off means the job was ac­ accomplished in spite of resource limitations, organizational dilemmas, and pressures. Those mechanics who are most adept are valued for their productive capacity, even at higher organizational levels. Unacknowledged by those levels, though, are the vast informal work systems that develop so mechanics can get work done, advance their skills at improvising and satisficing, impart them to one another, and condense them in unofficial, self-made documentation (McDonald et al., 2002). Seen from the outside, a defining characteristic of such informal work systems would be routine nonconformity. But from the inside, the same behavior is a mark of expertise, fueled by professional and interpreter pride. And, of course, informal work systems emerge and thrive in the first place because procedures are in­ adequate to cope with local challenges and surprises and because procedures' conception of work collides with the scarcity, pressure, and multiple goals of real work. Some of the safest complex, dynamic work not only occurs despite the procedures—such as aircraft line maintenance—but without procedures altogether. Rochlin et al. (1987, p. 79), commenting on the introduction of ever heavier and capable aircraft onto naval aircraft carriers, noted that "there were no books on the integration of this new hardware into existing routines and no other place to practice it but at sea. Moreover, little of the process was written down, so that the ship in operation is the only reliable manual." Work is "neither standardized across ships nor, in fact, written down systematically and formally anywhere." Yet naval aircraft carriers with inherent high-risk operations have a remarkable safety record, like other so-called high-reliability organizations (Rochlin, 1999; Rochlin, LaPorte, & Roberts, 1987). Documentation cannot present any close relationship to situated action because of the unlimited uncertainty and ambiguity involved in the activity. Especially where normal work mirrors the uncertainty and criticality of emergencies, rules emerge from practice and experience rather than preceding it. Procedures, in other words, end up following work instead of specifying action beforehand. Human factors have so far been unable to trace and model such coevolution of humans and systems, of work and rules. Instead, it has typically imposed a mechanistic, static view of one best practice from the top down. Procedure-following can also be antithetical to safety. In the 1949 U.S. Mann Gulch disaster, firefighters who perished were the ones sticking to the organizational mandate to carry their tools everywhere (Weick, 1993). In this case, as in others (e.g., Carley, 1999), people faced the choice be­ tween following the procedure or surviving. Procedures Are Limited in Rationalizing Human Work This, then, is the tension. Procedures are seen as an investment in safety— but it turns out that they are not always. Procedures are thought to be required to achieve safe practice—yet they are not always necessary, nor likely ever sufficient for creating safety. Procedures spell out how to do the job safely—yet following all the procedures can lead to an inability to get the job done. Though a considerable practical problem, such tensions are underreported and under-analyzed in the human factors literature. There is always a distance between a written rule and an actual task. This distance needs to be bridged; the gap must be closed, and the only thing that can close it is human interpretation and application. Ethnographer Ed Hutchins has pointed out how procedures are not just externalized cognit asks (Wright & McCarthy, 2003). Externalizing a cognitive task would transplant it from the head to the world, for example, onto a checklist. Rather, following a procedure requires cognitive tasks that are not specified in the procedure; transforming the written procedure into an activity requires cognitive work. Procedures are inevitably incomplete specifications of action: They contain abstract descriptions of objects and actions that relate only loosely to particular objects and actions that are encountered in the actual situation (Suchman, 1987). Take, for example, the lubrication of the jackscrew on MD-80s from chapter 2—something that was done incompletely and at increasingly greater intervals before the crash of Alaska 261. This is part of the written procedure that describes how the lubrication work should be done (NTSB, 2002, pp. 29-30): A. Open access doors 6307, 6308, 6306, and 6309 B. Lube per the following . . . 3. JACKSCREW Apply light coat of grease to threads, then operate mechanism through full range of travel to distribute lubricant over length of jackscrew. C. Close doors 6307, 6308, 6306, and 6309. This leaves a lot to the imagination or to the mechanic's initiative. How much is a "light" coat? Do you do apply the grease with a brush (if a "light coat" is what you need), or do you pump it onto the parts directly with the grease gun? How often should the mechanism (jackscrew plus nut) be operated through its full range of travel during the lubrication procedure? None of this is specified in the written guidance. It is little wonder that: Investigators observed that different methods were used by maintenance personnel to accomplish certain steps in the lubrication procedure, including the manner in which grease was applied to the acme nut fitting and the acme screw and the number of times the trim system was cycled to distribute the grease immediately after its application. (NTSB, 2002, p. 116) In addition, actually carrying out the work is difficult enough. As noted in Chapter 2, the access panels of the horizontal stabilizer were just large enough to allow a hand through, which would then block the view of any­ thing that went on inside. As a mechanic, you can either look at what you have to do or what you have just done or actually do it. You cannot do both at the same time, because the access doors are too small. This makes judgments about how well the work is being done rather difficult. The investigation discovered as much when they interviewed the mechanic responsible for the last lubrication of the accident airplane: "When asked how he deter­ mined whether the lubrication was being accomplished properly and when to stop pumping the grease gun, the mechanic responded, 'I don't' " (NTSB, 2002, p. 31). The time the lubrication procedure took was also unclear, as there was ambiguity about which steps were included in the procedure. Where does the procedure begin and where does it end, after access has been created to the area, or before? And is closing the panels part of it as well, as far as time estimates are concerned? Having heard that the entire lubrication process takes "a couple of hours," investigators learned from the mechanic of the airplane accident that the lubrication task took "roughly. . . probably an hour" to accomplish. It was not entirely clear from his testimony whether he was including the removal of the access panels in his estimate. When asked whether his 1-hour estimate included gaining access to the area, he replied, "No, that would probably take a little—well, you've got probably a dozen screws to take out of the one panel, so that's—I wouldn't think any more than an hour." The questioner then stated, "Including access?" and the mechanic responded, 'Yeah." (NTSB, 2002, p. 32) As the procedure for lubricating the MD-80 jackscrew indicates, and Mc­ Donald et al. (2002) remind us, formal documentation cannot be relied on, nor is it normally available in a way that supports a close relationship to action. There is a distinction between universalistic and particularistic rules: Universalistic rules are very general prescriptions (e.g., "Apply a light coat of grease to threads") but remain at a distance from their actual application. In fact, all universalistic rules or general prescriptions develop into particularistic rules as experience accumulates. With experience, people encounter the conditions under which universalistic rules need to be applied and become increasingly able to specify those conditions. As a result, universalistic rules assume appropriate local expressions through practice. Wright and McCarthy (2003) have pointed out that procedures come out of the scientific management tradition, where their main purpose was a minimization of human variability, maximization of predictability, a rationalization of work. Aviation contains a strong heritage: Procedures in commercial aviation represent and allow a routinization that makes it possible to conduct safety-critical work with perfect strangers. Procedures are a substitute for knowing coworkers. The actions of a copilot are predictable not because the copilot is known (in fact, you may never have flown with him or her) but because the procedures make them predictable. Without such standardization, it would be impossible to cooperate safely and smoothly with unknown people. In the spirit of scientific management, human factors also assume that order and stability in operational systems are achieved rationally and mechanistically and that control is implemented vertically (e.g., through task analyses that produce prescriptions of work to be carried out). In addition, the strong influence of information-processing psychology on human factors has reinforced the idea of procedures as IF-THEN rule-following, where procedures are akin to a program in a computer that, in turn, serves as input signals to the human information processor. The algorithm specified by the procedure becomes the software on which the human processor runs. But it is not that simple. Following procedures in the sense of applying them in practice requires more intelligence. It requires additional cognitive work. This brings us to the second model of procedures and safety. PROCEDURE APPLICATION AS SUBSTANTIVE COGNITIVE ACTIVITY People at work must interpret procedures with respect to a collection of actions and circumstances that the procedures themselves can never fully specify (e.g., Suchman, 1987). In other words, procedures are not the work itself. Work, especially in complex, dynamic workplaces, often requires subtle, local judgments with regard to the timing of subtasks, relevance, importance, prioritization, and so forth. For example, there is no technical rea­ son why a before-landing checklist in a commercial aircraft could not be automated. The kinds of items on such a checklist (e.g., hydraulic pumps, gear, flaps) are mostly mechanical and could be activated on the basis of predetermined logic without having to rely on or constantly remind a human to do so. Yet, no before-landing checklist is fully automated today. The reason is that approaches for landing differ—they can differ in terms of timing, workload, or other priorities. Indeed, the reason is that the check­ list is not the job itself. The checklist is, to repeat Suchman, a resource for action; it is one way for people to help structure activities across roughly similar yet subtly different situations. Variability in this is inevitable. Circumstances change or are not as foreseen by those who designed the procedures. Safety, then, is not the result of rote rule-following; it is the result of people's insight into the features of situations that demand certain actions and people being skillful at finding and using a variety of resources (including written guidance) to accomplish their goals. This suggests a second model on procedures and safety: • Procedures are resources for action. Procedures do not specify all circumstances to which they apply. Procedures cannot dictate their own application. • Applying procedures successfully across situations can be a substantive and skillful cognitive activity. • Procedures cannot, in themselves, guarantee safety. Safety results from people being skillful at judging when and how (and when not) to adapt procedures to local circumstances. • For progress on safety, organizations must monitor and understand the reasons behind the gap between procedures and practice. Additionally, organizations must develop ways that support people's skill at judging when and how to adapt. Procedures and Unusual Situations Although there is always a distance between the logic dictated in written guidance and real actions to be taken in the world, prespecified guidance is especially inadequate in the face of novelty and uncertainty. Adapting procedures to fit unusual circumstances is a substantive cognitive activity. Take for instance the crash of a large passenger aircraft near Halifax, Nova Scotia in 1998. After an uneventful departure, a burning smell was detected, and not much later, smoke was reported inside the cockpit. Carley (1999) characterized the two pilots as respective embodiments of the models of procedures and safety: The co-pilot preferred a rapid descent and suggested dumping fuel early so that the aircraft would not be too heavy to land. But the captain told the copilot, who was flying the plane, not to descend too fast and insisted they cover applicable procedures (checklists) for dealing with smoke and fire. The captain delayed a decision on dumping fuel. With the fire developing, the aircraft became uncontrollable and crashed into the sea, taking all 229 lives onboard with it. There were many good reasons for not immediately diverting to Halifax: Neither pilot was familiar with the airport, they would have to fly an approach procedure that they were not very proficient at, applicable charts and information on the airport were not easily available, and an extensive meal service had just been started in the cabin. Yet, part of the example illustrates a fundamental double bind for those who encounter surprise and have to apply procedures in practice (Woods& Shattuck, 2000): • If rote rule following persists in the face of cues that suggest procedures should be adapted, this may lead to unsafe outcomes. People can get blamed for their inflexibility, their application of rules without sensitivity to context. • If adaptations to unanticipated conditions are attempted without complete knowledge of circumstance or certainty of outcome, unsafe results may occur, too. In this case, people get blamed for their deviations and nonadherence. In other words, people can fail to adapt or attempt adaptations that may fail. Rule following can become a desynchronized and increasingly irrelevant activity, decoupled from how events and breakdowns are really unfolding and multiplying throughout a system. In the Halifax crash, as is often the case, there was uncertainty about the very need for adaptations (How badly ailing was the aircraft, really?) as well as uncertainty about the effect and safety of adapting: How much time would the crew have to change their plans? Could they skip fuel dumping and still attempt a landing? Potential adaptations and the ability to project their potential for success were not necessarily supported by specific training or overall professional indoctrination. Civil aviation, after all, tends to emphasize the first model: Stick with procedures, and you will most likely be safe (e.g., Lautman & Gallimore, 1987). Tightening procedural adherence through threats of punishment or other supervisory interventions does not remove the double bind. In fact, it may tighten the double bind—making it more difficult for people to develop judgment of how and when to adapt. Increasing the pressure to com­ ply increases the probability of failures to adapt—compelling people to adopt a more conservative response criterion. People will require more evidence for the need to adapt, which takes time, and time may be scarce in cases that call for adaptation (as in the aforementioned case). Merely stressing the importance of following procedures can increase the number of cases in which people fail to adapt in the face of surprise. Letting people adapt without adequate skill or preparation, on the other hand, can increase the number of failed adaptations. One way out of the double bind is to develop people's skill at adapting. This means giving them the ability to balance the risks between the two possible types of failure: failing to adapt or attempting adaptations that may fail. It requires the development of judgment about local conditions and the opportunities and risks they present, as well as an awareness of larger goals and constraints that operate on the situation. Development of this skill could be construed, to paraphrase Rochlin (1999), as planning for surprise. Indeed, as Rochlin (p. 1549) observed, the culture of safety in high-reliability organizations anticipates and plans for possible failures in "the continuing expectation of future surprise." Progress on safety also hinges on how an organization responds in the wake of failure (or even the threat of failure). Post-mortems can quickly reveal a gap between procedures and local practice, and hindsight inflates the causal role played by unofficial action (McDonald et al., 2002). The response, then, is often to try to forcibly close the gap between procedures and practice by issuing more procedures or policing practice more closely. The role of informal patterns of behavior and what they represent (e.g., re­ source constraints, organizational deficiencies or managerial ignorance, countervailing goals, peer pressure, professionalism, and perhaps even better ways of working) all go misunderstood. Real practice, as done in the vast in­ formal work systems, is driven and kept underground. Even though failures offer each sociotechnical system an opportunity for critical self-examination, accident stories are developed in which procedural deviations play a major, evil role and are branded as deviant and causal. The official reading of how the system works or is supposed to work is once again re-invented: Rules mean safety, and people should follow them. High-reliability organizations, in contrast, distinguish themselves by their constant investment in trying to monitor and understand the gap between procedures and practice. The common reflex is not to try to close the gap but to understand why it exists. Such understanding provides insight into the grounds for informal patterns of activity and opens ways to improve safety by sensitivity to people's local operational context. The Regulator: From Police to Partner, there is always a tension between centralized guidance and local practice, creating a clear dilemma for those tasked with regulating safety-critical industries. The dominant regulatory instrument consists of rules and checking that those rules are followed. However, forcing operational people to stick to rules can lead to ineffective, unproductive, or even unsafe local actions. For various jobs, following the rules and getting the task done are mutually exclusive. On the other hand, letting people adapt their local practices in the face of pragmatic demands can make them sacrifice global system goals or miss other constraints or vulnerabilities that operate on the system. Helping people solve this fundamental trade-off is not a matter of pushing the criterion one way or the other. Discouraging people's attempts at adaptation can increase the number of failures to adapt in situations where adaptation is necessary. Allowing procedural leeway without encouraging organizations to invest in people's skills at adapting, on the other hand, can increase the number of failed attempts at adaptation. This means that the gap between rule and task, between written procedure and actual job, needs to be bridged by the regulator as much as by the operator. Inspectors who work for regulators need to apply rules as well: find out what exactly the rules mean and what their implications are when imposed on a field of practice. The development from universalism to particularism applies to regulators, too. This raises questions about the role that inspectors should play. Should they function as police—checking to what extent the market is abiding by the laws they are supposed to uphold? In that case, should they apply a black-and-white judgment (which would ground a number of companies immediately)? Or, if there is a gap between procedure and practice that inspectors and operators share and both need to bridge, can inspectors be partners in joint efforts toward progress on safety? The latter role is one that can only develop in good faith, though such good faith may be the very by-product of the development of a new kind of relationship, or partnership, towards progress on safety. Mis­ matches between rules and practice are no longer seen as the logical conclusion of an inspection but rather as the starting point, the beginning of joint discoveries about real practice and the context in which it occurs. What are the systemic reasons (organizational, regulatory, resource-related) that help create and sustain the mismatch? The basic criticism of an inspector's role as a partner is easy to anticipate: Regulators should not come too close to the ones they regulate, lest their relationship become too cozy and objective judgment of performance against safety criteria become impossible. But regulators need to come close to those they regulate in any case. Regulators (or their inspectors) need to be insiders in the sense of speaking the language of the organization they inspect and understanding the kind of business they are in order to gain the respect and credibility of the informants they need most. At the same time, regulators need to be outsiders—resisting getting integrated into the worldview of the one they regulate. Once on the inside of that system and its worldview, it may be increasingly difficult to discover the potential drift into failure. What is normal to the operator is normal to the inspector. The tension between having to be an insider and an outsider at the same time is difficult to resolve. The conflictual, adversarial model of safety regulation has, in many cases, not proven productive. It leads to window dressing and posturing on the part of the operator during inspections, as well as secrecy and obfuscation of safety- and work-related information at all other times. As airline maintenance testifies, the real practice is easily driven underground. Even for regulators who apply their power as police rather than as partners, the struggle of having to be insider and outsider at the same time is not automatically resolved. Issues of access to information (the relevant information about how people really do their work, even when the inspector is not there) and inspector credibility demand that there be a relationship between regulator and operator that allows such access and credibility to develop. Organizations (including regulators) who wish to make progress on safety with procedures need to: • Monitor the gap between procedure and practice and try to understand why it exists (and resist trying to close it by simply telling people to comply). • Help people develop skills to judge when and how to adapt (and resist only telling people they should follow procedures). But many organizations or industries do neither. They may not even know or want to know (or be able to afford to know) about the gap. Take aircraft maintenance again. A variety of workplace factors (communication problems, physical or hierarchical distance, industrial relations) obscure the gap. For example, continued safe outcomes of existing practice give super­ visors no reason to question their assumptions about how work is done (if they are safe, they must be following procedures down there). There is wider industry ignorance, however (McDonald et al., 2002). In the wake of failure, informal work systems typically retreat from view, gliding out of investigators' reach. What goes misunderstood or unnoticed is that informal work systems compensate for the organization's inability to provide the basic resources (e.g., time, tools, documentation with a close relationship to action) needed for task performance. Satisfied that violators got caught and that formal prescriptions of work were once again amplified, the organizational system changed little or nothing. It completes another cycle of stability, typified by stagnation of organizational learning and no progress on safety (McDonald et al.). GOAL CONFLICTS AND PROCEDURAL DEVIANCE As discussed in Chapter 2, a major engine behind routine divergence from written guidance is the need to pursue multiple goals simultaneously. Mul­ tiple goals mean goal conflicts. As Dorner (1989) remarked, "Contradictory goals are the rule, not the exception, in complex situations" (p. 65). In a study of flight dispatchers, for example, Smith (2001) illustrated the basic dilemma. Would bad weather hit a major hub airport or not? What should the dispatchers do with all the airplanes en route? Safety (by making air­ craft divert widely around the weather) would be a pursuit that "tolerates a false alarm but deplores a miss" (p. 361). In other words, if safety is the ma­ jor goal, then making all the airplanes divert even if the weather would not end up at the hub (a false alarm) is much better than not making them di­ vert and sending them headlong into bad weather (a miss). Efficiency, on the other hand, severely discourages the false alarm, whereas it can actually deal with a miss. As discussed in Chapter 2, this is the essence of most operational systems. Though safety is a (stated) priority, these systems do not exist to be safe. They exist to provide a service or product, to achieve economic gain, and to maximize capacity utilization. But still, they have to be safe. One starting point, then, for understanding a driver behind routine deviations is to look deeper into these goal interactions, these basic incompatibilities in what people need to strive for in their work. Of particular interest is how people themselves view these conflicts from inside their operational reality and how this contrasts with management (and regulator) views of the same activities. NASA's "Faster, Better, Cheaper" organizational philosophy in the late 1990s epitomized how multiple, contradictory goals are simultaneously present and active in complex systems. The loss of the Mars Climate Orbiter and the Mars Polar Lander in 1999 was ascribed in large part to the irreconcilability of the three goals (faster and better and cheaper), which drove down the cost of launches, made for shorter, aggressive mission schedules, eroded personnel skills and peer interaction, limited time, reduced the workforce, and lowered the level of checks and balances normally found (National Aeronautics and Space Administration, 2000). People argued that national aeronautics and space administration should pick any two of the three goals. Faster and cheaper would not mean better. Better and cheaper would mean slower. Faster and better would be more expensive. Such reduction, however, obscures the actual reality facing operational personnel in safety-critical settings. These people are there to pursue all three goals simultaneously—fine-tuning their operation, as Starbuck and Milliken (1988) said, to "render it less redundant, more efficient, more profitable, cheaper, or more versatile" (p. 323), fine-tuning, in other words, to make it faster, better, cheaper. The 2003 Space Shuttle Columbia accident focused attention on the maintenance work that was done on the Shuttle's external fuel tank, once again revealing the differential pressures of having to be safe and getting the job done (better, but also faster and cheaper). A mechanic working for the contractor, whose task it was to apply the insulating foam to the external fuel tank, testified that it took just a couple of weeks to learn how to get the job done, thereby pleasing upper management and meeting production schedules. An older worker soon showed him how he could mix the base chemicals of the foam in a cup and brush it over scratches and gouges in the insulation without reporting the repair. The mechanic soon found himself doing this hundreds of times, each time without filling out the required paperwork. Scratches and gouges that were brushed over with the mixture from the cup basically did not exist as far as the organization was concerned. Those that did not exist could not maintain the production schedule for the external fuel tanks. Inspectors often did not check. A company program that once had paid workers hundreds of dollars for finding defects had been watered down, virtually inverted by incentives for getting the job done now. Goal interactions are critical in such experiences, which contain all the ingredients of procedural fluidity, maintenance pressure, the meaning of incidents worth reporting, and their connections to drift into failure. As in most operational work, the distance between formal, externally dictated logics of action and actual work is bridged with the help of those who have been there before, who have learned how to get the job done (without apparent safety consequences), and who are proud to share their professional experience with younger, newer workers. Actual practice by newcomers settles at a distance from the formal description of the job. Deviance becomes routinized. This is part of the vast informal networks characterizing much maintenance work, including informal hierarchies of teachers and apprentices, informal documentation of how to actually get work done, informal procedures and tasks, and informal teaching practices. Inspectors did not check, did not know, or did not report. Managers were happy that production schedules were met and happy that fewer defects were being discovered—normal people doing normal work in a normal organization. Or that is what it seemed to everybody at the time. Once again, the notion of an incident, of something that was worthy of reporting (a defect), got blurred against a background of routine nonconformity. What was normal versus what was deviant was no longer so clear. Goal conflicts between safer, better, and cheaper were reconciled by doing the work more cheaply, superficially better (brushing over gouges), and apparently without cost to safety. As long as orbiters kept coming back safely, the contractor must have been doing something right. Understanding the potential side effects was very difficult, given the historical mission success rate. The lack of failures was seen as a validation that current strategies to prevent hazards were sufficient. Could anyone foresee, in a vastly complex system, how local actions as trivial as brushing chemicals from a cup could one day align with other factors to push the system over the edge? Re­ call from Chapter 2: What cannot be believed cannot be seen. Past success was taken as a guarantee of continued safety. The Internalization of External Pressure Some organizations pass on their goal conflicts to individual practitioners quite openly. Some airlines, for example, pay their crews a bonus for on-time performance. An aviation publication commented on one of those operators (a new airline called Excel, flying from England to holiday destinations): "As part of its punctuality drive, Excel has introduced a bonus scheme to give employees a bonus should they reach the agreed target for the year. The aim of this is to focus everyone's attention on keeping the air­ craft on schedule" (Airliner World, 2001, p. 79). Such plain acknowledgment of goal priorities, however, is not common. Most important goal conflicts are never made so explicit, arising rather from multiple irreconcilable directives from different levels and sources, from subtle and tacit pressures, from management or customer reactions to particular trade-offs. Organizations often resort to "conceptual integration, or plainly put doublespeak" (Dorner, 1989, p. 68). For example, the operating manual of another air­ line opens by stating that "(1) our flights shall be safe; (2) our flights shall be punctual; (3) our customers will find value for money." Conceptually, this is Dorner's (1989) doublespeak documentary integration of incompatibles. It is impossible, in principle, to do all three simultaneously, as with NASA's faster, better, cheaper. Whereas incompatible goals arise at the level of an organization and its interaction with its environment, the actual managing of goal conflicts under uncertainty gets pushed down into local operating units—control rooms, cockpits, and the like. There, the conflicts are to be negotiated and resolved in the form of thousands of little and larger daily decisions and trade-offs. These are no longer decisions and trade-offs made by the organization but by individual operators or crews. It is this insidious delegation, this hand-over, where the internalization of external pressure takes place. Crews of one airline describe their ability to negotiate these multiple goals while under the pressure of limited resources as "the blue feeling" (referring to the dominant color of their fleet). This feeling represents the willingness and ability to put in the work to actually deliver on all three goals simultaneously (safety, punctuality, and value for money). This would confirm that practitioners do pursue incompatible goals of faster, better, and cheaper all at the same time and are aware of it, too. In fact, practitioners take their ability to reconcile the irreconcilable as a source of considerable professional pride. It is seen as a strong sign of their expertise and competence. The internalization of external pressure, this usurpation of organizational goal conflicts by individual crews or operators, is not yet well described or modeled. This, again, is a question about the dynamics of the macro-micro connection that we saw in Chapter 2. How is it that a global tension between efficiency and safety seeps into local decisions and tradeoffs by individual people or groups? These macrostructural forces, which operate on an entire company, find their most prominent expression in how local work groups make assessments about opportunities and risks (see also Vaughan, 1996). Institutional pressures are reproduced, or per­ haps really manifested, in what individual people do, not by the organization as a whole. But how does this connection work? Where do external pressures become internal? When do the problems and interests of an organization under the pressure of resource scarcity and competition become the problems and interests of individual actors at several levels within that organization? The connection between external pressure and its internalization is relatively easy to demonstrate when an organization explicitly advertises how operators' pursuit of one goal will lead to individual rewards (a bonus scheme to keep everybody focused on the priority of schedule). However, such cases are probably rare, and it is doubtful whether they represent the actual internalization of a goal conflict. It becomes more difficult when the connection and the conflicts are more deeply buried in how operators transpose global organizational aims onto individual decisions. For example, the blue feeling signals aircrews' strong identification with their organization (which flies blue aircraft) and what it and its brand stand for (safety, reliability, value for money). Yet it is a feeling that only individuals or crews can have, a feeling because it is internalized. Insiders point out how some crews or commanders have the blue feeling, whereas others do not. It is a personal attribute, not an organizational property. Those who do not have the blue feeling are marked by their peers—seldom supervisors—for their insensitivity to, or disinterest in, the multiplicity of goals and their unwillingness to do substantive cognitive work necessary to reconcile the irreconcilable. These practitioners do not reflect the corps' professional pride because they will always make the easiest goal win over the others (e.g., "Don't worry about customer service or capacity utilization, it's not my job"), choosing the path of least resistance and least work in the eyes of their peers. In the same airline, those who try to adhere to minute rules and regulations are called "Operating Manual worshippers"—a clear signal that their way of dealing with goal contradictions is not only perceived as cognitively cheap (just go back to the book, it will tell you what to do) but as hampering the collective ability to actually get the job done, diluting the blue feeling. The blue feeling, then, is also not just a personal attribute but an interpeer commodity that affords comparisons, categorizations, and competition among members of the peer group, independent of other layers or levels in the organization. Similar interpeer pride and perception operate as the subtle engine behind the negotiation among different goals in other professions, too, for example, flight dispatchers, air-traffic controllers, or aircraft maintenance workers (McDonald et al., 2002). The latter group (aircraft maintenance) has incorporated even more internal mechanisms to deal with goal interactions. The demand to meet technical requirements clashes routinely with time or other resource constraints such as inadequate time, personnel, tools, parts, or functional work environment (McDonald et al., 2002). The vast internal, sub-surface networks of routines, illegal documentation, and shortcuts, which from the outside would be seen as massive infringement of existing procedures, are a result of the pressure to reconcile and compromise. Actual work practices constitute the basis for technicians' strong professional pride and sense of responsibility for delivering safe work that exceeds even technical requirements. Seen from the inside, it is the role of the technician to apply judgment founded on his or her knowledge, experience, and skill—not on for­ mal procedure. Those most adept at this are highly valued for their productive capacity, even at higher organizational levels. Yet upon formal scrutiny (e.g., an accident inquiry), informal networks and practices often retreat from view, yielding only a bare-bones version of work in which the nature of goal compromises and informal activities is never explicit, acknowledged, understood, or valued. Similar to the British Army on the Somme, management in some maintenance organizations occasionally de­ sides (or pretends) that there is no local confusion, that there are no con­ traditions or surprises. In their official understanding, there are rules, and people follow the rules, resulting in safe outcomes. People who do not follow the rules are more prone to causing accidents, as the hindsight bias inevitably points out. To people on the work floor, in contrast, manage­ do not even understand the fluctuating pressures on their work, let alone the strategies necessary to accommodate those (McDonald et al.). Both cases (the blue feeling and maintenance work) challenge human factors' traditional reading of violations as deviant behavior. Human factors want work to mirror prescriptive task analyses or rules, and violations breach vertical control implemented through such managerial or design directives. Seen from the inside of people's own work, however, violations become compliant behavior. Cultural understandings (e.g., expressed in notions of a blue feeling) affect interpretative work so that even if people's behavior is objectively deviant, they will see their own conduct as conforming (Vaughan, 1999). Their behavior is compliant with the emerging, local, internalized ways to accommodate multiple goals important to the organization (maximizing capacity utilization but doing so safely, meeting technical requirements, but also deadlines). It is compliant, also, with a complex of peer pressures and professional expectations in which unofficial action yields better, quicker ways to do the job, in which unofficial action is a sign of competence and expertise, where unofficial action can override or out­ smart hierarchical control and compensate for higher level organizational deficiencies or ignorance. ROUTINE NONCONFORMITY The gap between procedures and practice is not constant. After the creation of new work (e.g., through the introduction of new technology), time can go by before applied practice stabilizes, likely at a distance from the rules as written for the system on the shelf. Social science has characterized this migration from tightly coupled rules to more loosely coupled practice, variously as "fine-tuning" (Starbuck & Milliken, 1988) or "practical drift" (Snook, 2000). Through this shift, applied practice becomes the pragmatic imperative; it settles into a system as normative. Deviance (from the original rules) becomes normalized; nonconformity becomes routine (Vaughan, 1996). The literature has identified important ingredients in the normalization of deviance, which can help organizations understand the nature of the gap between procedures and practice: • Rules that are overdesigned (written for tightly coupled situations, for the worst case) do not match actual work most of the time. In real work, there is slack: time to recover, opportunity to reschedule, and getting the job done better or more smartly (Starbuck & Milliken). This mismatch creates an inherently unstable situation that generates pressure for change (Snook). • Emphasis on local efficiency or cost-effectiveness pushes operational people to achieve or prioritize one goal or a limited set of goals (e.g., customer service, punctuality, capacity utilization). Such goals are typically easily measurable (e.g., customer satisfaction, on-time performance), whereas it is much more difficult to measure how much is borrowed from safety. • Past success is taken as a guarantee of future safety. Each operational success achieved at incremental distances from the formal, original rules can establish a new norm. From here a subsequent departure is once again only a small incremental step (Vaughan). From the outside, such fine-tuning constitutes incremental experimentation in uncontrolled settings (Starbucks & Milliken). On the inside, incremental nonconformity is an adaptive response to scarce resources, multiple goals, and often competition. • Departures from the routine become routine. Seen from the inside of people's own work, violations become compliant behavior. They are compliant with the emerging, local ways to accommodate multiple goals important to the organization (maximizing capacity utilization but doing so safely; meeting technical requirements, but also deadlines). They are com­ pliant, also, with a complex of peer pressures and professional expectations in which unofficial action yields better, quicker ways to do the job, in which unofficial action is a sign of competence and expertise, where unofficial action can override or outsmart hierarchical control and compensate for higher level organizational deficiencies or ignorance. Although a gap between procedures and practice always exists, there are different interpretations of what this gap means and what to do about it. As pointed out in Chapter 6, human factors may see the gap between procedures and practice as a sign of complacency—operators' self-satisfaction with how safe their practice or their system is or a lack of discipline. Psychologists may see routine nonconformity as expressing a fundamental tension between multiple goals (production and safety) that pull workers in oppo­ site directions: getting the job done but also staying safe. Others highlight the disconnect that exists between distant supervision or preparation of the work (as laid down in formal rules) on the one hand, and local, situated ac­ tion on the other. Sociologists may see in the gap a political lever applied on management by the work floor, overriding or outsmarting hierarchical control and compensating for higher-level organizational deficiencies or ignorance. To the ethnographer, routine nonconformity would be interest­ ing not just because of what it says about the work or the work context but because of what it says about what the work means to the operator. The distance between procedures and practice can create widely divergent images of work. Is routine nonconformity an expression of elitist operators who consider themselves to be above the law, of people who demonstrate a willingness to ignore the rules? Work, in that case, is about individual choices, supposedly informed choices between doing that work well or badly, between following the rules or not. Or is routine nonconformity a systematic by-product of the social organization of work, where it emerges from the interactions between the organizational environment (scarcity and competition), internalized pressures, and the underspecified nature of written guidance? In that case, work is seen as fundamentally contextualized, constrained by environmental uncertainty and organizational characteristics, and influenced only to a small extent by individual choice. People's ability to balance these various pressures and influences on procedure following depends in large part on their history and experience. As Wright and McCarthy (2003) pointed out, there are currently very few ways in which this experience can be given a legitimate voice in the design of procedures. As Chapter 8 shows, a more common way of responding to what is seen as human unreliability is to introduce more automation. Automation has no trouble following algorithms. In fact, it could not run without any. Yet, such literalism can be a mixed blessing. Chapter 8 Can We Automate Human Error Out of the System? If people cannot be counted on to follow procedures, should we not simply marginalize human work? Can automation eliminate human unreliability and error? Automation extends our capabilities in many, if not all, transportation modes. In fact, automation is often presented and implemented precisely because it helps systems and people perform better. It may even make operational lives easier: reducing task load, increasing access to information, helping the prioritization of attention, providing reminders, and doing work for us where we cannot. What about reducing human error? Many indeed have the expectation that automation will help reduce human error. Just look at some of the evidence: All kinds of transport achieve higher navigation accuracy with satellite guidance; pilots are now able to circumvent pitfalls such as thunderstorms, wind shear, mountains, and collisions with other aircraft; and situation awareness improves dramatically with the introduction of moving map displays. So, with these benefits, can we automate human error out of the system? The thought behind the question is simple. If we automate part of a task, then the human does not carry out that part. If humans do not carry out that part, there is no possibility of human error. As a result of this logic, there was a time (and in some quarters, there perhaps still is) when automating everything we technically could was considered the best idea. The Air Transport Association of America (ATA) observed, for example, that "during the 1970s and early 1980s. . . the concept of automating as much as possible was considered appropriate" (ATA, 1989, p. 4). It would lead to greater safety, greater capabilities, and other benefits. NEW CAPABILITIES, NEW COMPLEXITIES But, really, can we automate human error out of the system? There are problems. With new capabilities come new complexities. We cannot just automate part of a task and assume that the human-machine relationship remains unchanged. Though it may have shifted (with the humans doing less and the machines doing more), there is still an interface between humans and technology. And the work that goes on at that interface has likely changed drastically. Increasing automation transforms hands-on operators into supervisory controllers and managers of a suite of automated and other human resources. With their new work come new vulnerabilities and new error opportunities. With new interfaces (from pointers to pictures, from single parameter gauges to computer displays) come new pathways to human-machine coordination breakdown. Transportation has witnessed the transformation of work by automation first-hand and documented its consequences widely. Automation does not do away with what we typically call human error, just as (or precisely because) it does not do away with human work. There is still work to do for people. It is not that the same kinds of errors occur in automated systems as in manual systems. Automation changes the expression of expertise and error; it changes how people can perform well and changes how their performance breaks down, if and when it does. Automation also changes opportunities for error recovery (often not for the better) and, in many cases, delays the visible consequences of errors. New forms of coordination breakdowns and accidents have emerged as a result. Data Overload Automation does not replace human work. Instead, it changes the work it is designed to support. And with these changes come new burdens. Take system monitoring, for example. There are concerns that automation can create data overload. Rather than taking away cognitive burdens from people, automation introduces new ones, creating new types of monitoring and memory tasks. Because automation does so much, it also can show much (and indeed, there is much to show). If there is much to show, data overload can occur, especially in pressurized, high-workload, or unusual situations. Our ability to make sense of all the data generated by automation has not kept pace with systems' ability to collect, transmit, transform, and present data. But data overload is a pretty complex phenomenon, and there are different ways of looking at it (see Woods, Patterson, & Roth, 2002). For example, we can see it as a workload bottleneck problem. When people experience data overload, it is because of fundamental limits in their internal information-processing capabilities. If this is the characterization, then the solution lies in even more automation. More automation, after all, will take work away from people. And taking work away will reduce workload. One area where the workload-reduction solution to the data-overload problem has been applied is in the design of warning systems. It is there that fears of data overload are often most prominent. Incidents in aviation and other transportation modes keep stressing the need for better support of human problem solving during dynamic fault scenarios. People complain of too much data, of illogical presentations, of warnings that interfere with other work, of a lack of order, and of no rhyme or reason to the way in which warnings are presented. Workload reduction during dynamic fault management is so important because problem solvers in dynamic domains need to diagnose malfunctions while maintaining process integrity. Not only must failures be managed while keeping the process running (e.g., keeping the aircraft flying); their implications for the ability to keep the process running in the first place need to be understood and acted on. Keeping the process intact and diagnosing failures are interwoven cognitive demands in which timely understanding and intervention are often crucial. A fault in a dynamic processes typically produces a cascade of disturbances or failures. Modern airliners and high-speed vessels have their systems tightly packed together because there is not much room onboard. Systems are also cross-linked in many intricate ways, with electronic interconnections increasingly common as a result of automation and computerization. This means that failures in one system quickly affect other systems, perhaps even along nonfunctional propagation paths. Failure crossover can occur simply because systems are located next to one another, not because they have anything functional in common. This may defy operator logic or knowledge. The status of single components or systems, then, may not be that interesting for a operator. In fact, it may be highly confusing. Rather, the operator must see, through a forest of seemingly disconnected failures, the structure of the problem so that a solution or countermeasure becomes evident. Also, given the dynamic process managed, which issue should be addressed first? What are the postconditions of these failures for the remainder of operations (i.e., what is still operational, how far can I go, what do I need to reconfigure)? Is there any trend? Are there noteworthy events and changes in the monitored process right now? Will any of this get worse? These are the types of questions that are critical to answer in successful dynamic fault management. Current warning systems in commercial aircraft do not go far in answering these questions, something that is confirmed by pilots' assessments of these systems. For example, pilots comment on too much data, particularly all kinds of secondary and tertiary failures, with no logical order, and primary faults (root causes) that are rarely, if ever, highlighted. The representation is limited to message lists, something that we know hampers operators' visualization of the state of their system during dynamic failure scenarios. Yet not all warning systems are the same. Current warning systems show a range of automated support, from not doing much at all, through prioritizing and sorting warnings, to doing something about the failures, to doing most of the fault management and not showing much at all anymore. Which works best? Is there any merit to seeing data overload as a workload bottleneck problem, and do automated solutions help? An example of a warning system that basically shows everything that goes wrong inside an aircraft's systems, much in order of appearance, is that of the Boeing 767. Messages are presented chronologically (which may mean the primary fault appears somewhere in the middle or even at the bottom of the list) and failure severity is coded through color. A warning system that departs slightly from this baseline is for example the Saab 2000, which sorts the warnings by inhibiting messages that do not require pilot actions. It displays the remaining warnings chronologically. The primary fault (if known) is placed at the top, however, and if a failure results in an automatic system reconfiguration, then this is shown too. The result is a shorter list than the Boeing's, with a primary fault at the top. Next as an example comes the Airbus A320, which has a fully defined logic for warning-message prioritization. Only one failure is shown at the time, together with immediate action items required of the pilot. Subsystem information can be displayed on demand. Primary faults are thus highlighted, together with guidance on how to deal with them. Finally, there is the MD-11, which has the highest degree of autonomy and can respond to failures without asking the pilot to do so. The only exceptions are nonreversible actions (e.g., an engine shutdown). For most failures, the system informs the pilot of system reconfiguration and presents system status. In addition, the system recognizes combinations of failures and gives a common name to these higher order failures (e.g., Dual Generator). As could be expected, response latency on the Boeing 767-type warning system is longest (Singer & Dekker, 2000). It takes a while for pilots to sort through the messages and figure out what to do. Interestingly, they also get it wrong more often on this type of system. That is, they misdiagnose the primary failure more often than on any of the other systems. A nonprioritized list of chronological messages about failures seems to defeat even the speed-accuracy trade-off: Longer dwell times on the display do not help people get it right. This is because the production of speed and accuracy are cognitive: Making sense of what is going wrong inside an aircraft's systems is a demanding cognitive task, where problem representation has a profound influence on people's ability to do it successfully (meaning fast and correct). Modest performance gains (faster responses and fewer misdiagnoses) can be seen on a system like that of the Saab 2000, but the Airbus A320 and MD-11 solutions to the workload bottleneck problem really seem to pay off. Performance benefits really accrue with a system that sorts through the failures, shows them selectively, and guides the pilot in what to do next. In our study, pilots were quickest to identify the primary fault in the failure scenario with such a system, and made no misdiagnoses in assessing what it was (Singer & Dekker). Similarly, a warning system that itself contains or counteracts many of the failures and shows mainly what is left to the pilot seems to help people in quickly identifying the primary fault. These results, however, should not be seen as justification for simply automating more of the failure-management task. Human performance difficulties associated with high-automation participation in difficult or novel circumstances are well known, such as brittle procedure following where operators follow heuristic cues from the automation rather than actively seeking and dealing with information related to the disturbance chain. Instead, these results indicate how progress can be made by changing the representational quality of warning systems altogether, not just by automating more of the human task portion. If guidance is beneficial, and if knowing what is left is useful, then the results of this study tell designers of warning systems to shift to another view of referents (the thing in the process that the symbol on the display refers to). Warning-system designers would have to get away from relying on single systems and their status as referents to show on the display, and move toward referents that fix on higher order variables that carry more meaning relative to the dynamic fault-management task. Referents could integrate current status with future predictions, for example, or cut across single parameters and individual systems to reveal the structure behind individual failures and show consequences in terms that are operationally immediately meaningful (e.g., loss of pressure, loss of thrust). Another way of looking at data overload is as a clutter problem—there is simply too much on the display for people to cope with. The solution to data overload as a clutter problem is to remove stuff from the display. In warning-system design, for example, this may result in guidelines that stress how no more than a certain number of lines must be filled up on a warning screen. Seeing data overload as clutter, however, is completely insensitive of context. What seems clutter in one situation may be highly valuable, or even crucial, in another situation. The crash of an Airbus A330 during a test flight at the factory field in Toulouse, France in 1994 provides a good demonstration of this (see Billings, 1997). The aircraft was on a certification test flight to study various pitch-transition control laws and how they worked during an engine failure at low altitude, in a lightweight aircraft with a rearward center of gravity (CG). The flight crew included a highly experienced test pilot, a copilot, a flight-test engineer, and three passengers. Given the lightweight and rearward CG, the aircraft got off the runway quickly and easily and climbed rapidly, with a pitch angle of almost 25° nose-up. The autopilot was engaged 6 seconds after takeoff. Immediately after a short climb, the left engine was brought to idle power and one hydraulic system was shut down in preparation for the flight test. Now the autopilot had to simultaneously manage a very low speed, an extremely high angle of attack, and asymmetrical engine thrust. After the captain disconnected the autopilot (this was only 19 seconds after takeoff) and reduced power on the right engine to regain control of the aircraft, even more airspeed was lost. The aircraft stalled, lost altitude rapidly, and crashed 36 seconds after takeoff. When the airplane reached a 25° pitch angle, autopilot, and flight director mode information was automatically removed from the primary flight display in front of the pilots. This is a sort of declutter mode. It was found that, because of the high rate of ascent, the autopilot had gone into altitude-acquisition mode (called ALT* in the Airbus) shortly after takeoff. In this mode there is no maximum pitch protection in the automatic flight system software (the nose can go as high as the autopilot commands it to go, until the laws of aerodynamics intervene). In this case, at low speed, the autopilot was still trying to acquire the altitude commanded (2,000 feet), pitching up to it, and sacrificing airspeed in the process. But ALT* was not shown to the pilots because of the declutter function. So the lack of pitch protection was not announced, and may not have been known to them. Declutter has not been a fruitful or successful way of trying to solve data overload (see Woods et al., 2002), precisely because of the context problem. Reducing data elements on one display calls for that knowledge to be represented or retrieved elsewhere (people may need to pull it from memory instead), lest it be altogether unavailable. Merely seeing data overload as a workload or clutter problem is based on false assumptions about how human perception and cognition work. Questions about maximum human data-processing rates are misguided because this maximum, if there is one at all, is highly dependent on many factors, including people's experience, goals, history, and directed attention. As alluded to earlier in the book, people are not passive recipients of observed data; they are active participants in the intertwined processes of observation, action, and sense making. People employ all kinds of strategies to help manage data, and impose meaning on it. For example, they redistribute cognitive work (to other people, to artifacts in the world), they represent problems themselves so that solutions or countermeasures become more obvious. Clutter and workload characterizations treat data as a unitary input phenomenon, but people are not interested in data, they are interested in meaning. And what is meaningful in one situation may not be meaningful in the next. Declutter functions are context insensitive, as are workload reduction measures. What is interesting, or meaningful, depends on context. This makes designing a warning or display system highly challenging. How can a designer know what the interesting, meaningful or relevant pieces of data will be in a particular context? This takes a deep understanding of the work as it is done, and especially as it will be done once the new technology has been implemented. Recent advances in cognitive work analysis (Vicente, 1999) and cognitive task design (Hollnagel, 2003) presented ways forward, and more is said about such envisioning of future work toward the end of this chapter. Adapting to Automation, Adapting the Automation In addition to knowing what (automated) systems are doing, humans are also required to provide the automation with data about the world. They need to input things. In fact, one role for people in automated systems is to bridge the context gap. Computers are dumb and dutiful: They will do what they are programmed to do, but their access to context, to a wider environment, is limited—limited, in fact, to what has been predesigned or preprogrammed into them. They are literalist in how they work. This means that people have to jump in to fill a gap: They have to bridge the gulf between what the automation knows (or can know) and what really is happening or relevant out there in the world. The automation, for example, will calculate an optimal descent profile in order to save as much fuel as possible. But the resulting descent may be too steep for crew (and passenger) taste, so pilots program in an extra tailwind, tricking the computers into descending earlier and eventually more shallow (because the tailwind is fictitious). The automation does not know about this context (preference for certain descent rates over others), so the human has to bridge the gap. Such tailoring of tools is a very human thing to do: People will shape tools to fit the exact task they must fulfill. But tailoring is not risk- or problem-free. It can create additional memory burdens, impose cognitive load when people cannot afford it, and open up new error opportunities and pathways to coordination breakdowns between humans and machines. Automation changes the task for which it was designed. Automation, though introducing new capabilities, can increase task demands and create new complexities. Many of these effects are, in fact, unintended by the designers. Also, many of these side effects remain buried in actual practice and are hardly visible to those who only look for the successes of new machinery. Operators who are responsible for (safe) outcomes of their work are known to adapt technology so that it fits their actual task demands. Operators are known to tailor their working strategies so as to insulate themselves from the potential hazards associated with using the technology. This means that the real effects of technology change can remain hidden beneath a smooth layer of adaptive performance. Operational people will make it work, no matter how recalcitrant or ill suited to the domain the automation, and its operating procedures, really may be. Of course, the occasional breakthroughs in the form of surprising accidents provide a window onto the real nature of automation and its operational consequences. But such potential lessons quickly glide out of view under the pressure of the fundamental surprise fallacy. Apparently successful adaptation by people in automated systems, though adaptation in unanticipated ways, can be seen elsewhere in how pilots deal with automated cockpits. One important issue on high-tech flight decks is knowing what mode the automation is in (this goes for other applications such as ship's bridges too: Recall the Royal Majesty from chap. 5). Mode confusion can lie at the root of automation surprises, with people thinking that they told the automation to do one thing whereas it was actually doing another. How do pilots keep track of modes in an automated cockpit? The formal instrument for tracking and checking mode changes and status is the FMA, or flight-mode annunciator, a small strip that displays contractions or abbreviations of modes (e.g., Heading Select mode is shown as heading or heading SEL) in various colors, depending on whether the mode is armed (i.e., about to become engaged) or engaged. Most airline procedures require pilots to call out the mode changes they see on the FMA. One study monitored flight crews during a dozen return flights between Amsterdam and London on a full flight simulator (Bjorklund, Alfredsson, & Dekker, 2003). Where both pilots were looking and how long was measured by EPOG (eye-point-of-gaze) equipment, which uses different kinds of techniques ranging from laser beams to measuring and calibrating saccades, or eye jumps that can track the exact focal point of a pilot's eyes in a defined visual field (see Fig. 8.1). Pilots do not look at the FMA much at all. And they talk even less about it. Very few call-outs are made the way they should be (according to the procedures). Yet this does not seem to have an effect on automation-mode awareness, nor on the airplane's flight path. Without looking or talking, most pilots apparently still know what is going on inside the automation. In this one study, 521 mode changes occurred during the 12 flights. About 60% of these were pilot-induced (i.e., because the pilot changed a setting in the automation), and the rest were automation-induced. Two out of five mode changes were never visually verified (meaning neither pilot looked at their FMA during 40% of all mode changes). The pilot flying checked a little less than the pilots not flying, which could be a natural reflection of the role division: Pilots who are flying the aircraft have other sources of flight-related data they need to look at, whereas the pilot not flying can oversee the entire process, thereby engaging more often in checks of what the automation modes are. There are also differences between captains and first officers as well (even after you correct for pilot-flying vs. pilot-not-flying roles). Captains visually verified the transitions in 72% of the cases, versus 47% for first officers. This may mirror the ultimate responsibility that captains have for safety of flight, yet there was no expectation that this would translate into such concrete differences in automation monitoring. Amount of experience on automated aircraft types was ruled out as being responsible for the difference. Of 512 mode changes, 146 were called out. If that does not seem like much, consider this: Only 32 mode changes (that is about 6%) were called out after the pilot looked at the FMA. The remaining call-outs continuing airworthines management exposition either before looking at the FMA, or without looking at the FMA at all. Such a disconnect between seeing and saying suggests that there are other cues that pilots use to establish what the automation is doing. The FMA does not serve as a major trigger for getting pilots to call out modes. Two out of five mode transitions on the FMA are never even seen by entire flight crews. In contrast to instrument monitoring in non-glass-cockpit aircraft, monitoring for mode transitions is based more on a pilot's mental model of the automation (which drives expectations of where and when to look) and an understanding of what the current situation calls for. Such models are often incomplete and buggy and it is not surprising that flight crews neither visually nor verbally verify many mode transitions. At the same time, a substantial number of mode transitions are actually anticipated correctly by flight crews. In those cases where pilots do call out a mode change, four out of five visual identifications of those mode changes are accompanied or preceded by a verbalization of their occurrence. This suggests that there are multiple under-investigated resources that pilots rely on for anticipating and tracking automation-mode behavior (including pilot mental models). The FMA, designed as the main source of knowledge about automation status, actually does not provide a lot of that knowledge. It triggers a mere one out of five call-outs and gets ignored altogether by entire crews for a whole 40% of all mode transitions. Proposals for new regulations are, unfortunately, taking shape around the same old display concepts. For example, Joint Advisory Circular ACJ 25.1329 (Joint Aviation Authorities, 2003, p. 28) said that: "The transition from an armed mode to an engaged mode should provide an additional attention-getting feature, such as boxing and flashing on an electronic display (per AMJ25-11) for a suitable, but brief, period (e.g., ten seconds) to assist in flight crew awareness." But flight-mode annunciators are not at all attention getting, whether there is boxing or flashing or not. Indeed, empirical data show (as it has before, see Mumaw, Sarter, & Wickens, 2001) that the FMA does not "assist in flight crew awareness" in any dominant or relevant way. If design really is to cap­ ture crew's attention about automation status and behavior, it will have to do radically better than annunciating abstruse codes in various hues and boxing or flashing times. The call-out procedure appears to be mis calibrated with respect to real work in a real cockpit, because pilots basically do not follow formal verification and call-out procedures at all. Forcing pilots to visually verify the FMA first and then call out what they see bears no similarity to how actual work is done, nor does it have much sensitivity to the conditions under which such work occurs. Call-outs may well be the first task to go out the window when workload goes up, which is also confirmed by this type of research. In addition to the few formal call-outs that do occur, pilots communicate implicitly and informally about mode changes. Implicit communication surrounding altitude capture could for example be "Coming up to one-three-zero, (capture)" (referring to flight level 130). There appear to be many different strategies to support mode awareness, and very few of them actually overlap with formal procedures for visual verification and call-outs. Even during the 12 flights of the Bjorklund et al. (2003) study, there were at least 18 different strategies that mixed checks, timing, and participation. These strategies seem to work as well as, or even better than, the official procedure, as crew communications on the 12 flights revealed no automation surprises that could be traced to a lack of mode awareness. Perhaps mode awareness does not matter that much for safety after all. There is an interesting experimental side effect here: If mode awareness is measured mainly by visual verification and verbal call-outs, and crews neither look nor talk, then are they unaware of modes, or are the researchers unaware of pilots' awareness? This poses a puzzle: Crews who neither talk nor look can still be aware of the mode their automation is in, and this, indeed seems to be the case. But how, in that case, is the researcher (or your company, or line-check pilot) to know? The situation is one answer. By simply looking at where the aircraft is going, and whether this overlaps with the pilots' intentions, an observer can get to know something about apparent pilot awareness. It will show whether pilots missed something or not. In the research reported here however, pilots missed nothing: There were no unexpected aircraft behaviors from their perspective (Bjorklund et al., 2003). This can still mean that the crews were either not aware of the modes and it did not matter, or they were aware but the research did not capture it. Both may be true. MABA-MABA OR ABRACADABRA The diversity of experiences and research results from automated cockpits shows that automation creates new capabilities and complexities in ways that may be difficult to anticipate. People adapt to automation in many different ways, many of which have little resemblance to formally established procedures for interacting with the automation. Can automation, in a very Cartesian, dualistic sense, replace human work, thereby reducing human error? Or is there a more complex coevolution of people and technology? Engineers and others involved in automation development are often led to believe that there is a simple answer and, in fact, a simple way of getting the answer. MABA-MABA lists, or "Men-Are-Better-At, Machines-Are-Better-At" lists have appeared over the decades in various guises. What these lists basically do is try to enumerate the areas of machine and human strengths and weaknesses in order to provide engineers with some guidance on which functions to automate and which ones to give to humans. The process of function allocation as guided by such lists sounds straightforward but is actually fraught with difficulty and often unexamined assumptions. One problem is that the level of granularity of functions to be considered for function allocation is arbitrary. For example, it depends on the information processing model on which the MABA-MABA method is based (Hollnagel, 1999). In Parasuraman, Sheridan, and Wickens (2000), four stages of information processing (acquisition, analysis, selection, response) form the guiding principle to which functions should be kept or given away, but this is an essentially arbitrary decomposition based on a notion of a human-machine ensemble that resembles a linear input-output device. In cases where it is not a model of information processing that determines the categories of functions to be swapped between humans and machines, the technology itself often determines it (Hollnagel, 1999). MABA-MABA attributes are then cast in mechanistic terms, derived from technological metaphors. For example, Fitts (1951) applied terms such as information capacity and computation in his list of attributes for both the human and the machine. If the technology gets to pick the battlefield (i.e., determine the language of attributes), it will win most of them back for itself. This results in human-uncentered systems where typically heuristic and adaptive human abilities such as not focusing on irrelevant data, scheduling, and reallocating activities to meet current constraints, anticipating events, making generalizations and inferences, learning from past experience, and collaborating (Hollnagel) easily fall by the wayside. Moreover, MABA-MABA lists rely on a presumption of fixed human and machine strengths and weaknesses. The idea is that, if you get rid of the (human) weaknesses and capitalize on the (machine) strengths, you will end up with a safer system. This is what Hollnagel (1999) called "function allocation by substitution." The idea is that automation can be introduced as a straightforward substitution of machines for people—preserving the basic system while improving some of its output measures (lower workload, better economy, fewer errors, higher accuracy, etc.). Indeed, Parasuraman et al. (2000) recently defined automation in this sense: "Automation refers to the full or partial replacement of a function previously carried out by the human operator" (p. 287). But automation is more than replacement (although perhaps automation is about replacement from the perspective of the engineer). The really interesting issues from a human performance standpoint emerge after such replacement has taken place. Behind the idea of substitution lies the idea that people and computers (or any other machines) have fixed strengths and weaknesses and that the point of automation is to capitalize on the strengths while eliminating or compensating for the weaknesses. The problem is that capitalizing on some strength of computers does not replace a human weakness. It creates new human strengths and weaknesses—often in unanticipated ways (Bainbridge, 1987). For instance, the automation strength to carry out long sequences of action in predetermined ways without performance degradation amplifies classic human vigilance problems. It also exacerbates the system's reliance on the human strength to deal with the parametrization problem, or literalism (automation does not have access to all relevant world parameters for accurate problem solving in all possible contexts). As we have seen, however, human efforts to deal with automation literalism, by bridging the context gap, may be difficult because computer systems can be hard to direct (How do I get it to understand? How do I get it to do what I want?). In addition, allocating a particular function does not absorb this function into the system without further consequences. It creates new functions for the other partner in the human-machine equation—functions that did not exist before, for example, typing, or searching for the right display page, or remembering entry codes. The quest for a priori function allocation, in other words, is intractable (Hollnagel & Woods, 1983), and not only this: Such new kinds of work create new error opportunities (What was that code again? Why can't I find the right page?). TRANSFORMATION AND ADAPTATION Automation produces qualitative shifts. Automating something is not just a matter of changing a single variable in an otherwise stable system (Woods & Dekker, 2001). Automation transforms people's practice and forces them to adapt in novel ways: "It alters what is already going on—the everyday practices and concerns of a community of people—and leads to a resettling into new practices" (Flores, Graves, Hartfield, & Winograd, 1988, p. 154). Unanticipated consequences are the result of these much more profound, qualitative shifts. For example, during the Gulf War in the early 1990s, "almost without exception, technology did not meet the goal of unencumbering the personnel operating the equipment. Systems often required exceptional human expertise, commitment, and endurance" (Cordesman & Wagner, 1996, p. 25). Where automation is introduced, new human roles emerge. Engineers, given their professional focus, may believe that automation transforms the tools available to people, who will then have to adapt to these new tools. In chapter 9 we see how, according to some researchers, the removal of paper flight-progress strips in air traffic control represents a transformation of the workplace, to which controllers only need to adapt (they will compensate for the lack of flight progress strips). In reality, however, people's practice gets transformed by the introduction of new tools. New technology, in turn, gets adapted by people in locally pragmatic ways so that it will fit the constraints and demands of actual practice. For example, controlling without flight-progress strips (relying more on the indications presented on the radar screen) asks controllers to develop and refine new ways of managing airspace complexity and dynamics. In other words, it is not the technology that gets transformed and the people who adapt. Rather, people's practice gets transformed and they in turn adapt the technology to fit their local demands and constraints. The key is to accept that automation will transform people's practice and to be prepared to learn from these transformations as they happen. This is by now a common (but not often successful) starting point in contextual design. Here the main focus of system design is not the creation of artifacts per se, but getting to understand the nature of human practice in a particular domain, and changing those work practices rather than just adding new technology or replacing human work with machine work. This recognizes that: • Design concepts represent hypotheses or beliefs about the relationship between technology and human cognition and collaboration. • They need to subject these beliefs to empirical jeopardy by a search for disconfirming and confirming evidence. • These beliefs about what would be useful have to be tentative and open to revision as they learn more about the mutual shaping that goes on between artifacts and actors in a field of practice. Subjecting design concepts to such scrutiny can be difficult. Traditional validation and verification techniques applied to design prototypes may turn up nothing, but not necessarily because there is nothing that could turn up. Validation and verification studies typically try to capture small, narrow outcomes by subjecting a limited version of a system to a limited test. The results can be informative, but hardly about the processes of transformation (different work, new cognitive and coordination demands) and adaptation (novel work strategies, tailoring of the technology) that will determine the sources of a system's success and potential for failure once it has been fielded. Another problem is that validation and verification studies need a reasonably ready design in order to carry any meaning. This presents a dilemma: By the time results are available, so much commitment (financial, psychological, organizational, political) has been sunk into the particular design that any changes quickly become unfeasible. Such constraints through commitment can be avoided if human factors can say meaningful things early on in a design process. What if the system of interest has not been designed or fielded yet? Are there ways in which we can anticipate whether automation, and the human role changes it implies, will create new error problems rather than simply solving old ones? This has been described as Newell's catch: In order for human factors to say meaningful things about a new design, the design needs to be all but finished. Although data can then be generated, they are no longer of use, because the design is basically locked. No changes as a result of the insight created by human factors data are possible anymore. Are there ways around this catch? Can human factors say meaningful things about a design that is nowhere near finished? One way that has been developed is future incident studies, and the concept they have been tested on is exception management. AUTOMATION AND EXCEPTION MANAGEMENT One role that may fit the human well is that of exception manager. Introducing automation to turn people into exception managers can sound like a good idea. In ever busier systems, where operators are vulnerable to problems of data overload, turning humans into exception managers is a powerfully attractive concept. It has, for example, been practiced in the dark cockpit design that essentially keeps the human operator out of the loop (all the annunciator lights are out in normal operating conditions) until something interesting happens, which may then be the time for the human to intervene. This same envisioned role of exception manager dominates recent ideas about how to effectively let humans control ever-increasing air traffic loads. Perhaps, the thought goes, controllers should no longer be in charge of all the parameters of every flight in their sector. A core argument is that the human controller is a limiting factor in traffic growth. Too many aircraft under one single controller leads to memory overload and the risk of human error. Decoupling controllers from all individual flights in their sectors through greater computerization and automation on the ground and greater autonomy in the air is assumed to be the way around this limit. The reason we may think that human controllers will make good exception managers is that humans can handle unpredictable situations that machines cannot. In fact, this is often a reason why humans are still to be found in automated systems in the first place (see Bainbridge, 1987). Following this logic, controllers would be very useful in the role of traffic manager, waiting for problems to occur in a kind of standby mode. The view of controller practice is that of a passive observer who is ready to act when necessary. But intervening effectively from a position of disinvolvement has proven to be difficult—particularly in air-traffic control. For example, Endsley, Mogford, Allendoerfer, and Stein (1997) pointed out, in a study of direct routings that allowed aircraft deviations without negotiations, that with more freedom of action being granted to individual aircraft, it became more difficult for controllers to keep up with traffic. Controllers were less able to predict how traffic patterns would evolve over a foreseeable timeframe. In other studies too, passive monitors of traffic seemed to have trouble maintaining a sufficient understanding of the traffic under their control (Galster, Duley, Masolanis, & Parasuraman, 1999), and were more likely to overlook separation infringements (Metzger & Parasuraman, 1999). In one study, controllers effectively gave up control over an aircraft with communication problems, leaving it to other aircraft and their collision-avoidance systems to sort it out among themselves (Dekker & Woods, 1999). This turned out to be the controllers' only route out of a fundamental double bind: If they intervened early they would create a lot of workload problems for themselves (suddenly a large number of previously autonomous aircraft would be under their control). Yet if they waited on intervention (in order to gather more evidence on the aircraft's intentions), they would also end up with an unmanageable workload and very little time to solve anything in. Controller disinvolvement can create more work rather than less, and produce a greater error potential. This brings out one problem of envisioning practice, of anticipating how automation will create new human roles and what the performance consequences of those roles will be. Just saying "manager of exceptions" is insufficient: It does not make explicit what it means to practice. What work does an exception manager do? What cues does he or she base decisions on? The downside of under specification is the risk of remaining trapped in a disconnected, shallow, unrealistic view of work. When our view of (future) practice is disconnected from many of the pressures, challenges, and constraints operating in that world, our view of practice is distorted from the beginning. It misses how operational people's strategies are often intricately adapted to deal effectively with these constraints and pressures. There is an upside to under specification, however, and that is the freedom to explore new possibilities and new ways to relax and recombine the multiple constraints, all in order to innovate and improve. Will automation help you get rid of human error? With air traffic controllers as exception managers, it is interesting to think about how the various designable objects would be able to support them in exception management. For example, visions of future air traffic control systems typically include data linking as an advance that avoids the narrow bandwidth problem of voice communications—thus enhancing system capacity. In one study (Dekker & Woods, 1999), a communications failure affected an aircraft that had also suffered problems with its altitude reporting (equipment that tells controllers how high it is and whether it is climbing or descending). At the same time, this aircraft was headed for streams of crossing air traffic. Nobody knew exactly how datalink, another piece of technology not connected to altitude-encoding equipment, would be implemented (its envisioned use was, and is, to an extent underspecified). One controller, involved in the study, had the freedom to suggest that air-traffic control should contact the airline's dispatch or maintenance office to see whether the aircraft was climbing or descending or level. After all, data link could be used by maintenance and dispatch personnel to monitor the operational and mechanical status of an aircraft, so "if dispatch monitors power settings, they could tell us," the controller suggested. Others objected because of the coordination overheads this would create. The ensuing discussion showed that, in thinking about future systems and their consequences for human error, we can capitalize on under specification if we look for the so-called leverage points (in the example: data link and other resources in the system) and a sensitivity to the fact that envisioned objects only become tools through use—imagined or real (data links to dispatch become a backup air-traffic control tool). Anticipating the consequences of automation on human roles is also difficult because—without a concrete system to test—there are always multiple versions of how the proposed changes will affect the field of practice in the future. Different stakeholders (in air traffic control, this would be air carriers, pilots, dispatchers, air traffic controllers, supervisors, and flow controllers) have different perspectives on the impact of new technology on the nature of practice. The downside of this plurality is a kind of parochialism where people mistake their partial, narrow view for the dominant view of the future of practice and are unaware of the plurality of views across stakeholders. For example, one pilot claimed that greater autonomy for airspace users is "safe, period" (Baiada, 1995). The upside of plurality is the triangulation that is possible when multiple views are brought together. In examining the relationships, overlaps, and gaps across multiple perspectives, we are better able to cope with the inherent uncertainty built into looking into the future. A number of future incident studies (see Dekker & Woods, 1999) examined controllers' anomaly response in future air-traffic control worlds precisely by capitalizing on this plurality. To study anomaly response under envisioned conditions, groups of practitioners (controllers, pilots, and dispatchers) were trained on proposed future rules. They were brought together to try to apply these rules in solving difficult future airspace problems that were presented to them in several scenarios. These included aircraft decompression and emergency descents, clear air turbulence, frontal thunderstorms, corner-post overloading (too many aircraft going to one entry point for airport area), and priority air-to-air refueling and consequent airspace restrictions and communication failures. These challenges, interestingly, were largely rule or technology independent: They can happen in airspace systems of any generation. The point was not to test the anomaly response performance of one group against that of another, but to use triangulation of multiple stakeholder viewpoints—anchored in the task details of a concrete problem—to discover where the envisioned system would crack, where it would break down. Validity in such studies derives from: (a) the extent to which problems to be solved in the test situation represent the vulnerabilities and challenges that exist in the target world, and (b) the way in which real problem-solving expertise is brought to bear by the study participants. Developers of future air traffic control architectures have been envisioning a number of predefined situations that call for controller intervention, a kind of reasoning that is typical for engineering-driven decisions about automated systems. In air traffic management, for example, potentially dangerous aircraft maneuvers, local traffic density (which would require some density index), or other conditions that compromise safety would make it necessary for a controller to intervene. Such rules, however, do not reduce uncertainty about whether to intervene. They are all a form of threshold crossing—intervention is called for when a certain dynamic density has been reached or a number of separation miles has been transgressed. But threshold-crossing alarms are very hard to get right—they come either too early or too late. If too early, a controller will lose interest in them: The alarm will be deemed alarmist. If the alarm comes too late, its contribution to flagging or solving the problem will be useless and it will be deemed incompetent. The way in which problems in complex, dynamic worlds grow and escalate, and the nature of collaborative interactions, indicate that recognizing exceptions in how others (either machines or people) are handling anomalies is complex. The disappointing history of automating problem diagnosis inspires little further hope. Threshold-crossing alarms cannot make up for a disinvolvement—they can only make a controller acutely aware of those situations in which it would have been nice to have been involved from the start. Future incident studies allow us to extend the empirical and theoretical base on automation and human performance. For example, supervisory-control literature makes no distinction between anomalies and exceptions. This indistinction results from the source of supervisory-control work: How do people control processes over physical distances (time lag, lack of access, etc.). However, air-traffic control augments the issue of supervisory control with a cognitive distance: Airspace participants have some system knowledge and operational perspective, as do controllers, but there are only partial overlaps and many gaps. Studies on exception management in future air-traffic control force us to make a distinction between anomalies in the process, and exceptions from the point of view of the supervisor (controller). Exceptions can arise in cases where airspace participants are dealing with anomalies (e.g., an aircraft with pressurization or communications problems) in a way that forces the controller to intervene. An exception is a judgement about how well others are handling or going to handle disturbances in the process. Are airspace participants handling things well? Are they going to get themselves in trouble in the future? Judging whether airspace users are going to get in trouble in their dealings with a process disturbance would require a controller to recognize and trace a situation over time—contradicting arguments that human controllers make good stand-by interveners. Will Only the Predicted Consequences Occur? In developing new systems, it is easy for us to become mis calibrated. It is easy for us to become overconfident that if our envisioned system can be realized, the predicted consequences and only the predicted consequences will occur. We lose sight of the fact that our views of the future are tentative hypotheses and that we would actually need to remain open to revision, that we need to continually subject these hypotheses to empirical jeopardy. One way to fool ourselves into thinking that only the predicted consequences will occur when we introduce automation is to stick with the substitutional practice of function allocation. Substitution assumes a fundamentally uncooperative system architecture in which the interface between human and machine has been reduced to a straightforward "you do this, I do that" trade. If that is what it is, of course, we should be able to predict the consequences. But it is not that simple. The question for successful automation is not who has control over what or how much. That only looks at the first parts, the engineering parts. We need to look beyond this and start asking humans and automation the question: "How do we get along together?" Indeed, where we really need guidance today is in how to support the coordination between people and automation. In complex, dynamic, nondeterministic worlds, people will continue to be involved in the operation of highly automated systems. The key to a successful future of these systems lies in how they support cooperation with their human operators—not only in foreseeable standard situations, but also under novel, unexpected circumstances. One way to frame the question is how to turn automated systems into effective team players (Sarter & Woods, 1997). Good team players make their activities observable to fellow team players, and are easy to direct. To be observable, automation activities should be presented in ways that capitalize on well-documented human strengths (our perceptual system's acuity to contrast, change and events, our ability to recognize patterns and know how to act on the basis of this recognition, e.g., Klein). For example: • Event based: Representations need to highlight changes and events in ways that the current generation of state-oriented displays do not. • Future oriented: In addition to historical information, human operators in dynamic systems need support for anticipating changes and knowing what to expect and where to look next. • Pattern based: Operators must be able to quickly scan displays and pick up possible abnormalities without having to engage in difficult cognitive work (calculations, integrations, extrapolations of disparate pieces of data). By relying on pattern- or form-based representations, automation has an enormous potential to convert arduous mental tasks into straightforward perceptual ones. Team players are directable when the human operator can easily and efficiently tell them what to do. Designers could borrow inspiration from how practitioners successfully direct other practitioners to take over work. These are intermediate, cooperative modes of system operation that allow human supervisors to delegate suitable subproblems to the automation, just as they would be delegated to human crew members. The point is not to make automation into a passive adjunct to the human operator who then needs to micromanage the system each step of the way. This would be a waste of resources, both human and machine. Human operators must be allowed to preserve their strategic role in managing system resources as they see fit, given the circumstances.