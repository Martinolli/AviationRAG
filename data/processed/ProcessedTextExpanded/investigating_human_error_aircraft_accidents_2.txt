Title: Investigating Human Error Incidents, Accidents, and Complex Systems – Chapter(s) 1 -2 Author(s): Barry Strauch Category: Safety Tags: Human, Error, Investigation, Accidents, Incidents, System Foreword to Second Edition Many accident investigations make the same mistake in defining causes. They identify the widget that broke or malfunctioned, then locate the person most closely connected with the technical failure: the engineer who miscalculated an analysis, the operator who missed signals or pulled the wrong switches, the supervisor who failed to listen, or the manager who made bad decisions. When causal chains are limited to technical flaws and individual failures, the ensuing responses aimed at preventing a similar event in the future are equally limited: they aim to fix the technical problem and replace or retrain the individual responsible. Such corrections lead to a misguided and potentially disastrous belief that the underlying problem has been solved. (Columbia Accident Investigation Board, 2003, p. 97) I’ll never forget the day my phone rang early on a Sunday morning in 2006. The voice on the other end of the line informed me there had been an airline crash in Lexington, Kentucky. The airplane was still on fire, and multiple fatalities were expected. With that, I started packing my bags to head to Kentucky. Before I left my house, the images on TV pretty much told me what had happened. The wreckage was positioned a few thousand feet directly off the end of a runway that would have been too short for an airplane of that size to successfully takeoff. A broken fence at the end of the runway and tire marks through the grass to the initial impact point provided further clues. So, before even leaving my house, I had surmised the pilots had made an error in attempting to depart from the wrong runway. Error identified; case closed. Right? Well, actually not. A good friend, Captain Daniel Maurino, stated, “The discovery of human error should be considered as the starting point of the investigation, and not the ending point that has punctuated so many previous investigations” (Maurino, 1997). His words of wisdom are framed in my office to serve as a constant reminder of the necessity to look behind the obvious human error. It is one thing to say someone committed an error, but it is quite another to try to identify the underlying factors that influenced that error. And why do we care? Finding who or what is at “fault” should not be simply an exercise in attributing error, but rather should be done to identify the factors that influenced the error so those conditions can be corrected to prevent future errors. If we simply say “human error,” “pilot error,” or “operator error” and stop with that, we miss valuable learning opportunities. The Institute of Medicine noted in a seminal report on medical error that “blaming the individual does not change these factors, and the same error is likely to recur” (Institute of Medicine, 2000, p. 49). xvi In the case of the Lexington, Kentucky crash, the error was identified within hours, if not minutes, after it occurred. But, identifying the human error doesn’t mean the investigation is completed; instead, it should be, as Daniel Maurino stated, the starting point of the investigation. Once the human error was identified, the prevailing question should (and did) become “Why was the error committed?” Were the pilots fatigued? Did the fact that the airport was undergoing the construction of runways and taxiways somehow confuse the pilots during taxi-out? How did the disparity between taxiway signs and what was depicted on the pilots’ airport dia- gram charts affect their performance? Did organizational factors such as poor training or lack of company standardization somehow contribute to the error? What role did understaffing play in the control tower? Did the crew’s casual attitude enable their error? Why were two other flights successful- fully navigating the airport construction and taxiing to the right runway moments before the crash- but this crew did not? Only after questions such as these are answered can the human error be understood and the underlying conditions corrected. Since that accident in 2006, I’ve been involved in the deliberation of some 150 or so transportation accidents. From that experience, I have developed the belief that most, if not all, accidents or incidents have roots in human error. In some cases, it is a readily identifiable error of a frontline operator, such as a pilot, ship’s master, medical technician, air traffic controller, or control room operator. In other cases, the error(s) may not be obvious at all. It may be deeply embedded within the system, perhaps far, far away from the scene of the accident, such as decisions and actions/inactions made by organizations or regulators. As explained in this book, there are proximate errors—those that are closest to the accident in terms of timing or location, and there are underlying conditions that are factors in the accident causation but perhaps not readily apparent. Reason (1990, 1997) refers to these as active failures and latent conditions, respectively. Contemporary thinking views error as a “symptom of deeper trouble” (Dekker, 2002, p. 61) within the system. Maurino said human error should be “considered like fever: an indication of illness rather than its cause. It is a marker announcing problems in the architecture of the system” (Maurino, 1997). In the early 1990s, the National Transportation Safety Board (NTSB) board member John Lauber was one of the first to focus on how organizational factors can influence transportation safety (Meshkati, 1997). Lauber argued that the cause of a commuter airliner in-flight breakup due to faulty maintenance should be “the failure of Continental Express management to establish a corporate culture which encouraged and enforced adherence to approved maintenance and quality assurance procedures” (NTSB, 1992, p. 54). Of the five national transportation safety board board members, Lauber was alone in his belief. The conventional thinking at the time seemed to be to identify the proximate error that sparked the accident and call that the “cause” of the mishap. But, as discussed throughout this book, human error does not occur in a vacuum. It must, therefore, be examined in the context in which the error occurred. In other words, if an error occurs in the workplace, the workplace must be examined to look for conditions that could provoke error. What were the physical conditions at the workplace? Was lighting adequate to perform the task? Were the procedures and training adequate? Did the organizational norms and expectations prioritize safety over competing goals? Was the operational layout of the workplace conducive to error? Organizational factors have been implicated in accidents and incidents in many socio-technical industries. For example, the U.S. Chemical Safety and Hazard Investigation Board (Chemical Safety Board [CSB]) determined that a 2005 oil refinery explosion that claimed 15 lives and injured 180 people had numerous organizational-related factors, such as the company’s cost-cutting and overreliance on misleading safety metrics (CSB, 2007). The International Atomic Energy Agency (IAEA) stated that the Chernobyl nuclear power plant meltdown “flowed from a deficient safety culture, not only at the Chernobyl plant, but throughout the Soviet design, operating, and regulatory organizations for nuclear power” (IAEA, 1992, pp. 23–24). The National Transportation Safety Board (2010) found organizational issues to be a causal factor in the 2009 multi-fatality subway accident in Washington, DC. In 2015, I was involved in the final deliberation of an accident involving SpaceShipTwo, a commercial space vehicle that suffered an in-flight breakup during a test flight. From the onboard video recorder, it was evident that the copilot prematurely moved a lever, which led to an uncommanded movement of the vehicle’s tail feather—a device similar to a conventional aircraft’s horizontal and vertical stabilizer. The tail feather is actuated by a cockpit lever to pivot it upward 60° relative to the longitudinal axis of the aircraft; its purpose is to stabilize the aircraft during the reentry phase of flight. However, if the feather is deployed at the wrong time, as in this case, the resulting aerodynamic loads on the aircraft will lead to catastrophic in-flight breakup. The obvious “cause” of the accident was that the copilot committed an error in unlocking the feather at the wrong time, which led to the uncommanded actuation of the feather. But, this finding alone would serve no useful purpose for preventing similar errors in the future. After all, the copilot was killed, so surely he would not commit this error again. By digging deeper, the investigation found that influencing the ­ copilot’s error was the high workload he was experiencing during this phase of flight, along with the time pressure to complete critical tasks from memory— all while experiencing vibration and g-loads that he had not experienced recently. From a broader perspective, the spaceship designer/manufacturer, Scaled Composites, did not consider that this single error could lead to an unintended feather activation. Although the copilot had practiced for this flight several times in the simulator, this premature movement of the feather unlock lever occurred on the fourth powered flight of the SpaceShipTwo, indicating to me that the likelihood of this error was high. “By not considering human error as a potential cause of uncommanded feather extension of the SpaceShipTwo, Scaled Composites missed opportunities to identify the design and/or operational requirements that could have mitigated the consequences of human error during a high workload phase of flight” (NTSB, 2015, p. 67). Because of the underlying design implications, the National Transportation Safety Board issued a safety recommendation to the Federal Aviation Administration (FAA) to ensure that commercial space flight entities identify and address “single flight crew tasks that, if performed incorrectly or at the wrong time, could result in a catastrophic hazard” (NTSB, 2015, p. 70). In addition, the manufacturer added a safety interlock to ensure that this lever could not be activated during this critical flight regime. Not only can organizations create error-provoking conditions, but regulators can do so as well. Examples include failing to provide adequate oversight and enforcement or not developing adequate procedures. In 2009, a Pacific Gas & Electric Company (PG&E) 30-inch diameter natural gas transmission pipeline ruptured and exploded. The conflagration claimed eight lives in San Bruno, California, destroyed 38 homes, and damaged 70. The investigation determined that oversight and enforcement by both the state and federal regulators were ineffective, which “permitted PG&E’s organizational failures to continue over many years” (NTSB, 2011, p. 126). So, as you can see, human behavior, including errors, can be influenced by many factors. Therefore, the investigation of human error should not be a random hit-or-miss process. It should be conducted in an organized, methodical process with a clear purpose in mind. Dr. Barry Strauch has been on the frontlines for nearly 35 years, and he has provided human factors expertise to well over a hundred aviation and maritime accidents. Between the covers of this book, he lays out in clear terms the factors that enable human error, including individual factors such as fatigue, stress, and medical factors. He also examines in detail organizational and regulatory precursors to error. Each chapter provides a bulleted checklist to facilitate identifying relevant factors. This second edition provides an update to what I found to be an excellent reference—one that I often referred to in my decade of serving on an accident investigation board, as indicated by scores of dog-eared pages filled with underlining and highlighting. I encourage anyone involved with investigating any type of error—whether that error occurred in the hospital, on the hangar floor, in a nuclear control room, or on the flight deck of an airliner—to use this text as a resource for investigating human error. Using this book as a guide—I assure you—will not be an error. Like the rest of the modern world, I owe an enormous debt to the skills of professional accident investigators. As a traveler and a consumer, I am extremely grateful for what they have done to make complex technologies significantly safer; however, as an academic, I have also been especially dependent on their published findings. Although, mercifully, I have had very little first-hand experience of the real thing, this has not prevented me from writing, lecturing, and theorizing about the human contribution to the breakdown of complex systems for the past 30 years or so. There are perhaps two reasons why I have so far been able to pull this off. The first is that the ivory tower provided the time and resources to look for recurrent patterns in a large number of adverse events over a wide range of hazardous technologies, a luxury that few “real-world” people could enjoy. The second has been the high quality of most major accident reports. If such accounts had later been shown to lack accuracy, insight, analytical depth, or practical value, then my reliance upon them would have been foolish or worse. But while many have challenged the theories, very few have questioned the credibility of the sources. So, you might ask, if accident investigators are doing so well, why do they need this book? The most obvious answer is that human, organizational, and systemic factors, rather than technical or operational issues, now dominate the risks to most hazardous industries—yet the large majority of accident investigators are technical and operational specialists. Erik Hollnagel (1993) carried out a survey of the human factors literature over three decades to track the increasing prominence of the “human error” problem. In the 1960s, erroneous actions of one kind or another were estimated to contribute around 20% of the causal contributions to major accidents. By the 1990s, however, this figure had increased fourfold. One obvious explanation is that the reliability of mechanical and electronic components has increased markedly over this period, while complex systems are still being managed, controlled, and maintained by Mark I human beings. In addition, this period has seen some subtle changes in the way we perceive the “human error” problem and its contribution to accidents. For the most part, “human error” is no longer viewed as a single portmanteau category, a default bin into which otherwise unexplained factors can be dumped. We now recognize that erroneous actions come in a variety of forms and have different origins, both in regard to the underlying psychological mechanisms and their external shaping factors. It is also appreciated that front-line operators do not possess a monopoly on error. Slips, lapses, mistakes, and violations can occur at all levels of the system. We are now able to view errors as consequences rather than sole causes and see frontline operators more as the inheritors rather than the instigators of accidents in complex systems. System complexity derives in large part from the existence of diverse and redundant layers of defenses, barriers, and safeguards that are designed to prevent operational hazards from coming into damaging contact with people, assets, and the environment. The nuclear industry calls them “defenses- in-depth.” Such characteristics make it highly unlikely that accidents in complex systems arise from any single factor, be it human, technical, or environmental. The apparently diabolical conjunction of several different factors is usually needed to breach all of these defenses in depth at the same time. This makes such events less frequent, but the causes more complex. Some of the latent contributions have often lain dormant in the system for many years prior to the accident. Given the increasing recognition that contributing- ing factors can have both a wide scope and a long history, it is almost inevitable that investigators will net larger numbers of human and organizational shortcomings. Another associated change—at least within the human factors and investigative communities—has been a shift away from the “person model” of human error, in which the search for causes and their countermeasures is focused almost exclusively upon the psychology of individuals. Instead, there has been an increasing willingness to take a systems view of accident causation in which the important question is not “Who blundered?” but “How and why did the defenses fail?” Unfortunately, the person model is still deeply embedded in the human psyche and is especially pernicious in its moral (or legal) form. This is the widespread belief that responsible and highly trained professionals (pilots, surgeons, ship officers, control room operators, and the like) should not make errors. However, when such erroneous actions do occur, it is assumed that they are sufficient to cause bad accidents. The reality, of course, is quite different. Highly trained, responsible professionals make frequent errors, but most are inconsequential, or else they are detected and recovered (see, e.g., Amalberti and Wioland, 1997). Moreover, these errors are only occasionally necessary to add the final touches to an accident-in-waiting, a potential scenario that may have been lurking within a complex system for a long time. The achievements of accident investigators are all the more remarkable when one considers the snares, traps, and pitfalls that lie in their path. Aside from the emotional shock of arriving at often inaccessible and hostile locations to confront the horrors of an accident site, investigators are required to track backward—sometimes for many years—in order to create a coherent, accurate, and evidence-based account of how and why the disaster occurred, and to make recommendations to prevent the recurrence of other tragedies. The first and most obvious problem is that the principal witnesses to the accident are often dead or incapacitated. But this, as most investigators would acknowledge, goes with the territory. Other difficulties are less apparent and have to do with unconscious cognitive biases that influence the way people arrive at judgments about blame and responsibility and cause and effect. While human factors specialists have focused mainly on the error tendencies of the operators of complex systems, there has also been considerable interest in how people trying to make sense of past events can go astray. Let me briefly review some of these investigative error types. They fall into two related groups: those that can bias attributions of blame and responsibility and those that can distort perceptions of cause and effect. Here are some of the reasons why the urge to blame individuals is so strong. When looking for an explanation of an occurrence, we are biased in finding it among human actions that are close in time and place to the event, particularly if one or more of them are considered discrepant. This leads to what has been termed the counterfactual fallacy (Miller & Turnbull, 1990), where we confuse what might have been with what ought to have been, particularly in the case of bad outcomes. The fallacy goes as follows: Had things been otherwise (i.e., had this act not happened), there would have been no adverse result; therefore, the person who committed the act is responsible for the outcome. Another factor that leads to blaming is the fundamental attribution error (Fiske & Taylor, 1984). This is the universal human tendency to resort to dispositional rather than situational influences when explaining people’s actions, particularly if they are regarded as unwise or unsafe. We say that the person was stupid or careless, but if the individual in question were asked, he or she is most likely to point to the local constraints. The truth usually lies somewhere in between. The just world hypothesis (Lerner, 1970)—the view that bad things happen to bad people, and conversely—comes into play when there is an especially unhappy outcome. Such a belief is common among children, but it can often last into adulthood. A close variant is the representativeness heuristic (Tversky & Kahneman, 1974) or the tendency to presume a symmetrical relationship between cause and effect—thus, bad consequences must be caused by horrendous blunders, while particularly good events are seen as miracles. Yet another reason why people are so quick to assign blame is the illusion of free will (Lefcourt, 1973). People, particularly in Western cultures, place great value on the belief that they are the controllers of their own fate. They can even become mentally ill when deprived of this sense of personal freedom. Feeling capable of choice naturally leads them to assume that other people are the same. They are also seen as free agents, able to choose between right and wrong and between correct and erroneous actions. But our actions are often more constrained by circumstances than we are willing to admit or understand. All accident investigators are faced with the task of digitizing an essentially analog occurrence; in other words, they have to chop up continuous and interacting sequences of prior events into discrete words, paragraphs, conclusions, and recommendations. If one regards each sequence as a piece of string (though it is a poor analogy), then it is the investigator’s task to tie knots at those points, marking what appear to be significant stages in the development of the accident. Such partitioning is essential for simplifying the causal complexity, but it also distorts the nature of reality (Woods, 1993). If this parsing of events correctly identifies proper areas for remediation, then the problem is a small one, but it is important for those who rely on accident reports to recognize that they are—even the best of them—only a highly selected version of the actuality and not the whole truth. It is also a very subjective exercise. Over the years, I have given students the task of translating these accident narratives into event trees. Starting with the accident itself, they were required to track back in time, asking themselves at each stage what factors were necessary to bring about the subsequent events—or, to put it another way, which elements, if removed, would have thwarted the accident sequence. Even simple narratives produced a wide variety of event trees, with different nodes and different factors represented at each node. While some versions were simply inaccurate, most were perfectly acceptable accounts. The moral was clear: the causal features of an accident are to the analyst what a Rorschach test (inkblot test) is to the psychologist—something that is open to many interpretations. The test of a good accident report is not so much its fidelity to the often-irrecoverable reality but the extent to which it directs those who regulate, manage, and operate hazardous technologies toward appropriate and workable countermeasures. A further problem in determining cause and effect arises from the human tendency to confuse the present reality with that facing those who were directly involved in the accident sequence. A well-studied manifestation of this is hindsight bias or the knew-it-all-along effect (Fischhoff, 1975; Woods et al., 1994). Retrospective observers who know the outcome tend to exaggerate what the people on the spot should have appreciated. Those looking back on an event see all the causal sequences homing in on that point in time at which the accident occurred, but those involved in the prior events, armed only with limited foresight, see no such convergence. With hindsight, we can easily spot the indications and warning signs that should have alerted those involved to the imminent danger. However, most “warning” signs are only effective if you know in advance what kind of accident you are going to have. Sydney Decker (2001) has added two further phenomena to this catalog of investigative pitfalls: he termed them micro-matching and cherry-picking. Both arise, he argues, from the investigator’s tendency to treat actions in isolation. He calls this “the disembodiment of human factors data.” Micro-matching is a form of hindsight bias in which investigators evaluate discrete performance fragments against standards that seem applicable from their after-the-fact perspective. It often involves comparing human actions against written guidance or data that were accessible at the time and should have indicated the true situation. As Decker puts it: “Knowledge of the ‘critical’ data comes only with the omniscience of hindsight, but if data can be shown to have been physically available, it is assumed that it should have been picked up by xxv the practitioners in the situation.” The problem, he asserts, is that such judgments do not explain why this did not happen at the time. Cherry-picking, another variant of hindsight bias, involves identifying patterns of isolated behavioral fragments on the basis of post-event knowledge. This grouping is not a feature of reality but an artifact introduced by the investigator. Such tendencies, he maintains, derive from the investigator’s excessive reliance upon inadequate folk models of behavior and upon human reactions to failure. Fortunately, he outlines a possible remedy “in the form of steps investigators can take to reconstruct the unfolding mindset of the people they are investigating, in parallel and tight connection with how the world was evolving around these people at the time.” Clearly, accident investigators need help in making sense of human factors data. But I am not sure that Olympian pronouncements (or even Sinaian tab- lets) are the way to provide it, nor am I convinced that investigators can ever “reconstruct the unfolding mindset of the people they are investigating”—I can’t even construct my present mindset with any confidence. This book, on the other hand, delivers the goods in a way that is both useful and meaningful to hard-pressed accident investigators with limited resources. It is well written, well researched, extremely well informed, and offers its guidance in a down-to-earth, practical, and modular form (i.e., it can be read via the contents page and index rather than from cover to cover). It is just the thing, in fact, to assist real people doing a vital job. And, as far as I know, there is nothing else like it in the bookshops. Preface to Second Edition I have seen many changes in the understanding of human error as well as in the role that it plays in accident causation in the 15 years since this book was first published. In that time, considerable research has been conducted in areas such as automation, team performance, safety management, and fatigue, which has given investigators additional knowledge with which to assess the causes of human error. In this interval, we have also witnessed a worldwide decline in major aircraft accidents. Unfortunately, some of the accidents that have occurred since then appear to have been influenced by the same antecedents to the error that we have seen all too frequently over the years. For example, the accident used in the case study in Chapter 16, while more current and with more complex errors than was true of the case study in the first edition, illustrates automation-related errors that are almost identical to those seen in previous accidents, including one committed almost 30 years earlier. Because people and the systems they operate do not always learn from their mistakes, the need for thorough and systematic human factors investigations of error becomes that much more critical. Hopefully, the lessons to be learned from these investigations can be used to avoid further accidents. Not only have operators continued to make errors that have led to accidents, but mishaps in which the antecedents were well known but ignored have occurred in other systems as well. For example, the case of Bernard Madoff, whose Ponzi scheme cost many investors their life savings, illustrates how ineffective oversight can exacerbate system errors. The U.S. regulator of financial securities had been informed of the Madoff Ponzi scheme well before the scheme was exposed, yet nothing was done to stop him, despite its own (flawed) investigation and the presence of publicly available information that could have pointed out the fraud. The regulator did not cause the scheme, but by failing to properly oversee the financial system in which it operated, it contributed to losses of millions of investor dollars beyond what would have been the case had it acted effectively when it initially learned of the scheme. In the years since the first edition, we have also witnessed the world’s third major civilian nuclear reactor accident, the March 2011 meltdown in the Fukushima Daiichi nuclear-generating plant in Japan. A reactor core melted after coolant ceased flowing to it, following flooding of the backup diesel generators, a result of a devastating tsunami. Because the plant was located in a seismic zone near the ocean, it was potentially prone to tsunamis. Regulators, therefore, required protection against them. In addition to building a seawall, designers installed backup generators to enable the coolant to be pumped to the core in the event that primary power was lost. xxviii Preface to Second Edition However, although backup generators were required and installed, designers and regulators failed to consider the possibility of a tsunami of sufficient magnitude that would exceed the seawall limits and flood the backup generators, which had been placed at ground level behind the seawall. In this manner, designers, regulators, operators, and all who play integral roles in complex systems have continued to create antecedents to errors that appear, in hindsight, to have been preventable. While this book does not attempt to provide foresight to those designing or operating complex systems, I do hope that it will provide the knowledge needed to effectively investigate the results of their errors to identify their antecedents. Because of the findings of both accident investigations and of the human factors research conducted in the interim, we have a better understanding of error causation than we did 15 years ago. We know more, for example, than we did then about automation’s effects on operators, how fatigue can adversely affect cognitive performance, and how organizations can contribute to operator errors, and the research cited in this text reflects these advances. My experience as an investigator, with over 30 years of conducting error investigations in major modes of transportation, has reinforced my belief that error investigators need to be aware of basic human factors research findings. The ability to identify necessary data, along with interviewing and analytical skills, are all necessary. However, without an understanding of human error, even the most skilled investigators will have difficulty explicating the error causing the accidents they investigate. In the first edition, I suggested that accident investigation is a special calling, and my experience since then has only reinforced that view. Being able to understand and identify errors in a constructive way that can be used to prevent accidents is indeed a privilege. I hope that you will find this text helpful and contribute to your own endeavors. Should you investigate an accident, I hope that you will make a positive contribution to safety by helping to reduce its likelihood in the future. Finally, this book is dedicated to the memory of Howard B. Brandon, Jr., whose father has been a colleague, mentor, and friend. Preface to First Edition From the time I was a boy growing up in Brooklyn, I have been fascinated with New York’s subway system. When I was 11, I began a tradition that lasted 3 years. To celebrate the last day of school, I would ride at the head end of a subway train on a route that I had not taken before. I never told my parents. I doubt they would have understood. I loved the subways, and riding at the very front of the train allowed me to see not only the track ahead but also to watch the operator, who then called the motorman. I would stand there for hours, fascinated, watching the train’s movements and the train operator as he would move the train forward and then slow it down and stop it at each station. From these beginnings, my interest in complex systems, especially transportation systems, has grown. I later became fascinated with another system, aviation, and I tried to learn as much as I could about that field. After completing graduate school, I indulged myself in learning to fly. I became hooked. All of my free time and disposable income went to pay for lessons and flight time. After several years, I was fortunate enough to be able to acquire several pilot ratings. I even briefly considered trying to become an airline pilot. However, the airlines weren’t hiring many pilots in those days, and I had to enter the field in another way. I became an accident investigator with the National Transportation Safety Board (NTSB). I joined the national transportation safety board as a human performance investigator in 1983, with several other young human factors professionals. We were among the first at the agency, or anywhere for that matter, to systematically examine the role of operator error in accidents in complex systems. They wanted us to provide more insight into the cause of an accident than to attribute it solely to operator error, the standard practice of the day. The national transportation safety board was, and is a special place. Its investigators are thoroughly dedicated to its mission—to learn what causes an accident in order to prevent future accidents. Often, at considerable personal sacrifice, they travel to inhospitable locales and work under great stress to get to the bottom of terrible tragedies. In those days, there wasn’t much to guide us beyond the standard human factors design texts. Researchers at national aeronautics and space administration Ames have been actively engaged in studying team errors and crew resource management for several years, but the fruits of their efforts will still be several years away. The Danish researcher Jens Rasmussen, and the British researchers James Reason, Neville Moray, and their colleagues in Europe were just beginning to examine error as a systems construct after the nuclear accident at Three Mile Island. Elsewhere, the field of human error was only just beginning to emerge as a field worthy of extensive study in and of itself. Much has happened to the field of human error since 1983 and to me as well. I have held a variety of positions at the NTSB, all related to either investigating or training others to investigate errors in both the United States and abroad. I have met many people involved in transportation safety, all of whom are as dedicated and committed as my colleagues at the NTSB. But many have asked the same question—given the prominence of human error in the cause of accidents, is there anything written on how to investigate error? Unfortunately, I would have to answer that although there was much-written error, little was available to explain how to investigate it. I wrote this text to remedy that situation. I have based it not on any formal method that the national transportation safety board has adopted but on my own reading, experience, and belief in what works. It is intended for those who are interested in human error and for those who investigate errors in the course of an incident or accident investigation.* I am indebted to many people who have helped me along the way, and without whose help, this text would not have been possible. Although I can- not name them all, I would like to thank several whose assistance was invaluable. Dr. Michael Walker of the Australian Transportation Safety Bureau commented on the organization of the text when it was still in its formative stage. Drs. Evan Byrne and Bart Elias of the National Transportation Safety Board provided beneficial comments and suggestions on an early draft. Dr. Douglas Wiegmann of the University of Illinois took time out of his schedule to review a draft, and his comments are greatly appreciated. The questions that Dr. John Stoop of the Delft University of Technology in the Netherlands raised were incisive and helped guide my thinking on subsequent drafts. Dr. Mitchell Garber, the medical officer of the National Transportation Safety Board, meticulously read and offered suggestions on several drafts. His guidance went well beyond medical and human factors issues and greatly improved both the content and structure of the text. My editor, Ms. Joanne Sanders-Reio, worked with me to arrange my thoughts and, more importantly, helped refine and organize the text. Carol Horgan reviewed the final draft for clarity. My publisher, John Hindley, provided ongoing support and encouragement from the beginning. Professor James Reason provided invaluable encouragement in these efforts. I am especially indebted to my wife Maureen, my son Sean, and my daughter Tracy. They have put up with the over three and half years that I have spent on this project, with the attendant absences from their lives and the frustrations these efforts produced. Without their patience and encouragement- this book would not have been possible. * The text reflects my views and opinions and not necessarily those of the National Transportation Safety Board. xxxi Finally, although he passed away over two decades ago, my father, Samuel A. Strauch, encouraged and supported a quest for learning that has remained with me to this day. This book is dedicated to his memory. 1 1 Introduction The ValuJet accident continues to raise troubling questions—no longer about what happened but about why it happened and what is to keep something similar from happening in the future. As these questions lead into the complicated and human core of flight safety, they become increasingly difficult to answer. Langewiesche, 1998 The Atlantic Monthly Introduction “To err is human,” it is said, and people make mistakes—it is part of the human condition. When people err, they may be embarrassed or angry with themselves, but most often, the errors are minor, and little attention is paid to the consequences. However, sometimes the errors lead to more serious consequences. Occasionally, people working in hospitals, airlines, power stations, chemical refineries, or similar settings commit errors−errors that may cause accidents with catastrophic consequences, potentially leading to injury or death to those who played no part in the error. Such work settings, known as “complex systems” (Perrow, 1999), generate electricity, refine crude oil, manage air traffic, transport products and people, and treat the sick, to name a few. They have brought substantial benefits to our way of life and permitted a standard of living to which many have become accustomed, but when someone who works in these systems makes an error, the consequences may be severe. Although companies and their regulators typically establish extensive performance standards to prevent errors, these errors, which in other environments may be inconsequential, can, in these settings, result in severe consequences. A new catastrophe seems to occur somewhere in the world with regularity, often one that is later attributed to someone doing something wrong. Whether it is an airplane accident, a train derailment, a tanker grounding, or any of the myriad events that seem to occur with regularity, the tendency of often simple errors to wreak havoc continues. Despite the progress made, two systems have not yet been developed that are immune to the errors of those who operate them. The human genetic structure has been mapped, the Internet has developed, and cell phones have been designed with more computing power than most computers had, but a few short years ago, human error had not yet been eliminated from complex systems. However, while error has not been eliminated, our understanding of the causes of errors has increased. Particularly in complex systems where there is little tolerance for errors, regulators, system designers, and operators have developed and implemented techniques that anticipate and address potential opportunities for error and, it is hoped, prevent errors from being committed that can jeopardize system safety. The Crash of ValuJet Flight 592 To illustrate how even simple errors can lead to a catastrophic accident, let us look at an event in one of our safest complex systems—commercial air trans- portation. Despite numerous measures that had been developed to prevent the very types of errors that occurred, several people, including some who were not even involved in the conduct of the accident flight, committed criti- cal errors that led to an accident. On May 11, 1996, just minutes after it had taken off from nearby Miami, Florida, a McDonnell Douglas DC-9 crashed into the Florida Everglades (National Transportation Safety Board, 1997). Investigators determined that the cause of the accident was relatively simple and straightforward; an intense fire broke out in the airplane’s cargo compartment and within minutes burned through the compartment into the cabin, quickly spreading through the cabin. The pilots were unable to land before the fire degraded the airplane’s structural integrity. All onboard were killed in the accident (Figure 1.1). The investigation led to considerable worldwide media attention. As with any large-scale event involving a substantial loss of life, this was under- standable. But other factors played a part as well. The airline had been operating for less than 3 years, and it had employed what were then nontra- ditional airline practices. It had expanded rapidly, and in the months before the accident experienced two nonfatal accidents. After this accident, many criticized the airline, questioning its management practices and its safety record. Government officials initially defended the airline’s practices, but then reversed themselves. Just over a month after the accident, government regulators, citing deficiencies in the airline’s operations, forced it to suspend operations until it could satisfy their demands for reform. This led to even more media attention. 3 As details about the crash emerged and more was learned, the scope of the tragedy increased. Minutes after takeoff, the pilots had declared an emer- gency, describing smoke in the cockpit. Within days investigators learned that despite strict prohibitions, canisters of chemical oxygen generators had been loaded onto the aircraft. It was believed that the canisters, the report of smoke in the cockpit, and the accident were related. Oxygen generators provide oxygen to airline passengers in the event of a cabin depressurization and are therefore designed to be safely transported in aircraft, provided the canisters are properly installed within protective housings. However, if the canisters are not packaged properly, or are shipped without locks to prevent initiation of oxygen generation, they could inadver- tently generate oxygen. The process creates heat as a by-product, bringing the surface temperature of the canisters to as high as 500°F (260°C). Investigators believed that boxes of canisters that lacked locks or other protection were placed loosely in boxes and loaded into the airplane’s cargo hold underneath the cabin. After being jostled during takeoff and climb out, the canisters began generating oxygen. The canister surfaces became heated to the point that adjacent material in the cargo compartment was ignited and a fire began. The canisters then fed the fire with pure oxygen, produc- ing one of extraordinary intensity that quickly penetrated the fire resistant material lining the cargo hold, material that had not been designed to protect against an oxygen-fed fire. The fire burned through the cabin floor and, with the pure oxygen continuing to feed it, grew to the point where the struc- ture weakened and the airplane become uncontrollable. It crashed into the FIGURE 1.1 The ValuJet accident site in the Florida Everglades. (Courtesy of the National Transportation Safety Board, 1997 .) 4 Everglades, a body of shallow water, becoming submerged under its soft silt floor (Figure 1.2). Because of the potential danger that unprotected oxygen generators pose, they are considered hazardous and airlines are prohibited from loading unexpended and unprotected canisters of oxygen generators onto aircraft. Yet, after the accident, it was clear that someone had placed the canisters on the airplane. As a result, a major focus of the investigation emerged to deter- mine how and why the canisters were loaded onto the airplane. Investigators learned that no single error led to loading the canisters onto the aircraft. To the contrary, about 2 months before the accident, several indi- viduals committed relatively insignificant errors, in a particular sequence. Each error, in itself, was seemingly minor—the type that people may commit when rushed, for example. Rarely do these errors cause catastrophic conse- quences. However in this accident, despite government-approved standards and procedures designed and implemented to prevent them, people still committed critical errors that resulted in a maintenance technician shipping three boxes of unexpended oxygen generators on the accident airplane. Although the errors may have appeared insignificant, a complex system such as commercial aviation has little room for even insignificant errors. Investigators seeking to identify the errors to determine their role in the cause of the accident faced multiple challenges. Many specialists had to methodically gather and examine a vast amount of information, then ana- lyze it to identify the critical errors, the persons who committed them, and the context in which the errors occurred. FIGURE 1.2 Unexpended, unburned chemical oxygen generator, locking cap in place, but open. (Courtesy of the National Transportation Safety Board, 1997 .) 5 It took substantial effort to understand the nature of the errors that led to this accident, and investigators succeeded in learning how the errors were committed. The benefits of their activities were as substantial. By meticu- lously collecting and analyzing the necessary data, investigators were able to learn what happened and why—information that managers and regulators then applied to system operations to make them safer. Many learned lessons from this accident, and they applied what they learned to their own opera- tions. While the tragedy of the accident cannot be diminished, it made the aviation industry a safer one; it has not witnessed a similar type of accident. This is the hope that guides error investigations, that circumstances similar to the event being investigated will not recur and that those facing the same circumstances will not repeat the errors made earlier. Investigating Error Today, in many industrialized countries, government agencies or commis- sions generally investigate major incidents and accidents. Some countries have established agencies that are dedicated to that purpose. For example, the National Transportation Safety Board in the United States, the Transportation Safety Board of Canada, and the Australian Transport Safety Bureau, inves- tigate incidents and accidents across transportation modes in their respec- tive countries. In other countries, government agencies investigate accidents in selected transportation modes, such as the Air Accidents Investigation Branch of Great Britain and the BEA (Bureau d’Enquêtes et d’Analyses pour la sécurité de l’aviation civile) of France, which investigate commercial avia- tion accidents and incidents. However, when relatively minor accidents or incidents occur, organiza- tions with little, if any, experience may need to conduct the investigations themselves. Without the proper understanding, those investigating error may apply investigative procedures incorrectly or fail to recognize how the error continuing airworthines management exposition about. Although researchers have extensively examined error (e.g., Reason, 1990, 1997; Woods, Johannesen, Cook, and Sarter, 1994), there is little available to guide those wishing to investigate error. Despite the many accidents and incidents that are caused by operator error, it appears that few know a formal process to investigate errors or how to apply such a method during the course of an investigation. This book presents a method of investigating errors believed to have led to an accident or incident. It can be applied to error investigations in any complex system, although most of the examples presented are aviation related. This primarily reflects the long tradition and experience of agencies that investigate aviation accidents, and the author’s experience participating in such investigations. Please consider the examples presented as tools to 6 illustrate points made in the book and not as reflections on the susceptibility of any one system or transportation mode to incidents or accidents. Neither the nature of the errors nor the process of investigating errors differs sub- stantially among systems. This book is designed for practitioners and investigators, as well as for stu- dents of error. It is intended to serve as a roadmap to those with little or no experience in human factors or in conducting error investigations. Though formal training in human factors, psychology, or ergonomics, or experience in formal investigative methodology is helpful, it is not required. The ability to understand and effectively apply an investigative discipline to the process is as important as formal training and experience. Chapters begin with reviews of the literature and, where appropriate, fol- low with explicit techniques on documenting data specific to the discussion in that chapter. Most chapters also end with “helpful techniques,” designed to serve as quick investigative references. Outline of the Book The book is divided into five sections, each addressing a different aspect of error in complex systems. Section I defines concepts that are basic to the book, errors and complex systems, Section II focuses on types of antecedents to error, Section III describes data sources and analysis techniques, Section IV discusses three contemporary issues in human error, and Section V reviews an accident in detail and presents thoughts on selected issues important to error investigations. Chapter 2 defines error in complex systems and introduces such critical concepts as operator, incident, accident, and investigation. Contemporary error theories are discussed, with particular attention devoted to Perrow’s description of system accidents (1999) and Moray (2000) and Reason’s (1990, 1997), models of error in complex systems. Changes in views of error over the years are discussed. Chapter 3 discusses the analysis of data obtained in a human error inves- tigation. Different types of analyses are described and their relationship to human error explored. A hypothetical illustration of the application of the analysis methodology to an accident involving human error is presented, with the logic involved in each of the steps examined. Chapter 4 begins the focus on antecedents to error by examining the role of equipment in creating error antecedents, the source of much of the early scientific work in the field of human factors. Information display and control features that affect operator performance are discussed and illustrations of their relationship to operator errors in selected accidents are presented. 7 Chapter 5 discusses antecedents pertaining to the system operator, his- torically the primary focus of those investigating error. Behavioral and physiological antecedents to error are examined, and antecedents that are operator-initiated or caused are differentiated from company-influenced antecedents. Chapter 6 reviews antecedents pertaining to companies that operate com- plex systems. These antecedents incorporate many that are discussed in ear- lier chapters, including operating procedures and company oversight of the application of those procedures to system operations. Chapter 7 examines antecedents related to regulators. It discusses the importance of regulators in both creating the rules under which complex sys- tems operate, and enforcing those rules to insure safe operation. Instances of lax regulation in which the regulator created antecedents to organizational errors in a variety of settings, including the financial sector, are discussed. Chapter 8 assesses the impact of culture on error. Two types of culture, national and company related, are examined. Although they are distinct in terms of their relationship to antecedents, they share characteristics that influence operator performance. Several accidents, which illustrate the types of antecedents that can arise from cultural factors, are reviewed. Chapter 9 reviews operator teams and error antecedents that are unique to teams. The complexities of contemporary systems often call for operator teams with diverse skills to operate the systems. System features that neces- sitate the use of operator teams, the errors that members of these teams could commit, and their antecedents, are examined. Chapter 10 addresses the first of the data sources investigators rely on, electronic data that system recorders capture and record. Types of recorders used in different systems are examined and their contribution to the inves- tigation of error in those systems discussed. A recent accident is presented to illustrate how recorded data can provide a comprehensive view of the system state and an understanding of the errors leading to an accident. Chapter 11 discusses written documentation, an additional data source for investigators. Documentation critical to investigations including records that companies and government agencies maintain, such as medical and personnel records, and factors that affect the quality of that information, are discussed. Several accidents are reviewed to illustrate how written docu- mentation can help investigators understand both the errors that may have led to events in complex systems and their antecedents. Chapter 12 focuses on a third type of data for investigators, interview data, and their use in error investigations. Memory and memory errors are reviewed, and their effects on interviewee recall discussed. Types of inter- viewees are discussed and the factors pertaining to each, such as the type of information expected, the interview location, and the time since the event, examined. Suggestions to enhance interview quality and maximize the information they can provide are offered. 8 Chapter 13 begins Section IV of the book, contemporary issues in error in complex systems. This chapter examines antecedents that are exclusive to the maintenance and inspection environment. With the exceptions of Reason and Hobbs (2003) and Drury (1998), researchers have generally paid little attention to understanding maintenance and inspection errors. Antecedents to these errors include environmental factors, tool design, the tasks themselves, and other fac- tors related to the distinctive demands of system maintenance and inspection. Chapter 14 reviews situation awareness and decision making, and their rela- tionship to system safety. Factors that can influence situation awareness are discussed, many of which are also reviewed as error antecedents elsewhere in the book. The relationship of situation awareness to decision making is out- lined. Two models of decision making are reviewed, classical decision making, applied to relatively static domains and naturalistic decision making, employed in dynamic environments. A case study involving a critical decision-making error is presented to illustrate the role of decision making in system safety. Chapter 15 examines a third issue in error, automation, a subject that has received considerable attention in the literature on error and complex sys- tems, and in accident investigations. Automated systems have introduced unique antecedents. Their effects on operator performance in an accident involving a marine vessel are examined. Chapter 16 begins the section that reviews issues previously discussed in the book. It focuses on an accident in detail to illustrate many of the concepts and methodology presented throughout the book. An automation-related accident involving a Boeing 777, in which a series of interacting antecedents led to a basic and rather simple operator error, is detailed. The roles of the manufacturer, the company, and the regulator are examined in detail. In the final Chapter 17, goals outlined in the first chapter are reexamined. Major principles of human error investigation, as discussed in earlier chap- ters are reviewed, and ways that investigations into error can be used proac- tively to enhance system safety, suggested. Each chapter is meant to stand alone, so that those interested in a spe- cific issue or technique can readily refer to the section of interest. The chap- ters may also be read out of sequence if desired. Nonetheless, reading them sequentially will provide a logical overview of the literature and the field itself. It is hoped that by the end of the book the reader will feel confident to effectively investigate error in a complex system. Section I Errors and Complex Systems Errors, Complex Systems, Accidents, and Investigations Patient accident reconstruction reveals the banality and triviality behind most catastrophes. Perrow, 1999, p. 9 Normal Accidents Operators and Complex Systems There have been extraordinary changes in the machines that affect our daily lives. The equipment has become more complex, more sophisticated and more automated, while becoming more central to our activities. In commercial aviation, for example, two pilots were needed to fly the first commercially successful air transport aircraft, the Douglas DC-3, an air- craft that was designed over 80 years ago. The DC-3 could carry about 20 passengers at a speed of about 200 miles an hour over several hundred miles. Today, two pilots are also needed to operate a passenger-carrying aircraft, the Airbus A-380, but this aircraft transports over 500 passengers, several thousand miles, at speeds in excess of 500 miles an hour. Although the acquisition and operating costs of the A-380 are many times those of its predecessor, the per-seat operating costs are lower. This has helped to make air transportation affordable to many more people than in the DC-3 era. Yet, there is a price that is paid for these technological advances. While the cost of travel has gone down substantially since the DC-3 era because mod- ern aircraft transport more people at lower cost than previously, more people are also exposed to the consequences of operator errors than was true of the earlier era. Accidents that occurred a century ago, such as ship fires, exposed relatively fewer people to risk whereas today thousands have been lost in 14 single events, such as the 1987 sinking of a ferry in the Philippines, or in the 1984 chemical accident in Bhopal, India.* Complex Systems People work with machines routinely and when they do they are machine operators. Whether operating lawn mowers, automobiles, tablets, or power saws, people use machines to perform tasks that they either cannot do them- selves, or can perform more quickly, accurately, or economically with the machines. Together the operator and the machine form a system in which each is a critical and essential system component. As Chapanis (1996) defines, A system is an interacting combination, at any level of complexity, of people, materials, tools, machines, software, facilities, and procedures designed to work together for some common purpose. (p. 22) Complex systems, which employ machines that require multiple operators with extensive training, support our way of life. They provide clean water and sewage treatment, electrical power, and facilitate global finance, to name but a few. These systems, considerably more sophisticated than, say a person operating a lawn mower, have become so integral to our daily activities that in the event they fail whole economies can be threatened. However, as Perrow (1999) notes, the complexity of such systems has increased inordinately. We have produced designs so complicated that we cannot anticipate all the possible interactions of the inevitable failures; we add safety devices that are deceived or avoided or defeated by hidden paths in the systems. The systems have become more complicated because either they are deal- ing with more deadly substances, or we demand they function in ever more hostile environments or with ever greater speed and volume. (p. 12) As our dependence on systems increases, more is asked of them, and with their increasing technical capabilities we have witnessed increased com- plexity. Complex systems are more than merely operators and equipment working together, they are entities that typically perform numerous tasks of considerable import to both companies and individuals. Although complex systems need not necessarily be high-risk systems, that is, systems in which the consequences of failure can be catastrophic, many authors apply the terms interchangeably. Systems that are sufficiently com- plex are often high-risk systems, if for no other reason than because so many people depend on them and thus interruptions from service can dramatically affect our lives. Nonetheless, while the focus of this book is on complex sys- tems, the methodology to investigate human error described can be readily applied to simple systems as well—even to the system in which one person operates a lawn mower. Operators Operators interact with and control complex systems, and consequently play a central role in system safety. Despite the diversity of skills they need, equipment used, and settings in which they operate, one term can be used to describe them. While some have used terms such as “actor,” “technician,” “pilot,” “controller,” and “worker,” the term operator will be used presently. In reference to maintenance activities, the terms technician and inspector will be used, as appropriate. Whether it is a financial, air transport, or electrical generating system, operators essentially perform two functions: they monitor the system and they control its operations. To do so, they obtain information from the system and its operating environment, using their knowledge and experience, with the information, to understand the system state. Based on their understand- ing of the system, they modify operations, as needed, according to operational phase and the system-related information they perceive. Because of the potential severity of the consequences of error in complex systems, operators are expected to be skilled and qualified. They are the first line of defense in trying to limit the effects of system anomalies from becom- ing catastrophic. However, operators sometimes precipitate rather than pre- vent system incidents or accidents. Normal Accidents and Complex Systems The changes that have taken place over time in the complexity of these sys- tems have fundamentally altered the relationship between operators and the machines they control. Once directly controlling the machines, operators now largely supervise their operations. These tasks are typically performed at a higher cognitive and a lower physical level than was true of operators of earlier times who largely controlled the machines manually. Charles Perrow (1999) suggests that complex systems have changed to the extent that “interactive complexity” and “tight coupling” have made “nor- mal accidents” inevitable. That is, as systems have become more efficient, powerful, and diverse in the tasks they perform, the consequences of system failures have grown. In response, designers have increased the number of defenses against system malfunctions and operator errors, thus increas- ing internal system complexity. At the same time, systems have become tightly coupled, so that processes occur in strict, time-dependent sequences, with little tolerance for variability. Should a component or subsystem expe- rience even a minor failure, little or no “slack” would be available within 16 the system, and the entire process could be impacted. The combination of increased complexity and tight coupling has created system states that nei- ther designers nor operators had anticipated. Perrow suggests that unanticipated events in tightly coupled and highly complex systems will inevitably lead to accidents. As he explains, If interactive complexity and tight coupling—system characteristics— inevitably will produce an accident, I believe we are justified in calling it a normal accident, or a system accident. The odd term normal accident is meant to signal that, given the system characteristics, multiple and unexpected interactions of failures are inevitable. (p. 5) It seems difficult to accept that fundamental characteristics of complex systems have made catastrophic accidents “normal.” Perrow, however, has greatly influenced how incidents and accidents in complex systems are con- sidered by focusing not on the operator as the cause of an accident or inci- dent but on the system itself and its design. James Reason (1990, 1997), the British human factors researcher, expanded on Perrow’s theory by focusing on the manner in which system operation as well as system design can lead to errors. He suggests that two kinds of accidents occur in complex systems: one results from the actions of people, which he terms “individual accidents,” and the other “organizational acci- dents,” which results largely from the actions of companies and their man- agers. Reason’s (1997) description of organizational accidents has much in common with Perrow’s normal accidents, These [organizational accidents] are the comparatively rare, but often catastrophic, events that occur within complex modern technologies such as nuclear power plants, commercial aviation, the petrochemical industry, chemical process plants, marine and rail transport, banks and stadiums. Organizational accidents have multiple causes involving many people operating at different levels of their respective companies. Organizational accidents…can have devastating effects on uninvolved populations, assets and the environment. (p. 1) Both Reason and Perrow suggest that, given changes in the nature and function of these systems, new and largely unanticipated opportunities for human error have been created. Vicente (1999) elaborates on the work of Reason and Perrow and identi- fies elements of what he refers to as “sociotechnical systems,” which have increased the demands on system operators. These include the social needs and different perspectives of team members that often operate complex sys- tems, the increasing distance among operators and between operators and equipment, the dynamic nature of systems, increasing system automation, and uncertain data. By escalating the demands on operators, each element has increased the pressure on them to perform without error. 17 Human fallibilities being what they are, there will always be a possibil- ity that an operator will commit an error, and that the consequences of even “minor” errors will present a threat to the safety of complex systems. Some, such as Senders and Moray (1991), Hollnagel (1993), and Reason (1997), suggest that the impossibility of eliminating operator error should be recognized, by focusing not on error but instead on minimizing the consequences of errors. Human Error Most errors are insignificant and quickly forgotten. The relatively minor consequences of most human errors justify the relative inattention we pay them. Some circumstances even call for errors, such as when learning new skills. Children who learn to ride bicycles are expected to make numerous errors initially, but fewer errors as they become more proficient, until they reach the point of riding without error. Designers and training professionals, recognizing the value of errors in learning environments, have developed system simulators that enable operators to be trained in operating systems in realistic environments, free of the consequences of error. People require feedback after they have erred; without it, they may not even realize that they have committed errors. Someone who forgets to deposit money into a checking account may continue to write checks with- out recognizing that the account lacks sufficient funds. That person would not likely be considered to be committing an error each time he or she wrote a check. Rather, most would consider the person to have committed only one error—the initial failure to deposit funds into the account. It should be apparent that the nature of errors and the interpretation and determination of their significance are largely contextual. Turning a crank the wrong way to close an automobile window is a minor error that would probably be quickly forgotten. On the other hand, turning a knob in the control room of a nuclear power plant in the wrong direction can lead to a nuclear accident. Both errors are similar—relatively simple acts of rotating a control in the wrong direction—yet under certain conditions an otherwise minor error can cause catastrophic consequences. What ultimately differentiates errors are their contexts and the relative severity of their consequences. Theories of Error Modern error theory suggests that in complex systems, operator errors are the logical consequences of antecedents or precursors that had been present in the systems. Theorists have not always considered system antecedents to play as large a role in error causation as is considered today. Freud Freud and his students believe that error is a product of the unconscious drives of the person (e.g., Brenner, 1964). Those who erred are considered less effective and possibly more deficient than those who do not, an inter- pretation that has had wide influence on theories of error and on subsequent research. For example, the concept of “accident proneness,” influenced by Freud’s view of error, attributed to certain people a greater likelihood of committing errors than to others because of their personal traits. However, studies (e.g., Rodgers and Blanchard, 1993; Lawton and Parker, 1998) have found serious methodological deficiencies in the initial studies upon which much of the later assumptions about error proneness had been based. For example, the failure to control the rates of exposure to risk minimized the applicability of conclusions derived. Lawton and Parker conclude, “…it proved impossible to produce an overall stable profile of the accident-prone individual or to determine whether someone had an accident-prone personality” (p. 656). The application of Freud’s theories (he used multiple theories to explain human behavior) outside of clinical settings has largely fallen into disfavor as both behavioral and cognitive psychological theories have gained increasing acceptance. Unlike Freud, error theorists since his day consider the setting in which errors are committed when examining error to be far more important than the characteristics of the person committing the error. Heinrich Heinrich (1931, 1941) was among the first to systematically study accident causation in industrial settings. He suggested that incidents and accidents can be prevented by breaking the causal link in the sequence or chain of events that led up to them. Focusing on occupational injuries, that is job-related injuries, he suggested that accidents result from a sequence of events involving people’s interactions with machines. One step leads to others in a fixed and logical order, much as a falling domino causes subsequent stand- ing dominoes to fall, ultimately leading to an incident or accident. Heinrich suggested that incidents and accidents form a triangle or pyramid of fre- quency, with non-injury incidents, which occur the least often, located at the bottom of the pyramid, incidents with minor injuries, which occur more often than non-injury incidents, at the middle of the pyramid, and accidents with serious injuries, which occur the least often, at the top of the pyramid. To Heinrich, two critical underlying factors leading to accidents were personal or mechanical hazards resulting from carelessness and poorly designed or improperly maintained equipment. Carelessness and other “faults” were, to Heinrich, the result of environmental influences, that is, the environment in which people were raised, or traits that they inherited. Heinrich’s work, with its systematic study of accident causation, had considerable influence on our view of accident causation. Coury, Ellingstad, and Kolly (2010) wrote that as a result of Heinrich’s work, many have come to view accident causation as a series of links in a chain, to be prevented by breaking the link or sequence of events. Norman Norman (1981, 1988) studied both cognitive and motor errors and differenti- ated between two types of errors: slips and mistakes. Slips are action errors or errors of execution that are triggered by schemas, a person’s organized knowledge, memories, and experiences. Slips can result from errors in the formation of intents to act, faulty triggering of schemas, or mental images of phenomena, among other factors. He categorized six types of slips, exempli- fied by such relatively minor errors as striking the wrong key on a computer keyboard, pouring coffee into the cereal bowl instead of the cup adjacent to the bowl, and speaking a word other than the one intended. Mistakes are errors of thought in which a person’s cognitive activities lead to actions or decisions that are contrary to what was intended. To Norman, slips are errors that logically result from the combination of environmental triggers and schemas. Applying the lessons of slips to design, such as standardizing the direction of rotation of window cranks in automobiles, would, to Norman, reduce the number of environmental triggers and there- fore reduce the likelihood of slips. Rasmussen Jens Rasmussen (1983), a Danish researcher, expanded the cognitive aspects of error that Norman and others described, by defining three levels of opera- tor performance and three types of associated errors: skill-, knowledge-, and rule-based. Skill-based performance, the simplest of the three, relies on skills that a person acquires overtime and stores in memory. Skill-based perfor- mance errors are similar to Norman’s slips in that they are largely errors of execution. With rule-based performance, more advanced than skill-based, operators apply rules to situations that are similar to those that they have encountered through experience and training. Rule-based performance errors result from the inability to recognize or understand the situations or circumstances encountered. This can occur when the information necessary to understand the situation is unavailable, or the operator applies the wrong rule to unfamiliar circumstances. Rasmussen maintains that the highest level of performance is knowledge- based. Rather than applying simple motor tasks or rules to situations that are similar to those previously encountered, the operator applies previously learned information, or information obtained through previous experience, 20 to novel situations to analyze or solve problems associated with those situations. Knowledge-based performance errors result primarily from short- comings in operator knowledge or limitations in his or her ability to apply existing knowledge to new situations. Reason James Reason (1990) enlarged the focus of earlier definitions of errors and further distinguished among basic error types. He defines slips as others have—relatively minor errors of execution, but he also identifies an additional type of error, a lapse, which he characterizes as primarily a memory error. A lapse is less observable than a slip and occurs when a person becomes distracted when about to perform a task, or omits a step when attempting to complete the task. Reason also distinguishes between mistakes and violations. Both are errors of intent—mistakes result from inappropriate intentions or incorrect diagnoses of situations, violations are actions that are deliberately nonstandard or contrary to procedures. Reason does not necessarily consider violations to be negative. Operators often develop violations to accomplish tasks in ways they believe would be more efficient than those accomplished by following procedures that design- ers and managers developed. By contrast, Reason considers a deliberate act, intended to undermine the safety of the system, to be sabotage. Reason’s categorization of errors corresponds to Rasmussen’s perfor- mance-based errors. Slips and lapses are action errors that involve skill- based performance while mistakes involve either rule- or knowledge-based performance. Reason, however, added to previous error theories by addressing the role of designers and company managers in operator errors, that is, those who function at the higher levels of system operations, at what he labels the “blunt end” of a system. Those at the blunt end commit what he terms “latent errors” (but later (1997) referred to as “latent conditions”) within a system. Operators, located at the “sharp end” of a system, commit what he calls “active errors,” errors that directly lead to accidents. Operators’ active errors are influenced, Reason argues, by latent errors that those at the blunt end have committed, errors that lie hidden within the system. Although active errors lead to consequences that are almost imme- diately recognized, the consequences of latent errors may go unnoticed for some time, becoming manifest only when a combination of factors weaken system defenses against active errors. Designers and managers place internal defenses in systems to prevent errors from leading to incidents and accidents in recognition of the potential fallibility of human performance. However, should the defenses fail when an operator commits an error, catastrophic consequences could occur. 21 Reason (1990) uses a medical analogy to explain how latent errors can affect complex systems, There appear to be similarities between latent failures in complex technological systems and resident pathogens in the human body. The resi- dent pathogen metaphor emphasises the significance of causal factors present in the system before an accident sequence actually begins. All man-made systems contain potentially destructive agencies, like the pathogens within the human body. At any one time, each complex system will have within it a certain number of latent failures, whose effects are not immediately apparent but that can serve both to promote unsafe acts and to weaken its defence mechanisms. For the most part, these are tolerated, detected and corrected…but every now and again a set of external circumstances—called here local triggers—arise that com- bines with these resident pathogens in subtle and often unlikely ways to thwart the system’s defences and to bring about its catastrophic break- down. (p. 197) Reason illustrates how company-related defenses and resident pathogens affect safety by pointing to slices of Swiss cheese that are lined up against each other (Figure 2.1). Unforeseen system deficiencies, such as questionable managerial and design decisions, precede managers’ actions. These lead to “psychological precursors” among operators such as reactions to stress or to other aspects of the “human condition,” and to unsafe acts. These rep- resent the holes in the Swiss cheese whereas the solid parts of the cheese slices represent company defenses against the hazards of unsafe acts. If the Swiss cheese slices were placed one against the other, the holes or deficien- cies would be unlikely to line up in sequence. Some “holes” due to active failures Defenses in depth Other “holes” due to latent conditions. Company-related defenses, the solid portions of the cheese, would block an error from penetrating. However, should the deficiencies (holes) line up uniquely, an active error could breach the system, much as an object could move through the holes in the slices, an unsafe act would not be prevented from affecting the system, and an accident could result. To Reason, even though managerial and design errors are unlikely to lead directly to accidents and incidents, an examination of human error should assess the actions and decisions of managers and designers at the blunt end at least as much, if not more, than the actions of the system operators at the sharp end. His description of the role of both design and company-related or managerial antecedents of error has greatly influenced our understanding of error, largely because of its simplicity, rationality, and ease of understanding. Further, his approach to developing a model to explain error causation was also influential. For example, the International Civil Aviation Organization (ICAO) has formally adopted Reason’s model of error for its member states to facilitate their understanding of human factors issues and aviation safety (ICAO, 1993). Dekker and Pruchnicki (2014) updated Reason’s model, in the light of several major accidents and theoretical work that had been conducted since his initial work on error was published. Errors in complex systems that lead to accidents and incidents, they argue, are often preceded by extensive periods, which they refer to as “incubation periods,” in which the latent errors of which Reason speaks, or organizational shortcomings, gradually increase but remain unrecognized. These shortcomings maybe taken for granted, or are unrecognized over time as the risks increase and the organization or company gradually “drifts” toward an accident. As they note: Pressures of scarcity and competition, the intransparency and size of complex systems, the patterns of information that surround decision makers, and the incremental nature of their decisions over time, all enter into the incubation period of future accidents. Incubation hap- pens through normal processes of reconciling differential pressures on an organisation (efficiency, capacity utilisation, safety) against a back- ground of uncertain technology and imperfect knowledge. Incubation is about incremental, or small, seemingly insignificant steps eventually contributing to extraordinary unforeseen events. (p. 541) What Is Error Researchers generally agree on the meaning of an error. To Senders and Moray (1991), it is “something [that] has been done which was not intended by the actor, not desired by a set of rules or an external observer, or that led the task or system outside its acceptable limits” (p. 25). Reason (1990) sees an error as “a generic term to encompass all those occasions in which a planned sequence of mental or physical activities fails to achieve its intended outcome, and when these failures cannot be attributed to the intervention of some chance agency” (p. 5). Woods, Johannesen, Cook, and Sarter (1994) define error as “a specific variety of human performance that is so clearly and significantly substandard and flawed when viewed in retrospect that there is no doubt that it should have been viewed by the practitioner as substandard at the time the act was committed or omitted” (emphasis in original, p. 2). Hollnagel (1993) believes that the term “human error” is too simplistic and that “erroneous action” should be used in its place. An erroneous action, he explains, “is an action which fails to produce the expected result and which therefore leads to an unwanted consequence” (p. 67). He argues that one should not make judgments regarding the cause of the event. The term erro- neous action, unlike error, implies no judgment and accounts for the context in which the action occurs. Despite some disagreement in defining error, most researchers agree on the fundamental aspects of error, seeing it as the result of something that people do or intend to do that leads to outcomes different from what they had expected. Therefore, to be consistent with these views, error will be defined in this book as an action or decision that results in one or more unintended negative outcomes. Errors that occur in learning or training environments, where they are expected, tolerated, and used to enhance and enlarge a person’s repertoire of skills and knowledge, will not be considered further. For our purposes even though researchers have described multiple types of errors, insofar as accident or incident investigations are concerned, only two types of errors are important, action errors and decision errors. In an action error, an operator does something wrong, such as shuts a system down that should have continued in operation, or does something contrary to what had been called for by company procedures. Decision errors refer to incorrect decisions that operators make, such as misinterpreting weather information and proceeding into an area of adverse weather. In general, errors related to equipment control design antecedents tend to be action errors. Errors that call for interpretation, such as navigation or understanding the meaning of multiple alarms, tend to be decision errors. Error Taxonomies Senders and Moray (1991) developed an error taxonomy based largely on the work of Rasmussen, Reason, and others, to better understand errors and the circumstances in which people commit errors. Their taxonomy suggests that error results from one or more of the following factors, operating alone or together, the person’s “information-processing system” or cognitive pro- cesses; environmental effects; pressures on and biases of the individual; and the individual’s mental, emotional, and attentional states. This taxonomy describes errors in terms of four levels; “phenomenological” or observable 24 manifestations of error, cognitive processes, goal-directed behaviors, and external factors, such as environmental distractions or equipment design factors. Shappell and Wiegmann (1997, 2001) propose a taxonomy to apply to the investigation of human error in aircraft accidents, a model that has since been embraced and applied by such U.S. agencies as the U.S. Coast Guard, in the investigation of marine accidents. Expanding on Reason’s work, their taxonomy differentiates among operations that are influenced by unsafe supervision, unsafe conditions, and unsafe acts. Unsafe acts include various error categories, while unsafe conditions include both behavioral and physiological states and conditions. Unsafe supervision, which distinguishes between unsafe supervisory actions that are unforeseen and those that are foreseen, incorporates elements that Reason would likely term latent errors or latent conditions. Sutcliffe and Rugg (1998) propose a taxonomy based on Hollnagel’s (1993), that distinguishes between error phenotypes (the manifestation of errors) and genotypes (their underlying causes). They group the operational descrip- tions of errors into six categories and divide causal factors into three groups: cognitive, social and company-related, and equipment or tool design. O’Hare (2000) proposed a taxonomy, referred to as the “Wheel of Misfortune,” to serve as a link between researchers in human error and acci- dent investigators seeking to apply research findings to incidents or acci- dents. As with Reason, he delineates company-related defenses that could allow operator error to affect system operations unchecked. Incidents, Accidents, and Investigations Incidents and Accidents Loimer and Guarnieri (1996), in a review of accident history, described how the meaning of term has changed over the years. Aristotle, for example, used accidents to refer to nonessential or extrinsic characteristics of people and things. Thus, someone could have accidental qualities, for example, one leg, and still retain human characteristics. About the fourteenth century, the English began to use a more modern understanding of the term, closer to that of contemporary times, that is, “to happen by chance; a misfortune; an event that happens without foresight or expectation” (p. 102), a usage initially found in Chaucer in 1374. As the industrial revolution developed in the late eighteenth century, injuries of workers in the textile, railroad, and mining industries began to emerge. These were new types of accidents that occurred among workers who were operating what were then complex systems, but of course system operations required considerably more muscular effort than 25 is true today, with little design and training consideration directed to worker safety. Loimer and Guarnieri noted that accident attribution began to change around that time as well, from being considered the result of divine influence that had been common in the middle ages to that of worker causation, for example, carelessness, of the industrial revolution. Coury et al. (2010) wrote that World War II brought about considerable complexity in systems such as aircraft used in the war effort. System complexity was also influenced by the rapid development of and the need to quickly uti- lize these systems, which called for hastily training people to operate them, factors that contributed to high rates of training accidents. In attempting to understand the reasons for the accident rates, researchers focused on opera- tor error from the perspective of factors related to the design of the system controls and displays, rather than on the operator himself or herself, a focus that led to research to better understand how machine design can lead to error. Process Accidents Today, researchers devote considerable attention to examining on the job injuries, especially in such industries as petrochemical processing and mining (e.g., Flin, Mearns, O’Connor, and Bryden, 2000). But the nature of accident causation is typically different in worker injury accidents than it is in process accidents. In the former, accident causation is largely considered the result of flaws in control design, training, or worker attention. In the latter, the type that is the focus of this book, causation is generally attributed to flaws in the system itself, which can include design, training, and worker attention but typically involves elements of the entire system. Certainly, the consequences of the two are different as well. Occupational accident con- sequences primarily affect system operators while process accidents may affect the workers or operators, but as often affect those uninvolved in sys- tem operations, such as passengers in transportation accidents, or residents near a nuclear generating station that sustained a radiation leak. Senders and Moray (1991), focusing on process accidents, term an accident “a manifestation of the consequence of an expression of an error” (p. 104). Others suggest that accidents are events that are accompanied by injury to persons or damage to property. In this way, even minor injuries can change the categorization of an incident, typically involving an occurrence of more minor consequences, to that of an accident, an occurrence with often major or severe consequences. Those consequences can be injuries to persons, damage to property, and or pollution of the air, water, or land environment. Perrow (1999) distinguished between accidents and incidents largely by the extent of the damage to property and injuries to persons. He consid- ers incidents to be events that damage parts of the system, and accidents events that damage subsystems or the system as a whole, resulting in the 26 immediate shutdown of the system. Although a system accident may start with a component failure, it is primarily distinguished by the occurrence of multiple failures interacting in unanticipated ways. Catastrophic system accidents may bring injury or death to bystanders uninvolved with the sys- tem, or even to those not yet born. For example, accidents in nuclear generating stations can lead to birth defects and fertility difficulties among those exposed to radiation released in the accident. Legal Definitions Whether an event is classified as an incident or an accident can have considerable influence on data analysis, research, as well as on civil or criminal proceedings. Therefore, much attention has been devoted to the classification of accidents. Loimer and Guarnieri (1996) describe the historic tradition, dating to the middle ages, of accident causation being attributed to acts of god as compared to acts of people. Today, they note, any accident that is caused, directly or indirectly, by natural causes “without human intervention” is considered to be “an act of god.” In this respect, the March 11, 2011, accident at the Fukushima Daiichi nuclear power plant, which occurred in the aftermath of a magnitude 9 earthquake and subsequent tsunami, may be considered an act of god, despite the fact that the direct cause of the nuclear accident was the flooding of the diesel generators that provided electric power for emergency water cooling to the nuclear core. Water from the tsunami entered and contaminated the generators, which had been placed at ground level, thereby making them susceptible to flooding given the reac- tor’s proximity to the sea. For our purposes, even though the flooding was a naturally caused event, the placement of the generators adjacent to the sea was not, and thus investigators would still want to examine the system shortcomings that allowed the tsunami to result in a nuclear accident. In general, the contemporary classification of events into accidents ignores the natural- versus person-caused aspect to focus on the severity of the con- sequences. Consequently, specific definitions in both international law and in the laws of individual nations define accidents. For example, international civil aviation organization (1970) defines an aircraft accident as, An occurrence associated with the operation of an aircraft which takes place between the time any person boards the aircraft with the intention of flight until such time as all such persons have disembarked, in which: a person is fatally injured…or the aircraft sustains damage or structural failure…or the aircraft is missing or is completely inaccessible. (p. 1) international civil aviation organization also precisely defines injury and death associated with an accident. Injuries include broken bones other than fingers, toes, or noses, or any of the following: hospitalization for at least 48 hours within 7 days of the event, severe lacerations, internal organ damage, second- or third-degree burns 27 over 5% or more of the body, or exposure to infectious substances or injurious radiation. A fatal injury is defined as a death from accident-related inju- ries that occurred within 30 days of the accident. An incident is an event that is less serious than an accident. Other government or international agencies use similar definitions, albeit specific to the particular domain. For example, the U.S. Coast Guard defines a marine accident as, Any casualty or accident involving any vessel other than public vessels if such casualty or accident occurs upon the navigable waters of the United States, its territories or possessions or any casualty or accident wherever such casualty or accident may occur involving any United States vessel which is not a public vessel…[including] any accidental grounding, or any occurrence involving a vessel which results in damage by or to the vessel, its apparel, gear, or cargo, or injury or loss of life of any person; and includes among other things, collisions, strandings, groundings, founderings, heavy weather damage, fires, explosions, failure of gear and equipment and any other damage which might affect or impair the seaworthiness of the vessel…[and] occurrences of loss of life or injury to any person while diving from a vessel and using underwater breathing apparatus. (46 Code of Federal Regulations 4.03-1 (a) and (b)) Under U.S. law, 46 U.S. Code § 6101, a major marine accident, referred to as a “major marine casualty,” is defined as: … a casualty involving a vessel, other than a public vessel, that results in— 1. The loss of 6 or more lives. 2. The loss of a mechanically propelled vessel of 100 or more gross tons. 3. Property damage initially estimated at $500,000 or more. 4. Or serious threat, as determined by the Commandant of the Coast Guard with concurrence by the Chairman of the National Transportation Safety Board, to life, property, or the environ- ment by hazardous materials. To avoid confusion among the various definitions, both incidents and accidents in complex systems will be defined as: unexpected events that cause substantial property or environmental damage and/or serious injuries to people. Accidents lead to consequences that are more severe than those of incidents. Investigations Coury et al. (2010) reviewed the history of accident investigations in complex systems, focusing on transportation accident investigations, and noted how the evolution of accident investigation matched the corresponding evolution 28 in technology. As technology became more reliable, investigations focused less on hardware and more on the role of those who operate the systems. Although companies often investigated the accidents of systems they owned and operated, governments also played a role in the investigations, often initially in the role of coroners’ inquests. Eventually, investigations went beyond identifying the accident cause as operator error, or pilot error in the case of aviation, to focus on the nature of the interaction between the operator and the system being operated. Coury et al. (2010) note that with the advent of World War II, human factors emerged as a major element of accident invstigations. “No longer was it acceptable,” they note, “to merely identify the type of pilot error; now the design of the system and its contribution to the error must also be considered” (p. 16). Further, as investigators gained a more sophisticated understanding of error in accident causation, in aviation accident investigations, Pilot and operator error were no longer simply categories within causal taxonomies but instead reflected a more complex interaction between people and machines that could be empirically studied and even “designed out” of human machine systems. As a result, human factors and human performance assumed a larger role in accident investigation, in which safety issues were related to potential incompatibilities with human information processing, and cognition and influenced the way accident investigators thought about pilot actions. (p. 16) Le Coze (2013), in a review of major models of investigations, describes two “waves” of highly visible major accidents that occurred in the past 20–30 years that have impacted our view of accidents. The recent accidents, which involved a variety of complex systems, …all come under the same intense national and often also international interest and scrutiny by the media, justice systems, civil society, states, financial markets, industry and professions. They have a strong symbolic component, where each time, and probably at Fukushima more than elsewhere, a belief about the safety of these systems that had previously been taken for granted has seriously been undermined. (p. 201) The accidents to which he referred include, in the first wave, the 1986 explosion of the space shuttle Challenger, the ground collision at Tenerife of two Boeing 747s, and the grounding of the tanker Exxon Valdez. The acci- dents in the second wave include the grounding of the cruise vessel Costa Concordia, and the meltdown at the Fukushima Daiichi nuclear power plant that followed the earthquake and tsunami. Although today accident investigations are conducted to identify the cause or causes of accidents and thereby develop ways of mitigating future opportunities for error and malfunctions, investigations may fulfill multiple missions as well. Senders and Moray (1991) acknowledge that investigations can 29 be conducted for a variety of purposes. “What is deemed to be the cause of an accident or error,” they write, “depends on the purpose of the inquiry. There is no absolute cause” (emphasis in original, p. 106). For example, law enforcement personnel conduct criminal investigations to identify perpetrators of crimes and to collect sufficient evidence to prosecute and convict them. Governments investigate accidents to protect the public by ensuring that the necessary steps are taken to prevent similar occurrences, mandating necessary changes to the system or changing the nature of its oversight of the system. Kahan (1999) notes that governments have become increasingly involved in investigating transportation accidents. Whereas governments initially investigated accidents on an individual basis and assigned investigators to the investigations as they occurred, many governments have established agencies with full-time investigative staffs for the exclusive purpose of investigating accidents. Rasmussen, Pejtersen, and Goodstein (1994) contend that investigators examine system events according to a variety of viewpoints. These include a common sense one, and those of the scientist, reliability analyst, therapist, attorney, and designer, respectively. Each influences what Rasmussen et al. (1994) refer to as an investigation’s “stopping point,” that is, the point at which the investigator believes that the objectives of the investigation have been met. For example, an investigator with a common sense perspective stops the investigation when satisfied that the explanation of the event is reasonable and familiar. The scientist concludes the investigation when the mechanisms linking the error antecedent to the operator who committed the error are known, and the attorney concludes the investigation when the one responsible for the event, usually someone directly involved in the operation who can be punished for his or her actions or decisions, is identified. The objective advocated in this book is based on the suggestions of Rasmussen et al. (1994). Investigators should conduct investigations to learn what caused an incident or accident by establishing a link between antecedent and error, so that changes can be implemented to prevent future occurrences. Dekker (2015) identified four purposes of accident investigations, epistemological, that is, establishing what happened; preventive, identifying path- ways to avoidance; moral, tracing the transgressions that were committed and reinforcing moral and regulatory boundaries; and existential, finding an explanation for the suffering that occurred. These purposes affect the conduct of accident investigations. For example, the existential and moral needs Dekker identified, and the public policy implications Le Coze (2013) described, are addressed by the direct role of governments in investigations. Relying on government rather than industry to conduct such investigations, for example, satisfies the public need for answers to what happened, and the need for reassurance that action will be taken to address the shortcomings that led to the accident. Stoop and Dekker (2012), focusing on aviation acci- dent investigations, also note the evolution of investigations as technology has advanced, to where today we accept failure as “normal,” where resilient 30 systems can allocate scarce safety resources as needed in response to differ- ent system states. Today, it can be said that investigations, particularly those of major acci- dents, serve multiple functions. These serve not just to determine how the accident developed and was caused, so that changes in the system can be implemented to prevent similar accidents in the future, but for needs that transcend those of the accident itself. As Le Coze (2008) writes, Accident investigations are not research works with the aim of theoris- ing. They are investigative projects, set up in a specific political context following a disaster, for understanding its circumstances and for mak- ing recommendations. They also serve a societal need for transparency. These are big projects carried out through a short period of time, often within months. The number of staff is important. This staff includes people collecting the data, advisors and consultants from university and industry for various aspects ranging from technical to organisational and human factors issues, administrative people, etc. (p. 140) Accident investigations, where investigators identify the factors that led to an accident, analyze how those factors played a role in the circumstances in which the accident occurred, and ultimately suggest ways to prevent their recurrence, call for data collection and analysis skills. Unlike empiri- cal research, which is overseen through peer review, theory testing, and/or experimental replication, major accident investigations are typically subject to governmental or corporate review. In addition, investigations face time pressures that can be considerable. Unless the investigations can be con- ducted quickly, the findings of the investigation could have little significance in terms of risk mitigation and public need. Further, analytical rules of accident investigations tend to be legalistic, using logical consistency and the preponderance of evidence. Based on the facts gathered, investigators develop a logical explanation of the events that led to an accident. This generally results in identifying errors on the part of individual operators or operator teams (including maintenance personnel), failures of some mechanical component or system, a failure that may have been the result of an operator error, and/or errors in actions, inactions and/ or shortcomings in decisions of organizational managers. Although some investigative agencies shy away from identifying operator errors, the practice is still commonplace among such investigative agencies as the United States National Transportation Safety Board, the British Air Accidents Investigation Branch and the Marine Accidents Investigation Branch, and the French Bureau d’Enquêtes et d’Analyses pour la sécurité de l’aviation civile, when investigators believe that this is warranted. These aspects of investigations affect the way in which investigations are conducted, by emphasizing the investigators’ ability to complete the investigation in a timely manner (i.e., “getting the job done”), while simultaneously following rigorous rules of logic. Assumptions Several assumptions about operator error in complex systems form the foun- dation of the investigative approach of this book. These are • The more complex the task, the greater the likelihood that an error will be committed. • The more people involved in performing a task, the greater the like- lihood that an error will be committed. • People behave rationally and operate systems in a way to avoid accidents. • Errors cannot be eliminated, but opportunities for error can be reduced. Although the first two assumptions may seem rather obvious, some assume the contrary, that by adding steps and operators to a task the chances of error decrease. In fact, with certain exceptions, the opposite is true. As a task becomes more complex and more people are needed to perform it, opportunities for error increase. In addition, operators are ratio- nal in that they want to avoid accidents and operate systems accordingly. Those who mean to cause accidents in effect intend criminal acts, which call for a different investigative approach than that used in this book. It should be noted, however, that on occasion criminal acts have been initially investigated as accidents, until evidence of operator planning to make the event appear to be an accident emerged (e.g., National Transportation Safety Board, 2002). Systems that people design, manage, and operate, are not immune to the effects of error. Because people are not perfect, designers and managers cannot design and oversee a perfect system and operators cannot ensure error- free performance. Operators of any system, irrespective of its complexity, purpose, or application, commit errors. As Gilbert, Amalberti, Laroche, and Paries (2007) observed, Observations of operators’ practices show that they regularly make errors, which are therefore not abnormalities or exceptions. The errors are accepted, remedied or ignored. Errors are a price to pay, a necessity for adjustment, mere symptoms of good cognitive functioning. Errors (and all failures) can neither be reduced to departures from the rules, nor considered as abnormalities or exceptions. They are an integral part of habitual, normal functioning, irrespective of the level on which they are situated. (p. 968) 32 The task of investigators, therefore, is to determine the cause of errors so that modifications to the system can be proposed, so that the circumstances that led to the errors are prevented from recurring. General Model of Human Error Investigation Researchers have proposed different accident causation and investigation models, to explain how error affects operator performance in investigations. Some, like Leveson’s (2004) systems-theoretic accident model and processes (STAMP) model, seek to integrate accident causation analysis with hazard analysis and accident prevention strategies. Others, like Shappell and Wiegmann’s human factors analysis and classification system (HFACS) model (1997, 2001), which is directly based on Reason’s model of error causa- tion (1990, 1997), have been widely used to analyze the role of human factors in accident causation (e.g., Li and Harris, 2005; Schröder-Hinrichs, Baldauf, and Ghirxi, 2011). However, models, largely because they are directly based on theory, may be difficult to apply in actual investigations. Accidents are unique events and investigators must be prepared to identify data to be collected and analyzed according to the needs of the investigation, rather than of particular theories. As Reason et al. (2006) note, Accidents come in many sizes, shapes and forms. It is therefore naïve to hope that one model or one type of explanation will be universally applicable. Some accidents are really simple, and therefore only need simple explanations and simple models. Some accidents are complex, and need comparable models and methods to be analysed and pre- vented. (p. 21) Neville Moray (1994, 2000), a British human factors researcher, contends that error in complex systems results from elements that form the systems and to investigate system errors, one must examine the pertinent elements. He out- lines these features with concentric squares that show the equipment as a core component of the system (Figure 3.2). These elements shape the system: •Equipment •Individual operator Operator team • Company and management • Regulator • Society and cultural factors. Each system component affects the quality of the system operation, and can create opportunities for operator error. For example, the information operators obtain about the system affects their perception of the system state. Displayed information that is difficult to interpret can increase the likelihood of error. Each of these elements can lead to error in itself, or can interact with the others to create opportunities for error. To be useful for those investigating accidents, models must be practical and if not investigators will have difficulty applying them to investigations. Models should also be simple by avoiding complexity in explaining error or accident causation. For optimum benefit, models should also be practical, while still adhering to research findings on error causation. This text will eschew models in favor of a method that, based in the theories of both Moray (1994, 2000) and Reason (1990, 1997), is designed to facilitate the task of data identification, collection, and analysis for those investigating the role of human error in accident and incident investigations. Antecedents Because errors are unintended, one assumes that operators want to operate systems correctly. Using Moray’s (1994, 2000) model, with that of Reason (1990, 1997), their errors are considered to reflect system influences on their performance. That is, the operators wanted to perform well but did not because of shortcomings within the system. I refer to these characteristics as precursors or antecedents to error. As Reason argues, antecedents may be hidden within systems, such as in equip- ment design, procedures, and training, where they remain unrecognized but can still degrade system operators’ performance. The mechanisms by which each antecedent or precursor exerts its influence varies with the context and nature of both the system element and the antecedent itself. For example, an antecedent may distract an operator during a critical task, hinder his or her ability to obtain critical information, or limit his or her ability to recall or apply the proper procedure. The focus of the accident investigator therefore should be to identify those shortcomings within the system that led to the accident. Investigators identify the presence of an antecedent in two ways, by identifying an action, situation, or factor that influenced the operator’s performance during the event, and more importantly, by obtaining evidence demonstrating that the operator’s performance was affected by the antecedent. The evidence, which can take many forms, will be discussed in subsequent chapters. Antecedents and Errors Antecedents in complex systems contribute to errors through unrecognized or unacted upon shortcomings in the system. While complex systems are composed of a multitude of components, the elements of the system in this book are general, derived from the antecedents identified in both Moray (1994, 2000) and Reason’s (1990) models. They can be considered latent errors or latent conditions within the system as well as system shortcomings, inadequacies, in sum, any other system action or decision that adversely influenced an operator’s performance (Figure 2.3). The errors that led to accidents and incidents, whether committed by operators or system managers, are either action errors, that is, someone did something wrong, or decision errors, that is, someone made a decision that proved to be erroneous. Further, because in accident causation failure to take an action or make a decision may be as critical to the cause of the accident as taking the wrong action or making a decision that proved to be erroneous, errors of omission should be considered as well as errors of commission. The logic used in this process will be discussed in more detail in Chapter 3, Analyzing the Data. Keep in mind though, that the steps to be conducted in identifying both antecedents and errors, and relating them to the accident or incident, are ongoing through the investigation. That is, when identifying errors and searching for their antecedents, investigators should always keep in mind the role antecedents may play in the critical error or errors that led to the event under investigation. Summary Complex systems are those combinations of people, materials, tools, machines, software, facilities, and procedures designed to work together for a common purpose. Perrow argues that the interactive complexity and “tight coupling” or close interrelationships among complex system elements create conditions that make accidents and incidents “normal.” When component malfunctions occur, the combination of interactive complexity and tight coupling within the system can create system states that neither operators nor designers had anticipated. Error is defined as an action or decision that results in one or more unintended negative outcomes. Perrow’s work has influenced theories of error, and has changed the way a system’s influence on operator performance is viewed. Where researchers had seen errors as primarily reflecting on the person committing them, contemporary views of error see it originating within the operating system. Reason likens these elements to pathogens residing within the body. As pathogens can cause illness when certain conditions are met, system-related deficiencies (latent errors or latent conditions) cause the normal defenses to fail and lead to an operator error, which causes an event. Moray delineates system elements that can lead to error in complex systems. Error investigations can have many objectives and purposes, depending on the investigator’s perspective. The objective of an investigation should be to mitigate future opportunities for error by identifying the critical errors and their antecedents, and eliminating them or reducing their influence in the system. The model of error that is proposed in the book describes six types of antecedents, each of which, alone or in combination, can adversely affect operator performance and lead to an error.