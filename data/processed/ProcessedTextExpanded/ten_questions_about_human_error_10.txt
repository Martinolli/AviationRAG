Title: Ten Questions About Human Error: A New View of Human Factors And System Safety Chapters 9 - 10 Author(s): Sidney W. A. Dekker Category: Analysis, Human, Factors, Errors Tags: human, error, factors, accident Chapter 9 Will the System Be Safe? How do you know whether a new system will be safe? As chapter 8 showed, automating parts of human work may make a system safer, but they may not. The Alaska Airlines 261 accident discussed in chapter 2 illustrates how difficult it is to know whether a system is going to be safe during its operational lifetime. In the case of the DC-9 trim system, bridging the gap between producing a system and running it proved quite difficult. Certifying that the system was safe, or airworthy, when it rolled out of the factory with zero flying hours was one thing. Certifying that it would stay safe during a projected lifetime proved to be quite another. Alaska 261 shows how large the gulf between making a system and maintaining it can be. The same is true for sociotechnical systems. Take the issue of flightprogress strips in air-traffic control. The flight strip is a small paper slip with flight-plan data about each controlled aircraft's route, speed, altitude, times over waypoints, and other characteristics (see Fig. 9.1). It is used by air-traffic controllers in conjunction with a radar representation of air traffic. A number of control centers around the world are doing away with these paper strips, to replace them with automated flight-tracking systems. Each of these efforts requires, in principle, a rigorous certification process. Different teams of people look at color coding, letter size and legibility, issues of human-computer interaction, software reliability and stability, seating arrangements, button sensitivities, and so forth, and can spend a decade following the footsteps of a design process to probe and poke it with methods and forms and questionnaires and tests and checklists and tools and guidelines—all in an effort to ensure that local human factors or ergonomics standards have been met. But such static snapshots may mean little. A lineup of microcertificates of usability does not guarantee safety. As soon as they hit the field of practice, systems start to drift. A year (or a month) after its inception, no sociotechnical system is the same as it was in the beginning. As soon as a new technology is introduced, the human, operational, organizational system that is supposed to make the technology work forces it into locally practical adaptations. Practices (procedures, rules) adapt around the new technology, and the technology in turn is reworked, revised, and amended in response to the emergence of practical experience. THE LIMITS OF SAFETY CERTIFICATION System safety is more than the sum of the certified parts. A redundant torque tube inside of a jackscrew, for example, does nothing to maintain the integrity of a DC-9 trim system without a maintenance program that guarantees continued operability. But ensuring the existence of such a maintenance system is nothing like understanding how the local rationality of such a system can be sustained (we're doing the right thing, the safe thing) while safety standards are in fact continually being eroded (e.g., from 350- to 2,550-hour lubrication interval). The redundant components may have been built and certified. The maintenance program (with 2,550-hour lubrication intervals—certified) may be in place. But safe parts do not guarantee system safety. Certification processes do not typically take lifetime wear of parts into account when judging an aircraft airworthy, even if such wear will render an aircraft, like Alaska 261, quite unworthy of flying. Certification processes certainly do not know how to take sociotechnical adaptation of new equipment, and the consequent potential for drift into failure, into account when looking at nascent technologies. Systemic adaptation or wear is not a criterion in certification decisions, nor is there a requirement to put in place an organization to prevent or cover for anticipated wear rates or pragmatic adaptation, or fine-tuning. As a certification engineer from the regulator testified, "Wear is not considered as a mode of failure for either a system safety analysis or for structural considerations" (NTSB, 2002, p. 24). Because how do you take wear into account? How can you even predict with any accuracy how much wear will occur? McDonnell-Douglas surely had it wrong when it anticipated wear rates on the trim jackscrew assembly of its DC-9. Originally, the assembly was designed for a service life of 30,000 flight hours without any periodic inspections for wear. But within a year, excessive wear had been discovered nonetheless, prompting a reconsideration. The problem of certifying a system as safe to use can become even more complicated if the system to be certified is sociotechnical and thereby even less calculable. What does wear mean when the system is sociotechnical rather than consisting of pieces of hardware? In both cases, safety certification should be a lifetime effort, not a still assessment of decomposed system status at the dawn of a nascent technology. Safety certification should be sensitive to the coevolution of technology and its use, its adaptation. Using the growing knowledge base on technology and organizational failure, safety certification could aim for a better understanding of the ecology in which technology is released—the pressures, resource constraints, uncertainties, emerging uses, fine-tuning, and indeed lifetime wear. Safety certification is not just about seeing whether components meet criteria, even if that is what it often practically boils down to. Safety certification is about anticipating the future. Safety certification is about bridging the gap between a piece of gleaming new technology in the hand now, and its adapted, coevolved, grimy, greased-down wear and use further down the line. But we are not very good at anticipating the future. Certification practices and techniques oriented toward assessing the standard of current components do not translate well into understanding total system behavior in the future. Making claims about the future, then, often hangs on things other than proving the worthiness of individual parts. Take the trim system of the DC-9 again. The jackscrew in the trim assembly had been classified as a "structure" in the 1960s, leading to different certification requirements from when it would have been seen as a system. The same piece of hardware, in other words, could be looked at as two entirely different things: a system, or a structure. In being judged a structure, it did not have to undergo the required system safety analysis (which may, in the end, still not have picked up on the problem of wear and the risks it implied). The distinction, this partition of a single piece of hardware into different lexical labels, however, shows that airworthiness is not a rational product of engineering calculation. Certification can have much more to do with localized engineering judgments, with argument and persuasion, with discourse and renaming, with the translation of numbers into opinion, and opinion into numbers— all of it based on uncertain knowledge. As a result, airworthiness is an artificially binary black-or-white verdict (a jet is either airworthy or it is not) that gets imposed on a very grey, vague, uncertain world—a world where the effects of releasing a new technology into actual operational life are surprisingly unpredictable and incalculable. Dichotomous, hard yes or no meets squishy reality and never quite gets a genuine grip. A jet that was judged airworthy, or certified as safe, may or may not be in actual fact. It may be a little bit unairworthy. Is it still airworthy with an end-play check of .0042 inches, the set limit? But "set" on the basis of what? Engineering judgment? Argument? Best guess? Calculations? What if a following end-play check is more favorable? The end-play check itself is not very reliable. The jet may be airworthy today, but no longer tomorrow (when the jackscrew snaps). But who would know? The pursuit of answers to such questions can precede or accompany certification efforts. Research, that putatively objective scientific encounter with empirical reality, can assist in the creation of knowledge about the future, as shown in chapter 8. So what about working without paper flight strips? The research community has come to no consensus on whether air traffic control can actually do without them, and if it does, how it succeeds in keeping air-traffic under control. Research results are inconclusive. Some literature suggested that flight strips are expendable without consequences for safety (e.g., Albright, Truitt, Barile, Vortac, & Manning, 1996), whereas others argued that air-traffic control is basically impossible without them (e.g., Hughes, Randall, & Shapiro, 1993). Certification guidance that could be extracted from the research base can go either way: It is either safe or unsafe to do away with the flight strips, depending on whom you listen to. What matters most for credibility is whether the researcher can make statements about human work that a certifier can apply to the coming, future use of a system. In this, researchers appeared to rely on argument and rhetoric, as much as on method, to justify that the results they found are applicable to the future. LEIPZIG AS LEGITIMATE For human factors, the traditionally legitimate way of verifying the safety of new technology is to conduct experiments in the laboratory. Say that researchers want to test whether operators can safely use voice-input systems, or whether their interpretation of some target is better on three-dimensional displays. The typical strategy is to build microversions of the future system and expose a limited number of participants to various conditions, some or all of which may contain partial representations of a target system. Through its controlled settings, laboratory research already makes some sort of verifiable step into the future. Empirical contact with a world to be designed is ensured because some version of that future world has been prefabricated in the lab. This also leads to problems. Experimental steps into the future are necessarily narrow, which affects the generalizability of research findings. The mapping between test and target situations may miss several important factors. In part as a result of a restricted integration of context, laboratory studies can yield divergent and eventually inconclusive results. Laboratory research on decision making (Sanders & McCormick, 1997), for example, has found several biases in how decision makers deal with information presented to them. Can new technology circumvent the detrimental aspects of such biases, which, according to some views, would lead to human error and safety problems? One bias is that humans are generally conservative and do not extract as much information from sources as they optimally should. Another bias, derived from the same experimental research, is that people have a tendency to seek far more information than they can absorb adequately. Such biases would seem to be in direct opposition to each other. It means that reliable predictions of human performance in a future system may be difficult to make on the basis of such research. Indeed, laboratory findings often come with qualifying labels that limit their applicability. Sanders and McCormick (1997), for example, advised: "When interpreting the . . . findings and conclusions, keep in mind that much of the literature is comprised of laboratory studies using young, healthy males doing relatively unmotivating tasks. The extent to which we can generalize to the general working population is open to question" (p. 572). Whether the question remains open does not seem to matter. Experimental human factors research in the laboratory holds a special appeal because it makes mind measurable, and it even allows mathematics to be applied to the results. Quantitativism is good: It helps equate psychology with natural science, shielding it from the unreliable wanderings through mental life using dubious methods like introspection. The large-scale university laboratories that are now a mainstay of many human factors departments were a 19th-century European invention, pioneered by scientists such as the chemist Justus Liebig. Wundt of course started the trend in psychology with his Leipzig laboratory (see chap. 5). Leipzig did psychologya great service: Psychophysics and its methods of inquiry introduced psychology as a serious science, as something realist, with numbers, calculations, and equations. The systematization, mechanization, and quantification of psychological research in Leipzig, however, must be seen as an antimovement against earlier introspection and rationalism. Echoes of Leipzig still sound loudly today. A quantitativist preference remains strong in human factors. Empiricist appeals (the pursuit of real measurable facts through experiment) and a strong reliance on Cartesian-Newtonian interpretations of natural science equal to those of, say, physics, may help human factors retain credibility in a world of constructed hardware and engineering science, where it alone dabbles in the fuzziness of psychology. In a way, then, quantitativist human factors or engineering psychology is still largely the sort of antimovement that Wundt formed with his Leipzig laboratory. It finds its expression in a pursuit of numbers and statistics, lest engineering consumers of the research results (and their government or other sponsors) suspect the results to be subjective and untrustworthy. The quantification and mechanization of mind and method in human factors are good only because they are not something else (i.e., foggy rationalism or unreliable introspection), not because they are inherently good or epistemologically automatically justifiable. The experimental method is good for what it is not, not for what it is. One can see this in the fact that quantitative research in mainstream human factors never has to justify its method (that method is good because at least it is not that other, vague stuff). Qualitative research, on the other hand, is routinely dismissed as insufficiently empirical and will always be required to justify its method. Anything perceived to be sliding toward rationalism, subjectivism, and nonsystematic introspection is highly suspicious, not because it is, but because of what it evokes: a fear that human factors will be branded unscientific. Now these fears are nothing new. They have inspired many a split or departure in the history of psychology. Recall Watson's main concern when launching behaviorism. It was to rescue psychology from vague subjectivist introspection (by which he even meant Wundt's systematic, experimental laboratory research) and plant it firmly within the natural science tradition. Ever since Newton read the riot act on what scientific was to be, psychology and human factors have struggled to find an acceptance and an acceptability within that conceptualization. Misconceptions About the Qualitative-Quantitative Relationship Whether quantitative or qualitative research can make more valid claims about the future (thereby helping in the certification of a system as safe to use) is contested. At first sight, qualitative, or field studies, are about the present (otherwise there is no field to study). Quantitative research may test actual future systems, but the setting is typically so contrived and limited that its relationship to a real future is tenuous. As many have pointed out, the difference between quantitative and qualitative research is actually not so great (e.g., Woods, 1993; Xiao & Vicente, 2000). Claims of epistemological privilege by either are counterproductive, and difficult to substantiate. A method becomes superior only if it better helps researchers answer the question they are pursuing, and in this sense, of course, the differences between qualitative and quantitative research can be real. But dismissing qualitative work as subjective misses the point of quantitative work. Squeezing numbers out of an experimental encounter with reality, and then closing the gap to a concept-dependent conclusion on what you just saw, requires generous helpings of interpretation. As we see in the following discussion, there is a great deal of subjectivism in endowing numbers with meaning. Moreover, seeing qualitative inquiry as a mere protoscientific prelude to real quantitative research misconstrues the relationship and overestimates quantitative work. A common notion is that qualitative work should precede quantitative research by generating hypotheses that can then be tested in more restricted settings. This may be one relationship. But often quantitative work only reveals the how or what (or how much) of a particular phenomenon. Numbers in themselves can have a hard time revealing the why of the phenomenon. In this case, quantitative work is the prelude to real qualitative research: It is experimental number crunching that precedes and triggers the study of meaning. Finally, a common claim is that qualitative work is high in external validity and low in internal validity. Quantitative research, on the other hand, is thought to be low in external validity and high in internal validity. This is often used as justification for either approach and it must rank among the most misconstrued arguments in scientific method. The idea is that internal validity is high because experimental laboratory research allows an investigator almost full control over the conditions in which data are gath­ ered. If the experimenter did not make it happen, either it did not happen, or the experimenter knows about it, so that it can be dealt with as a confound. But the degree of control in research is often overestimated. Laboratory settings are simply another kind of contextualized setting, in which all kinds of subtle influences (social expectations, people's life histories) enter and influence performance just like they would in any other contextualized setting. The degree of control in qualitative research, on the other hand, is often simply assumed to be low. And much qualitative work indeed adds to that image. But rigor and control is definitely possible in qualitative work: There are many ways in which a researcher can become confident about systematic relationships between different factors. Subjectivism in interpretation is not more necessary in qualitative than in quantitative research. Qualitative work, on the other hand, is not automatically externally valid simply because it takes place in a field (applied) setting. Each encounter with empirical reality, whether qualitative or quantitative, generates context-specific data—data from that time and place, from those people, in that language—that are by definition nonexportable to other settings. The researcher has to engage in analysis of those data in order to bring them up to a concept-dependent level, from which terms and conclusions can be taken to other settings. The examples that follow play out these issues. But the account is about more than the real or imagined opposition between qualitative and quantitative work. The question is how human factors research, quantitative or qualitative, can contribute to knowing whether a system will be safe to use. EXPERIMENTAL HUMAN FACTORS RESEARCH ON FLIGHT STRIPS: AN EXAMPLE One way to find out if controllers can control air-traffic without the airport information desk of flight strips is to test it in an experimental setting. You take a limited number of controllers, and put them through a short range of tasks to see how they do. In their experiments, Albright et al. (1996) deployed a wide array of measurements to find out if controllers perform just as well in a condition with no strips as in a condition with strips. The work they performed was part of an effort by the U.S. Federal Aviation Administration, a regulator (and ultimately the certifier of any future air-traffic control system in the U.S.). In their study, the existing air-traffic control system was retained, but to compare stripped versus stripless control, the researchers removed the flight strips in one condition: The first set of measurements consisted of the following: total time watching the PVD [plan view display, or radar screen], number of FPR [flight plan requests], number of route displays, number of J-rings used, number of conflict alerts activated, mean time to grant pilot requests, number of unable requests, number of requests ignored, number of controller-to-pilot requests, number of controller-to-center requests, and total actions remaining to complete at the end of the scenario. (Albright et al., p. 6) The assumption that drives most experimental research is that reality (in this case about the use and usefulness of flight strips) is objective and that it can be discovered by the researcher wielding the right measuring instruments. This is consistent with the structuralism and realism of human factors. The more measurements, the better, the more numbers, the more you know. This is assumed to be valid even when an underlying model that would couple the various measurements together into a coherent account of expert performance is often lacking (as it is in Albright et al., 1996, but also in many folk models in human factors). In experimental work, the number and diversity of measurements can become the proxy indicator of the accuracy of the findings, and of the strength of the epistemological claim (Q: So how do you know what you know? A: Well, we measured this, and this, and this, and that, and . . .). The assumption is that, with enough quantifiable data, knowledge can eventually be offered that produces an accurate and definitive account of a particular system. More of the same will eventually lead to something different. The strong influence that engineer­ ing has had on human factors (Batteau, 2001) makes this appear as just common sense. In engineering, technical debates are closed by amassing results from tests and experience; the essence of the craft is to convert uncertainty into certainty. Degrees of freedom are closed through numbers; ambiguity is worked out through numbers; uncertainty is reduced through numbers (Vaughan, 1996). Independent of the number of measurements, each empirical encounter is of necessity limited, in both place and time. In the case of Albright et al. (1996), 20 air-traffic controllers participated in two simulated airspace conditions (one with strips and one without strips) for 25 minutes each. One of the results was that controllers took longer to grant pilot requests when they did not have access to flight strips, presumably because they had to assemble the basis for a decision on the request from other information sources. The finding is anomalous compared to other results, which showed no significant difference between workload and ability to keep control over the traffic situation across the strip-no strip conditions, leading to the conclusion that "the presence or absence of strips had no effect on either performance or perceived workload. Apparently, the compensatory behaviors were sufficient to maintain effective control at what controllers perceived to be a comparable workload" (Albright et al., p. 11). Albright et al. explained the anomaly as follows: "Since the scenarios were only 25 minutes in length, controllers may not have had the opportunity to formulate strategies about how to work without flight strips, possibly contributing to the delay" (p. 11). At a different level, this explanation of an anomalous datum implies that the correspondence between the experimental setting and a future system and setting may be weak. Lacking a real chance to learn how to formulate strategies for controlling traffic without flight strips, it would be interesting to pursue the question of how controllers in fact remained in control over the traffic situation and kept their workload down. It is not clear how this lack of a developed strategy can affect the number of requests granted but not the perceived workload or control performance. Certifiers may, or perhaps should, wonder what 25 minutes of undocumented struggle tells them about a future system that will replace decades of accumulated practice. The emergence of new work and establishment of new strategies is a fundamental accompaniment to the introduction of new technology, representing a transformation of tasks, roles, and responsibilities. These shifts are not something that could easily be noticed within the confines of an experimental study, even if controllers were studied for much longer than 25 minutes. Albright et al. (1996), resolved this by placing the findings of control performance and workload earlier in their text: "Neither performance nor perceived workload (as we measured them in this study) was affected when the strips were removed" (p. 8). The qualification that pulled the authority of the results back into the limited time and place of the experimental encounter (how we measured them in this study), were presented parenthetically and thus accorded less central importance (Golden-Biddle & Locke, 1993). The resulting qualification suggests that comparable performance and workload may be mere artifacts of the way the study was conducted, of how these things were measured at that time and place, with those tools, by those researchers. The qualification, however, was in the middle of the paper, in the middle of a paragraph, and surrounded by other paragraphs adorned with statistical allusions. Nothing of the qualification remained at the end of the paper, where the conclusions presented these localized findings as universally applicable truths. Rhetoric, in other words, is enlisted to deal with problematic areas of epistemological substance. The transition from localized findings (in this study the researchers found no difference in workload or performance the way they measured them with these 20 controllers) to generalizable principles (we can do away with flight strips) essentially represents a leap of faith. As such, central points of the argument were left unsaid or were difficult for the reader to track, follow, or verify. By bracketing doubt this way, Albright et al. (1996) communicated that there was nothing, really, to doubt. Authority (i.e., true or accurate knowledge) derives from the replicable, quantifiable experimental approach. As Xiao and Vicente (2000) argued, it is very common for quantitative human factors research not to spend much time on the epistemological foundation of its work. Most often it moves unreflectively from a particular context (e.g., an experiment) to concepts (not having strips is safe), from data to conclusions, or from the modeled to the model. The ultimate resolution of the fundamental constraint on em­ pirical work (i.e., each empirical encounter is limited to a time and place) is that more research is always necessary. This is regarded as a highly reasonable conclusion of most quantitative human factors, or indeed any, experimental work. For example, in the Albright et al. study, one constraint was the 25-minute time limit on the scenarios played. Does flight-strip removal actually change controller strategies in ways that were not captured by the present study? This would seem to be a key question. But again, the reservation was bracketed. Whether or not the study answered this question does not in the end weaken the study's main conclusion: "(Additional research is necessary to determine if there are more substantial long term effects to strip removal)" (p. 12). In addition, the empirical encounter of the Albright et al. (1996) study was limited because it only explored one group of controllers (upper airspace). The argument for more research was drafted into service for legitimizing (not calling into question) results of the study: "Additional studies should be conducted with field controllers responsible for other types of sectors (e.g., low altitude arrival, or non-radar) to determine when, or if, controllers can compensate as successfully as they were able to in the current investigation" (p. 12). The idea is that more of the same, eventually, will lead to something different, that a series of similar studies over time will produce a knowledge increment useful to the literature and useful to the consumers of the research (certifiers in this case). This, once again, is largely taken for granted in the human factors community. Findings will invariably get better next time, and such successive, incremental enhancement is a legitimate route to the logical human factors end point: the discovery of an objective truth about a particular human-machine system and, through this, the revelation of whether it will be safe to use or not. Experimental work relies on the production of quantifiable data. Some of this quantification (with statistical ornaments such as F-values and standard deviations) was achieved in Albright et al. (1996) by converting tickmarks on lines of a questionnaire (called the "PEQ," or post-experimental questionnaire) into an ordinal series of digits: The form listed all factors with a 9.6 centimeter horizontal line next to each. The line was marked low on the left end and high on the right end. In addition, a vertical mark in the center of the line signified the halfway mark. The controllers were instructed to place an X on the line adjacent to the factor to indicate a response. . . . The PEQ scales were scored by measuring distance from the right anchor to the mark placed by the controller on a horizontal line (in centimeters). . . . Individual repeated measures ANOVAs [were then conducted], (pp. 5-8) The veneration of numbers in this case, however, went a step too far. ANOVAs cannot be used for the kind of data gathered through PEQ scales. The PEQ is made up of so-called ordinal scales. In ordinal scales, data categories are mutually exclusive (a tickmark cannot be at two distances at the same time), they have some logical order, and they are scored according to the amount of a particular characteristic they possess (in this case, distance in centimeters from the left anchor). Ordinal scales, however, do not represent equal differences (a distance of 2 configuration management does not represent twice as much of the category measured as a distance of 1 cm), as interval and ratio scales do. esides, reducing complex categories such as "usefulness" or "likeability" to distances along a few lines probably misses out on an interesting ideographic reality beneath all of the tickmarks. Put in experimental terms, the operationalization of usefulness as the distance from a tickmark along a line is not particularly high on internal validity. How can the researcher be sure that usefulness means the same thing to all responding controllers? If different respondents have different ideas of what usefulness meant during their particular experimental scenario, and if different respondents have different ideas of how much usefulness a tickmark, say, in the middle of the line represents, then the whole affair is deeply confounded. Researchers do not know what they are asking and do not know what they are getting in reply. Further numeric analysis is dealing with apples and oranges. This is one of the greater risks of folk modeling in human factors. It assumes that everybody understands what usefulness means, and that everybody has the same definition. But these are generous and untested assumptions. It was only with qualitative inquiry that researchers could ensure that there was some consensus on understandings of usefulness with respect to the controlling task with or without strips. Or they could discover that there was no consensus and then control for it. This would be one way to deal with the confound. It may not matter, and it may not have been noticed. Numbers are good. Also, the linear, predictable format of research writing, as well as the use of abbreviated statistical curios throughout the results section, represent a rhetoric that endows the experimental approach with its authority—authority in the sense of privileged access to a particular layer or slice of em­ pirical reality that others outside the laboratory setting do or do not have admittance to. Other rhetoric invented particularly for the study (e.g., PEQ scales for questions presented to participants after their trials in Albright et al., 1996) certifies the researchers' unique knowledge of this slice of reality. It validates the researcher's competence to tell readers what is really going on there. It may dissuade second-guessing. Empirical results are deemed accurate by virtue of a controlled encounter, a standard reporting format that shows logical progress to objective truths and statements (introduction, method, results, discussion, and summary), and an authoritative dialect intelligible only to certified insiders. Closing the Gap to the Future Because of some limited correspondence between the experiment and the system to be designed, quantitative research seemingly automatically closes the gap to the future. The stripless condition in the research (even if contrived by simply leaving out one artifact [the flight strip] from the present) is a model of the future. It is an impoverished model to be sure, and one that offers only a partial window onto what future practice and performance may be like (despite the epistemological reservations about the authenticity of that future discussed earlier). The message from Albright et al.'s (1996) encounter with the future is that controllers can compensate for the lack of flight strips. Take flight strips away, and controllers compensate for the lack of information by seeking information elsewhere (the radar screen, flight-plan readouts, controller-to-pilot requests). Someone might point out that Albright et al. prejudged the use and usefulness of flight strips in the first few sentences of their introduction, that they did not see their data as an opportunity to seek alternative interpretations: "Currently, en route control of high altitude flights between airports depends on two primary tools: the computer-augmented radar information available on the Plan View Display (PVD) and the flight information available on the Flight Progress Strip" (p. 1). This is not really an enabling of knowledge, it is the imposition of it. Here, flight strips are not seen as a problematic core category of controller work, whose use and usefulness would be open to negotiation, disagreement, or multiple interpretations. Instead, flight strips function as information-retrieval devices. Framed as such, the data and the argument can really only go one way: By removing one source of information, controllers will redirect their information-retrieving strategies onto other devices and sources. This displacement is possible, it may even be desirable, and it is probably safe: "Complete removal of the strip information and its accompanying strip marking responsibilities resulted in controllers compensating by retrieving information from the computer" (Albright et al., p. 11). For a certifier, this closes a gap to the future: Removing one source of information will result in people finding the information elsewhere (while showing no decrement in performance or increment in workload). The road to automation is open and people will adapt successfully, for that has been scientifically proven. Therefore, doing away with the flight strips is (probably) safe, and certifiable as such. If flight strips are removed, then what other sources of information should remain available? Albright et al. (1996) inquired about what kind of information controllers would minimally like to preserve: Route of flight scored high, as did altitude information and aircraft call sign. Naming these categories gives developers the opportunity to envision an automated version of the flight strip that presents the same data in digital format, one that substitutes a computer-based format for the paper-based one, without any consequences for controller performance. Such a substitution, however, may overlook critical factors associated with flight strips that contribute to safe practice, and that would not be incorporated or possible in a computerized version (Mackay, 2000). Any signs of potential ambiguity or ambivalence about what else flight strips may mean to those working with them were not given further consideration beyond a brief mention in the experimental research write-up—not because these signs were actively, consciously stifled, but because they were inevitably deleted as Albright et al. (1996) carried out and wrote up their encounter with empirical reality. Albright et al. explicitly solicited qualitative, richer data from their participants by asking if controllers themselves felt that the lack of strips impaired their performance. Various controllers indicated how strips help them preplan and that, without strips, they cannot preplan. The researchers, however, never unpacked the notion of preplanning or investigated the role of flight strips in it. Again, such notions (e.g., preplanning) are assumed to speak for themselves, taken to be selfevident. They require no deconstruction, no further interpretive work. Paying more attention to these qualitative responses could create noise that confounds experimental accuracy. Comments that preplanning without strips was impossible hinted at flight strips as a deeper, problematic category of controller work. But if strips mean different things to different controllers, or worse, if preplanning with strips means different things to different controllers, then the experimental bedrock of comparing comparable people across comparable conditions would disappear. This challenges in a profound way the nomothetic averaging out of individual differences. Where individual differences are the nemesis of experimental research, interpretive ambiguity can call into question the legitimacy of the objective scientific enterprise. QUALITATIVE RESEARCH ON FLIGHT STRIPS: AN EXAMPLE Rather than looking at people's work from the outside in (as do quantitative experiments), qualitative research tries to understand people's work from the inside out. When taking the perspective of the one doing the work, how does the world look through his or her eyes? What role do tools play for people themselves in the accomplishments of their tasks; how do tools affect their expression of expertise? An interpretive perspective is based on the assumption that people give meaning to their work and that they can express those meanings through language and action. Qualitative research interprets the ways in which people make sense of their work experiences by examining the meanings that people use and construct in light of their situation (Golden-Biddle & Locke, 1993). The criteria and end points for good qualitative research are different than from those in quantitative research. As a research goal, accuracy is practically and theoretically unobtainable. Qualitative research is relentlessly empirical, but it rarely achieves finality in its findings. Not that quantitative research ever achieves finality (remember that virtually every ex­ perimental report finishes with the exhortation that more research is necessary). But qualitative researchers admit that there is never one accurate description or analysis of a system in question, no definitive account— only versions. What flight strips exactly do for controllers is forever subject to interpretation; it will never be answered objectively or finitely, never be closed to further inquiry. What makes a version good, though, or credible, or worth paying attention to by a certifier, is its authenticity. The researcher has to not only convince the certifier of a genuine field experience in writing up the research account, but also make intelligible what went on there. Validation from outside the field emerges from an engagement with the literature (What have others said about similar contexts?) and from interpretation (How well are theory and evidence used to make sense of this particular context?). Field research, though critical to the ethnographic community as a system-theoretic accident model and processes of authenticity, is not necessarily the only legitimate way to generate qualitative data. Surveys of user populations can also be tools that support qualitative inquiry. Find Out What the Users Think The reason that qualitative research may appeal to certifiers is that it lets the informants, the users, speak—not through the lens of an experiment, but on the users' terms and initiative. Yet this is also where a central problem lies. Simply letting users speak can be of little use. Qualitative research is not (or should not be) plain conversational mappings—a direct transfer from field setting to research account. If human factors would (or continues to) practice and think about ethnography in these terms, doubts about both the method and the data it yields will continue to surface. What certifiers, as consumers of human factors research, care about is not what users say in raw, unpacked form, but about what their remarks mean for work, and especially for future work. As Hughes et al. (1993) put it: "It is not that users cannot talk about what it is they know, how things are done, but it needs bringing out and directing toward the concerns of the design itself" (p. 138). Within the human factors community, qualitative research seldom takes this extra step. What human factors requires is a strong ethnography, one that actually makes the hard analytical move from user statements to a design language targeted at the future. A massive qualitative undertaking related to flight strips was the Lancaster University project (Hughes et al., 1993). Many man-months were spent (an index of the authenticity of the research) observing and documenting air-traffic control with flight strips. During this time the researchers developed an understanding of flight strips as an artifact whose functions derive from the controlling work itself. Both information and annotations on the strip and the active organization of strips among and between controllers were essential: "The strip is a public document for the members of the (controlling) team; a working representation of an aircraft's control history and a work site of controlling. Moving the strips is to organize the information in terms of work activities and, through this, accomplishing the work of organizing the traffic" (Hughes et al., pp. 132-133). Terms such as working representation and organizing traffic are concepts, or categories, that were abstracted well away from the masses of deeply context-specific field notes and observations gathered in the months of research. Few controllers would themselves use the term working representation to explain what flight strips mean to them. This is good. Conceptual abstraction allows a researcher to reach a level of greater generality and increased generalizability (see Woods, 1993; Xiao & Vicente, 2000). Indeed, working representation may be a category that can lead to the future, where a designer would be looking to computerize a working representation of flight information, and a certifier would be evaluating whether such a computerized tool is safe to use. But such higher order interpretive work is seldom found in human factors research. It would separate ethnography and ethnographic argument from research that simply makes claims based on authenticity. Even Hughes et al. (1993) relied on authenticity alone when they told of the various annotations made on flight strips, and did little more than parrot their informants: Amendments may be done by the controller, by the chief, or less often, by one of the "wings." "Attention-getting" information may also be written on the strips, such as arrows indicating unusual routes, symbols designating "crossers, joiners and leavers" (that is, aircraft crossing, leaving or joining the major traffic streams), circles around unusual destinations, and so on. (p. 132) Though serving as evidence of socialization, of familiarity and intimacy, speaking insider language is not enough. By itself it is not helpful to certifiers who may be struggling with evaluating a version of air-traffic control without paper flight strips. Appeals to authenticity ("Look, I was there, and I understand what the users say") and appeals to future relevance ("Look, this is what you should pay attention to in the future system") can thus pull in opposite directions: the former toward more the context specific that is hardly generalizable, the latter toward abstracted categories of work that can be mapped onto yet-to-be-fielded future systems and conceptions of work. The burden to resolve the tension should not be on the certifier or the designer of the system, it should be on the researcher. Hughes et al. (1993) agreed that this bridge-building role should be the researcher's: Ethnography can serve as another bridge between the users and the designers. In our case, controllers have advised on the design of the display tool with the ethnographer, as someone knowledgeable about but distanced from the work, and, on the one hand able to appreciate the significance of the controllers' remarks for their design implications and, on the other hand, familiar enough with the design problems to relate them to the controllers' experiences and comments, (p. 138) Hostage to the Present, Mute About the Future Hughes et al. (1993) research account actually missed the "significance of controller remarks for their design implications" (p. 138). No safety implications were extracted. Instead the researchers used insider language to forward insider opinions, leaving user statements unpacked and largely underanalyzed. Ethnography essentially gets confused with what informants say and consumers of the research are left to pick and choose among the statements. This is a particularly naive form of ethnography, where what informants can tell researchers is equated or confused with what strong, analytical ethnography (and ethnographic argument) could reveal. Hughes et al. relied on informant statements to the extent they did because of a common belief that the work that their informants did, and the foundational categories that informed it are for the most part self-evident; close to what we would regard as common sense. As such, they require little, if any, analytic effort to discover. It is an ethnography reduced to a kind of mediated user show-and-tell for certifiers—not as thorough analysis of the foundational categories of work. For example, Hughes et al concluded that "(flight strips) are an essential feature of 'getting the picture,' 'organising the traffic,' which is the means of achieving the orderliness of the traffic" (p. 133). So flight strips help controllers get the picture. This kind of statement is obvious to controllers and merely repeats what everyone already knows. If ethnographic analysis cannot go beyond common sense, it merely privileges the status quo. As such, it offers certifiers no way out: A system without flight strips would not be safe, so forget it. There is no way for a certifier to circumvent the logical conclusion of Hughes et al. (1993): "The importance of the strip to the controlling process is difficult to overestimate" (p. 133). So is it safe? Going back to Hughes et al.: "For us, such questions were not easily answerable by reference to work which is as subtle and complex as our ethnographic analysis had shown controlling to be" (p. 135). Such surrender to the complexity and intricacy of a particular phenomenon is consistent with what Dawkins (1986, p. 38) called the "argument from personal incredulity." When faced with highly complicated machinery or phenomena, it is easy to take cover behind our own sense of extreme wonder, and resist efforts at explanation. In the case of Hughes et al. (1993), it recalls an earlier reservation: "The rich, highly detailed, highly textured, but nevertheless partial and selective descriptions associated with ethnography would seem to contribute little to resolving the designers problem where the objective is to determine what should be designed and how" (p. 127). Such justification ("It really is too complex and subtle to communicate to you") maneuvers the entire ethnographic enterprise out of the certifier's view as something not particularly helpful. Synthesizing the complexity and subtlety of a setting should not be the burden of the certifier. Instead, this is the role of the researcher; it is the essence of strong ethnography. That a phenomenon is remarkable does not mean it is inexplicable; so if we are unable to explain it, "we should hesitate to draw any grandiose conclusions from the fact of our own inability" (Dawkins, 1986, p. 39). Informant remarks such as "Flight strips help me get the mental picture" should serve as a starting point for qualitative research, not as its conclusion. But how can researchers move from native category to analytic sense? Qualitative work should be hermeneutic and circular in nature: not aiming for a definitive description of the target system, but rather a continuous reinterpretation and reproblematization of the successive layers of data mined from the field. Data demand analysis. Analysis in turn guides the search for more data, which in turn demand further analysis: Categories are continually revised to capture the researcher's (and, hand in hand, the practitioner's) evolving understanding of work. There is a constant interplay between data, concepts, and theory. The analysis and revision of categories is a hallmark of strong ethnography, and Ross's (1995) study of flight-progress strips in Australia serves as an interesting example. Qualitative in nature, Ross's research relied on surveys of controllers using flight strips in their current work. Surveys are often derided by qualitative researchers for imposing the researcher's understanding of the work onto the data, instead of the other way around (Hughes et al., 1993). Demonstrating that it is not just the empirical encounter or rhetorical appeals to authenticity that matter (through large numbers of experimental probes or months of close observation), the survey results Ross gathered were analyzed, coded, categorized, receded and recategorized until the inchoate masses of context-specific controller remarks began to form sensible, generalizable wholes that could meaningfully speak to certifiers. Following previous categorizations of flight-strip work (Delia Rocco, Manning, & Wing, 1990), Ross (1995) moves down from these conceptual descriptions of controller work and up again from the context-specific details, leaving several layers of intermediate steps. In line with characterizations of epistemological analysis through abstraction hierarchies (see Xiao & Vicente, 2000), each step from the bottom up is more abstract than the previous one; each is cast less in domain-bound terms and more in concept-dependent terms than the one before. Beyer and Holtzblatt (1998) referred to this process as induction: reasoning from the particular to the general. One example from Ross (p. 27) concerns domain-specific controller activities such as "entering a pilot report; composing a flight plan amendment." These lower level, context-specific data are of course not without semantic load themselves: it is always possible to ask further questions and descend deeper into the world of meanings that these simple, routine activities have for the people who carry them out. Indeed, we have to ask if we can only go up from the context-specific level—maintained in human factors as the most atomistic, basic, low-level data set (see Woods, 1993). In Ross's data, researchers should still question the common sense behind the otherwise taken-for-granted entering of a pilot report: What does a pilot report mean for the controller in a particular context (e.g., weather related), what does entering this report mean for the controller's ability to manage other traffic issues in the near future (e.g., avoiding sending aircraft into severe turbulence)? While alluding to even more fine-grained details and questions later, these types of activities also point to an intentional strategy at a higher level of analysis (Delia Rocco et al., 1990): that of the "transformation or translation of information for entry into the system," which, at an even higher level of analysis, could be grouped under a label coding, together with other such strategies (Ross, 1995, p. 27). Part of this coding is symbolic, in that it uses highly condensed markings on flight strips (underlining, black circles, strike-throughs) to denote and represent for controllers, what is going on. The highly intricate nature of even one flight (where it crosses vs. where it had planned to cross a sector boundary, what height it will be leaving when, whether it has yet contacted another frequency, etc.) can be collapsed or amortized by simple symbolic notation—one line or circle around a code on the strip that stands for a complex, multidimensional problematic that other controllers can easily recognize. Unable to keep all the details of what a flight would do stable in the head, the controller compresses complexity, or amortizes it, as Hollan, Hutchins, and Kirsh (2000) would say, by letting one symbol stand for complex concepts and interrelationships, some even temporal. Similarly, "recognizing a symbol for a handoff' (on a flight strip), though allowing further unpacking (e.g., what do you mean "recognize"?), is an instance of a tactic that "transforms or translates information received," which in turn represents a larger controller competency of "decoding," which in its turn is also part of a strategy to use symbolic notation to collapse or amortize complexity (Ross, 1995, p. 27). From recognizing a symbol for a hand-off to the collapsing of complexity, there are four steps, each more abstract and less in domain terms than the one before. Not only do these steps allow others to assess the analytical work for its worth, but the destination of such induction is actually a description of work that can be used for guiding the evaluation of a future system. Inspired by Ross's analysis, we can surmise that controllers rely on flight strips for: • Amortizing or collapsing complexity (what symbolic notation conveys) . • Supporting coordination (who gets which flight strip next from whom). • Anticipating dynamics (how much is to come, from where, when, in what order). These (no longer so large) jumps to the highest level of abstraction can now be made—identifying the role the flight strip has in making sense of workplace and task complexity. Although not so much a leap of faith any longer (because there are various layers of abstraction in between), the final step, up to the highest level conceptual description, still appears to hold a certain amount of creative magic. Ross (1995) revealed little of the mechanisms that actually drive his analysis. There is no extensive record that tracks the transformation of survey data into conceptual understandings of work. Perhaps these transformations are taken for granted too: The mystery is left unpacked because it is assumed to be no mystery. The very process by which the researcher manages to migrate from user-language descriptions of daily activities to conceptual languages less anchored in the present, remains largely hidden from view. No ethnographic literature guides specifically the kinds of inferences that can be drawn up to the highest level of conceptual understanding. At this point, a lot of leeway is given (and reliance placed on) the researcher and his or her (keenly) developed insight into what activities in the field really mean or do for people who carry them out. The problems of this final step are known and acknowledged in the qualitative research community. Vaughan (1996) and other sociologists referred to it as making the macro-micro connection: locating general meaning systems (e.g., symbolic notation, off-loading) in local contexts (placing a circle around a set of digits on the flight strip). Geertz (1973) noted how inferences that try to make the macro-micro connection often resemble "perfected impressionism" in which "much has been touched but little grasped" (p. 312). Such inferences tend to be evocative, resting on suggestion and insinuation more than on analysis (Vaughan, 1996). In qualitative research, lower levels of analysis or understanding always underconstrain the inferences that can be drawn further on the way to higher levels (see Hollan et al., 2000). At each step, alternative interpretations are possible. Qualitative work does not arrive at a finite description of the system or phenomenon studied (nor does quantitative research, really). But qualitative work does not even aim or pretend to do so (Batteau, 2001). Results are forever open to further interpretation, forever subject to increased problematization. The main criterion, therefore, to which we should hold the inferences drawn is not accuracy (Golden-Biddle & Locke, 1993), but plausibility: Does the conceptual description make sense—especially to the informants, to the people who actually do the work? This also motivates the continuous, circular nature of qualitative analysis: reinterpreting results that have been interpreted once already, gradually developing a theory—a theory of why flight strips help controllers know what is going on that is anchored in the researcher's continually evolving understanding of the informants' work and their world. Closing the Gap to the Future The three high-level categories of controller (flight-strip) work tell certifiers that air-traffic controllers have developed strategies for dealing with the communication of complexity to other controllers, for predicting workload and planning future work. Flight strips play a central, but not necessarily exclusive, role. The research account is written up in such a way that the status quo does not get the prerogative: Tools other than flight strips could conceivably help controllers deal with complexity, dynamics, and coordination issues. Complexity and dynamics, as well as coordination, are critical to what makes air-traffic control what it is, including difficult. Whatever certifiers will want to brand as safe to use, they would do well to take into account that controllers use their artifact(s) to help them deal with complexity, to help them anticipate dynamic futures, and to support their coordination with other controllers. This resembles some kind of human factors requirements that could provide a certifier with meaningful input. CERTIFYING UNDER UNCERTAINTY One role of human factors is to help developers and certifiers judge whether a technology is safe for future use. But quantitative and qualitative human factors communities both risk taking the authority of their findings for granted and regarding the translation to future, and claims about the future being either safe or unsafe, as essentially nonproblematic. At least the literature (both literatures) are relatively silent on this fundamental issue. Yet neither the legitimacy of findings nor the translation to claims about the future is in fact easily achieved, or should be taken for granted. More work needs to be done to produce findings that make sense for those who have to certify a system as safe to use. Experimental human factors research can claim empirical legitimacy by virtue of the authority vested in the laboratory researcher and the control over the method used to get data. Such research can speak meaningfully to future use because it tests microversions of a future system. Researchers, however, should explicitly indicate where the versions of the future they tested are impoverished, and what subtle effects of context on their experimental settings could produce findings that diverge from what future users will encounter. Qualitative research in human factors can claim legitimacy, and relevance to those who need to certify the next system, because of its authentic encounters with the field where people actually carry out the work. Validation emerges from the literature (what others have said about the same and similar contexts) and from interpretation (how theory and evidence make sense of this particular context). Such research can speak meaningfully to certification issues because it allows users to express their preferences, choices and apprehensions. Qualitative human factors research, however, must not stop at recording and replaying informant statements. It must deconfound informant understandings with understandings informed by concepts, theory, analysis and literature. Human factors work, of whatever kind, can help bridge the gap from research findings to future systems. Research accounts need to be both convincing as science and cast in a language that allows a certifier to look ahead to the future: looking ahead to work and a co-evolution of people and technology in a system that does not yet exist. Chapter 10 Should We Hold People Accountable for Their Mistakes? Transportation human factors has made enormous progress over the past decades. It would be easy to claim that transportation systems have become safer in part through human factors efforts. As a result of such work over the past decades, progress on safety has become synonymous with: • Taking a systems perspective: Accidents are not caused by failures of individuals, but emerge from the conflux or alignment of multiple contributory system factors, each necessary and only jointly sufficient. The source of accidents is the system, not its component parts. • Moving beyond blame: Blame focuses on the supposed defects of individual operators and denies the import of systemic contributions. In addition, blame has all kinds of negative side effects. It typically leads to defensive posturing, obfuscation of information, protectionism, polarization, and mute reporting systems. Progress on safety coincides with learning from failure. This makes punishment and learning two mutually exclusive activities: Organizations can either learn from an accident or punish the individuals involved in it, but hardly do both at the same time. The reason is that punishment of individuals can protect false beliefs about basically safe systems, where humans are the least reliable components. Learning challenges and potentially changes the belief about what creates safety. Moreover, punishment emphasizes that failures are deviant, that they do not naturally belong in the organization. Learning means that failures are seen as normal, as resulting from the in herent pursuit of success in resource-constrained, uncertain environments. Punishment turns the culprits into unique and necessary ingredients for the failure to happen. Punishment, rather than helping people avoid or better manage conditions that are conducive to error, actually conditions people not to get caught when errors do occur. This stifles learning. Finally, punishment is about the search for closure, about moving beyond and away from the adverse event. Learning is about continuous improvement, about closely integrating the event in what the system knows about itself. Making these ideas stick, however, is not proving as easy as it was to develop them. In the aftermath of several recent accidents and incidents, the operators involved (pilots or air-traffic controllers in these cases) were charged with criminal offenses (e.g., professional negligence, manslaughter). In some accidents even organizational management has been held criminally liable. Criminal charges differ from civil lawsuits in many respects. Most obviously, the target is not an organization, but individuals (air-traffic controllers, flight crew, maintenance technicians). Punishment consists of possible incarceration or some putatively rehabilitative alternative—not (just) financial compensation. Unlike organizations covered against civil suits, few operators or managers themselves have insurance to pay for legal defense against criminal charges that arise from doing their jobs. Some maintain that criminally pursuing operators or managers for erring on the job is morally unproblematic. The greater good befalls the greater number of people (i.e., all potential passengers) by protecting them from unreliable operators. A lot of people win, only a few outcasts lose. To human factors, however, this may be utilitarianism inverted. Everybody loses when human error gets criminalized: Upon the threat of criminal charges, operators stop sending in safety-related information; incident reporting grinds to a halt. Criminal charges against individual operators also polarize industrial relations. If the organization wants to limit civil liability, then official blame on the operator could deflect attention from up­stream organizational issues related to training, management, supervision, and design decisions. Blaming such organizational issues, in contrast, can be a powerful ingredient in an individual operator's criminal defense—certainly when the organization has already rendered the operator expendable by euphemism (standby, ground duty, administrative leave) and without legitimate hope of meaningful re-employment. In both cases, industrial relations are destabilized. Intra-organizational battles become even more complex when individual managers get criminally pursued; defensive maneuvering by these managers typically aims to off-load the burden of blame onto other departments or parts of the organization. This easily leads to poisonous relations and a crippling of organizational functioning. Finally, incarceration or alternative punishment of operators or managers has no demonstrable rehabilitative effect (perhaps because there is nothing to rehabilitate) . It does not make an operator or manager any safer, nor is there evidence of vicarious learning (learning by example and fear of punishment) . Instead, punishment or its threat merely leads to counterproductive responses, to people ducking the debris. The transportation industry itself shows ambiguity with regard to the criminalization of error. Responding to the 1996 Valujet accident, where mechanics loaded oxygen generators into the cargo hold of a DC-9, which subsequently caught fire, the editor of Aviation Week and Space Technology "strongly believed the failure of SabreTech employees to put caps on oxygen generators constituted willful negligence that led to the killing of 110 passengers and crew. Prosecutors were right to bring charges. There has to be some fear that not doing one's job correctly could lead to prosecution" (North, 2000, p. 66). Rescinding this 2 years later, however, North (2002) opined that learning from accidents and criminal prosecution go together like "oil and water, cats and dogs," that "criminal probes do not mix well with aviation accident inquiries" (p. 70). Most other cases reveal similar instability with regard to prosecuting operators for error. Culpability in aviation does not appear to be a fixed notion, connected unequivocally to features of some incident or accident. Rather, culpability is a highly flexible category. Culpability is negotiable, subject to national and professional interpretations, influenced by political imperatives and organizational pressures, and part of personal or institutional histories. As psychologists point out, culpability is also about assumptions we make about the amount of control people had when carrying out their (now) controversial acts. The problem here is that hindsight deeply confounds such judgments of control. In hindsight, it may seem obvious that people had all the necessary data available to them (and thus the potential for control and safe outcomes). Yet they may have willingly ignored this data in order to get home faster, or because they were complacent. Retrospect and the knowledge of outcome deeply affect our ability to judge human performance, and a reliance on folk models of phenomena like complacency, situation awareness, and stress does not help. All too quickly we come to the conclusion that people could have better controlled the outcome of a situation, if only they had invested a little more effort. ACCOUNTABILITY What is accountability, and what does it actually mean to hold people accountable for their mistakes? Social cognition research shows that accountability or holding people accountable is not that simple. Accountability is fundamental to any social relation. There is always an implicit or explicit expectation that we may be called on to justify our beliefs and actions to oth­ ers. The social-functionalist argument for accountability is that this expectation is mutual: As social beings we are locked into reciprocating relationships. Accountability, however, is not a unitary concept—even if this is what many stakeholders may think when aiming to improve people's performance under the banner of holding them accountable. There are as many types of accountability as there are distinct relationships among people, and between people and organizations, and only highly specialized subtypes of accountability actually compel people to expend more cognitive effort. Expending greater effort, moreover, does not necessarily mean better task performance, as operators may become concerned more with limiting exposure and liability than with performing well (Lerner & Tetlock, 1999), something that can be observed in the decline of incident reporting with threats of prosecution (North, 2002). What is more, if accounting is perceived as illegitimate, for example, intrusive, insulting, or ignorant of real work, then any beneficial effects of accountability will vanish or backfire. Effects that have been experimentally demonstrated include a decline in motivation, excessive stress, and attitude polarization, and the same effects can be seen in recent cases where pilots and air-traffic controllers were held accountable by courts and other parties ignorant of the real trade-offs and dilemmas that make up actual operational work. The research base on social cognition, then, tells us that accountability, even if inherent in human relationships, is not unambiguous or unproblematic. The good side of this is that, if accountability can take many forms, then alternative, perhaps more productive avenues of holding people accountable are possible. Giving an account, after all, does not have to mean exposing oneself to liability, but rather, telling one's story so that others can learn vicariously. Many sources, even within human factors, point to the value of storytelling in preparing operators for complex, dynamic situations in which not everything can be anticipated. Stories are easily remembered, scenario-based plots with actors, intentions, clues, and outcomes that in one way or another can be mapped onto current difficult situations and matched for possible ways out. Incident-reporting systems can capitalize on this possibility, whereas more incriminating forms of accountability actually retard this very quality by robbing from people the incentive to tell stories in the first place. ANTHROPOLOGICAL UNDERSTANDINGS OF BLAME The anthropologist is not intrigued by flaws in people's reasoning process that produce for example, the hindsight bias, but wants to know something about casting blame. Why is blame a meaningful response for those doing the blaming? Why do we turn error into crime? Mary Douglas (1992) described how peoples are organized in part by the way in which they explain misfortune and subsequently pursue retribution or dispense justice. Societies tend to rely on one dominant model of possible cause from which they construct a plausible explanation. In the moralistic model, for example, misfortune is seen as the result of offending ancestors, of sinning, or of breaking some taboo. The inflated, exaggerated role that procedure violations (one type of sinning or taboo breaking) are given in retrospective accounts of failure represent one such use for moralistic models of breakdown and blame. The moralistic explanation (you broke the rule, then you had an accident) is followed by a fixed repertoire of obligatory actions that follow on that choice. If taboos have been broken, then rehabilitation can be demanded through expiatory actions. Garnering forgiveness through some purification ritual is one example. Forcing operators to publicly offer their apologies is a purification ritual seen in the wake of some accidents. Moreover, the rest of the community is reminded to not sin, to not break the taboos, lest the same fate befall them. How many reminders are there in the transportation industry imploring operators to "follow the rules," to "follow the procedures"? These are moralistic appeals with little demonstrable effect on practice, but they may make industry participants feel better about their systems; they may make them feel more in control. In the extrogenous model, external enemies of the system are to blame for misfortune, a response that can be observed even today in the demotion or exile of failed operators: pilots or controllers or technicians. These people are expost facto relegated to a kind of underclass that no longer represents the professional corps. Firing them is one option, and is used relatively often. But there are more subtle expressions of the extrogenous model too. The ritualistic expropriation of badges, certificates, stripes, licenses, uniforms, or other identity and status markings in the wake of an accident delegitimizes the errant operator as a member of the operational community. A part of such derogation, of course, is psychological defense on the part of (former) colleagues who would need to distance themselves from a realization of equal vulnerability to similar failures. Yet such delegitimization also makes criminalization easier by beginning the incremental process of dehumanizing the operator in question. Wilkinson (1994) presented an excellent example of such demonizing in the consequences that befell a Boeing 747 pilot after allegedly narrowly missing a hotel at London Heathrow airport in thick fog. Demonizing there was incremental in the sense that it made criminal pursuit not only possible in the first place, but subsequently necessary. It fed on itself: Demons such as this pilot would need to be punished, demoted, exorcised. The press had a large share in dramatizing the case, promoting the captain's dehumanization to the point where his suicide was the only way out. Failure and Fear Today, almost every misfortune is followed by questions centering on "whose fault?" and "what damages, compensation?" Every death must be chargeable to somebody's account. Such responses approximate the primitives' resistance to the idea of natural death remarkably well (Douglas, 1992). Death, even today, is not considered natural—it has to arise from some type of identifiable cause. Such resistance to the notion that deaths actually can be accidental is obvious in responses to recent mishaps. For example, Snook (2000) commented on his own disbelief, his struggle, in analyzing the friendly shoot-down of two U.S. Black Hawk helicopters by U.S. Fighter Jets over Northern Iraq in 1993: This journey played with my emotions. When I first examined the data, I went in puzzled, angry, and disappointed—puzzled how two highly trained Air Force pilots could make such a deadly mistake; angry at how an entire crew of AWACS controllers could sit by and watch a tragedy develop without taking action; and disappointed at how dysfunctional Task Force OPC must have been to have not better integrated helicopters into its air operations. Each time I went in hot and suspicious. Each time I continuing airworthines management exposition out sympathetic and unnerved. ... If no one did anything wrong; if there were no unexplainable surprises at any level of analysis; if nothing was abnormal from a behavioral and organizational perspective; then what have we learned? (p. 203) Snook (2000) confronted the question of whether learning, or any kind of progress on safety, is possible at all if we can find no wrongdoing, no surprises, if we cannot find some kind of deviance. If everything was normal, then how could the system fail? Indeed, this must be among the greater fears that define Western society today. Investigations that do not turn up a "Eureka part," as the label became in the TWA800 probe, are feared not because they are bad investigations, but because they are scary. Philosophers like Nietzsche pointed out that the need for finding a cause is fundamental to human nature. Not being able to find a cause is profoundly distressing; it creates anxiety because it implies a loss of control. The desire to find a cause is driven by fear. So what do we do if there is no Eureka part, no fault nucleus, no seed of destruction? Is it possible to acknowledge that failure results from normal people doing business as usual in normal organizations? Not even many accident investigations succeed at this. As Galison (2000) noted: If there is no seed, if the bramble of cause, agency, and procedure does not issue from a fault nucleus, but is rather unstably perched between scales, between human and non-human, and between protocol and judgment, then the world is a more disordered and dangerous place. Accident reports, and much of the history we write, struggle, incompletely and unstably, to hold that nightmare at bay. (p. 32) Galison's (2000) remarks remind us of this fear (this nightmare) of not being in control over the systems we design, build, and operate. We dread the possibility that failures emerge from the intertwined complexity of normal everyday systems interactions. We would rather see failures emanate from a traceable, controllable single seed or nucleus. In assigning cause, or in identifying our imagined core of failure, accuracy does not seem to matter. Being afraid is worse than being wrong. Selecting a scapegoat to carry the interpretive load of an accident or incident is the easy price we pay for our illusion that we actually have control over our risky technologies. This price is the inevitable side effect of the centuries-old pursuit of Baconian control and technological domination over nature. Sending controllers, or pilots, or maintenance technicians to jail may be morally wrenching (but not unequivocally so—remember North, 2000), but it is preferable over its scary alternative: acknowledging that we do not enjoy control over the risky technologies we build and consume. The alternative would force us to really admit that failure is an emergent property, that "mistake, mishap and disaster are socially organized and systematically produced by social structures," that these mistakes are normal, to be expected because they are "embedded in the banality of organizational life" (Vaughan, 1996, p. xiv). It would force us to acknowledge the relentless inevitability of mistake in organizations, to see that harmful outcomes can occur in the organizations constructed to prevent them, that harmful consequences can occur even when everybody follows the rules. Preferring to be wrong over being afraid in the identification of cause overlaps with the common reflex toward individual responsibility in the West. Various transportation modes (particularly aviation) have exported this bias to less individually oriented cultures as well. In the Western intellectual tradition since the Scientific Revolution, it has seemed self-evident to evaluate ourselves as individuals, bordered by th limits of our minds and bodies, and evaluated in terms of our own personal achievements. From the Renaissance onward, the individual became a central focus, fueled in part by Descartes' psychology that created "self-contained individuals" (Heft, 2001). The rugged individualism developed on the back of mass European immigration into North America in the late 19th and early 20th centuries accelerated the image of independent, free heroes accomplishing greatness against all odds, and antiheroes responsible for disproportionate evildoing (e.g., Al Capone). Lone antiheroes still play the lead roles in our stories of failure. The notion that it takes teamwork, or an entire organization, an entire industry (think about Alaska 261) to break a system is just too eccentric relative to this cultural prejudice. There are earlier bases for the dominance of individualism in Western traditions as well. Saint Augustine, the deeply influential moral thinker for Judeo-Christian societies, saw human suffering as occurring not only because of individual human fault (Pagels, 1988), but because of human choice, the conscious, deliberate, rational choice to err. The idea of a rational choice to err is so pervasive in Western thinking that it goes virtually unnoticed, unquestioned, because it makes such common sense. The idea, for example, is that pilots have a choice to take the correct runway but fail to take it. Instead, they make the wrong choice because of attentional deficiencies or motivational shortcomings, despite the cues that were available nd the time they had to evaluate those cues. Air-traffic controllers have a choice to see a looming conflict, but elect to pay no attention to it because they think their priorities should be elsewhere. After the fact, it often seems as if people chose to err, despite all available evidence indicating they had it wrong. The story of Adam's original sin, and especially what Saint Augustine made of it, reveals the same space for conscious negotiation that we retrospectively invoke on behalf of people carrying out safety-critical work in real conditions. Eve had a deliberative conversation with the snake on whether to sin or not to sin, on whether to err or not to err. The allegory emphasizes the same conscious presence of cues and incentives to not err, of warnings to follow rules and not sin, and yet Adam and Eve elected to err anyway. The prototypical story of error and violation and its consequences in Judeo-Christian tradition tells of people who were equipped with the requisite intellect, who had received the appropriate indoctrination (don't eat that fruit), who displayed capacity for reflective judgment, and who actually had the time to choose between a right and a wrong alternative. They then proceeded to pick the wrong alternative, a choice that would make a big difference for their lives and the lives of others. It is likely that, rather than causing the fall into continued error, as Saint Augustine would have it, Adam's original sin portrays how we think about error, and how we have thought about it for ages. The idea of free will permeates our moral thinking, and most probably influences how we look at human performance to this day. MISMATCH BETWEEN AUTHORITY AND RESPONSIBILITY Of course this illusion of free will, though dominant in post hoc analyses of error, is at odds with the real conditions under which people perform work: where resource limitations and uncertainty severely constrain the choices open to them. Van den Hoven (2001) called this "the pressure condition." Operators such as pilots and air-traffic controllers are "narrowly embedded"; they are "configured in an environment and assigned a place which will provide them with observational or derived knowledge of relevant facts and states of affairs" (p. 3). Such environments are exceedingly hostile to the kind of reflection necessary to meet the regulative ideal of individual moral responsibility. Yet this is exactly the kind of reflective idyll we read in the story of Adam and Eve and the kind we retrospectively presume on behalf of operators in difficult situations that led to a mishap. Human factors refers to this as an authority-responsibility double bind: A mismatch occurs between the responsibility expected of people to do the right thing, and the authority given or available to them to live up to that responsibility. Society expresses its confidence in operators' responsibility through payments, status, symbols, and the like. Yet operators' authority may fall short of that responsibility in many important ways. Operators typically do not have the degrees of freedom assumed by their professional responsibility because of a variety of reasons: Practice is driven by multiple goals that may be incompatible (simultaneously having to achieve maximum capacity utilization, economic aims, customer service, and safety). As Wilkinson (1994, p. 87) remarked: "A lot of lip service is paid to the myth of command residing in the cockpit, to the fantasy of the captain as ultimate decision-maker. But today the commander must first consult with the accountant." Error, then, must be understood as the result of constraints that the world imposes on people's goal-directed behavior. As the local rationality principle dictates, people want to do the right thing, yet features of their work environment limit their authority to act, limit their ability to live up to the responsibility for doing the right thing. This moved Claus Jensen (1996) to say: there is no longer any point in appealing to the individual worker's own sense of responsibility, morality or decency, when almost all of us are working within extremely large and complex systems . . . According to this perspective, there is no point in expecting or demanding individual engineers or managers to be moral heroes; far better to put all of one's efforts into reinforcing safety procedures and creating structures and processes conducive to ethical behavior, (p. xiii) Individual authority, in other words, is constrained to the point where moral appeals to individual responsibility are becoming useless. And authority is not only restricted because of the larger structures that people are only small parts of. Authority to assess, decide, and act can be in limited simply because of the nature of the situation. Time and other resources for making sense of a situation are lacking; information may not be at hand or may be ambiguous; there may be all kinds of subtle organizational pressures to prefer certain actions over others; and there may be no neutral or additional expertise to draw on. Even Eve was initially alone with the snake. Where was Adam, the only other human available in paradise during those critical moments of seduction into error? Only recent additions to the human factors literature (e.g., naturalistic decision making, ecological task analyses) explicitly took these and other constraints on people's practice into consideration in the design and understanding of work. Free will is a logical impossibility in cases where there is a mismatch between responsibility and authority, which is to say that free will is always a logical impossibility in real settings where real safety-critical work is carried out. This should invert the culpability criterion when operators or others are being held accountable for their errors. Today it is typically the defendant who has to explain that he or she was constrained in ways that did not allow adequate control over the situation. But such defenses are often hopeless. Outsider observers are influenced by hindsight when they look back on available data and choice moments. As a consequence, they consistently overestimate both the clarity of the situation and the ability to control the outcome. So rather than the defendant having to show that insufficient data and control made the outcome inevitable, it should be up to the claimants, or prosecution, to prove that adequate control was in fact available. Did people have enough authority to live up to their responsibility? Such a proposal, however, amounts to only a marginal adjustment of what may still be dysfunctional and counterproductive accountability relationships. What different models of responsibility could possibly replace current accountability relationships, and do they have any chance? In the adversarial confrontations and defensive posturing that the criminalization of error generates today, truth becomes fragmented across multiple versions that advocate particular agendas (staying out of jail, limiting corporate liability). This makes learning from the mishap almost impossible. Even making safety improvements in the wake of an accident can get construed as an admission of liability. This robs systems of their most concrete demonstration that they have learned something from the mishap: an actual implementation of lessons learned. Indeed, lessons are not learned before organizations have actually made the changes that those lessons prescribe. BLAME-FREE CULTURES? Ideally, there should be accountability without invoking defense mechanisms. Blame-free cultures, for example, though free from blame and associated protective plotting, are not without member responsibility. But blamefree cultures are extremely rare. Examples have been found among Sherpas in Nepal (Douglas, 1992), who pressure each other to settle quarrels peacefully and reduce rivalries with strong informal procedures for reconciliation. Laying blame accurately is considered much less important than a generous treatment of the victim. Sherpas irrigate their social system with a lavish flow of gifts, taxing themselves collectively to ensure nobody goes neglected, and victims are not left exposed to impoverishment or discrimination (Douglas). This mirrors the propensity of Scandinavian cultures for collective taxation to support dense webs of social security. Prosecution of individuals or especially civil lawsuits in the wake of accidents are rare. U.S. responses stand in stark contrast (although criminal prosecution of operators is rare there). Despite a plenitude of litigation (which inflates and occasionally exceeds the compensatory expectations of a few), victims as a group are typically undercompensated. Blame-free cultures may hinge more on consistently generous treatment of victims than on denying that professional accountability exists. They also hinge on finding other expressions of responsibility, of what it means to be a responsible member of that culture. Holding people accountable can be consistent with being blame-free if transportation industries think in novel ways about accountability. This would involve innovations in relationships among the various stakeholders. Indeed, in order to continue making progress on safety, transportation industries should reconsider and reconstruct accountability relationships between its stakeholders (organizations, regulators, litigators, operators, passengers). In a new form of accountability relationships, operators or managers involved in mishaps could be held accountable by inviting them to tell their story (their account). Such accounts can then be systematized and distributed, and used to propagate vicarious learning for all. Microversions of such accountability relationships have been implemented in many incident-reporting systems, and perhaps their examples could move industries in the direction of as yet elusive blame-free cultures. The odds, however, may be stacked against attempts to make such progress. The Judeo-Christian ethic of individual responsibility is not just animated by a basic Nietzschean anxiety of losing control. Macrostructural forces are probably at work too. There is evidence that episodes of renewed enlightenment, such as the Scientific Revolution, are accompanied by violent regressions toward supernaturalism and witch hunting. Prima facie, this would be an inconsistency. How can an increasingly illuminated society simultaneously retard into superstition and scapegoating? One answer may lie in the uncertainties and anxieties brought on by the technological advances and depersonalization that inevitably seem to come with such progress. New, large, complex, and widely extended technological systems (e.g., global aviation that took just a few decades to expand into what it is today) create displacement, diffusion, and causal uncertainty. A reliance on individual culpability may be the only sure way of recapturing an illusion of control. In contrast, less technologically or industrially developed societies (take the Sherpas as example again) appear to rely on more benign models of failure and blame, and more on collective responsibility. In addition, those who do safety-critical work often tie culpability conventions to aspects of their personal biographies. Physician Atul Gawande (2002, p. 73), for example, commented on a recent surgical incident and observed that terms such as systems problems are part of a "dry language of structures, not people . . . something in me, too, demands an acknowledgement of my autonomy, which is also to say my ultimate culpability ... although the odds were against me, it wasn't as if I had no chance of succeeding. Good doctoring is all about making the most of the hand you're dealt, and I failed to do so." The expectation of being held accountable if things go wrong (and, conversely, being responsible if things go right) appears intricately connected to issues of self-identity, where accountability is the other side of professional autonomy and a desire for control. This expectation can engender considerable pride and can make even routine operational work deeply meaningful. But although good doctoring (or any kind of practice) may be making the most of the hand one is dealt, human factors has always been about providing that hand more and better opportunities to do the right thing. Merely leaving the hand with what it is dealt and banking on personal motivation to do the rest takes us back to prehistoric times, when be­ haviorism reigned and human factors had yet to make its entry in system safety thinking. Accountability and culpability are deeply complex concepts. Disentangling their prerational influences in order to promote systems thinking, and to create an objectively fairer, blame-free culture, may be an uphill struggle. They are, in any case, topics worthy of more research.