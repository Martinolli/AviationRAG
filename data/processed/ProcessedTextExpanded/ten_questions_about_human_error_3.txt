Title: Ten Questions About Human Error: A New View Of Human Factors And System Safety Chapter 3 Author(s): Sidney W. A. Dekker Category: Analysis, Human, Factors, Errors Tags: human, error, factors, accident Why Are Doctors More Dangerous Than Gun Owners? There are about 700,000 physicians in the United States. The U.S. Institute of Medicine estimates that each year between 44,000 and 98,000 people die as a result of medical errors (Kohn, Corrigan, & Donaldson, 1999). This makes for a yearly accidental death rate per doctor of between 0.063 and 0.14. In other words, up to one in seven doctors will kill a patient each year by mistake. Take gun owners in contrast. There are 80,000,000 gun owners in the United States. Yet their errors lead to "only" 1,500 accidental gun deaths per year. This means that the accidental death rate, caused by gun- owner error, is 0.000019 per gun owner per year. Only about 1 in 53,000 gun owners will kill somebody by mistake. Doctors then, are 7,500 times more likely to kill somebody by mistake. While not everybody has a gun, al­ most everybody has a doctor (or several doctors), and is thus severely ex­ posed to the human error problem. As organizations and other stakeholders (e.g., trade and industry groups, regulators) try to assess the "safety health" of their operations, counting and tabulating errors appears to be a meaningful measure. Not only does it provide an immediate, numeric estimate of the probability of accidental death, injury, or any other undesirable event, it also allows the comparison of systems and components in it (this hospital vs. that hospital, this airline vs. that one, this aircraft fleet or pilot vs. that one, these routes vs. those routes). Keeping track of adverse events is thought to provide rela­ tively easy, quick, and accurate access to the internal safety workings of a sys­ tem. Moreover, adverse events can be seen as the start of—or reason for— deeper probing, in order to search for environmental threats or unfavor­ able conditions that could be changed to prevent recurrence. There is, of course, also the sheer scientific curiosity of trying to understand different types of adverse events, different types of errors. Categorizing, after all, has been fundamental to science since modern times. Over the past decades, transportation human factors has endeavored to quantify safety problems and find potential sources of vulnerability and fail­ ure. It has spawned a number of error-classification systems. Some classify decision errors together with the conditions that helped produce them; some have a specific goal, for example, to categorize information transfer problems (e.g., instructions, errors during watch change-over briefings, co­ ordination failures); others try to divide error causes into cognitive, social and situational (physical, environmental, ergonomic) factors; yet others at­ tempt to classify error causes along the lines of a linear information- processing or decision-making model, and some apply the Swiss-cheese metaphor (i.e., systems have multiple layers of defense, but all of them have holes) in the identification of errors and vulnerabilities up the causal chain. Error classification systems are used both after an event (e.g., during inci­ dent investigations) or for observations of current human performance. THE MORE WE MEASURE, THE LESS WE KNOW In pursuing categorization and tabulation of errors, human factors makes a number of assumptions and takes certain philosophical positions. Little of this is made explicit in the description of these methods, yet it carries con­ sequences for the utility and quality of the error count as a measure of safety health and as a tool for directing resources for improvement. Here is an example. In one of the methods, the observer is asked to distinguish be­ tween "procedure errors" and "proficiency errors." Proficiency errors are related to a lack of skills, experience, or (recent) practice, whereas proce­ dure errors are those that occur while carrying out prescribed or normative sequences of action (e.g., checklists). This seems straightforward. Yet, as Croft (2001) reported, the following problem confronts the observer: one type of error (a pilot entering a wrong altitude in a flight computer) can le­ gitimately end up in either of two categories of the error counting method (a procedural error or proficiency error). "For example, entering the wrong flight altitude in the flight management system is considered a pro­ cedural error. . . . Not knowing how to use certain automated features in an aircraft's flight computer is considered a proficiency error" (p. 77). If a pilot enters the wrong flight altitude in the flight-management sys­ tem, is that a procedural or a proficiency issue, or both? How should it be categorized? Thomas Kuhn (1962) encouraged science to turn to creative philosophy when confronted with the inklings of problems in relating the­ ory to observations (as in the problem of categorizing an observation into theoretical classes). It can be an effective way to elucidate and, if necessary, weaken the grip of a tradition on the collective mind, and suggest the basis for a new one. This is certainly appropriate when epistemological questions arise: questions on how we go about knowing what we (think we) know. To understand error classification and some of the problems associated with it, we should try to engage in a brief analysis of the contemporary philosophi­ cal tradition that governs human factors research and the worldview in which it takes place. Realism: Errors Exist: You Can Discover Them With a Good Method The position human factors takes when it uses observational tools to meas­ ure "errors" is a realist one: It presumes that there is a real, objective world with verifiable patterns that can be observed, categorized, and predicted. Errors, in this sense, are a kind of Durkheimian fact. Emile Durkheim, a founding father of sociology, believed that social reality is objectively "out there," available for neutral, impartial empirical scrutiny. Reality exists, truth is worth striving for. Of course, there are obstacles to getting to the truth, and reality can be hard to pin down. Yet pursuing a close mapping or correspondence to that reality is a valid, legitimate goal of theory develop­ ment. It is that goal, of achieving a close mapping to reality, that governs er- ror-counting methods. If there are difficulties in getting that correspon­ dence, then these difficulties are merely methodological in nature. The difficulties call for refinement of the observational instruments or addi­ tional training of the observers. These presumptions are modernist; inherited from the enlightened ideas of the Scientific Revolution. In the finest of scientific spirits, method is called on to direct the searchlight across empirical reality, more method is called on to correct ambiguities in the observations, and even more method is called on to break open new portions of hitherto unexplored empirical reality, or to bring into focus those portions that so far were vague and elusive. Other labels that fit such an approach to empirical reality could include positivism, which holds that the only type of knowledge worth bothering with is that which is based directly on experience. Positivism is as­ sociated with the doctrine of Auguste Comte: The highest, purest (and per­ haps only true) form of knowledge is a simple description of sensory phe­ nomena. In other words, if an observer sees an error, then there was an error. For example, the pilot failed to arm the spoilers. This error can then be written up and categorized as such. But positivist has obtained a negative connotation, really meaning "bad" when it comes to social science research. Instead, a neutral way of describing the position of error-counting methods is realist, if naively so. Operating from a realist stance, researchers are concerned with validity (a measure of that correspondence they seek) and reliability. If there is a reality that can be captured and described objectively by outside observers, then it is also possible to generate converging evidence with multiple observers, and consequently achieve agreement about the nature of that reality. This means reliability: Reliable contact has been made with empirical reality, generating equal access and returns across observations and observers. Er­ ror counting methods rely on this too: It is possible to tabulate errors from different observers and different observations (e.g., different flights or air­ lines) and build a common database that can be used as some kind of ag­ gregate norm against which new and existing entrants can be measured. But absolute objectivity is impossible to obtain. The world is too messy for that, phenomena that occur in the empirical world too confounded, and methods forever imperfect. It comes as no surprise, then, that error-counting methods have different definitions, and different levels of definitions, for er­ ror, because error itself is a messy and confounded phenomenon: • Error as the cause of failure, for example, the pilot's failure to arm the spoilers led to the runway overrun. • Error as the failure itself: Classifications rely on this definition when categorizing the kinds of observable errors operators can make (e.g., deci­ sion errors, perceptual errors, skill-based errors; Shappell & Wiegmann, 2001) and probing for the causes of this failure in processing or perform­ ance. According to Helmreich (2000), "Errors result from physiological and psychological limitations of humans. Causes of error include fatigue, workload, and fear, as well as cognitive overload, poor interpersonal com­ munications, imperfect information processing, and flawed decision mak­ ing" (p. 781). • Error as a process, or, more specifically, as a departure from some kind of standard: This standard may consist of operating procedures. Violations, whether exceptional or routine (Shappell & Wiegmann), or intentional or unintentional (Helmreich), are one example of error according to the process definition. Depending on what they use as standard, observers of course come to different conclusions about what is an error. Not differentiating among these different possible definitions of error is a well-known problem. Is error a cause, or is it a consequence? To the error- counting methods, such causal confounds and messiness are neither really surprising nor really problematic. Truth, after all, can be elusive. What mat­ ters is getting the method right. More method may solve problems of method. That is, of course, if these really are problems of method. The modernist would say "yes." "Yes" would be the stock answer from the Scien­ tific Revolution onward. Methodological wrestling with empirical reality, where empirical reality plays hard to catch and proves pretty good at the game, is just that: methodological. Find a better method, and the problems go away. Empirical reality will swim into view, unadulterated. Did You Really See the Error Happen? The postmodernist would argue something different. A single, stable reality that can be approached by the best of methods, and described in terms of correspondence with that reality, does not exist. If we describe reality in a particular way (e.g., this was a "procedure error"), then that does not imply any type of mapping onto an objectively attainable external reality—close or remote, good or bad. The postmodernist does not deal in referentials, does not describe phenomena as though they reflect or represent some­ thing stable, objective, something "out there." Rather, capturing and de­ scribing a phenomenon is the result of a collective generation and agree­ ment of meaning that, in this case, human factors researchers and their industrial counterparts have reached. The reality of a procedure error, in other words, is socially constructed. It is shaped by and dependent on mod­ els and paradigms of knowledge that have evolved through group consen­ sus. This meaning is enforced and handed down through systems of ob­ server training, labeling and communication of the results, and industry acceptance and promotion. As philosophers like Kuhn (1962) have pointed out, these paradigms of language and thought at some point adopt a kind of self-sustaining energy, or "consensus authority" (Angell & Straub, 1999). If human factors auditors count errors for managers, they, as (puta­ tively scientific) measurers, have to presume that errors exist. But in order to prove that errors exist, auditors have to measure them. In other words, measuring errors becomes the proof of their existence, an existence that was preordained by their measurement. In the end, everyone agrees that counting errors is a good step forward on safety because almost everyone seems to agree that it is a good step forward. The practice is not questioned because few seem to question it. As the postmodernist would argue, the procedural error becomes true (or appears to people as a close correspon­ dence to some objective reality) only because a community of specialists have contributed to the development of the tools that make it appear so, and have agreed on the language that makes it visible. There is nothing in­ herently true about the error at all. In accepting the utility of error count­ ing, it is likely that industry accepts its theory (and thereby the reality and validity of the observations it generates) on the authority of authors, teach­ ers, and their texts, not because of evidence. In his headline, Croft (2001) announced that researchers have now perfected ways to monitor pilot per­ formance in the cockpit. "Researchers" have "perfected." There is little that an industry can do other than to accept such authority. What alternatives have they, asks Kuhn, or what competence? Postmodernism sees the "reality" of an observed procedure error as a ne­ gotiated settlement among informed participants. Postmodernism has gone beyond common denominators. Realism, that product and accompani­ ment of the Scientific Revolution, assumes that a common denominator can be found for all systems of belief and value, and that we should strive to converge on those common denominators through our (scientific) meth­ ods. There is a truth, and it is worth looking for through method. Postmodernism, in contrast, is the condition of coping without such com­ mon denominators. According to postmodernism, all beliefs (e.g., the be­ lief that you just saw a procedural error) are constructions, they are not un­ contaminated encounters with, or representations of, some objective empirical reality. Postmodernism challenges the entire modernist culture of realism and empiricism, of which error counting methods are but an in­ stance. Postmodernist defiance not only appears in critiques against error- counting but also reverberates throughout universities and especially the sciences (e.g., Capra, 1982). It never comes away unscathed, however. In the words of Varela, Thompson, and Rosch (1991), we suffer from "Carte­ sian anxiety." We seem to need the idea of a fixed, stable reality that sur­ rounds us, independent of who looks at it. To give up that idea would be to descend into uncertainty, into idealism, into subjectivism. There would be no more groundedness, no longer a set of predetermined norms or stan­ dards, only a constantly shifting chaos of individual impressions, leading to relativism and, ultimately, nihilism. Closing the debate on this anxiety is im­ possible. Even asking which position is more "real" (the modernist or the postmodernist one) is capitulating to (naive) realism. It assumes that there is a reality that can be approximated better either by the modernists or postmodernists. Was This an Error? It Depends on Who You Ask Here is one way to make sense of the arguments. Although people live in the same empirical world (actually, the hard-core constructionist would ar­ gue that there is no such thing), they may arrive at rather different, yet equally valid, conclusions about what is going on inside of it, and propose different vocabularies and models to capture those phenomena and activi­ ties. Philosophers sometimes use the example of a tree. Though at first sight an objective, stable entity in some external reality, separate from us as observers, the tree can mean entirely different things to someone in the logging industry as compared to, say, a wanderer in the Sahara. Both inter­ pretations can be valid because validity is measured in terms of local rele­ vance, situational applicability, and social acceptability—not in terms of correspondence with a real, external world. Among different characteriza­ tions of the world there is no more real or more true. Validity is a function of how the interpretation conforms to the worldview of those to whom the ob­ server makes his appeal. A procedure error is a legitimate, acceptable form of capturing an empirical encounter only because there is a consensual system of like-minded coders and consumers who together have agreed on the lin­ guistic label. The appeal falls onto fertile ground. But the validity of an observation is negotiable. It depends on where the appeal goes, on who does the looking and who does the listening. This is known as ontological relativism: There is flexibility and uncertainty in what it means to be in the world or in a particular situation. The ontological rela­ tivist submits that the meaning of observing a particular situation depends entirely on what the observer brings to it. The tree is not just a tree. It is a source of shade, sustenance, survival. Following Kant's ideas, social scien­ tists embrace the common experience that the act of observing and perceiving objects (including humans) is not a passive, receiving process, but an active one that engages the observer as much as it changes or affects the ob­ served. This relativism creates the epistemological uncertainty we see in error-counting methods, which, after all, attempt to shoehorn observations into numerical objectivity. Most social observers or error coders will have felt this uncertainty at one time or another. Was this a procedure error, or a proficiency error, or both? Or was it perhaps no error at all? Was this the cause, or was it the consequence? If it is up to Kant, not having felt this uncertainty would serve as an indication of being a particularly obtuse ob­ server. It would certainly not be proof of the epistemological astuteness of either method or error counter. The uncertainty suffered by them is epistemological because it is realized that certainty about what we know, or even about how to know whether we know it or not, seems out of reach. Yet those within the ruling paradigm have their stock answer to this challenge, just as they have it whenever confronted with problems of bringing observa­ tions and theories in closer correspondence. More methodological agree­ ment and refinement, including observer training and standardization, may close the uncertainty. Better trained observers will be able to distin­ guish between a procedure error and proficiency error, and an improve­ ment to the coding categories may also do the job. Similar modernist ap­ proaches have had remarkable success for five centuries, so there is no reason to doubt that they may offer routes to some progress even here. Or is there? Perhaps more method may not solve problems seemingly linked to method. Consider a study reported by Hollnagel and Amalberti (2001), whose purpose was to test an error-measurement instrument. This instrument was designed to help collect data on, and get a better understanding of, air-traffic controller errors, and to identify areas of weakness and find possibilities for improvement. The method asked observers to count er­ rors (primarily error rates per hour) and categorize the types of errors using a taxonomy proposed by the developers. The tool had already been used to pick apart and categorize errors from past incidents, but would now be put to test in a real-time field setting—applied by pairs of psychologists and air-traffic controllers who would study air-traffic control work going on in real time. The observing air traffic controllers and psychologists, both trained in the error taxonomy, were instructed to take note of all the errors they could see. Despite common indoctrination, there were substantial differences between the numbers of errors each of the two groups of observers noted, and only a very small number of errors were actually observed by both. People watching the same performance, using the same tool to classify behavior, continuing airworthines management exposition up with totally different error counts. Closer inspection of the score sheets revealed that the air-traffic controllers and psychologists tended to use different subsets of the error types available in the tool, indicating just how negotiable the notion of error is: The same fragment of performance means entirely different things to two different (but similarly trained and standardized) groups of observers. Air-traffic controllers relied on external working conditions (e.g., interfaces, personnel and time resources) to refer to and categorize errors, whereas psychologists preferred to locate the er­ ror somewhere in presumed quarters of the mind (e.g., working memory) or in some mental state (e.g., attentional lapses). Moreover, air-traffic controllers who actually did the work could tell both groups of error coders that they both had it wrong. Debriefing sessions exposed how many observed errors were not errors at all to those said to have committed them, but rather normal work, expressions of deliberate strategies intended to manage problems or foreseen situations that the error counters had either not seen, or not understood if they had. Croft (2001) reported the same re­ sult in observations of cockpit errors: More than half the errors revealed by error counters were never discovered by the flight crews themselves. Some realists may argue that the ability to discover errors that people themselves do not see is a good thing: It confirms the strength or superiority of method. But in Hollnagel and Amalberti's (2001) case, error coders were forced to disavow such claims to epistemological privilege (and embrace ontological relativism instead). They reclassified the errors as normal ac­ tions, rendering the score sheets virtually devoid of any error counts. Early transfers of aircraft were not an error, for example, but turned out to corre­ spond to a deliberate strategy connected to a controller's foresight, planning ahead, and workload management. Rather than an expression of weakness, such strategies uncovered sources of robustness that would never have come out, or would even have been misrepresented and mischaracterized, with just the data in the classification tool. Such normalization of actions, which at first appear deviant from the outside, is a critical aspect to really understanding human work and its strengths and weaknesses (see Vaughan, 1996). Without understanding such processes of normalization, it is impossible to penetrate the situated meaning of errors or violations. Classification of errors crumbles on the inherent weakness of the naive realism that underlies it. The realist idea is that errors are "out there," that they exist and can be observed, captured, and documented independently of the observer. This would mean that it makes no difference who does the observing (which it patently does). Such presumed realism is naive because all observations are ideational—influenced (or made possible in the first place) to a greater or lesser extent by who is doing the observing and by the worldview governing those observations. Realism does not work because it is impossible to separate the observer from the observed. Acknowledging some of these problems, the International Civil Aviation Organization (ICAO, 1998) has called for the development of human performance data- collection methods that do not rely on subjective assessments. But is this possible? Is there such a thing as an objective observation of another human's behavior? The Presumed Reality of Error The test of the air-traffic control error counting method reveals how "an action should not be classified as an 'error' only based on how it appears to an observer" (Hollnagel & Amalberti, 2001, p. 13). The test confirms ontologi­ cal relativism. Yet sometimes the observed "error" should be entirely non-controversial, should it not? Take the spoiler example from chapter 1. The flight crew for­ got to arm the spoilers. They made a mistake. It was an error. You can apply the new view to human error, and explain all about context and situation and mitigating factors. Explain why they did not arm the spoilers, but that they did not arm the spoilers is a fact. The error occurred. Even multiple different observers would agree on that. The flight crew failed to arm the spoilers. How can one not acknowledge the existence of that error? It is there, it is a fact, staring us in the face. But what is a fact? Facts always privilege the ruling paradigm. Facts always favor current interpretations, as they fold into existing constructed render­ ings of what is going on. Facts actually exist by virtue of the current para­ digm. They can neither be discovered nor given meaning without it. There is no such thing as observations without a paradigm; research in the ab­ sence of a particular worldview is impossible. In the words of Paul Feyera­ bend (1993, p. 11): "On closer analysis, we even find that science knows no 'bare facts' at all, but that the 'facts' that enter our knowledge are already viewed in a certain way and are, therefore, essentially ideational." Feyera­ bend called the idea that facts are available independently and can thereby objectively favor one theory over another, the autonomy principle (p. 26). The autonomy principle asserts that the facts that are available as empirical content of one theory (e.g., procedural errors as facts that fit the threat and error model) are objectively available to alternative theories too. But this does not work. As the spoiler example from chapter 1 showed, errors occur against and because of a background, in this case a background so systemic, so structural, that the original human error pales against it. The error al­ most becomes transparent, it is normalized, it becomes invisible. Against this backdrop, this context of procedures, timing, engineering trade-offs, and weakened hydraulic systems, the omission to arm the spoilers dissolves. Figure and ground trade places: No longer is it the error that is really ob­ servable or even at all interesting. With deeper investigation, ground be­ comes figure. The backdrop begins to take precedence as the actual story, subsuming, swallowing the original error. No longer can the error be distin­ guished as a singular, failed decision moment. Somebody who applies a the­ ory of naturalistic decision making will not see a procedure error. What will be seen instead is a continuous flow of actions and assessments, coupled and mutually cued, a flow with nonlinear feedback loops and interactions, inextricably embedded in a multilayered evolving context. Human interac­ tion with a system, in other words, is seen as a continuous control task. Such a characterization is hostile to the digitization necessary to fish out individ­ ual human errors. Whether individual errors can be seen depends on the theory used. There are no objective observations of facts. Observers in error counting are themselves participants, participating in the very creation of the ob­ served fact, and not just because they are there, looking at how other peo­ ple are working. Of course, through their sheer presence, error counters probably distort people's normal practice, perhaps turning situated per­ formance into a mere window-dressed posture. More fundamentally, how­ ever, observers in error counting are participants, because the facts they see would not exist without them. They are created through the method. Observers are participants because it is impossible to separate observer and object. None of this, by the way, makes the procedure error less real to those who observe it. This is the whole point of ontological relativism. But it does mean that the autonomy principle is false. Facts are not stable aspects of an independent reality, revealed to scientists who wield the right instruments and methods. The discovery and description of every fact is dependent on a particular theory. In the words of Einstein, it is the theory that determines what can be seen. Facts are not available "out there," independent of the­ ory. To suppose that a better theory should come along to account for pro­ cedure errors in a way that more closely matches reality is to stick with a model of scientific progress that was disproved long ago. It follows the idea that theories should not be dismissed until there are compelling reasons to do so, and compelling reasons arise only because there is an overwhelming number of facts that disagree with the theory. Scientific work, in this idea, is the clean confrontation of observed fact with theory. But this is not how it works, for those facts do not exist without the theory. Resisting Change: The Theory Is Right. Or Is It? The idea of scientific (read: theoretical) progress through the accumulation of observed disagreeing facts that ultimately manage to topple a theory also does not work because counterinstances (i.e., facts that disagree with the the­ ory) are not seen as such. Instead, if observations reveal counterinstances (such as errors that resist unique classification in any of the categories of the error-counting method), then researchers tend to see these as further puz­ zles in the match between observation and theory (Kuhn, 1962)—puzzles that can be addressed by further refinement of their method. Counterinstances, in other words, are not seen as speaking against the theory. According to Kuhn (1962), one of the defining responses to para­ digmatic crisis is that scientists do not treat anomalies as counterinstances, even though that is what they are. It is extremely difficult for people to re­ nounce the paradigm that has led them into a crisis. Instead, the epistemo­ logical difficulties suffered by error-counting methods (Was this a cause or a consequence? Was this a procedural or a proficiency error?) are dismissed as minor irritants and reasons to engage in yet more methodological refine­ ment consonant with the current paradigm. Neither scientists nor their supporting communities in industry are will­ ing to forego a paradigm until and unless there is a viable alternative ready to take its place. This is among the most sustained arguments surrounding the continuation of error counting: Researchers engaging in error classifi­ cation are willing to acknowledge that what they do is not perfect, but vow to keep going until shown something better. And industry concurs. As Kuhn pointed out, the decision to reject one paradigm necessarily coin­ cides with the acceptance of another. Proposing a viable alternative theory that can deal with its own facts, however, is exceedingly difficult, and has proven to be so even historically (Feyerabend, 1993). Facts, after all, privilege the status quo. Galileo's tele­ scopic observations of the sky generated observations that motivated an al­ ternative explanation about the place of the earth in the universe. His ob­ servations favored the Copernican heliocentric interpretation (where the earth goes around the sun) over the Ptolomeic geocentric one (where the sun goes around the earth). The Copernican interpretation, however, was a worldview away from what was currently accepted, and many doubted Gali- leo's data as a valid empirical window on that heliocentric reality. People were highly suspicious of the new instrument: Some asked Galileo to open up his telescope to prove that there was no little moon hiding inside of it. How, otherwise, could the moon or any other celestial body be seen so closely if it was not itself hiding in the telescope? One problem was that Ga­ lileo did not offer a theoretical explanation for why this could be so, and why the telescope was supposed to offer a better picture of the sky than the naked eye. He could not, because relevant theories (optica) were not yet well developed. Generating better data (like Galileo did), and developing entirely new methods for better access to these data (such as a telescope), does in itself little to dislodge an established theory that allows people to see the phenomenon with their naked eye and explain it with their com­ mon sense. Similarly, people see the error happen with their naked eye, even without the help of an error-classification method: The pilot fails to arm the spoilers. Even their common sense confirms that this is an error. The sun goes around the earth. The earth is fixed. The Church was right, and Galileo was wrong. None of his observed facts could prove him right, because there was no coherent set of theories ready to accommodate his facts and give them meaning. The Church was right, as it had all the facts. And it had the theory to deal with them. Interestingly, the Church kept closer to reason as it was defined at the time. It considered the social, political, and ethical implications of Galileo's alternatives and deemed them too risky to accept—certainly on the grounds of tentative, rickety evidence. Disavowing the geocentric idea would be disavowing Creation itself, removing the common ontological de­ nominator of the past millennium and severely undermining the authority and political power the Church derived from it. Error-classification meth­ ods too, guard a piece of rationality that most people in industry and else­ where would be loathe to see disintegrate. Errors occur, they can be distin­ guished objectively. Errors can be an indication of unsafe performance. There is good performance and bad performance; there are identifiable causes for why people perform well or less well and for why failures happen. Without such a supposedly factual basis, without such hopes of an objective rationality, traditional and well-established ways for dealing with threats to safety and trying to create progress could collapse. Cartesian anxiety would grip the industry and research community. How can we hold people ac­ countable for mistakes if there are no "errors"? How can we report safety occurrences and maintain expensive incident-reporting schemes if there are no errors? What can we fix if there are no causes for adverse events? Such questions fit a broader class of appeals against relativism. Postmodernism and relativism, according to their detractors, can lead only to moral ambiguity, nihilism and lack of structural progress. We should instead hold onto the realist status quo, and we can, for most observed facts still seem to privilege it. Errors exist. They have to. To the naive realist, the argument that errors exist is not only natural and necessary, it is also quite impeccable, quite forceful. The idea that er­ rors do not exist, in contrast, is unnatural, even absurd. Those within the es­ tablished paradigm will challenge the sheer legitimacy of questions raised about the existence of errors, and by implication even the legitimacy of those who raise the questions: "Indeed, there are some psychologists who would deny the existence of errors altogether. We will not pursue that doubtful line of argument here" (Reason & Hobbs, 2003, p. 39). Because the current paradigm judges it absurd and unnatural, the question about whether errors exist is not worth pursuing: It is doubtful and unscientific—and in the strictest sense (when scientific pursuits are measured and defined within the ruling paradigm), that is precisely what it is. If some scientists do not succeed in bringing statement and fact into closer agreement (they do not see a procedure error where others would), then this discredits the scientist rather than the theory. Galileo suffered from this too. It was the scientist who was discredited (for a while at least), not the prevailing paradigm. So what does he do? How does Gali­ leo proceed once he introduces an interpretation so unnatural, so absurd, so countercultural, so revolutionary? What does he do when he notices that even the facts are not (interpreted to be) on his side? As Feyerabend (1993) masterfully described it, Galileo engaged in propaganda and psy­ chological trickery. Through imaginary conversations between Sagredo, Salviati, and Simplicio, written in his native tongue rather than in Latin, he put the ontological uncertainty and epistemological difficulty of the geocentric interpretation on full display. The sheer logic of the geocen­ tric interpretation fell apart whereas that of the heliocentric interpreta­ tion triumphed. Where the appeal to empirical facts failed (because those facts will still be forced to fit the prevailing paradigm rather than its alter­ native), an appeal to logic may still succeed. The same is true for error counting and classification. Just imagine this conversation: Simplicio: Errors result from physiological and psychological limita­ tions of humans. Causes of error include fatigue, workload, and fear, as well as cognitive overload, poor interpersonal communications, imper­ fect information processing, and flawed decision making. Sagredo: But are errors in this case not simply the result of other er­ rors? Flawed decision making would be an error. But in your logic, it causes an error. What is the error then? And how can we categorize it? Simplicio: Well, but errors are caused by poor decisions, failures to ad­ here to brief, failures to prioritize attention, improper procedure, and so forth. Sagredo: This appears to be not causal explanation, but simply re­ labeling. Whether you say error, or poor decision, or failure to prioritize attention, it all still sounds like error, at least when interpreted in your worldview. And how can one be the cause of the other to the exclusion of the other way around? Can errors cause poor decisions just like poor de­ cisions cause errors? There is nothing in your logic that rules this out, but then we end up with a tautology, not an explanation. And yet, such arguments may not help either. The appeal to logic may fail in the face of overwhelming support for a ruling paradigm—support that derives from consensus authority, from political, social, and organiza­ tional imperatives rather than a logical or empirical basis (which is, after all, pretty porous). Even Einstein expressed amazement at the common reflex to rely on measurements (e.g., error counts) rather than logic and argument: " 'Is it not really strange,' he asked in a letter to Max Born, 'that human beings are normally deaf to the strongest of argument while they are always inclined to overestimate measuring accuracies?' " (Feyerabend, 1993, p. 239). Numbers are strong. Arguments are weak. Error counting is good because it generates numbers, it relies on accurate measurements (recall Croft, 2001, who announced that "researchers" have "perfected" ways to monitor pilot performance), rather than on argument. In the end, no argument, none of this propaganda or psychological trickery can serve as a substitute for the development of alternative theory, nor did it in Galileo's case. The postmodernists are right and the realists are wrong: Without a paradigm, without a worldview, there are no facts. People will reject no theory on the basis of argument or logic alone. They need an­ other to take its place. A paradigmatic interregnum would produce paraly­ sis. Suspended in a theoretical vacuum, researchers would no longer be able to see facts or do anything meaningful with them. So, considering the evidence, what should the alternative theory look like? It needs to come with a superior explanation of performance varia­ tions, with an interpretation that is sensitive to the situatedness of the per­ formance it attempts to capture. Such a theory sees no errors, but rather performance variations—inherently neutral changes and adjustments in how people deal with complex, dynamic situations. This theory will resist coming in from the outside, it will avoid judging other people from a posi­ tion external to how the situation looked to the subject inside of it. The outlines of such a theory are developed further in various places in this book. SAFETY AS MORE THAN ABSENCE OF NEGATIVES First, though, another question: Why do people bother with error counts in the first place? What goals do they hope these empirical measures help them accomplish, and are there better ways to achieve those goals? A final aim of error counting is to help make progress on safety, but this puts the link between errors and safety on trial. Can the counting of negatives (e.g., these errors) say anything useful about safety? What does the quantity meas­ ured (errors) have to do with the quality managed (safety)? Error-classi- fication methods assume a close mapping between these two, and assume that an absence or reduction of errors is synonymous with progress on safety. By treating safety as positivistically measurable, error counting may be breathing the scientific spirit of a bygone era. Human performance in the laboratory was once gauged by counting errors, and this is still done when researchers test limited, contrived task behavior in spartan settings. But how well does this export to natural settings where people carry out ac­ tual complex, dynamic, and interactive work, where determinants of good and bad outcomes are deeply confounded? It may not matter. The idea of a realist count is compelling to industry for the same reasons that any numerical performance measurement is. Managers get easily infatuated with "balanced scorecards" or other faddish figures of performance. Entire business models depend on quantifying per­ formance results, so why not quantify safety? Error counting becomes yet another quantitative basis for managerial interventions. Pieces of data from the operation that have been excised and formalized away from their origin can be converted into graphs and bar charts that subsequently form the inspiration for interventions. This allows managers, and their airlines, to elaborate their idea of control over operational practice and its outcomes. Managerial control, however, exists only in the sense of purposefully for­ mulating and trying to influence the intentions and actions of operational people (Angell & Straub, 1999). It is not the same as being in control of the consequences (by which safety ultimately gets measured industry-wide), be­ cause for that the real world is too complex and operational environments too stochastic (e.g., Snook, 2000). There is another tricky aspect of trying to create progress on safety through error counting and classification. This has to do with not taking context into regard when counting errors. Errors, according to realist inter­ pretations, represent a kind of equivalent category of bad performance (e.g., a failure to meet one's objective or intention), no matter who com­ mits the error or in what situation. Such an assumption has to exist, other­ wise tabulation becomes untenable. One cannot (or should not) add ap­ ples and oranges, after all. If both apples and oranges are entered into the method (and, given that the autonomy principle is false, error-counting methods do add apples and oranges), silly statistical tabulations that claim doctors are 7,500 times more dangerous than gun owners can roll out the other end. As Hollnagel and Amalberti (2001) showed, attempts to map sit­ uated human capabilities such as decision making, proficiency, or delibera­ tion onto discrete categories are doomed to be misleading. They cannot cope with the complexity of actual practice without serious degeneration (Angell & Straub, 1999). Error classification disembodies data. It removes the context that helped produce the behavior in its particular manifesta­ tion. Such disembodiment may actually retard understanding. The local ra­ tionality principle (people's behavior is rational when viewed from the in­ side of their situations) is impossible to maintain when context is removed from the controversial action. And error categorization does just that: It re­ moves context. Once the observation of some kind of error is tidily locked away into some category, it has been objectified, formalized away from the situation that brought it forth. Without context, there is no way to reestab­ lish local rationality. And without local rationality, there is no way to under­ stand human error. And without understanding human error, there may be no way to learn how to create progress on safety. Safety as Reflexive Project Safety is likely to be more than the measurement and management of nega­ tives (errors), if it is that at all. Just as errors are epistemologically elusive (How do you know what you know? Did you really see a procedure error? Or was it a proficiency error?), and ontologically relativist (what it means "to be" and to perform well or badly inside a particular situation is different from person to person), the notion of safety may similarly lack an objective, common denominator. The idea behind measuring safety through error counts is that safety is some kind of objective, stable (and perhaps ideal) re­ ality, a reality that can be measured and reflected, or represented, through method. But does this idea hold? Rochlin (1999, p. 1550), for example, proposed that safety is a "constructed human concept" and others in hu­ man factors have begun to probe how individual practitioners construct safety, by assessing what they understand risk to be, and how they perceive their ability of managing challenging situations (e.g., Orasanu, 2001). A substantial part of practitioners' construction of safety turns out to be re­ flexive, assessing the person's own competence or skill in maintaining safety across different situations. Interestingly, there may be a mismatch be­ tween risk salience (how critical a particular threat to safety was perceived to be by the practitioner) and frequency of encounters (how often these threats to safety are in fact met in practice). The safety threats deemed most salient were the ones least frequently dealt with (Orasanu, 2001). Safety is more akin to a reflexive project, sustained through a revisable narrative of self-identity that develops in the face of frequently and less frequently en­ countered risks. It is not something referential, not something that is objec­ tively "out there" as a common denominator, open to any type of approxi­ mation by those with the best methods. Rather, safety may be reflexive: something that people relate to themselves. The numbers produced by error counts are a logical endpoint of a struc­ tural analysis that focuses on (supposed) causes and consequences, an anal­ ysis that defines risk and safety instrumentally, in terms of minimizing er­ rors and presumably measurable consequences. A second, more recent approach is more socially and politically oriented, and places emphasis on representation, perception, and interpretation rather than on structural features (Rochlin, 1999). The managerially appealing numbers generated by error counts do not carry any of this reflexivity, none of the nuances of what it is to "be there," doing the work, creating safety on the line. What it is to be there ultimately determines safety (as outcome): People's local ac­ tions and assessments are shaped by their own perspectives. These in turn are embedded in histories, rituals, interactions, beliefs and myths, both of people's organization and organizational subculture and of them as indi­ viduals. This would explain why good, objective, empirical indicators of so­ cial and organizational definitions of safety are difficult to obtain. Opera­ tors of reliable systems "were expressing their evaluation of a positive state mediated by human action, and that evaluation reflexively became part of the state of safety they were describing" (Rochlin, 1999, p. 1550). In other words, the description itself of what safety means to an individual operator is a part of that very safety, dynamic and subjective. "Safety is in some sense a story a group or organization tells about itself and its relation to its task environment" (Rochlin, p. 1555). Can We Measure Safety? But how does an organization capture what groups tell about themselves; how does it pin down these stories? How can management measure a medi­ ated, reflexive idea? If not through error counts, what can an organization look for in order to get some measure of how safe it is? Large recent acci­ dents provide some clues of where to start looking (e.g., Woods, 2003). A main source of residual risk in otherwise safe transportation systems is the drift into failure described in chapter 2. Pressures of scarcity and competi­ tion narrow an organization's focus on goals associated with production. With an accumulating base of empirical success (i.e., no accidents, even if safety is increasingly traded off against other goals such as maximizing profit or capacity utilization), the organization, through its members' mul­ tiple little and larger daily decisions, will begin to believe that past success is a guarantee of future safety, that historical success is a reason for confi­ dence that the same behavior will lead to the same (successful) outcome the next time around. The absence of failure, in other words, is taken as evidence that hazards are not present, that countermeasures already in place are effective. Such a model of risk is embedded deeply in the reflexive sto­ ries of safety that Rochlin (1999) talked about, and it can be made explicit only through qualitative investigations that probe the interpretative aspect of situated human assessments and actions. Error counts do little to eluci­ date any of this. More qualitative studies could reveal how currently traded models of risk may increasingly be at odds with the actual nature and prox­ imity of hazard, though it may of course be difficult to establish the objec­ tive, or ontologically absolutist, presence of hazard. Particular aspects of how organization members tell or evaluate safety stories, however, can serve as markers. Woods (2003, p. 5), for example, has called one of these markers "distancing through differencing." In this proc­ ess, organizational members look at other failures and other organizations as not relevant to them and their situation. They discard other events be­ cause they appear at the surface to be dissimilar or distant. Discovering this through qualitative inquiry can help specify how people and organizations reflexively create their idea, their story of safety. Just because the organiza­ tion or section has different technical problems, different managers, differ­ ent histories, or can claim to already have addressed a particular safety con­ cern revealed by the event, does not mean that they are immune to the problem. Seemingly divergent events can represent similar underlying pat­ terns in the drift towards hazard. High-reliability organizations characterize themselves through their preoccupation with failure: continually asking themselves how things can go wrong and could have gone wrong, rather than congratulating themselves on the fact that things went right. Dis­ tancing through differencing means underplaying this preoccupation. It is one way to prevent learning from events elsewhere, one way to throw up ob­ stacles in the flow of safety-related information. Additional processes that can be discovered include to what extent an organization resists oversimplifying interpretations of operational data, whether it defers to expertise and expert judgment rather than manage­ rial imperatives. Also, it could be interesting to probe to what extent prob- lem-solving processes are fragmented across organizational departments, sections, or subcontractors. The 1996 Valujet accident, where flammable oxygen generators were placed in an aircraft cargo hold without shipping caps, subsequently burning down the aircraft, was related to a web of sub­ contractors that together made up the virtual airline of Valujet. Hundreds of people within even one subcontractor logged work against the particu­ lar Valujet aircraft, and this subcontractor was only one of many players in a network of organizations and companies tasked with different aspects of running (even constituting) the airline. Relevant maintenance parts (among them the shipping caps) were not available at the subcontractor, ideas of what to do with expired oxygen canisters were generated ad hoc in the absence of central guidance, and local understandings for why ship­ ping caps may have been necessary were foggy at best. With work and re­ sponsibility for it distributed among so many participants, nobody may have been able anymore to see the big picture, including the regulator. Nobody may have been able to recognize the gradual erosion of safety constraints on the design and operation of the original system. If safety is a reflexive project rather than an objective datum, human fac­ tors researchers must develop entirely new probes for measuring the safety health of an organization. Error counts do not suffice. They uphold an illu­ sion of rationality and control, but may offer neither real insight nor pro­ ductive routes for progress on safety. It is, of course, a matter of debate whether the vaguely defined organizational processes that could be part of new safety probes (e.g., distancing through differencing, deference to ex­ pertise, fragmentation of problem-solving, incremental judgments into dis­ aster) are any more real than the errors from the counting methods they seek to replace or augment. But then, the reality of these phenomena is in the eye of the beholder: Observer and observed cannot be separated; object and subject are largely indistinguishable. The processes and phenomena are real enough to those who look for them and who wield the theories to accommodate the results. Criteria for success may lie elsewhere, for exam­ ple in how well the measure maps onto past evidence of precursors to fail­ ure. Yet even such mappings are subject to paradigmatic interpretations of the evidence base. Indeed, consonant with the ontological relativity of the age human factors has now entered, the debate can probably never be closed. Are doctors more dangerous than gun owners? Do errors exist? It depends on who you ask. The real issue, therefore, lies a step away from the fray, a level up, if you will. Whether we count errors as Durkheimian fact on the one hand or see safety as a reflexive project on the other, competing premises and practices reflect particular models of risk. These models of risk are interesting not because of their differential abilities to access empirical truth (because that may all be relative), but because of what they say about us, about human fac­ tors and system safety. It is not the monitoring of safety that we should sim­ ply pursue, but the monitoring of that monitoring. If we want to make prog­ ress on safety, one important step is to engage in such metamonitoring, to become better aware of the models of risk embodied in our assumptions and approaches to safety.