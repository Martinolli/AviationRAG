Title: A Human Error Approach to Aviation Accident Analysis The Human Factors Analysis and Classification System, Chapters 1 – 2 – 3 Author(s): Douglas A. Wiegmann, Scott A. Shappell Category: Human, Error, Analysis Tags: Human, Error, human factor analysis and classification sytems Preface As aircraft have become more reliable, humans have played a progressively more important causal role in aviation accidents. Consequently, a growing number of aviation organizations are tasking their safety personnel with developing accident investigation and other safety programs to address the highly complex and often nebulous issue of human error. Unfortunately, many of today's aviation safety personnel have little formal education in human factors or aviation psychology. Rather, most are professional pilots with general engineering or other technical backgrounds. Thus, many safety professionals are ill-equipped to perform these new duties and, to their dismay, soon discover that an "off-the-shelf' or standard approach for investigating and preventing human error in aviation does not exist. This is not surprising, given that human error is a topic that researchers and academicians in the fields of human factors and psychology have been grappling with for decades. Indeed, recent years have seen a proliferation of human error frameworks and accident investigation schemes to the point where there now appears to be as many human error models as there are people interested in the topic (Senders and Moray, 1991). Even worse, most error models and frameworks tend to be either too "academic" or abstract for practitioners to understand or are too simple and "theoretically void" to get at the underlying causes of human error in aviation operations. Having been left without adequate guidance to circumnavigate the veritable potpourri of human error frameworks available, many safety professionals have resorted to developing accident investigation and error-management programs based on intuition or "pop psychology" concepts, rather than on theory and empirical data. The result has been accident analysis and prevention programs that, on the surface, produce a great deal of activity (e.g., incident reporting, safety seminars and "error awareness" training), but in reality only peck around the edges of the true underlying causes of human error. Demonstrable improvements in safety are therefore hardly ever realized. The purpose of the present book is to remedy this situation by presenting a comprehensive, user-friendly framework to assist practitioners in effectively investigating and analyzing human error in aviation. Coined the Human Factors Analysis and Classification System (HFACS), its framework is based on James Reason's (1990) well-known "Swiss cheese" model of accident causation. In essence, human factor analysis and classification sytems bridges the gap between theory and practice in a way that helps improve both the quantity and quality of information gathered in aviation accidents and incidents. The human factor analysis and classification sytems framework was originally developed for, and subsequently adopted by, the U.S. Navy/Marine Corps as an accident investigation and data analysis tool. The U.S. Army, Air Force, and Coast Guard, as well as other military and civilian aviation organizations around the world are also currently using human factor analysis and classification sytems to supplement their preexisting accident investigation systems. In addition, human factor analysis and classification sytems has been taught to literally thousands of students and safety professionals through workshops and courses offered at professional meetings and universities. Indeed, human factor analysis and classification sytems is now relatively well known within many sectors of aviation and an increasing number of organizations worldwide are interested in exploring its usage. Consequently, we currently receive numerous requests for more information about the system on what often seems to be a daily basis. To date, however, no single document containing all the information on the development and application of human factor analysis and classification sytems exists. Most of our previous work on this topic has been published in either technical reports, scientific journals or conference proceedings. Furthermore, given the development of human factor analysis and classification sytems has been an evolving process, our early publications and presentations contain much older, less complete versions of the system. Yet given the popularity and accessibility of the World Wide Web, many of these older versions are currently being circulated via documents and presentations that are available and downloadable over the Internet. As a result, some organizations are using older versions of human factor analysis and classification sytems and are not benefiting from the use of the latest and greatly improved version. Our goals in writing this book, therefore, are to integrate our various writings in this area and to expand upon them in a way not suitable for technical journals or other scientific publications. This book, therefore, will serve as a common resource for all who are interested in obtaining the most up-to-date and comprehensive description of the human factor analysis and classification sytems framework. We have written this book primarily for practitioners (not necessarily academicians) in the field of aviation safety. Therefore, we intentionally describe human error and human factor analysis and classification sytems from an applied perspective. In doing so, our hope is that practitioners will find in this book the necessary ingredients to effectively investigate and analyze the role of human error in aviation accidents and incidents. Perhaps then, definitive improvements in aviation safety will be more readily forthcoming. To set the stage for our discussion of HFACS, Chapter 1 provides an overview of the historical role that human error has played in aviation accidents. This chapter also examines the possible systemic reasons for the limited effectiveness of many accident prevention programs and highlights the need for the development of a comprehensive framework of human error. Toward these ends, the prominent human error perspectives commonly discussed in the literature are presented in Chapter 2, serving as a foundation for the development of HFACS. The strengths and weaknesses of each perspective are discussed with an eye toward a unifying theory of human error that incorporates the best aspects of each. One of the most influential unifying theories, James Reason's "Swiss cheese" model of accident causation, is presented in Chapter 3. With Reason's model as its theoretical basis, the human factor analysis and classification sytems framework is then laid out in detail to describe the latent and active failures or "holes in the cheese" as postulated by Reason. Simply describing human factor analysis and classification sytems however, is not enough. After all "the proof of the pudding is in the eating". Therefore, a better way to gain an appreciation of how human factor analysis and classification sytems can be applied to aviation accident analysis is to demonstrate its utility using a series of case studies. With this in mind, Chapter 4 presents several examples of how human factor analysis and classification sytems can be applied to explain the human causal factors associated with actual aviation accidents. Moving beyond the realm of accident investigation, Chapter 5 illustrates how human factor analysis and classification sytems can be used to perform comprehensive human factors analyses of existing accident databases. Examples will also be provided of how the results of such analyses have helped to identify key human factors problems within Naval aviation, so that successful interventions could be developed and implemented. Still, how is one to know whether human factor analysis and classification sytems will have utility in an operational setting? One obvious way is simply to implement it and see if it works. However, in today's world, most organizations cannot absorb the cost in both time and money to wait and see if human factor analysis and classification sytems proves useful. Clearly, a better approach would be to use some sort of objective criteria for evaluating the framework. Chapter 6, therefore, describes the set of design criteria and the validation process used to ensure that human factor analysis and classification sytems would have utility as an accident investigation and data analysis tool. As the final chapter, aptly named "But What About...?", Chapter 7 addresses some of the common questions and concerns that people often have about HFACS. While we would like to think that the preceding chapters adequately speak to these issues, we have chosen to meet them head-on in this chapter in order to help readers better determine the appropriateness of human factor analysis and classification sytems for their organization. Disclaimer The views expressed in this book are our own. They do not necessarily reflect those of the Federal Aviation Administration or the U.S. Department of Transportation. Nor do they necessarily reflect those of the U.S. Navy, Department of Defense or any other branch of the Federal Government. We have made an earnest attempt to provide proper citation to the work of others, but we do apologize if we have failed to provide appropriate credit to anyone for their efforts or ideas. 1 Errare Humanum eastern standard time To Err is Human On September 17th ... at 4:46 pm, the aeroplane was taken from the shed, moved to the upper end of the field and set on the starting track. Mr. Wright and Lieutenant Selfridge took their places in the machine, and it started at 5:14, circling the field to the left as usual. It had been in the air four minutes and 18 seconds, had circled the field 4 /2 times and had just crossed the aeroplane shed at the lower end of the field when I heard a report then saw a section of the propeller blade flutter to the ground. I judged the machine at the time was at a height of about 150 feet. It appeared to glide down for perhaps 75 feet, advancing in the meantime about 200 feet. At this point it seemed to me to stop, turn so as to head up the field towards the hospital, rock like a ship in rough water, then drop straight to the ground the remaining 75 feet..The pieces of propeller blade [were] picked up at a point 200 feet west of where the airplane struck. It was 2½ feet long, was a part of the right propeller, and from the marks on it had apparently come in contact with the upper guywire running to the rear rudder. ... [The propeller] struck [the guywire] hard enough to pull it out of its socket and at the same time to break the propeller. The rear rudder then fell to the side and the air striking this from beneath, as the machine started to glide down, gave an upward tendency to the rear of the machine, which increased until the equilibrium was entirely lost. Then the aeroplane pitched forward and fell straight down, the left wings striking before the right. It landed on the front end of the skids, and they, as well as the front rudder was crushed. Lieutenant Selfridge ... died at 8:10 that evening of a fracture of the skull over the eye, which was undoubtedly caused by his head striking one of the wooden supports or possibly one of the wires. ... Mr. Wright was found to have two or three ribs broken, a cut over the eye, also on the lip, and the left thigh broken between the hip and the knee (1st Lieutenant Frank P. Lalm, 1908). Note, this pioneer of aviation safety was actually Frank P. Lahm, not Lalm as identified in this letter to the Chief of the Army Signal Corps. What began as an unofficial orientation flight at Fort Meyer, Virginia in the summer of 1908, ended in tragedy, as have many flights since. Sadly, the annals of aviation history are littered with accidents and tragic losses such as this (Figure 1.1). Since the late 1950s, however, the drive to reduce the accident rate has yielded unprecedented levels of safety. In fact, today it is likely safer to fly in a commercial airliner than to drive a car or walk across a busy New York City street. Still, it is interesting that while historians can recount in detail the strides that the aviation industry has made over the last half century, one fundamental question remains generally unanswered: "Why do aircraft crash?" The answer may not be as straightforward as you think. For example, in the early years of aviation it could reasonably be said that the aircraft itself was responsible for the majority of aircraft accidents. That is, early aircraft were intrinsically unforgiving and, relative to their counterparts today, mechanically unsafe. However, the modern era of aviation has witnessed an ironic reversal of sorts. It now appears to some that the aircrew themselves are more deadly than the aircraft they fly (Mason, 1993; cited in Murray, 1997). Indeed, estimates in the literature indicate that somewhere between 70 and 80 percent of all aviation accidents can be attributed, at least in part, to human error (Shappell and Wiegmann, 1996). So, maybe we can answer the larger question of why aircraft crash, if only we could define what really constitutes that 70 to 80 percent of human error referred to in the literature. But, even if we did know, could we ever really hope to do anything about it? After all, “errare humanum est” – to err is human (Plutarch, c.100 AD). So, isn't it unreasonable to expect error-free human performance? Maybe ... but, perhaps a lesson in how far aviation safety has come since its inauspicious beginnings nearly a century ago will provide us with some clues about where we need to go next. Aviation Safety Trends Most aviation accident statistics cited in the literature today begin with data collected in the late 1950s and early 1960s. Representative of this type of data are the two graphs presented in Figure 1.2. In the top graph, the number of commercial aviation accidents that have occurred worldwide since 1961 are plotted annually against the number of departures. When the data are depicted in this manner, a sharp decline in the accident rate since the early 1960s becomes readily apparent. In fact, the number of commercial accidents has decreased to a point where today, fewer than two accidents occur worldwide for every one million departures (Boeing, 2000; Flight Safety Foundation [FSF], 1997). What's more, this trend is generally the same (albeit not as dramatic), whether you consider the overall number of commercial aviation accidents, or just those associated with fatalities. In either case, it can reasonably be said that commercial aviation safety has steadily improved over the last 40 years. Indeed, aviation has become one of the safest forms of transportation, leading the National Transportation Safety Board to proclaim in 1990 that a passenger boarding a U.S. carrier then had over a 99.99 percent chance of surviving the flight (NTSB, 1994a). Improvements in aviation safety, however, are not unique to commercial aviation. General aviation accident rates have also plummeted over the last several decades (Figure 1.3, top). A similar trend can also be seen when accident data from the U.S. Navy/Marine Corps (middle) and U.S. Air Force (bottom) are plotted across years. Indeed, the accident rates among these diverse types of flying operations have dropped impressively since the late 1950s and early 1960s, indicating that all aspects of aviation have benefited from advances aimed at making the skies safer. So, what can we attribute these improvements in aviation safety to over the last half-century? Given the rather dramatic changes evident in the accident record, it is doubtful that any single intervention was responsible for the decline in the accident rate. Rather, it is likely the result of a variety of factors, such as advancements in technology, equipment, operating procedures, and training practices (Nagel, 1988; Yacavone, 1993). To give you a better feel for how various interventions have improved aviation safety, let us consider several of these initiatives within the context of Naval aviation. In Figure 1.4, a number of technical innovations and standardization programs introduced into the U.S. Navy/Marine Corps over the last several decades have been superimposed on the annual accident rate. Arguably, these efforts were not solely responsible for the decline observed in the accident rate. After all, nowhere does this figure address improvements in aircraft design and the introduction of new aircraft in the fleet. Still, there is little question among Naval experts that these interventions played a significant role in the level of safety currently enjoyed by the U.S. Navy/Marine Corps. Consider, for example, the development of the angled carrier deck aboard Naval aircraft carriers in the early to mid-1950s. Many Naval history buffs may recall that early aircraft carrier flight decks were single straight runways, which created a number of safety problems – especially when one aircraft was trying to take off from the bow of the ship while another was nexpectedly aborting a landing on the stern (Figure 1.5, top). Not surprising, aircraft would occasionally collide! To remedy this safety hazard, the angled carrier deck was developed, which allowed aircraft to take off from the bow of the ship in a different direction from those landing on the stern, avoiding any potential conflict in their flight paths; a very wise intervention indeed (Figure 1.5, bottom). Another major factor affecting safety in the U.S. Navy/Marine Corps was the establishment of the Naval Aviation Safety Center (now known as the Naval Safety Center) in the mid-1950s. On the surface, this might not seem to be particularly revolutionary given today's standards. However, for the first time, a major command in the U.S. Navy was assigned the sole responsibility and authority for monitoring and regulating safety issues in the fleet. This single act elevated aviation safety to the highest levels of the U.S. Navy/Marine Corps, as the command reported directly to the Chief of Naval Operations. Still, other safety programs have helped improve Naval aviation safety as well. For example, in the early 1960s, the replacement air group concept was created, requiring pilots to receive specialized training in advanced aircraft before flying them in the fleet. While it may sound intuitive to some that pilots should gain some tactical experience in their aircraft before flying them in combat or other operations, this was not always the case. As recently as WWII, pilots would receive basic flight training and then transition to the fleet, entering the operational arena with very little time in their combat aircraft. More recently, the establishment of formal squadron safety programs, the development of aircrew coordination training, and the implementation of a periodic human factors review of fleet aviators have all contributed significantly to Naval aviation safety by identifying problems and hazards before they resulted in accidents. Undeniably, safety initiatives such as these have saved countless lives in the U.S. Navy/Marine Corps and have elevated Naval aviation safety to unprecedented levels. Beyond saving lives, the military, like any other business, is often driven by the so-called "bean counters." Yet, even the bean counters have to be smiling when you consider the cost savings realized as a result of improvements in aviation safety. Consider that until recently the U.S. Navy/Marine Corps flew an average of 2 million flight hours per year (today it's closer to 1.5 million flight hours per year). If the rate of major accidents today were still at levels observed in 1950, over 800 aircraft would have been lost in 2000 alone! Needless to say, the U.S. Navy/Marine Corps would be quickly out of the aviation business altogether, if that were the case. Thankfully, improvements in all forms of aviation safety, including Naval aviation, have remedied this trend. Some Reasons for Concern Even though the overall accident rate in civil and military aviation is indeed excellent, certain aspects of the data are "unsettling" (Nagel, 1988, p. 264). As can be seen from the graphs presented earlier, improvements in aviation safety have slowed substantially during the last few decades. This plateau has led some to conclude that further reductions in the accident rate are improbable, if not impossible. In other words, we have reached a point at which accidents may simply be the "cost of doing business." However, if we accept this philosophy we must also be prepared to accept the consequences. For example, on the military side of aviation, the financial cost of aircraft accidents is growing astronomically. As illustrated in Figure 1.6, the amount of money and resources lost due to U.S. Naval aviation accidents is enormous, even though these accidents occur much less frequently than other types. Indeed, the loss incurred from aviation accidents cost the U.S. Navy/Marine Corps some 3.3 billion in the last five years alone — more than five times that seen with all other accidents combined. Obviously, if the mishap rate were allowed to continue at its current level, either taxes would have to go up to buy more airplanes (not a politically popular option), or the military would have to operate with fewer and fewer aircraft (not a strategically savvy move either). There may be reason for concern within commercial aviation as well. Consider, for example, that worldwide air traffic is expected to increase significantly over the next several years as the industry continues to grow (FSF, 1997). Now let's assume for the moment that the current commercial accident rate is already "as good as it's going to get." Naturally, if you increase the number of flights while maintaining the same accident rate, the overall frequency of accidents will inevitably increase as well. To illustrate this point, the current commercial jet accident rate, expected traffic growth, and frequency of accidents have been plotted together in Figure 1.7. Sadly, if these estimates remain unchanged, there may be as many as 50 major airline accidents occurring worldwide per year during the first decade of the new millennium. This equates to nearly one accident a week! Given the intense media coverage that major airline accidents often receive, combined with the rapid dissemination of information worldwide, there is little doubt that the traveling public will be made well aware of these accidents in the most explicit detail, even if they do occur half-way around the world. As such, the airline industry will likely suffer as public confidence erodes and a general mistrust permeates the aviation industry. Simply trotting out the industry "talking heads" and releasing statements such as "the accident rate has not changed" or that "we are as safe as we have ever been" will likely have little or no effect on public confidence, nor will it likely appease the flying public's demand for the safest form of transportation possible. One alternative may be to post the national transportation safety board safety statistic cited earlier on the bulkhead of each airplane. Can you imagine reading the following placard as you board the airplane with the rest of your family for that fun-filled trip to Disneyland? Welcome aboard Doug and Scott's airline. You have a 99.99% chance of surviving this flight. Not a particularly comforting thought, is it? Well ... then again, public relations were never our strong suit. Beside, the national transportation safety board statistic cited earlier only refers to survival. There are no guarantees that you will not be involved in an accident or maimed – only that you will likely survive the ordeal. But seriously, "accident-prevention steps must be taken now to stop the accident rate from exceeding its current level, and even greater effort must be taken to further reduce the current accident rate" (FSF, 1997). After all, even if the industry was willing to accept the monetary cost of accidents, the loss of lives alone makes further reductions a necessity, not a commodity to be traded. Still, the days of sweeping reductions and sharp drops in the accident rate due to a few innovations or interventions have been over for nearly 30 years. Any change will likely be measured as a reduction in only a few accidents a year – and the cost of those interventions will be the result of millions of dollars worth of research and investigation. Therefore, with limited budgets and the stakes so high, accident prevention measures must target the primary cause of accidents, which in most cases, is the human (ICAO, 1993). Recall, that roughly 70 to 80 percent of all aviation accidents are attributable, at least in part, to some form of human error. Notably, however, as the accident rate has declined over the last half century, reductions in human error-related accidents have not kept pace with the reduction of accidents related to mechanical and environmental factors (NTSB, 1990; Nagel, 1988; O'Hare et al., 1994; Shappell and Wiegmann, 1996; Yacavone, 1993). In fact, humans have played a progressively more important causal role in both civilian and military aviation accidents as aircraft equipment has become more reliable (Nagel, 1988). For example, our previous analysis of Naval aviation mishap data (Shappell and Wiegmann, 1996), revealed, that in 1977, the number of Naval aviation accidents attributed solely to mechanical and environmental factors was nearly equal to those attributable, at least in part, to human error (Figure 1.8). Yet, by 1992, the number of solely mechanical accidents had been virtually eliminated, while the number of human-error related accidents had been reduced by only 50 percent. We have even argued that the reduction in accidents attributable to human error was not as much a function of interventions aimed at aircrew, as it was improvements made to the aircraft. After all, it is well known that the opportunity for human error will go up considerably when a mechanical failure occurs. Not surprising then, as aircraft have become more reliable, accidents due to human error would naturally decline as well. So it would appear that many of the interventions aimed at reducing the occurrence or consequence of human error have not been as effective as those directed at mechanical failures. Undeniably, there are many reasons for this disparity — some more obvious than others. Regardless of the reasons however, they can all be best understood within the context of the accident investigation and prevention process. Therefore, let us consider in more detail the differences in the accident investigation and intervention process from both the engineering and human factors side of the house. Although both processes normally occur simultaneously, each will be considered separately to illustrate their inherent differences. Engineering Aspects of an Investigation Although much less frequent today than in years past, mechanical failures occasionally do occur in flight, and in worst-case scenarios may even lead to an incident or accident as illustrated in Figure 1.9. Typically, an investigation will then take place involving a team of air-safety investigators and technical support personnel charged with sifting through the wreckage to uncover hidden clues as to the accident's cause. Collectively, this investigative team possesses a wide range of experience, including specialized knowledge of aircraft systems, aerodynamics, and other aerospace engineering topics. In addition, these highly trained accident sleuths often have access to an assortment of sophisticated technology and analytical techniques such as metallurgical tests, electron microscopy, and advanced computer modeling capabilities, all designed to enrich the investigative process. Armed with a blend of science and sophisticated instrumentation that would make even James Bond green with envy, it is no surprise that most, if not all, mechanical failures that result in accidents are often revealed during the engineering investigation. To illustrate this point, let us suppose for a moment that the structural integrity of an aircraft is compromised by fatigue fractures along a wing spar or among a series of bolts or rivets. These fractures, when viewed with an electron microscope, have unique patterns that can be easily identified by experienced engineers and metallurgists, leaving little doubt as to the origin of the failure. In much the same way, the presence of a system malfunction can be uncovered by a detailed examination of the electrical wiring of the aircraft, including the breakage pattern of light bulb filaments within the instrument panel. For example, if a particular system warning light was illuminated at the time of impact (presumably indicating that the system was inoperative) there is a distinctive stretch to the white-hot filament within the bulb. Combined with other supporting evidence such as frayed electrical wires, it can then be determined if a system failure contributed in a significant way to the accident. Regardless of the methods employed, what makes evidence gathered in engineering investigations so indisputable is that the techniques and analyses involved are grounded in the physical sciences. This fact alone allows investigators to move beyond simply identifying and cataloging what part of the aircraft failed, to the larger question of why the failure occurred in the first place. As a result, data gathered in engineering investigations have yielded revolutionary design changes and have contributed significantly to the evolution of today's modern aircraft. The collection of accident data alone, however, would be of little use if a repository/database did not exist to house it. Typically then, data from engineering investigations are entered into accident databases maintained by safety organizations like the National Transportation Safety Board (NTSB) in Washington, DC. Such databases are generally highly structured and well defined, being organized around traditional aircraft categories such as airframes, powerplants, and component systems. As a result, the data are easily accessible, allowing periodic analyses to be performed so that major causal trends or common problems can be identified across accidents. The results from these analyses in turn provide feedback to investigators, which improves their investigative methods and techniques while providing guidance on where to look during future investigations. For example, if analysts at the national transportation safety board were to find that a particular engine had a history of fatigue related failures, then this information could be distributed to investigators in the field for use during their next investigation. In effect, the accident database provides a rich source of clues when investigating future accidents. In addition, information from the database analyses provides a valuable resource for researchers within the FAA, NASA, department of defense and airplane manufacturers whose mission is to develop safer and more efficient aircraft. Ultimately, these needs-based, data-driven programs produce effective intervention strategies that either prevent mechanical failures from occurring or mitigate their consequences when they do. What's more, given that these interventions are "data-driven," their effectiveness can be objectively monitored and evaluated, so that they can be modified or reinforced to improve safety. The result of this engineering investigative and prevention process has been a dramatic reduction in the rate of accidents due to mechanical or systems failures. Human Factors Aspects of an Investigation In contrast to the engineering investigation just described, consider the occurrence of an aircrew error that results in an accident (Figure 1.10). As with mechanical failures, an investigation soon takes place to determine the nature and cause of these errors. However, unlike the engineering investigation that involved numerous technical experts, the human performance investigation typically involves only a single individual, who may or may not be trained in human factors. In fact, even at the world's premier safety organizations there may be only a handful of human factors professionals on the staff. Truth be told, if you were to knock on the door of the national transportation safety board or any of the U.S. military safety centers today and ask them to send out their human factors experts, only a few people would exit the building. Now, ask them to send out their engineering experts. It would look like a fire drill, as practically the whole building empties! Perhaps this is a bit of an exaggeration, but the point is that most human performance investigators are often a "one person show", with little assistance or support in the field or elsewhere. What makes matters worse is that unlike the tangible and quantifiable evidence surrounding mechanical failures, the evidence and causes of human error are generally qualitative and elusive. Even the analytical techniques used within the human factors investigation are generally less refined and sophisticated than those employed to analyze mechanical and engineering concerns. Consider, for example, the difference between fatigue in a bolt and a fatigued pilot. Unlike metal fatigue that can be readily identified using well-established technology and electron microscopy, pilot fatigue is difficult to observe directly, much less quantify. Instead, it must be inferred from a variety of factors such as the time an accident occurred and the pilot's 72-hour history, which includes, among other things, when he/she went to bed and how long they slept. In addition, other issues such as work tempo, experience, and flight duration may also come into play, all of which make any determination of pilot fatigue an inexact science at best. So, while engineers have little difficulty agreeing upon fatigue in a bolt, it remains virtually impossible to get a group of accident investigators to agree on the presence of fatigue in a pilot, even if all of the necessary information is available. Like pilot fatigue, the identification of other human factors causal to an accident is easier said than done. As a result, human factors investigations have traditionally focused on "what" caused the accident, rather than "why" it occurred. Indeed, many human causal factors in accident reports are not "really causes on which safety recommendations can be made, but rather merely brief descriptions of the accident" or error (ICAO, 1993, p. 32). Statements like the pilot "failed to maintain adequate clearance from the terrain" provide little insight into possible interventions. In effect, the only safety recommendations that could be derived from such a statement would be to either make a rubber airplane or make rubber ground — neither of which make much sense outside the confines of children's cartoons! Still, investigators identify human causal factors, and as with the engineering side of the investigation, the information gathered during the human performance investigation is entered into an accident database. However, unlike their engineering counterparts, databases that house human error data are often poorly organized and lack any consistent or meaningful structure. This should come as no surprise when you consider that "information management" technicians who possess expertise in archiving data but have little familiarity with human factors, design most accident databases. As a result, these data warehouses are quite effective in preserving the data (much like mummification preserves the body), but they have proven woefully inadequate for data retrieval and analysis. In fact, as the ardent researcher unwraps the proverbial database mummy, there is often considerable disappointment as he soon discovers that what's inside bears little resemblance to traditional human factors. That is to say, there is generally no theoretical or functional relationship between the variables, as they are often few in number and ill defined. Given the dearth of human factors data and the inherent problems associated with most databases, when aviation accident data are examined for human error trends, the result is typically less than convincing. Accordingly, many safety professionals have labeled the entire contents of the database as "garbage," a view not appreciated by those doing the investigations. Still, even with its shortcoming, analysts and academicians continue to wrestle with the data and are resolved to making something out of their contents. Unfortunately, many of these analyses simply focus on more reliable contextual information such as time of day, weather conditions, and geographic location of the accident or demographic data surrounding accidents, such as pilot gender, age, and flight time. In fact, few studies have attempted to examine the underlying human causes of accidents. Even those have generally been limited to a small subset of accidents that often only relate to the researchers particular area of interest. Rarely, if ever, has there been a comprehensive and systematic analysis of the entire database to discover the major human factors issues related to flight safety. Results from analyses of accident data have therefore provided little feedback to help investigators improve their investigative methods and techniques. The information is also of limited use to airlines and government agencies in determining the types of research or safety programs to sponsor. Not surprising then, many human factors safety programs tend to be intuitively- or fad-driven, rather than the data-driven programs initiated within the engineering side of the house. That is to say, interventions aimed at human factors are typically derived by well-meaning, "expert" opinion or group discussions about what many "believe" are the major safety issues. In truth however, many decisions about safety programs are based on statements like, "I've flown the line, and never crashed from being fatigued, so fatigue cannot be a big problem," or "the last accident was due to cockpit resource management problems, therefore we need to spend more money on improving CRM." Curiously, most would admit that this opinion-based process would not work on the engineering side. Imagine an engineer standing up in a meeting and emphatically stating that he or she has a "gut feeling" about the airworthiness of a particular aircraft. Such a statement not based on data, would clearly result in more than just a few odd looks from co-workers if not outright ridicule. Nevertheless, such is often the status quo on the human factors side and many don't think twice about it! Given that most human factors safety programs are not data-driven, it only stands to reason that they have produced intervention strategies that are only marginally effective at reducing the occurrence and consequences of human error. Furthermore, unlike the engineering side in which single interventions can often produce great strides in improving the structural integrity and reliability of mechanical systems, human factors interventions are often constrained by the limited improvements that can be achieved in the performance capabilities of humans. What's more, the lack of consistent human factors accident data has prohibited the objective evaluation of most interventions so that they might be revamped or reinforced to improve safety. As a result, the overall rate of human-error related accidents has remained high and constant over the last several years (Shappell and Wiegmann, 1996). Conclusion The current aviation safety system was built on issues that confronted aviation 50 years ago, when the aircraft was, in effect, the "weakest link." Today, however, accidents attributable to catastrophic failures of the aircraft are very infrequent. If the aviation industry is ever to realize a reduction in the aviation accident rate, the human causes of accidents need to be more effectively addressed. However, simply replacing all of the engineers and other technical experts with those versed in human factors is not the solution. That would be like "throwing the baby out with the bath water" and would likely result in an increase in accidents attributable to mechanical and engineering factors. Instead, the human factors aspects of aircraft accident investigations need to be enhanced. Nevertheless, one does not necessarily need a doctorate in human factors to perform a legitimate human performance investigation. Current air-safety investigators could effectively assume these responsibilities. This is not to say, however, that simply having a brain by default makes an engineer or a pilot a human factors expert. Just because we all eat, doesn't make us all experts in nutrition. Air-safety investigators need to be provided with a better understanding of human factors issues and analytical techniques. Increasing the amount of money and resources spent on human factors research and safety programs is not necessarily the answer to all of our safety problems either. After all, a great deal of resources and efforts are currently being expended and simply increasing these efforts would likely not make them more effective. To paraphrase Albert Einstein, "the definition of insanity is doing something over and over again and expecting different results." Instead, the solution may be to redirect safety programs so that they address important human factors issues. Regardless of the mechanism, safety efforts cannot be systematically refocused until a thorough understanding of the nature of human factors in aviation accidents is realized. Such an understanding can only be derived from a comprehensive analysis of existing accident databases. What is required to achieve these objectives is a general human error framework around which new investigative methods can be designed and existing post-accident databases restructured. Such a framework would also serve as a foundation for the development and tracking of intervention strategies, so that they can be modified or reinforced to improve safety. The question remains, as to whether such a human error framework exists – a topic we turn to in the next chapter. 2 Human Error Perspectives Recent years have witnessed a proliferation of human error frameworks to a point where today there appears to be as many human error models and taxonomies as there are people interested in the topic (Senders and Moray, 1991). What remains to be answered, however, is whether any of these frameworks can actually be used to conduct a comprehensive human error analysis of aviation accident data and/or provide a structure around which new human factors investigative techniques can be designed. After all, if an adequate "off-the-shelf' approach for addressing human error already exists, it would eliminate the need to develop yet another error framework. In other words, why reinvent the wheel if you don't have to? This is the very question that we have wrestled with within our own organizations. So how do you identify the right error framework for your purposes? Perhaps the best way is to do what we did and systematically examine the approaches others have taken to address human error (Wiegmann and Shappell, 2001a). Only then can you accurately determine which frameworks, if any, are suitable to meet your needs. At first glance, such a task can be daunting, particularly if one tries to survey each, and every one of the error frameworks that exist. However, what we have found is that when these different methods are sorted based upon the underlying assumptions made about the nature and causes of human error, a smaller, more manageable, collection of error systems will emerge. Using this approach, our previous forays into the human error literature have revealed six major human error perspectives, all of which have distinct advantages and disadvantages (Wiegmann et al., 2000; Wiegmann and Shappell, 2001a). In no particular order, they include the cognitive, ergonomic, behavioral, aeromedical, psychosocial, and organizational perspectives. In the next several pages we will explore each of these human error perspectives, focusing on selected frameworks that characterize each approach as well as their strengths and weaknesses. Then, after reviewing each perspective, we will once again return to the question of whether any of the existing frameworks provide a suitable foundation for conducting a comprehensive analysis of human error associated with aviation accidents and incidents. The Cognitive Perspective Let us begin by first examining one of the more popular approaches to human error analysis – the cognitive perspective. The principle feature of this approach is the assumption that the pilot's mind can be conceptualized as essentially an information processing system. Much like a modern computer, the cognitive viewpoint assumes that once information from the environment makes contact with one of the senses (e.g., vision, touch, smell, etc.), it progresses through a series of stages or mental operations, culminating in a response. Figure 2.1 Basic model of information processing Source: Adapted from Wickens and Flach (1988) The four-stage model of information processing described by Wickens and Flach (1988) is but one example of this view (Figure 2.1). In their model, stimuli from the environment (e.g., photons of light or sound waves) are converted into neural impulses and stored temporarily in a short-term sensory store (e.g., iconic or echoic memory). Provided sufficient attention is devoted to the stimulus, information from the short-term sensory store is then compared with previous patterns held in long-term memory to create a mental representation of the current state of the world. From there, individuals must decide if the information they glean requires a response or can simply be ignored until something significant occurs. But, let us assume for the moment that something important has happened, like an engine fire, and that a specific action is necessary to avert disaster. In this eventuality, information would then be passed to the response execution stage where the selection of appropriate motor programs would occur, enabling the pilot to activate the appropriate engine fire extinguishers. Still, the process doesn't stop there as the response is monitored via a sensory feedback loop, which in this case would ensure that the fire was put out, and if not, would stimulate the system to make the necessary modifications and adjustments until the situation was resolved. Using this four-stage model of information processing, Wickens and Flach (1988) proposed the general model of decision-making presented in Figure 2.2. In their model, an individual will sample a variety of cues in their environment to assess a given situation. These cues are then compared against a knowledge base contained within long-term memory so that an accurate diagnosis of the situation can take place. Then, given that a problem has been identified, choices have to be made regarding what action, or actions, should be taken. This process requires an evaluation of possible actions and utilizes risk assessment and criterion setting to ensure that an appropriate response will be employed. What's more, at any point in this decision-making process, individuals can seek out additional information (indicated by the lines to perception and attention) to improve situational assessment or enhance their response. Unfortunately, errors can arise at many points during this process. For example, cues can be absent or barely perceptible resulting in a poor or inaccurate assessment of the situation. Then again, individuals may correctly assess their current state of affairs, but choose the wrong solution or take unnecessary risks, resulting in failure. In fact, everything can be processed correctly and the right decision made, yet the pilot may not possess the skills necessary to avert disaster. Regardless of where the failure occurs, by capitalizing on our understanding of human information processing capabilities, decision-making models such as the one proposed by Wickens and Flach provide insight into why errors are committed, and why accidents happen. Using this same approach, Rasmussen (1982) developed a detailed taxonomic algorithm for classifying information processing failures. This algorithm, as employed within the context of aviation (e.g., O'Hare et al., 1994; Wiegmann and Shappell, 1997; Zotov, 1997), uses a six-step sequence to diagnose the underlying cognitive failure(s) responsible for an error (Figure 2.3). As described by O'Hare et al. in 1994, the algorithm includes stimulus detection, system diagnosis, goal setting, strategy selection, procedure adoption, and action stages, all of which can either fail independently or in conjunction with one another to cause an error. As one might expect, there is significant overlap between elements of Rasmussen's taxonomy and the four-stage model of information processing described earlier. For instance, Rasmussen's information processing errors correspond closely with the input of cues and short-term sensory storage of Wickens and Flach. Likewise, Rasmussen's diagnostic errors fit nicely with the pattern recognition stage, while goal, strategy and procedure errors are closely matched with decision-making and response selection. Finally, elements of Wickens and Flach's response execution stage are captured within Rasmussen's final category of action errors. Given the step-by-step, logical approach of cognitive models like the two presented above, this perspective remains popular among academicians and aviation psychologists for analyzing human error in complex systems. However, their appeal to those who actually do accident investigations is largely because they attempt to go beyond simply classifying "what" the aircrew did wrong (e.g., the pilot failed to lower the landing gear or the aircraft was flown into the terrain) to addressing the underlying causes of human error (e.g., the failure of attention, memory or specific types of decision errors). As a result, these cognitive models allow seemingly unrelated errors to be analyzed based on fundamental cognitive failures and scientific principles. Wiegmann and Shappell (1997), for example, used three cognitive models, including the four-stage model of information processing and the modified Rasmussen model to analyze over 4,500 pilot-causal factors associated with nearly 2,000 U.S. Naval aviation accidents. Although the models differed slightly in the types of errors that they captured, all three generally converged on the same conclusion. That is, judgment errors (e.g., decision making, goal setting and strategy selection errors) were associated more often with major accidents, while procedural and response execution errors were more likely to lead to minor accidents. These findings make intuitive sense if you consider them within the context of automobile accidents. For instance, if your timing is off a bit on the brake or your driving skill leaves something to be desired, our findings suggest that the odds are you are more likely to be involved in a minor fender-bender. On the other hand, if you elect to "run" a stoplight or drive at excessive speeds through a school zone, our findings would indicate that you are more likely to be involved in a major accident, or even worse, you may kill someone or yourself! But we were not the first ones to see this. In fact, findings similar to ours were found with other military (Diehl, 1992) and civilian aviation accidents (O'Hare et al., 1994; Jensen and Benel, 1977) using the cognitive approach. In the end, studies such as these have helped dispel the widely held belief that the only difference between a major accident and so-called "fender-bender" is little more than luck and timing. Indeed, it now appears to be much more. In theory, a better understanding of the types of cognitive failures that produce errors would in turn, allow for the identification and development of effective intervention and mitigation strategies. According to the cognitive perspective, these interventions would target the pilots' information processing capability. However, unlike computers that can be improved by simply upgrading the hardware, the information processing hardware of the human (i.e., the brain) is generally fixed inside the head. Therefore, in order to improve performance, cognitive psychologists typically attempt to capitalize on the manner in which pilots process information. For example, examining how expert pilots solve problems or distribute their attention in the cockpit can help scientists develop better methods for training novice aircrew. Another way of improving information processing is through the standardization of procedures and the use of checklists. These methods often facilitate information processing by reducing mental workload and task demands during normal operations and emergencies, thereby reducing the potential for errors and accidents. Nevertheless, as popular and useful as cognitive models are, they are not without their limitations where accident investigation is concerned. For instance, many cognitive theories are quite academic and difficult to translate into the applied world of error analysis and accident investigation. As a result, the application of these theoretical approaches often remains nebulous and requires analysts and investigators to rely as much on speculation and as they do on objective methods. What's more, cognitive models typically do not address contextual or task-related factors such as equipment design or environmental conditions like temperature, noise, and vibration. Nor do they consider conditions like fatigue, illness, and motivational factors, all of which impact pilot decision-making and information processing. Perhaps more important however, supervisory and other organizational factors that often impact performance are also overlooked by traditional cognitive models. Consequently, those that espouse the cognitive approach have been accused of encouraging an extreme, almost single-minded view that focuses solely on the operator (aircrew) as the "cause" of the error. This sort of single-mindedness often results in blame being unduly placed on the individual who committed the error rather than on its underlying causes which the individual may have little or no control over. Within the context of aviation, this view is sustained by those who regard pilots as the major cause of aircraft accidents or the weak link in the aviation safety chain. In effect then, pilots may be viewed as more dangerous than the aircraft they fly (Mason, 1993; cited in Murray, 1997). Clearly, such extreme views are detrimental to aviation safety in general, and may ultimately limit the advancement of the cognitive approach. The Ergonomic Perspective Now let us turn to the ergonomic or "systems perspective." According to this approach, the human is rarely, if ever, the sole cause of an error or accident. Rather, human performance involves a complex interaction of several factors including "the inseparable tie between individuals, their tools and machines, and their general work environment" (Heinrich, et al., 1980, p. 51). Perhaps the most well known of the systems perspectives is the SHEL model proposed by Edwards (1988), which describes four basic components necessary for successful man–machine integration and system design (Figure 2.4). SHEL, in this case, is an acronym representing the four components of the model, the first of which is software, represented by the letter "S". However, unlike the computer software we are all familiar with today, here software represents the rules and regulations that govern how a system operates. The "H," on the other hand, refers to the hardware associated with a given system, such as the equipment, material, and other physical assets. The "E" refers to the environment and was created to account for the physical working conditions that we as humans (liveware – symbolized by the letter L) are faced with. Edwards, recognizing that the four components of the SHEL model do not act in isolation, highlighted the interactions between components (indicated by the links in Figure 2.4). He felt that it was at the boundaries of these interfaces that many problems or mismatches occur. Within aviation for example, the focus has historically been on the liveware–hardware (better known as human–machine) interface, yielding significant improvements in cockpit layout and other so-called "knobs and dials" issues. In fact, the match between the human and the equipment within a given environment is viewed as so crucial to aircraft development today that human factors principles are often considered throughout the design process. However, even the two-dimensional interfaces between components do not sufficiently describe the SHEL model, as multi-dimensional models are more typical of normal day-to-day operations within a given system (represented by the multiple spheres in Figure 2.4). For example, with the development of datalink communications in aviation, the so-called liveware-hardware–liveware interface has been of great concern. In fact, before datalink is instituted, engineers and scientists will have to demonstrate that the proposed modifications to the liveware (air traffic controller) – hardware (datalink technology) – liveware (pilot) interface will enhance pilot–controller communications, or at a minimum produce no decrement in performance (Prinzo, 2001). Unfortunately, these multi-dimensional interactions are often hidden from the operator, producing opaque systems that, if not designed properly, can detract from the monitoring and diagnosing of system problems, thereby producing accidents (Reason, 1990). As popular as the SHEL model is, it is not the only ergonomics show in town. One alternative is the model of accident causation proposed by Firenze in 1971. His model is based on the premise that humans will make decisions based upon information they have acquired. Obviously, the better the information, the better the decision, and vice-versa. These decisions will allow the individual to take certain risks to complete a task (Figure 2.5, top). Like the SHEL model, Firenze's model predicts that system failures occur when there is a mismatch between the human, machine, and/or environmental components. But this assumes that the equipment (machine) functions properly and that the environment is conducive to a successful outcome. Problems arise when stressors such as anxiety, fatigue, and hazardous attitudes distort or impede the decision making process and lead to an accident (Figure 2.5, bottom). Clearly, improving information can prevent some accidents, but this may over-emphasize failures associated with the human, equipment, and/or environment – a point not lost on Firenze who felt that, "the probability of eliminating all failures where man interacts with machines is practically zero." Making matters worse, the environment often exacerbates whatever stressors an individual may be feeling at a given point in time. So if your goal is to reduce accidents, Firenze, and those that espouse his views, would argue that efforts must focus on the system as a whole, not just the human component. A close examination of the systems approach reveals some clear advantages over the cognitive failure models described earlier. For example, the systems perspective considers a variety of contextual and task-related factors that effect operator performance, including equipment design. In doing so, system models discourage analysts and investigators from focusing solely on the operator as the source or cause of errors. As a result, greater varieties of error prevention methods are available, including the possibility of designing systems that are more "error-tolerant." System approaches also have an intuitive appeal, particularly to those not formally trained in aviation psychology or human factors. In particular, approaches such as Edwards' SHEL model are very easy to comprehend, are relatively complete from an engineering point of view, and are generally well known across disciplines. In fact, in 1993, the International Civil Aviation Organization (the body governing aviation worldwide) recommended the use of the SHEL model as a framework for analyzing human factors during aviation accident investigations. Other organizations like the U.S. Air Force and Air Line Pilots Association have based portions of their investigative framework on this system as well. Nevertheless, even with their apparent popularity, the generality afforded by system models often comes at the cost of specificity. For instance, most system models lack any real sophistication when it comes to analyzing the human component of the system. Since system models focus on the interaction among components, emphasis is placed almost exclusively on the design aspects of the man—machine interface (e.g., the design of knobs, dials and displays), as well as the possible mismatch between the anthropometric requirements of the task and human characteristics. The effects of cognitive, social, and organizational factors therefore receive only tacit consideration, giving the impression that these components of the system are relatively unimportant. As a result, the systems perspective tends to promulgate the notion that all errors and accidents are design-induced and can therefore be engineered out of the system — a view not universally held within the aviation safety community. The Behavioral Perspective The behavioral perspective deals with the topic of pilot performance and aircrew error a bit differently than either the cognitive or ergonomic approaches. Rather than emphasizing an individual's ability to process information or how one integrates into the system as a whole, behaviorists believe that performance is guided by the drive to obtain rewards and avoid unpleasant consequences or punishments (Skinner, 1974). For example, the motivation-reward-satisfaction model proposed by Peterson in 1971, describes performance as dependent upon one's innate ability and motivation, which in turn is dependent upon a number of other factors (Figure 2.6). For instance, personnel selection plays a large role in determining whether someone has the aptitude to succeed; yet, without adequate training, performance will likely suffer. Likewise, motivation is critical to optimal performance regardless of where that motivation comes from — whether from the job, peers, unions, or internally derived. But motivation and ability alone cannot fully explain how people behave. Indeed, the cornerstone of Peterson's model is the extent to which individuals feel satisfied about their performance, which in turn is largely dependent on the rewards that they receive within an organization. Even one's sense of accomplishment and pride in a job well done can serve as a reward and thereby effect satisfaction. Ultimately, it is this feeling of satisfaction that motivates individuals to perform the same action again and again. Behavioral models like Peterson's have contributed greatly to our understanding, of how factors such as motivation, rewards, and past experience affect performance and safety. For instance, when individuals lack either the motivation to perform safely, or when conditions exist that reward unsafe actions, rather than those that are safe, accidents will likely occur. Even in the aviation industry, where safety is often highlighted, there are situational factors that reinforce unsafe behavior or punish individuals who emphasize safety at the expense of other organizational concerns. What's more, safe actions seldom lead to immediate tangible rewards, but serve only to prevent the occurrence of something aversive (e.g., crashing the airplane). It is not surprising then that some pilots have been known to bend, or worse yet, break the rules. As a result, recent years have witnessed the rise of behavioral-based safety programs that seek to reward safe behavior while at the same time enforcing the rules and holding aircrew and their supervisors accountable when unsafe acts occur. Even with its obvious benefits however, aviation safety professionals have never fully embraced the behavioral perspective. Still today, many question its applicability. This may be due in part to the fact that within the realm of aviation safety, the consequences of unsafe behavior are often fatal, and therefore it is hard to believe that someone would not be motivated to perform at their best. As Fuller (1997) has noted, "Perhaps we don't ask about motivation for air safety for the same reasons we don't ask about the motivation for breathing" (p. 175). Beyond that, it is hard to imagine how actions like misreading a flight management system (FMS) or forgetting to lower the landing gear can be linked to motivational factors. Still, there are unsafe acts that are obviously connected to motivation and should not be ignored during accident investigations. As a result, some human factors professionals and researchers, such as Reason (1990), have begun to distinguish between unsafe acts that are motivation-driven (i.e., violations) and those that are truly cognitive in nature (i.e., errors). Such a distinction is indeed important when it comes to developing interventions for reducing unsafe acts and improving safety. The Aeromedical Perspective Based largely upon the traditional medical model, the aeromedical perspective has been championed by those who feel that errors are merely the symptoms of an underlying mental or physiological condition such as illness or fatigue. The belief is that these so-called "pathogens" exist insidiously within the aircrew until they are triggered by environmental conditions or situations that promote their manifestation as symptoms (errors). In fact, some theorists believe that physiology affects virtually all aspects of safe behavior (Reinhart, 1996), and that the concept of being "medically airworthy" goes hand-in-hand with aviation safety and performance. Using traditional medical research methods, some safety experts have taken an epidemiological approach to analyzing accidents. The most common of which are those done by consumer product safety groups. One of the early epidemiological models of accident causation proposed by Suchman in 1961 is presented in Figure 2.7. Suchman's model is analogous to those used in medicine today to study the host, agent, and environmental factors that cause diseases. When applying the model, the investigator seeks an explanation for the occurrence of an accident within the host (accident victim), the agent (injury or damage deliverer), and environmental factors (physical, social and psychological characteristics of a particular accident setting). Above all else, the aeromedical approach highlights the crucial role that the physiological state of the pilot (i.e., the host) plays in safe performance and flight operations (Lauber, 1996). Although this may seem painfully obvious to those in the aerospace medicine community, others have not always taken the aeromedical perspective seriously. For example, while military pilots have long been taught about the adverse effects of hypoxia, decompression sickness, spatial disorientation, and other physiological factors by flight surgeons and aerospace physiologists, training in flight physiology within the civilian sector has typically been minimized. As a result, civilian pilots often have little respect for the significance of these factors within aviation (Reinhart, 1996). One aeromedical factor that has received considerable attention over the years in both military and civilian aviation is fatigue. As knowledge of the physiological underpinnings of circadian rhythms and jet lag has developed, an awareness of the adverse impact that fatigue has on aircrew performance has grown. This mounting appreciation was strengthened by the national transportation safety board (1994b) ruling that identified fatigue as a causal, rather than contributory, factor in an airline accident — one of the first rulings of its kind in the history of the Board. Without a doubt, the aeromedical community has taken the lead in shaping both the military's and industry's view of fatigue and has helped form policies on such contentious issues as work scheduling, shift-rotations, and crew-rest requirements. As with the other perspectives, the aeromedical approach is not without its critics. As mentioned above, some view pilot physiology and factors that influence it as relatively unimportant in the big picture of flight safety. This may be due to the fact that some pilots find it difficult to understand how adverse physiological states such as decompression sickness, trapped gases, and gravity-induced loss of consciousness (G-LOC) impact pilot performance in modern commercial and general aviation. Or even more unclear is how fatigued, self-medicated, or disoriented a pilot has to be before he or she commits an error that fatally jeopardizes the safety of flight. In short, it is not difficult to imagine how the presence of such factors may "contribute" to an error, but determining whether these factors "caused" an error or accident is another matter entirely. Although this "cause-effect" problem may seem trivial to some, to others in the aviation industry it weighs heavily on how resources and manpower are allocated to improve safety within their organizations. The Psychosocial Perspective The psychosocial perspective, unlike the others reviewed thus far, takes a more humanistic approach to behavior. Those that champion this approach view flight operations as a social endeavor that involves interactions among a variety of individuals, including pilots, air-traffic controllers, dispatchers, ground crew, maintenance personnel, and flight attendants. Incredibly, this cast of players from seemingly disparate organizations works closely together to ensure the level of safety we all enjoy in aviation today. Even the private pilot is seldom, if ever, entirely alone in the air or on the ground as air traffic control is only a button push away. These delicate, yet complex, interactions are at the center of the psychosocial perspective. Indeed, many aviation psychologists and safety professionals alike believe that pilot performance is directly influenced by the nature or quality of the interactions among group members (Helmreich and Foushee, 1993). These interactions in turn are influenced not only by the operating environment but also by the personalities and attitudes of individuals within each group. Given the inherent diversity and the sheer number of individuals involved day-to-day, one can only marvel at the precision and level of safety that modern aviation enjoys. According to this perspective, it is only when the delicate balance between group dynamics and interpersonal communications breaks down that errors and accidents occur (Figure 2.8). Historically, psychosocial models have been overlooked by those in the aviation industry (Kayten, 1993). In fact, it has only been in the past decade that aviation psychologists and accident investigators have truly begun to study the interpersonal aspects of human performance when examining aircrew errors. One such study involved an industry-wide analysis of aviation accidents and found that over 70 percent of all accidents resulted from aircrew coordination and communication problems (Lautman and Gallimore, 1987). However, this finding is not unique to commercial aviation. Aircrew coordination failures have been recognized as a major cause of military aviation accidents as well (Wiegmann and Shappell, 1999; Yacavone, 1993). As a result of these and other studies in the literature, many conventional engineering psychologists are now reaching beyond traditional design issues of the human–machine interface and beginning to address the exceedingly complex issues of human interpersonal relationships. Even those who promote the cognitive approach have begun to consider the possible impact that social factors have on processes such as decision making (Orasanu, 1993). As a direct result of the large number of accidents involving simple communication failures, more and more intervention strategies are being aimed at improving cockpit communications, including crew resource management (CRM) training. A staple within modern military and commercial aviation, cockpit resource management training involves educating and training aircrew to use techniques that enable individuals to communicate problems more effectively, divide task responsibilities during high workload situations, and resolve conflicts in the cockpit. In fact, one of the early aims of cockpit resource management training was to challenge and change pilots' traditional attitudes about differences in authority between the captain and the other aircrew (e.g., the co-pilot or first officer), an area that has been shown to impede communication and cause accidents (Wiegmann and Shappell, 1999). There is little debate that improvements in aircrew coordination and communication have reduced errors in the cockpit (Kern, 2001) and improved aviation safety. However, the psychosocial perspective hasn't always enjoyed the popularity within the aviation industry that it does today. This may be because many of the early approaches focused largely on personality variables rather than on crew coordination and communication issues that most contemporary approaches do. One of these early models included the concept of accident proneness, arguing that some individuals were simply predisposed toward making errors and causing accidents (Haddon et al., 1964). Today, however, the idea that accidents are inevitable among certain individuals is difficult for most theorists to accept. As a result, such fatalistic views have quickly fallen out of favor. But even more radical are those models that were based upon traditional psychoanalytic (Freudian) views of human behavior, which suggest that errors and accidents are caused by an individual's unconscious desire to harm others or to gratify unfulfilled sexual wishes. The following is an excerpt from Brenner (1964) illustrating this perspective: ...while driving her husband's car, [a woman], stopped so suddenly that the car behind her crumpled one of the rear fenders of the car she was in. The analysis of this mishap revealed a complicated set of unconscious motives. Apparently, three different, though related ones were present. For one thing, the [woman] was unconsciously angry at her husband because of the way he mistreated her. As she put it, he was always shoving her around. Smashing up his car was an unconscious expression of this anger, which she was unable to display openly and directly against him. For another thing, she felt very guilty as a result of what she unconsciously wanted to do to her husband in her rage at him and damaging his car was an excellent way to get him to punish her. As soon as the accident happened, she knew she was 'in for it.' For a third thing, the [woman] had strong sexual desires which her husband was unable to satisfy and which she herself had strongly repressed These unconscious, sexual wishes were symbolically gratified by having a man 'bang into [her] tail,' as she put it (p. 295). At the time, Brenner (1964) concluded that psychoanalytic explanations of accident causation were of "sufficient interest, influence, and plausibility to justify their scientific evaluation." However, in reality, such views have almost always been far outside the mainstream. Indeed, psychoanalytical models of accident causation and others like them were eventually rejected on both empirical and theoretical grounds. In fact, it can be argued that even current theories are at risk of suffering the same fate if more is not done to firm up the underlying psychosocial mechanisms that presumably lead to errors in the cockpit. With few exceptions (e.g., Helmreich and Foushee, 1993; Orasanu, 1993), little work has been done to empirically test predictions derived from psychosocial models of human error. Instead, most supporters of the psychosocial approach often reference the accident statistics cited earlier (e.g., Lautman and Gallimore, 1987; Wiegmann and Shappell, 1999; Yacavone, 1993) as confirmation of their perspective. However, these accident data are the very same data that were used to formulate such models and therefore, cannot logically be used again in reverse as supportive evidence. This lack of clarity is effected even more by the all-encompassing definition of cockpit resource management currently used in the industry, which describes cockpit resource management as the "effective use of all available resources [by the cockpit crew], including human resources, hardware, and information" (FAA, 1997, p. 2). As an anonymous reviewer once noted – given this "...broad definition, one might conclude that the only human error mishap [not caused by] cockpit resource management failures would be the deliberate crashing of the aircraft by a depressed or otherwise disturbed crew member." Indeed, what once appeared to be a useful concept has been expanded to a point where it may have lost some of its value. The Organizational Perspective Organizational approaches to understanding human error have been utilized in a variety of industrial settings for many years. However, it is only recently that the aviation community has embraced this point of view. This may be due to the fact that during the early days of aviation, emphasis was placed solely on the aircraft and those that flew them. Only now, are safety practitioners realizing the complex nature of accident/incident causation and the role organizations (not just aircrew and aircraft) play in the genesis and management of human error. In fact, it is the emphasis that organizational models place on the fallible decisions of managers, supervisors, and others in the organization that sets them apart from other perspectives. Perhaps the best-known organizational model of human error is the so-called "Domino Theory" described by Bird in 1974 (Figure 2.9). Bird's theory is based in large part on the premise that, "the occurrence of an [accident] is the natural culmination of a series of events or circumstances, which invariably occur in a fixed and logical order" (Heinrich et al., 1980, p. 23). That is, much like falling dominoes, Bird and others (Adams, 1976; Weaver, 1971) have described the cascading nature of human error beginning with the failure of management to control losses (not necessarily of the monetary sort) within the organization. Exactly how management does this is often difficult to put your finger on. What we do know is that virtually all managers are tasked with identifying and assigning work within the organization, establishing performance standards, measuring performance, and making corrections where appropriate to ensure that the job gets done. If management fails at any of these tasks, basic or underlying personal (e.g., inadequate knowledge/skill, physical and mental problems, etc.) and job-related factors (e.g., inadequate work standards, abnormal usage, etc.) will begin to appear. Often referred to as origins or root causes, these basic causes often lead to what Bird referred to as immediate causes that have historically been the focus of many safety programs. Specifically, immediate causes are those unsafe acts or conditions committed by employee/operators such as the unauthorized use of equipment, misuse of safety devices, and a veritable potpourri of other unsafe operations. Ultimately, it is these immediate causes that lead to accidents and injury. Several other organizational theorists have built upon Bird's Domino Theory, including the aforementioned Adams (1976) and Weaver (1971). For example, Adams renamed and expanded dominos one, two, and three to include elements of management structure, operational errors, and tactical errors respectively (Table 2.1). In so doing, Adams built upon Bird's original theory to more thoroughly address the relative contributions of employees, supervisors, and management to accident causation. Note, for example, that tactical errors focus primarily on employee behavior and working conditions, while operational errors are associated more with supervisory and manager behavior. Even domino one (Bird's "Loss Control") was modified to capture aspects of management structure that were not addressed by Bird. In many ways, what Adams really did was "operationalize" Bird's original ideas for use in industry – an approach still in vogue today in many settings. Weaver's (1971) contribution, although earlier than Bird's published ideas in 1974, viewed dominoes three, four and five as symptoms of underlying operational errors, much like Adams five years later. But Weaver's aim was to expose operational error by examining not only "what caused the accident", but also "why the unsafe act was permitted and whether supervisory-management had the safety knowledge to prevent the accident" (p. 24). In other words, was management knowledgeable of the laws, codes, and standards associated with safe operations; and if so, was there confusion on the part of employees regarding the goals of the organization, the roles and responsibilities of those participating in the work setting, accountability, and the like? Questions like these probe deeper into the underlying cause of operational errors, which all three theorists (Adams, Bird and Weaver) believe are founded in management. A slightly different approach has been proposed by Degani and Wiener (1994) for operations on the flight deck. As illustrated in Figure 2.10, their approach focuses on the relationship between the four "P's" of flight deck operations: 1) Management's philosophy or broad-based view about how they will conduct business; 2) Policies regarding how operations are to be performed; 3) Procedures and/or specifications concerning how certain actions are to be executed; and 4) Practices of aircrew as they perform their flight-related duties. According to Degani and Weiner, all of these factors interact to enhance flight safety. However, whenever ambiguous philosophies, policies, procedures, and practices exist, or when conflicts between the four "P's" arise, safety is jeopardized and accidents can occur. Consider, for example, the goal of most commercial airlines to have a safe and on-time departure. For obvious reasons, such a policy is critical to the success of any airline and deeply rooted within the management structure. Indeed, it is not surprising to see company slogans and advertisements promoting this philosophy. In translating philosophy into policy, many airlines have developed extensive plans to ensure that servicing of aircraft, routine maintenance, refueling, crew manning, and passenger and baggage loading all take place in a well orchestrated and lock-step fashion to ensure that the aircraft pushes back from the ramp "on-time." The procedures themselves are even more detailed as specific checks in the cockpit, sign-offs by ramp and gate personnel, and a variety of other safety and service checks are done in a very orderly and timely fashion. Ultimately however, the entire process is dependent on the men and women who perform these functions, and in doing so, put the philosophy, policies, and procedures of the organization into practice. That being said, the entire system can break down if, for example, the philosophy of the organization drives policies that are motivated more by profit than safety (e.g., an on-time departure at all costs). Misguided corporate attitudes such as these can lead to poor or misinterpreted procedures (e.g., abbreviated cockpit checklists or the absence of a thorough aircraft walk-around) and worse yet, unsafe practices by aircrew and other support personnel. Thankfully, this is rarely, if ever, the case. Traditional organizational theories like the Domino Theory and the Four P's have quickly gained acceptance within the aviation community and as with the other perspectives, have much to offer. The good news is that with the organizational perspective, aviation also gets the rich tradition and long established field of industrial and organizational (I/0) psychology. In fact, the methods that have proven valuable for error and accident prevention in aviation are not unlike those used in other industrial settings for controlling the quality, cost, and quantity of production (Heinrich et al., 1980). Consequently, the principles and methods developed and studied by 110 psychologists to improve worker behavior for decades (e.g., selection, training, incentives, and organizational design) should also be effective at reducing human error in aviation. Another advantage of the organizational approach is that it views all human error as something to be managed within the context of risk. The benefits of this operational risk management approach is that it allows the importance of specific errors to be determined objectively based on the relative amount of risk they impose on safe operations. This concept has not been lost on the U.S. military, as all branches utilize risk management in some fashion within their aviation and other operations. But even before the organizational perspective was considered within the aviation community, traditional I/O concepts had been employed for years. For example, to ensure that only skilled and safe pilots got into the cockpit, airlines and others within the aviation community employed the use of pilot selection tests. For those organizations that train their own pilots, as do most militaries around the world, these selection tests attempt to "weed out" those applicants who exhibit less than adequate mental aptitudes or psychomotor skills necessary for flying. Even organizations that hire pre-trained pilots (either from the military or general aviation sectors) often use background and flight experience as employment criteria, while others also use medical screenings and interviews to select their pilots. Another organizational approach, as illustrated by the Degani and Wiener (1994) model, to reducing errors in the cockpit is through the establishment of policies or rules that regulate what pilots can and cannot do in the cockpit. Such rules may restrict the type of weather in which pilots may operate their aircraft, or may limit the number of hours pilots can spend in the cockpit, in order to avoid the possible detrimental effects of fatigue on performance. By placing only safe and proficient pilots in the cockpit and limiting aircraft operations to only safe flying conditions, organizations are able to reduce the likelihood that pilots will make mistakes and cause accidents. Still, some have criticized that the "organizational causes" of operator errors are often several times removed, both physically and temporally, from the context in which the error is committed (e.g., the cockpit). As a result, there tends to be a great deal of difficulty linking organizational factors to operator or aircrew errors, particularly during accident investigations. Worse yet, little is known about the types of organizational variables that actually cause specific types of errors in the cockpit. Therefore, the practicality of an organizational approach for reducing or preventing operator error has been drawn into question. Furthermore, as with the other approaches described earlier, organizational models tend to focus almost exclusively on a single type of causal-factor (in this case, the fallible decisions of officials within the management hierarchy, such as line managers and supervisors) rather than the aircrew themselves. As a result, organizational models tend to foster the extreme view that "every accident, no matter how minor, is a failure of the organization" or that "...an accident is a reflection on management's ability to manage...even minor incidents are symptoms of management incompetence that may result in a major loss" (Ferry, 1988). Conclusion The preceding discussion of the different human-error perspectives was provided to help synthesize the different approaches or theories of human error in aviation. However, as mentioned earlier, there is no consensus within the field of aviation human factors regarding human error. Therefore, some human factors professionals may take issue, or at least partially disagree, with the way in which one or more of these perspectives and example frameworks were characterized or portrayed. Although this may provide academic fodder for those in the human factors field, that was not the intent of this chapter. Rather, the purpose was to address the question of whether any of these existing frameworks provides a foundation for conducting a comprehensive human error analysis of aviation accidents and incidents. The answer to the above question is clear. Although each human error perspective has its own strengths, each also has inherent weaknesses. Therefore, none of the perspectives reviewed, in and of themselves, were able to address the plethora of human causal factors associated with aviation accidents. So this leads us to the next logical question, "can an approach or model be developed that captures and capitalizes on the various strengths of each approach while eliminating or reducing their limitations?" We will explore the answer to this question in the next chapter. 3 The Human Factors Analysis and Classification System (HFACS) Several theorists and safety professionals have proposed "unifying frameworks" for integrating the diverse perspectives and models of human error described in the previous chapter (e.g., Degani and Wiener, 1994; Sanders and Shaw, 1988). While a few have enjoyed limited success, none has come close to the almost universal acceptance and praise that James Reason has received for his model of accident causation. The approach offered by Reason (1990) has literally revolutionized contemporary views of safety within aviation and throughout other industrial settings. For that reason alone, it is worth spending a few pages summarizing his perspective. Reason's Model of Accident Causation Elements of a Productive System Originally developed for the nuclear power industry, Reason's approach to accident causation is based on the assumption that there are fundamental elements of all organizations that must work together harmoniously if efficient and safe operations are to occur. Taken together, these elements comprise a "productive system" as depicted in Figure 3.1. Based on this model, the aviation industry can be viewed as a complex productive system whose "product" is the safe conduct of flight operations, regardless of whether it was for transportation, recreation, or national defense. As with any productive system, one of the key elements is the activity of front line operators (pilots, in the case of aviation) at the "pointy end" of the spear. These so-called "productive activities," in turn, require the effective integration of human and mechanical elements within the system, including among other things, effective pilot—cockpit interfaces so that safe flight operations can take place. Before productive activities can occur, certain "preconditions" such as reliable and well-maintained equipment, and a well-trained and professional workforce, need to exist. After all, few pilots are independently wealthy and own their own airline or fleet of aircraft. Rather, they dutifully work within a highly structured organization that requires effective management and careful supervision. Furthermore, such management and supervision is needed across numerous departments within the organization, including among others, operations, maintenance, and training. Even the best managers need guidance, personnel, and money to perform their duties effectively. This support comes from decision-makers who are even further up the chain-of-command, charged with setting goals and managing available resources. These same individuals have the unenviable task of balancing oft-competing goals of safety and productivity, which for airlines includes safe, on-time, cost-effective operations. Still, executive decisions are not made in a vacuum. Instead, they are typically based on social, economic, and political inputs coming from outside the organization, as well as feedback provided by managers and workers from within. In most organizations, the system functions well. But, what about those rare occasions when the wheels do come off? Now, the system that only moments earlier appeared safe and efficient, can find itself mired in doubt and mistrust by the workforce and those that it serves. Unfortunately, this is where many safety professionals are called in to pick up the pieces. Breakdown of a Productive System According to Reason, accidents occur when there are breakdowns in the interactions among the components involved in the production process. These failures degrade the integrity of the system making it more vulnerable to operational hazards, and hence more susceptible to catastrophic failures. As illustrated in Figure 3.2, these failures can be depicted as "holes" within the different layers of the system; thereby transforming what was once a productive process into a failed or broken down one. Given the image of Swiss cheese that this illustration generates, the theory is often referred to as the "Swiss cheese" model of accident causation. According to the "Swiss cheese" model, accident investigators must analyze all facets and levels of the system to understand fully the causes of an accident. For example, working backwards in time from the accident, the first level to be examined would be the unsafe acts of operators that have ultimately led to the accident. More commonly referred to in aviation as aircrew/pilot error, this level is where most accident investigations typically focus their efforts and consequently, where most causal factors are uncovered. After all, it is these active failures, or actions of the aircrew, that can be directly linked to the event. For instance, failing to lower the landing gear, or worse yet, improperly scanning the aircraft's instruments while flying in instrument meteorological conditions (IMC), may yield relatively immediate, and potentially grave, consequences. Represented as failed defenses or "holes" in the cheese, these active failures are typically the last unsafe acts committed by aircrew. However, what makes the "Swiss cheese" model particularly useful in accident investigation is that it forces investigators to address latent failures within the causal sequence of events as well. As their name suggests, latent failures, unlike their active counterparts, may lie dormant or undetected for hours, days, weeks, or even longer, until one day they adversely affect the unsuspecting aircrew. Consequently, investigators with even the best intentions may overlook them. Within this concept of latent failures, Reason described three more levels of human failure that contribute to the breakdown of a productive system. The first level involves conditions that directly affect operator performance. Referred to as preconditions for unsafe acts, this level involves conditions such as mental fatigue or improper communication and coordination practices, often referred to as crew resource management (CRM). Predictably, if fatigued aircrew fail to communicate and coordinate their activities with others in the cockpit or individuals external to the aircraft (e.g., air traffic control, maintenance, etc.), poor decisions are made and errors often result. But exactly why did communication and coordination break down in the first place? This is perhaps where Reason's work departs from traditional approaches to human error. In many instances, the breakdown in good cockpit resource management practices can be traced back to instances of unsafe supervision, the third level of human failure. If, for example, two inexperienced (and perhaps even, below average pilots) are paired with each other and sent on a flight into known adverse weather at night, is anyone really surprised by a tragic outcome? To make matters worse, if this questionable manning practice is coupled with the lack of quality cockpit resource management training, the potential for miscommunication and ultimately, aircrew errors, is magnified. In a sense then, the crew was "set up" for failure as crew coordination and ultimately performance would be compromised. This is not to lessen the role played by the aircrew, only that intervention and mitigation strategies might lie higher within the system. Reason's model did not stop at the supervisory level either; the organization itself can impact performance at all levels. For instance, in times of fiscal austerity, cash is at a premium, and as a result, training and sometimes even flight time are dramatically reduced. Consequently, supervisors are often left with no alternative but to task "non-proficient" aviators with complex tasks. Not surprisingly then, in the absence of good cockpit resource management training, communication and coordination failures will begin to appear as will a myriad of other preconditions, all of which will affect performance and elicit aircrew errors. Therefore, it makes sense that, if the accident rate is going to be reduced beyond current levels, investigators and analysts alike must examine the accident sequence in its entirety and expand it beyond the cockpit. Ultimately, causal factors at all levels within the organization must be addressed if any accident investigation and prevention system is going to succeed. Strengths and Limitations of Reason's Model It is easy to see how Reason's "Swiss cheese" model of human error integrates the human error perspectives described in Chapter 2 into a single unified framework. For example, the model is based on the premise that aviation operations can be viewed as a complex productive system (ergonomic perspective), that often breaks down because of ill-fated decisions made by upper level management and supervisors (organizational perspective). However, the impact that these fallible decisions have on safe operations may lie dormant for long periods of time until they produce unsafe operating conditions, such as poorly maintained equipment (ergonomic perspective), as well as unsafe aircrew conditions, such as fatigue (aeromedical perspective) or miscommunications among operators (psychosocial perspective). All of these factors, in turn affect an operators' ability to process information and perform efficiently (cognitive perspective). The result is often "pilot error," followed by an incident or accident. A limitation of Reason's model, however, is that it fails to identify the exact nature of the "holes" in the cheese. After all, as a safety officer or accident investigator, wouldn't you like to know what the holes in the "cheese" are? Wouldn't you like to know the types of organizational and supervisory failures that "trickle down" to produce failed defenses at the preconditions or unsafe acts level? It should also be noted that the original description of his model was geared toward academicians rather than practitioners. Indeed, some have suggested that the unsafe acts level as described by Reason and others was too theoretical. As a result, analysts, investigators, and other safety professionals have had a difficult time applying Reason's model to the "real-world" of aviation. This predicament is evidenced by ICAO's (1993) human-factors accident investigation manual. This manual describes Reason's model and touts it as a great advancement in our understanding of the human causes of aviation accidents. However, the manual then reverts to the SHEL model as a framework for investigating accidents. This is because Reason's model is primarily descriptive, not analytical. For the model to be systematically and effectively utilized as an analysis tool, the "holes in the cheese" need to be clearly defined. One needs to know what these system failures or "holes" are, so that they can be identified during accident investigations or better yet, detected and corrected before an accident occurs. Defining the Holes in the Cheese: The Human Factors Analysis and Classification System (HFACS) The Human Factors Analysis and Classification System (HFACS) was specifically developed to define the latent and active failures implicated in Reason's "Swiss cheese" model so it could be used as an accident investigation and analysis tool (Shappell and Wiegmann, 1997a; 1998; 1999; 2000a; 2001). The framework was developed and refined by analyzing hundreds of accident reports containing thousands of human causal factors. Although designed originally for use within the context of military aviation, human factor analysis and classification sytems has been shown to be effective within the civil aviation arena as well (Wiegmann and Shappell, 2001b). Specifically, human factor analysis and classification sytems describes four levels of failure, each of which corresponds to one of the four layers contained within Reason's model. These include: 1) Unsafe Acts, 2) Preconditions for Unsafe Acts, 3) Unsafe Supervision, and 4) Organizational Influences. The balance of this chapter describes the causal categories associated with each of these levels. Unsafe Acts of Operators The unsafe acts of operators can be loosely classified into two categories: errors and violations (Reason, 1990). In general, errors represent the mental or physical activities of individuals that fail to achieve their intended outcome. Not surprising, given the fact that humans by their very nature make errors, these unsafe acts dominate most accident databases. Violations, on the other hand, refer to the willful disregard for the rules and regulations that govern the safety of flight. The bane of many organizations, the prediction and prevention of these inexcusable and purely "preventable" unsafe acts, continue to elude managers and researchers alike. Still, distinguishing between errors and violations does not provide the level of granularity required of most accident investigations. Therefore, the categories of errors and violations were expanded here (Figure 3.3), as elsewhere (Reason, 1990; Rasmussen, 1982), to include three basic error types (skill-based, decision, and perceptual errors) and two forms of violations (routine and exceptional). Errors Skill-based errors. Skill-based behavior within the context of aviation is best described as "stick-and-rudder" and other basic flight skills that occur without significant conscious thought. As a result, these skill-based actions are particularly vulnerable to failures of attention and/or memory. In fact, attention failures have been linked to many skill-based errors such as the breakdown in visual scan patterns, task fixation, the inadvertent activation of controls, and the misordering of steps in a procedure, among others (Table 3.1). A classic example is an aircrew that becomes so fixated on trouble-shooting a burnt out warning light that they do not notice their fatal descent into the terrain. Perhaps a bit closer to home, consider the hapless soul who locks himself out of the car or misses his exit because he was either distracted, in a hurry, or daydreaming. These are all examples of attention failures that commonly occur during highly automatized behavior. While these attention/memory failures may be frustrating at home or driving around town, in the air, they can become catastrophic. In contrast to attention failures, memory failures often appear as omitted items in a checklist, place losing, or forgotten intentions. Indeed, these are common everyday occurrences for most of us. For example, who among us hasn't sent an email to someone with the intention of attaching a file, only to find out later that you forgot to attach the file? Likewise, many coffee drinkers have, at least one time in their life, brewed only water because they forgot to put coffee in the coffeemaker. If such errors can occur in seemingly benign situations such as these, it should come as no surprise that when under the stress of an inflight emergency, critical steps in emergency procedures can be missed. However, even when not particularly stressed, pilots have been known to forget to set the flaps on approach or lower the landing gear – at a minimum, an embarrassing gaffe. The third, and final, type of skill-based errors identified in many accident investigations involves technique errors. Regardless of one's training, experience, and educational background, the manner in which one carries out a specific sequence of actions may vary greatly. That is, two pilots with identical training, flight grades, and experience may differ significantly in the manner in which they maneuver their aircraft. While one pilot may fly smoothly with the grace of a soaring eagle, others may fly with the darting, rough transitions of a sparrow. Although both may be safe and equally adept at flying, the techniques they employ could set them up for specific failure modes. In fact, such techniques are as much a factor of innate ability and aptitude as they are an overt expression of one's own personality, making efforts at the prevention and mitigation of technique errors difficult, at best. Decision errors. The second error form, decision errors, represents intentional behavior that proceeds as planned, yet the plan itself proves inadequate or inappropriate for the situation (Table 3.1). Often referred to as "honest mistakes," these unsafe acts represent the actions or inactions of individuals whose "hearts are in the right place," but they either did not have the appropriate knowledge or just simply chose poorly. Perhaps the most heavily investigated of all error forms, decision errors can be grouped into three general categories: procedural errors, poor choices, and problem-solving errors. Procedural decision errors (Orasanu, 1993), or rule-based mistakes as described by Rasmussen (1982), occur during highly structured tasks of the sorts, if X, then do Y. Aviation, particularly within the military and commercial environments, by its very nature is highly structured, and consequently, much of pilot decision-making is procedural. There are very explicit procedures to be performed at virtually all phases of flight. Still, errors can, and often do, occur when a situation is either not recognized or misdiagnosed, and the wrong procedure is applied. This is particularly true when pilots are placed in time-critical emergencies like an engine malfunction on takeoff. However, even in aviation, not all situations have corresponding procedures to deal with them. Therefore, many situations require a choice to be made among multiple response options. Consider the pilot flying home after a long week away from the family who unexpectedly confronts a line of thunderstorms directly in his path. He can choose to fly around the weather, divert to another field until the weather passes, or penetrate the weather hoping to quickly transition through it. Confronted with situations such as this, choice decision errors (Orasanu, 1993), or knowledge-based mistakes as they are otherwise known (Rasmussen, 1982), may occur. This is particularly true when there is insufficient experience, time, or other outside pressures that may preclude safe decisions. Put simply, sometimes we chose well, and sometimes we do not. Finally, there are occasions when a problem is not well understood, and formal procedures and response options are not available. It is during these ill-defined situations that the invention of a novel solution is required. In a sense, individuals find themselves where they have not been before, and in many ways, must literally fly by the seat of their pants. Individuals placed in this situation must resort to slow and effortful reasoning processes where time is a luxury rarely afforded. Not surprisingly, while this type of decision-making is more infrequent than other forms, the relative proportion of problem-solving errors committed is markedly higher. Admittedly, there are a myriad of other ways to describe decision errors. In fact, numerous books have been written on the topic. However, the point here is that decision errors differ markedly from skill-based errors in that the former involve deliberate and conscious acts while the latter entail highly automatized behavior. Perceptual errors. Predictably, when one's perception of the world differs from reality, errors can, and often do, occur. Typically, perceptual errors occur when sensory input is either degraded or "unusual," as is the case with visual illusions and spatial disorientation or when aircrews simply misjudge the aircraft's altitude, attitude, or airspeed (Table 3.1). Visual illusions, for example, occur when the brain tries to "fill in the gaps" with what it feels belongs in a visually impoverished environment, such as that seen at night or when flying in adverse weather. Likewise, spatial disorientation occurs when the vestibular system cannot resolve one's orientation in space and therefore makes a "best guess" when visual (horizon) cues are absent. In either event, the unsuspecting individual is often left to make a decision that is based on faulty information, and the potential for committing an error is elevated. It is important to note, however, that it is not the illusion or disorientation that is classified as a perceptual error. Rather, it is the pilot's erroneous response to the illusion or disorientation. For example, many unsuspecting pilots have experienced "black-hole" approaches, only to fly a perfectly good aircraft into the terrain or water. This continues to occur, even though it is well known that flying at night over dark, featureless terrain (e.g., a lake or field devoid of trees), will produce the illusion that the aircraft is actually higher than it is. As a result, pilots are taught to rely on their primary instruments, rather than the outside world, particularly during the approach phase of flight. Even so, some pilots fail to monitor their instruments when flying at night. Tragically, these aircrew and others who have been fooled by illusions and other disorientating flight regimes may end up involved in a fatal aircraft accident. Violations By definition, errors occur within the rules and regulations espoused by an organization. In contrast, violations represent a willful disregard for the rules and regulations that govern safe flight and, fortunately, occur much less frequently since they often involve fatalities (Shappellet al., 1999). Routine Violations. While there are many ways to distinguish between types of violations, two distinct forms have been identified, based on their etiology, that will help the safety professional when identifying accident causal factors. The first, routine violations, tend to be habitual by nature and often tolerated by governing authority (Reason, 1990). Consider, for example, the individual who drives consistently 5-10 mph faster than allowed by law or someone who routinely flies in marginal weather when authorized for visual flight rules (VFR) only. While both are certainly against existing regulations, many others have done the same thing. Furthermore, those who regularly drive 64 mph in a 55-mph zone, almost always drive 64 mph in a 55-mph zone. That is, they "routinely" violate the speed limit. The same can typically be said of the pilot who routinely flies into marginal weather. What makes matters worse, these violations (commonly referred to as "bending" the rules) are often tolerated and, in effect, sanctioned by supervisory authority (i.e., you're not likely to get a traffic citation until you exceed the posted speed limit by more than 10 mph). If, however, the local authorities started handing out traffic citations for exceeding the speed limit on the highway by 9 mph or less (as is often done on military installations), then it is less likely that individuals would violate the rules. Therefore, by definition, if a routine violation is identified, one must look further up the supervisory chain to identify those individuals in authority who are not enforcing the rules. Exceptional Violations. On the other hand, unlike routine violations, exceptional violations appear as isolated departures from authority, not necessarily indicative of an individual's typical behavior pattern, nor condoned by management (Reason, 1990). For example, an isolated instance of driving 105 mph in a 55-mph zone is considered an exceptional violation since it is highly unlikely that the individual does this all the time. Likewise, flying under a bridge or engaging in other prohibited maneuvers, like low-level canyon running, would constitute an exceptional violation. However, it is important to note that, while most exceptional violations are heinous, they are not considered "exceptional" because of their extreme nature. Rather, they are considered exceptional because they are neither typical of the individual, nor condoned by authority. Still, what makes exceptional violations particularly difficult for any organization to deal with is that they are not indicative of an individual's behavioral repertoire and, as such, are particularly difficult to predict. In fact, when individuals are confronted with evidence of their dreadful behavior and asked to explain it, they are often left with little explanation. Indeed, those individuals who survived such excursions from the norm clearly knew that, if caught, dire consequences would follow. Nevertheless, defying all logic, many otherwise model citizens have been down this potentially tragic road. Preconditions for Unsafe Acts Arguably, the unsafe acts of aircrew can be directly linked to nearly 80% of all aviation accidents. However, simply focusing on unsafe acts is like focusing on a fever without understanding the underlying illness that is causing it. Thus, investigators must dig deeper into why the unsafe acts occurred in the first place. The process involves analyzing preconditions of unsafe acts, which includes the condition of the operators, environmental and personnel factors (Figure 3.4). Condition of Operators The condition of an individual can, and often does, influence performance on the job whether it is flying a plane, operating on a patient, or working on an assembly line. Unfortunately, this critical link in the chain of events leading up to an accident often goes unnoticed by investigators who have little formal training in human factors, psychology, or aerospace medicine. Still, it does not require a degree in any of those fields to thoroughly examine these potentially dangerous factors. Sometimes, it just takes pointing investigators in the right direction and letting their natural instincts take over. That is our purpose as we briefly describe three conditions of operators that directly impact performance: Adverse mental states, adverse physiological states, and physical/mental limitations. Adverse mental states. Being prepared mentally is critical in nearly every endeavor, but perhaps even more so in aviation. As such, the category of adverse mental states was created to account for those mental conditions that affect performance (Table 3.2). Principal among these are the loss of situational awareness, task fixation, distraction, and mental fatigue due to sleep loss or other stressors. Also included in this category are personality traits and pernicious attitudes such as overconfidence, complacency, and misplaced motivation. Predictably, if an individual is mentally tired for whatever reason, the likelihood that an error will occur increases. In a similar fashion, overconfidence and other hazardous attitudes such as arrogance and impulsivity will influence the likelihood that a violation will be committed. Clearly then, any framework of human error must account for these preexisting adverse mental states in the causal chain of events. Adverse physiological states. The second category, adverse physiological states, refers to those medical or physiological conditions that preclude safe operations (Table 3.2). Particularly important to aviation are such conditions as visual illusions and spatial disorientation as described earlier, as well as physical fatigue and the myriad of pharmacological and medical abnormalities known to affect performance. The effects of visual illusions and spatial disorientation are well known to most aviators. However, the effects on cockpit performance of simply being ill are less well known and often overlooked. Nearly all of us have gone to work sick, dosed with over-the-counter medications, and have generally performed well. Consider however, the pilot suffering from the common head cold. Unfortunately, most aviators view a head cold as only a minor inconvenience that can be easily remedied using over-the-counter antihistamines, acetaminophen, and other non-prescription pharmaceuticals. In fact, when confronted with a stuffy nose, aviators typically are only concerned with the effects of a painful sinus block as cabin altitude changes. But, it is not the overt symptoms that concern the local flight surgeon. Rather, it is the potential inner ear infection and increased likelihood of spatial disorientation while flying in instrument meteorological conditions that alarms them — not to mention the fatigue and sleep loss that often accompany an illness. Therefore, it is incumbent upon any safety professional to account for these sometimes subtle, yet potentially harmful medical conditions when investigating an accident or incident. Physical/Mental Limitations. The third and final category involves an individual's physical/mental limitations (Table 3.2). Specifically, this category refers to those instances when operational requirements exceed the capabilities of the individual at the controls. For example, the human visual system is severely limited at night; yet, automobile drivers do not necessarily slow down or take additional precautions while driving in the dark. In aviation, while slowing down is not really an option, paying additional attention to basic flight instruments and increasing one's vigilance will often add to the safety margin. Unfortunately, when precautions are not taken, the results can be catastrophic, as pilots will often fail to see other aircraft, obstacles, or power lines due to the size or contrast of the object in the visual field. There are also occasions when the time required to complete a task or maneuver exceeds one's ability. While individuals vary widely in their capacity to process and respond to information, pilots are typically noted for their ability to respond quickly. But faster does not always mean better. It is well documented, that if individuals are required to respond quickly (i.e., less time is available to consider all the possibilities or choices thoroughly), the probability of making an error goes up markedly. It should be no surprise then, that when faced with the need for rapid processing and reaction time, as is the case in many aviation emergencies, all forms of error would be exacerbated. Perhaps more important than these basic sensory and information processing limitations, there are at least two additional issues that need to be addressed – albeit they are often overlooked or avoided for political reasons by many safety professionals. These involve individuals who simply are not compatible with aviation, because they are either unsuited physically or do not possess the aptitude to fly. For instance, some people simply do not have the physical strength to operate in the potentially high-G environment of military or aerobatic aviation, or for anthropometric reasons, simply have difficulty reaching the controls or seeing out the windscreen. In other words, cockpits have traditionally not been designed with all shapes, sizes, and physical abilities in mind. Indeed, most cockpits have been designed around the average male, making flying particularly difficult for those less than 5 feet tall or over 6.5 feet tall. A much more sensitive topic to address as an accident investigator is the fact that not everyone has the mental ability or aptitude to fly. Just as not all of us can be concert pianists or NFL linebackers, not everyone has the innate ability to pilot an aircraft – a vocation that requires the unique, ability to make decisions quickly, on limited information, and correct the first time in life- threatening situations. This does not necessarily have anything to do with risk achievement worth intelligence. After all, we have argued that the eminent Albert Einstein would likely not have been a good pilot because, like many scientists, he always looked for the perfect answer, a luxury typically not afforded during an in-flight emergency. The difficult task for the safety professional is identifying whether aptitude might have contributed to the accident. Personnel Factors It is not difficult to envision how the condition of an operator can lead to the commission of unsafe acts. Nevertheless, there are a number of things that aircrew often do to themselves to create these preconditions for unsafe acts. We like to refer to them as personnel factors, and have divided them into two general categories: crew resource management and personal readiness. Crew Resource Management. Good communication skills and team coordination have been the mantra of I/O and personnel psychology for years. Not surprising then, crew resource management has been a cornerstone of aviation as well (Helmreich and Foushee, 1993). As a result, this category was created to account for occurrences of poor coordination among personnel (Table 3.2). Within the context of aviation, this includes coordination within and between aircraft, as well as with air traffic control, maintenance, and other support personnel. But aircrew coordination does not stop with the aircrew in flight. It also includes coordination before takeoff and after landing with the brief and debrief of the aircrew. It is not difficult to envision a scenario where the lack of crew coordination has led to confusion and poor decision-making in the cockpit. In fact, aviation accident databases are littered with instances of poor coordination among aircrew. One of the more tragic examples was the crash of a civilian airliner at night in the Florida Everglades as the crew was busily trying to troubleshoot what amounted to little more than a burnt out indicator light. Unfortunately, no one in the cockpit was monitoring the aircraft's altitude as the autopilot was inadvertently disconnected. Ideally, the crew would have coordinated the troubleshooting task ensuring that at least one crewmember was monitoring basic flight instruments and "flying" the aircraft. Tragically, this was not the case, as they entered a slow, unrecognized descent into the swamp resulting in numerous fatalities. Personal Readiness. In aviation, or for that matter in any occupational setting, individuals are expected to show up for work ready to perform at optimal levels. A breakdown in personal readiness can occur when individuals fail to prepare physically or mentally for duty (Table 3.2). For instance, violations of crew rest requirements, bottle-to-brief rules, and self-medicating will all affect performance on the job and are particularly detrimental in the aircraft. For instance, it is not hard to imagine that when individuals do not adhere to crew rest requirements, that they run the risk of suffering from mental fatigue and other adverse mental states, which ultimately lead to errors and accidents. Note however, that violations that affect personal readiness are not considered an "unsafe act, violation" since they typically do not happen in the cockpit, nor are they necessarily active failures with direct and immediate consequences. Still, not all personal readiness failures occur because rules or regulations have been broken. For example, jogging 10 miles before piloting an aircraft may not be against any existing regulations, yet it may impair the physical and mental capabilities of the individual enough to degrade performance and elicit unsafe acts. Likewise, the traditional "candy bar and coke" lunch of the military pilot may sound good, but is often not enough to sustain performance in the demanding environment of aviation. While there may be no rules governing such behavior, pilots must use good judgment when deciding whether they are "fit" to fly an aircraft. Environmental Factors In addition to personnel factors, environmental factors can also contribute to the substandard conditions of operators and hence to unsafe acts. Very broadly, these environmental factors can be captured within two general categories: the physical environment and the technological environment. Physical environment. The impact that the physical environment can have on aircrew has long been known and much has been documented in the literature on this topic (e.g., Nicogossian et al., 1994; Reinhart, 1996). The term physical environment refers to both the operational environment (e.g., weather, altitude, terrain), and the ambient environment, such as heat, vibration, lighting, toxins, etc. in the cockpit (Table 3.2). For example, as mentioned earlier, flying into adverse weather reduces visual cues, which can lead to spatial disorientation and perceptual errors. Other aspects of the physical environment, such as heat, can cause dehydration that reduces a pilot's concentration level, producing a subsequent slowing of decision-making processes or even the inability to control the aircraft. In military aircraft, and even occasionally during aerobatic flight in civil aircraft, acceleration forces can cause a restriction in blood flow to the brain, producing blurred vision or even unconsciousness. Furthermore, a loss of pressurization at high altitudes, or maneuvering at high altitudes without supplemental oxygen in unpressurized aircraft, can obviously result in hypoxia, which leads to delirium, confusion, and a host of unsafe acts. Technological environment. The technological environment that pilots often find themselves in can also have a tremendous impact on their performance. While the affect of some of these factors has been known for a long time, others have only recently received the attention they deserve. Within the context of HFACS, the term technological environment encompasses a variety of issues including the design of equipment and controls, display/interface characteristics, checklist layouts, task factors and automation (Table 3.2). For example, one of the classic design problems first discovered in aviation was the similarity between the controls used to raise and lower the flaps and those used to raise and lower the landing gear. Such similarities often caused confusion among pilots, resulting in the frequent raising of the landing gear while still on the ground. Needless to say, this made a seemingly routine task like taxiing for take-off much more exciting! A more recent problem with cockpit interfaces is the method used in some aircraft to communicate the location of a particular engine failure. Many of us have likely read accident reports or heard about pilots who experienced an engine failure in-flight and then inadvertently shut down the wrong engine, leaving them without a good propulsion system – an unenviable situation for any pilot to be in. After all, there is no worse feeling as a pilot than to be in a glider that only moments early was a powered airplane. The redesign of aircraft systems and the advent of more complex glass-cockpits have helped reduce a number of these problems associated with human error. However, they have also produced some new problems of their own. For example, human–automation interactions are extremely complex and frequently reveal nuances in human behavior that no one anticipated. Highly reliable automation, for instance, has been shown to induce adverse mental states such as over-trust and complacency, resulting in pilots following the instructions of the automation even when "common sense" suggests otherwise. In contrast, imperfectly reliable automation can often result in under-trust and disuse of automation even though aided performance is safer than unaided performance (Wickens and Hollands, 2000). Pilots turning off their traffic collision avoidance system (TCAS) because it often produces false alarms would be one example. In other cases, the interfaces associated with the automation can produce problems, such as the multiple modes associated with modern flight management systems (FMS). Pilots often suffer from "mode confusion" while interacting with these systems (Sarter and Woods, 1992). As a result, they may make dire decision errors and subsequently fly a perfectly good aircraft into the ground. Unsafe Supervision Recall that Reason's (1990) "Swiss cheese" model of accident causation includes supervisors who influence the condition of pilots and the type of environment they operate in. As such, we have identified four categories of unsafe supervision: Inadequate supervision, planned inappropriate operations, failure to correct a known problem, and supervisory violations (Figure 3.5). Each is described briefly below. Inadequate Supervision. The role of any supervisor is to provide their personnel the opportunity to succeed. To do this, they must provide guidance, training, leadership, oversight, incentives, or whatever it takes, to ensure that the job is done safely and efficiently. Unfortunately, this is not always easy, nor is it always done. For example, it is not difficult to conceive of a situation where adequate crew resource management training was either not provided, or the opportunity to attend such training was not afforded to a particular aircrew member. As such, aircrew coordination skills would likely be compromised, and if the aircraft were put into an adverse situation (an emergency, for instance), the risk of an error being committed would be exacerbated and the potential for an accident would increase significantly. In a similar vein, sound professional guidance and oversight are essential ingredients in any successful organization. While empowering individuals to make decisions and function independently is certainly important, this does not divorce the supervisor from accountability. The lack of guidance and oversight has proven to be a breeding ground for many of the violations that have crept into the cockpit. As such, any thorough investigation of accident causal factors must consider the role supervision plays (i.e., whether the supervision was inappropriate or did not occur at all) in the genesis of human error (Table 3.3). Planned Inappropriate Operations. Occasionally, the operational tempo and/or the scheduling of aircrew is such that individuals are put at unacceptable risk, crew rest is jeopardized, and ultimately performance is adversely affected. Such operations, though arguably unavoidable during emergencies, are otherwise regarded as unacceptable. Therefore, the second category of unsafe supervision, planned inappropriate operations, was created to account for these failures (Table 3.3). Consider, for example, the issue of improper crew pairing. It is well known that when very senior, dictatorial captains are paired with very junior, weak co-pilots, communication and coordination problems are likely to occur. Commonly referred to as the trans-cockpit authority gradient, such conditions likely contributed to the tragic crash of a commercial airliner into the Potomac River outside of Washington, DC, in January of 1982 (NTSB, 1982). In that accident, the captain of the aircraft repeatedly rebuffed the first officer when the latter indicated that the engine instruments did not appear normal. Undaunted, the captain continued a fatal takeoff in icing conditions with less than adequate takeoff thrust. Tragically, the aircraft stalled and plummeted into the icy river, killing the crew, and many of the passengers. Obviously, the captain and crew were held accountable – after all, they tragically died in the accident. But what was the role of the supervisory chain? Perhaps crew pairing was equally responsible. Although not specifically addressed in the report, such issues are clearly worth exploring in many accidents. In fact, in this particular instance, several other training and manning issues were also identified. Failure to Correct a Known Problem. The third category, failure to correct a known problem, refers to those instances when deficiencies among individuals, equipment, training or other related safety areas are "known" to the supervisor, yet are allowed to continue unabated (Table 3.3). For example, it is not uncommon for accident investigators to interview a pilot's friends, colleagues, and supervisors after a fatal crash only to find out that they "knew it would happen to him some day." If the supervisor knew that a pilot was incapable of flying safely, and allowed the flight anyway, he clearly did the pilot no favors. Some might even say that the failure to correct the behavior, either through remedial training or, if necessary, removal from flight status, essentially signed the pilot's death warrant – not to mention that of others who may have been on board. Likewise, the failure to consistently correct or discipline inappropriate behavior certainly fosters an unsafe atmosphere and promotes the violation of rules. Aviation history is rich with reports of aviators who tell hair-raising stories of their exploits and barnstorming low-level flights (the infamous "been there, done that"). While entertaining to some, they often serve to promulgate a perception of tolerance and "one-up-manship" until one day someone ties the low altitude flight record of ground-level! Indeed, the failure to report these unsafe tendencies and initiate corrective actions is yet another example of the failure to correct known problems. Supervisory Violations. Supervisory violations, on the other hand, are reserved for those instances when existing rules and regulations are willfully disregarded by supervisors (Table 3.3). Although arguably rare, supervisors have been known to occasionally violate the rules and doctrine when managing their assets. For instance, there have been occasions when individuals were permitted to operate an aircraft without current qualifications or license. Likewise, it can be argued that failing to enforce existing rules and regulations or flaunting authority are also violations at the supervisory level. While rare and possibly difficult to cull out, such practices are a flagrant violation of the rules and invariably set the stage for the tragic sequence of events that predictably follow. Organizational Influences As noted previously, fallible decisions of upper-level management can directly affect supervisory practices, as well as the conditions and actions of operators. Unfortunately, these organizational errors often go unnoticed by safety professionals, due in large part to the lack of a clear framework from which to investigate them. Generally speaking, the most elusive latent failures revolve around issues related to resource management, organizational climate, and operational processes, as detailed below and illustrated in Figure 3.6. Resource Management. This category encompasses the realm of corporate-level decision-making regarding the allocation and maintenance of organizational assets such as human resources (personnel), monetary assets, equipment, and facilities (Table 3.4). Generally speaking, corporate decisions about how such resources should be managed are typically based upon two, sometimes conflicting, objectives – the goal of safety and the goal of on-time, cost-effective operations. In times of relative prosperity, both objectives can be easily balanced and satisfied in full. However, as we mentioned earlier, there may also be times of fiscal austerity that demand some give-and-take between the two. Unfortunately, history tells us that safety and training are often the losers in such battles, and as such, the first to be cut in organizations having financial difficulties. Excessive cost-cutting could also result in reduced funding for new equipment, the purchase of low-cost, less effective alternatives, or worse yet, the lack of quality replacement parts for existing aircraft and support equipment. Consider this scenario recently played out in the military. While waiting on a back-ordered part, one of the squadron's aircraft is parked in the hangar in a down status. In the meantime, other aircraft in the squadron suffer failures to parts that are also not readily available from supply. Naturally, the creative maintenance officer orders that parts be scavenged from the aircraft in the hangar (facetiously referred to as the "hangar queen") and put on the other jets on the flightline to keep them fully operational. The problem is that the "hangar queen" has to be flown every few weeks if the squadron is going to be able to maintain its high level of readiness (albeit only on paper). So, the parts are taken off the aircraft on the line, put on the "hangar queen" so it can be flown around the pattern a couple of times. Then, the parts are taken off the "hangar queen," put back on the other aircraft and the process continues until replacement parts arrive, or something worse happens. Alas, most aircraft parts are not designed to be put on and taken off, repeatedly. Soon, the inevitable occurs and a critical part fails in flight causing an accident. As accident investigators, do we consider this unconventional approach to maintenance as causal to the accident? Certainly, but the lack of readily available replacement parts because of poor logistics and resource management within the organization is equally culpable. Organizational Climate. Organizational climate refers to a broad class of variables that influence worker performance (Table 3.4). Formally, it can be defined as the "situationally based consistencies in the organization's treatment of individuals" (Jones, 1988). While this may sound like psycho-babble to some, what it really means is that organizational climate can be viewed as the working atmosphere within the organization. One telltale sign of an organization's climate is its structure, as reflected in the chain-of-command, delegation of authority, communication channels, and formal accountability for actions. Just like in the cockpit, communication and coordination are also vital within an organization. If management and staff are not communicating, or if no one knows who is in charge, organizational safety clearly suffers and accidents can and will happen (Muchinsky, 1997). An organization's culture and policies are also important variables related to climate. Culture really refers to the unofficial or unspoken rules, values, attitudes, beliefs, and customs of an organization. Put simply, culture is "the way things really get done around here." In fact, you will see in Chapter 5 how the culture within the U.S. Navy/Marine Corps actually contributed to a number of Naval aviation accidents. Policies, on the other hand, are official guidelines that direct management's decisions about such things as hiring and firing, promotion, retention, sick leave, and a myriad of other issues important to the everyday business of the organization. When policies are ill-defined, adversarial, or conflicting, or when they are supplanted by unofficial rules and values, confusion abounds. Indeed, it is often the "unwritten policies" that are more interesting to accident investigators than the official ones. After all, it is safe to say that all commercial airlines have written policies on file that enable aircrew to request a relief pilot in the event they are too tired or ill to fly. While such policies exist on paper, there are some airlines whose "unwritten policies" make utilizing the relief pilot option difficult, and even career threatening in some instances. In fact, it can be argued that some corporate managers are quick to pay "lip service" to official safety policies while in the public eye, but then overlook such policies when operating behind the scenes. Organizational Process. This category refers to corporate decisions and rules that govern the everyday activities within an organization, including the establishment and use of standard operating procedures and formal methods for maintaining checks and balances (oversight) between the workforce and management (Table 3.4). Consider, for example, a young and inexperienced aircraft mechanic right out of school tasked with changing an engine on a military fighter aircraft. As he dutifully lays out his manual and begins changing the engine, following the procedures step-by-step, along comes the salty old crew chief with 25 years of experience in the field. During the ensuing conversation, the chief is heard to say, "Son, if you follow that book, we'll never get this finished on time. Let me show you how it's done." Unfortunately, rather than follow the procedures as outlined in the manual, the chief relies more on his own experiences and memory than on the actual procedures in the manual. Perhaps the procedures themselves are faulty and there is no way that an engine can be changed in the time allowed when using the manual. Nevertheless, the non-standard procedure the chief is using also introduces unwanted variability into the maintenance operation. While the latter requires a different sort of remedial action, the former implies that the procedures themselves may be flawed and points toward a failure within the organizational process. Other organizational factors such as operational tempo, time pressure, and work schedules are all variables that can adversely affect safety. As stated earlier, there may be instances when those within the upper echelon of an organization determine that it is necessary to increase the operational tempo to a point that overextends a supervisor's staffing capabilities. Therefore, a supervisor may have no recourse other than to utilize inadequate scheduling procedures that jeopardize crew rest or produce sub-optimal crew pairings, putting aircrew at an increased risk of a mishap. Clearly, organizations should have official procedures in place to address such contingencies, as well as oversight programs to monitor the risks. Regrettably, however, not all organizations have these procedures nor do they engage in an active process of monitoring aircrew errors and human factor problems via anonymous reporting systems and safety audits. As such, supervisors and managers are often unaware of the problems before an accident occurs. Conclusion Reason's Swiss cheese model provides a comprehensive theory of human error and accident causation. The Human Factors Analysis and Classification System (HFACS) was designed to define the "holes in the Swiss cheese" and to facilitate the application of this model to accident investigation and analysis. The purpose of this chapter, therefore, was to provide the reader with an overview of the categories contained within the framework. Figure 3.7 provides an illustration of all of these categories put together. We would like to emphasize that these categories were not just pulled out of thin air, or for that matter, out of a magician's hat. Nor were they developed only through brainstorming sessions with "expert" investigators. Rather, they were empirically derived and refined by analyzing hundreds of military and civil aviation accident reports that literally contained thousands of human causal factors. Still, human factor analysis and classification sytems must prove useful in the operational setting if it is to have any impact on aviation safety. Therefore, in the following chapters, we will demonstrate how human factor analysis and classification sytems can be used to investigate and analyze aviation accidents, as well as the new insights that can be gleaned from its application.