Title: Aircraft System Safety – Military and Civil Aeronautical Applications – Chapters 11 - 12 Author(s): Duane Kritzenger Category: System Safety Tags: Airworthiness, Aircraft, Safety, Chapter 11 Minimum equipment lists 11.1 Introduction For economic and/or operational requirements, an operator may require some leeway to enable flights to proceed with certain items of equipment or functions inoperative because: Σ systems and associated equipment suffer faults/failures, which will require maintenance time and effort to rectify Σ Some faults will be difficult to rectify before a scheduled flight. Others can only be rectified pending the incorporation of modifications. Σ Sometimes, the aircraft needs to be recovered to its main base, where repairs can more readily be made. However, at certification, the aircraft was designed to achieve a certain level of safety. When any one system, instrument, or equipment becomes inoperative, the designed level of safety is reduced. The question is, how can we be sure that under such conditions, the aircraft will continue to be operated safely? The solution lies in using the system safety assessment to define 'minimum equipment lists,' which will guide the operator in identifying allowable deficiencies, exposure times, and appropriate limitations of use. 11.2 The concept of minimum equipment lists The concept behind a minimum equipment list (MEL) is not new – aviation regulatory authorities have used it for many years (e.g., under the terminology 'Allowable Deficiencies, Go/No-go list, Dispatch Deviation Manual,' etc.). Initially, the concept was applied to allow operators to operate their aircraft with certain items of equipment or components inactive, provided that the Authority concerned was satisfied that an equivalent level of safety could be maintained either by (Christy, 1994): Σ introducing appropriate operating limitations. Σ transferring the function to another operating component or Σ refers to other instruments or components providing the required information. On early-generation aircraft, the systems employed were far less complex. Without recourse to extensive analysis, sound operational judgment was acceptable to decide which items of equipment or functions could be allowed to be inoperative at the time of dispatch. However, increased system complexity1 requires a more auditable approach to evaluate the remaining integrity of the aircraft system. If aircraft are to be allowed to operate with equipment or functions inoperative, it would seem logical for such allowable deviations to be taken into account in the safety assessment. AMC25.1309 (to CS25) advises that: A list may be developed of equipment and functions that need not be operative for flight based on stated compensating precautions that should be taken, e.g., operational or time limitations, flight crew procedures, or ground crew checks. The documents used to show compliance with CS 25.1309, together with any other relevant information, should be considered in the development of this list, which then becomes the basis for a Master Minimum Equipment List (MMEL). Experienced engineering and operational judgment should be applied during the development of the MMEL. The master minimum equipment list and minimum equipment list are alleviating documents: Σ The master minimum equipment list is a master list appropriate to an aircraft type that determines those instruments, items of equipment, or functions that, while maintaining the level of safety intended by the regulations, may temporarily be inoperative (refer to JARMMEL/MEL, 2000). The level of safety may be maintained due to: – the inherent redundancy of the design and/or – specified operational and maintenance procedures – specified conditions and limitations. Σ Depending on in-service experience, operational conditions, and maintenance procedures, the aircraft operator may wish to amend the master minimum equipment list by producing a minimum equipment list (MEL). This is allowed by the authorities on the condition that the minimum equipment list remains within the limitations of the master minimum equipment list (i.e., the minimum equipment list must not be less restrictive than the master minimum equipment list for the particular aircraft type. The minimum equipment list is (TGL 26, 2004) a joint operations and maintenance document prepared by an operator to: – identify the minimum equipment and conditions for an aircraft to maintain the Certificate of Airworthiness in force and to meet the operating rules for the type of operation – define operational procedures necessary to maintain an acceptable level of safety and to deal with inoperative equipment – define maintenance procedures necessary to maintain an acceptable level of safety and procedures necessary to secure any inoperative equipment. All items related to the airworthiness of the aircraft and not included in the list are automatically required for flight. Non-safety-related equipment (such as galley equipment, passenger, and convenience systems) need not be listed in the minimum equipment list and MMEL. 11.3 Generic approach The implications of the minimum equipment list requirements are that: Σ The safety assessment should highlight the level of criticality of the system and its function with respect to hazards that might arise in the event of a failure. Σ Items may be permitted to be inoperative, providing an equivalent level of safety is maintained by other reliable means (tailored from Christie (1994)): – There may be elements of redundancy that – although required for a safety measure – could be permitted to be inoperative for a short period (i.e., restricted flight time) without significantly affecting the required safety objectives. – In some instances, it may be possible to substitute redundancy with a maintenance check. – In some instances, it may be possible to reduce the severity of an adverse effect by restricting the aircraft's capability (e.g., limiting dispatch to VMC conditions). In practice, circumstances are not as clear-cut. Nevertheless, the general principles apply, and somehow, the manufacturer and Authority must be reasonably confident that these principles have been conscientiously applied when allowing an item of equipment or a function to be temporarily inoperative. 11.4 Process The following guidelines (tailored by Christie (1994)) are offered when considering the MEL/MMEL: Σ Establish the function or functions which make a system and its associated equipment 'safety critical' or 'safety significant' (i.e., those which lead to catastrophic and hazardous failure conditions).5 If the modification does not influence any of these functions, then no MEL/MMEL changes may be required. Σ Consider the effects of the occurrence of other probable events, not necessarily systems initiated (e.g., environmental conditions, time of day, daylight, darkness, etc.). Σ Take account of national operating regulations, which may require certain equipment or functions to be available at all times. Σ Check whether, in redundant systems, the non-availability of equipment or function will appreciably increase the likelihood of a hazardous event or whether there is sufficient built-in redundancy for such non-availability to have little or no adverse effect. Σ Consider whether increased maintenance inspections or pre-flight checks would provide adequate compensation (see Example 2). Σ Consider the practicality of temporary additions or changes to the flight crew procedures and assess the increase in workload thereof. 11.5 Equipment included in an MMEL/MEL Most aircraft are designed and certified with a significant amount of equipment redundancy, such that the airworthiness requirements are satisfied by a substantial margin. In addition, aircraft are generally fitted with equipment that is not required for safe operation under all operating conditions, e.g., instrument lighting in day VMC. Other equipment, such as entertainment systems or galley equipment, may be installed for passenger convenience. If this non-safety-related equipment does not affect the airworthiness or operation of the aircraft when inoperative, it need not be listed in the MMEL/MEL or be given a rectification interval. However, if the non-safety-related equipment has another function related to safety (such as the use of the entertainment system for passenger briefings), then this item must be included in the MMEL/MEL with an appropriate rectification interval. Put more simply, the MMEL/MEL lists required systems. The joint aviation authority decision process for inclusion of items in the minimum equipment list or master minimum equipment list is specified in JAR-MMEL/MEL and joint aviation authority TGM26. 11.6 Discussion The master minimum equipment list and associated minimum equipment list are alleviating documents. Their purpose is not to encourage the operation of aircraft with inoperative equipment. It is undesirable for aircraft to be dispatched with inoperative equipment, and such operations are permitted only as a result of careful analysis of each item to ensure that an acceptable level of safety is maintained. Fundamental considerations include: Σ All items related to the airworthiness of the aircraft and not included in the master minimum equipment list are automatically required to be operative prior to flight. Σ An operator or pilot retains the option to refuse any alleviation and may choose not to dispatch with any particular minimum equipment list item inoperative. Σ The continued operation of an aircraft under MEL/MMEL conditions should be minimized. Σ When considering redundancy techniques, system designers should provide for 'extra redundancy' in some systems to enable the aircraft to continue safe flight and landing with adequate safety margins (Lloyd and Tye, 1995, p. 147). Σ Where items are included in an existing master minimum equipment list or MEL, an account should be taken of them in the safety assessment (Lloyd and Tye, 1995, p. 147). Chapter 12 The safety management system 12.1 Introduction 12.1.1 Background A number of factors and inherent dangers exist that may influence the achievement of an acceptable level of system safety. Σ Aircraft are very complex and highly integrated with a multitude of critical systems involving interfaces between hardware, software, and operators. These configurations and interfaces are not stagnant and continue to evolve, introducing new situations and conditions. Σ Aircraft, especially military, are required to operate in very demanding environments. Actual testing under realistic environmental conditions is not possible in all cases. Σ Weight restrictions require aircraft designs to be optimized with minimum margins of safety. Σ Redundancy is often considered an unaffordable luxury, especially for military aircraft types. Σ Design restrictions often place limitations on safety measures. Σ During service life, the operational usage might change beyond that assumed in the original design and definition of the maintenance schedule. Σ Despite testing, unexpected hazardous conditions (such as flutter and store separation problems) may occur. Σ Cost-cutting measures may be implemented, e.g., extended maintenance intervals, less training, etc. Σ Other imperatives, such as mission accomplishment, available financial resources, and schedule constraints, may, at times, conflict with the technical airworthiness rules and standards. As a result, personnel associated with the design, manufacture, maintenance, and material support of aeronautical products may be exposed to an evolving, ever-changing level of risk. Until quite recently, only the people directly involved would have been held to blame for an accident. Now, it is recognized that safety is everybody's concern. However, whilst individuals are responsible for their own actions, only managers have the Authority and resources to correct the attitude and organizational deficiencies that commonly cause accidents. An accident is an indication of a failure on the part of management (David, 2002). What is required is an ordered approach to manage safety throughout the system's life cycle. This ordered approach is facilitated by the safety management system (SMS). This chapter provides some guidance on the philosophy and approach to a safety management system. 12.1.2 Regulatory requirements Various regulators require a safety management system, for instance: Σ JAR-OPS 1 (commercial air transport operation) and JAR-OPS 3 (rotorcraft operation) require that 'an operator shall establish an accident prevention and flight safety program to achieve and maintain risk awareness by all personnel involved with operations.' Σ The JAR OPS statement is derived from the international civil aviation organization recommended practice (Annex 5 part 1) for operators to have such a program in place. international civil aviation organization document 9422 (Accident Prevention Manual) gives appropriate guidance material and describes safety management systems. Σ The FAA's core approach to safety management is the air transport oversight system (ATOS). A key goal of ATOS is for the operator to implement its own system safety culture, including its own safety audits and self-correction programs. Σ The UK Health and Safety Executive requires all organizations to outline the overall philosophy, a chain of command, systems, and procedures in relation to health and safety management. 12.2 What is a safety management system? A company's safety management system (SMS) defines how the company intends to manage safety as an integral part of its business management activities. Profit (1999, p. 1) states that an safety management system is no more than a systematic and explicit approach to managing the risk of an accident (just as a quality management system is a systematic and explicit approach to improving a product or service). The prime purpose of an safety management system is to improve the level of safety by enabling the effective identification of hazards, the systematic introduction of control measures, and an audit trail for all of the safety-related decisions. This involves planning, organizing, monitoring, evaluating, and recording the arrangements for the management of safety. The actual content of an safety management system will be dependent upon each company's management system, but fundamentally, an safety management system has three basic characteristics: 1. A comprehensive corporate approach to managing safety, for instance: Σ leadership and commitment to safety Σ active involvement of top management Σ Clarity of policy objectives and safety improvement Σ Enhanced safety standards (relative to regulatory minima) Σ development and maintenance of a learning safety culture. 2. An effective organization for delivering safety, for instance: Σ committee structure for overseeing safety management Σ Management review mechanisms Σ Clarity of line management responsibilities Σ Coherent cascade of accountabilities for safety Σ role of accountable manager (CEO) and safety management system custodians Σ Change management process in place Σ effective competency and training requirements. 3. Robust systems for assuring safety, for instance: Σ Ensuring a pro-active approach to safety (e.g., through system safety assessments, safety cases, change assessments, etc.) Σ Maximizing the use of available information and a strong corporate knowledgebase for safety data exchange, training, and awareness. Σ Structured monitoring of safety management system compliance (e.g., safety/quality audits, process and practice monitoring, incident investigation, and follow-up to maintain or improve safety). 12.3 Safety culture Safety culture is (Kuo, 1995) 'The belief or philosophy on safety matters held by organizations and individuals, which is demonstrated in practice through their attitudes, actions, and behavior. An organization's safety culture becomes evident in 'the way we do things around here when no one is looking' (Matthew S., head of the Flight Safety Foundation). A safety culture is an attitude that exists when everyone recognizes and accepts their responsibilities for safety; the organization 'thinks safety' as a matter of course; and management realizes that the safety achievement of a system is not static and it may tend to degrade over time (e.g.,, as people become complacent or less vigilant, or when systems start to age). Example: Challenger Shuttle 'Shuttle program management made erroneous assumptions about the robustness of a system based on prior success rather than on dependable engineering data and rigorous testing.' Safety culture is significantly influenced by the following philosophical extremes (refer, inter alia, Kuo (1997a, Ch. 8)): Σ Blaming philosophy. When a failure/hazard occurs, someone must be at fault and should be punished. Although the guilty are punished and the immediate cause is found, this approach leads to the following: – a defensive attitude (due to a culture of blame and defensiveness) – The systematic causes behind the incident (e.g., management failures, commercial pressures, insufficient training, unsafe systems/processes, etc.) are often ignored. Reports of equipment failures, design faults, or procedures that might cause a hazard must be encouraged without the threat of disciplinary action wherever possible. An effective safety culture requires an atmosphere in which individuals are not unduly punished or blamed for their mistakes – a 'blame-free' environment. This is an ideal that is difficult to achieve in practice; when things really go wrong, people often react to protect themselves by pointing the finger of blame at others. Σ Collaborative philosophy. Best solutions are usually derived via close collaboration between the prescribing authorities, users, suppliers, and other stakeholders. This approach encourages shared responsibility and a willingness to improve safety. A collaborative safety culture is the most desired foundation for an effective SMS. It recognizes that there is no panacea for safety, that safety requires time to develop, and that factors keep changing (e.g., systems age, competence levels fluctuate, etc.). Even an organization that strives to achieve a blame-free environment is still subject to rules and legal regulation. A 'just' culture is one in which individuals are not free of blame if they are culpably negligent and where the organization gives due regard to honesty (David, 2002). Culture is the sense of values, beliefs, and norms that are practiced in business. Management creates culture, and it is the management's responsibility to influence it. Management has great leverage in affecting safety within an organization; through its attitudes and actions, management influences the attitudes and actions of line managers, who in turn repeat it to their employees (be they inspectors, quality control personnel, designers, operational personnel, etc.). A safety assessment must not be viewed as a one-off exercise; people should be continuously trying to make things safer. Errors and mistakes are inevitable, and safety can only be improved if the organization learns from its mistakes. The safety management system thus needs to enforce a culture of communication and continuous improvement. There are several ways to achieve this, for example: Σ Incident reporting, investigation, and feedback Σ Safety reviews and audits Σ Safety working groups and safety panels Σ Suggestion schemes which cover safety Σ incident reporting (including some sort of anonymous reporting scheme). 12.4 Developing a safety management system 12.4.1 Introduction Safety management is part of the overall management function, which determines and implements an organization's safety policy. It is the means by which the management principle is translated into front-line safety performance improvement activities. The implementation of a safety management system should thus follow a top-down program which ensures that: Σ safety policies are defined. These statements should define the organization's fundamental approach to the management of safety and should commit the organization at the highest level to the fulfillment of its stated safety policy. Σ from the policy statements, the organization should define its safety management principles, which specify the safety objectives with which the organization intends to comply. Σ Having defined the policy statement and principles, the organization should produce plans and procedures and define responsibilities that will ensure that the safety objectives are accomplished. Note that there is an ever-present danger of creating formal 'policies' or procedures to respond to each regulator or to each new hazard (Jenkins (1999), p4). At a strategic level, the safety aspiration and principles (policies) are common to all processes and hazards, whilst at the detailed level, the implementation is specific and tailored to the process and hazard. Organizations should take a pragmatic approach, building on existing procedures and practices. (particularly quality management). Σ procedures should be supported by advisory/ guidance material, working instructions, checklists, templates, etc. The advantage of keeping these separate from the procedures is that they should be easily tailored and improved upon as – part of the development of corporate memory – encouraging a safety culture – striving for a 'best-practice' approach. A fully-fledged safety management system is a formalized, company-wide system. It should be traceable from the aim of the policy statements through to the principles, individual responsibilities, and detailed procedures and instructions. This process is illustrated in Fig. 12.1. Where safety-sensitive functions are outsourced (e.g., maintenance), contractual arrangements should identify the need for an equivalent safety management system in the supplier's organization. 12.4.2 Safety Management Policy Statements The policy statements should define the fundamental approach to be adopted for managing safety and the organization's commitment to safety. The following bulleted items (tailored from SRG (1999)) provide typical safety policy topics: Σ Safety objective. Declare a top-level commitment to a business objective for safety that minimizes its contribution to accident risk as low as reasonably practicable. Rationale. This should be the key policy statement defining what the organization is striving to achieve through its safety management system. Σ Safety management. Make a commitment to the adoption of an explicit, proactive approach to systematic safety management. Rationale. An intuitive or ad hoc approach to safety is not acceptable. Σ Safety responsibility. Make a policy statement that confirms that everyone has an individual responsibility for the safety of their own actions and that managers are accountable for the safety performance of the activities, products, services, etc., in their charge. Flow down to departmental expositions, who is ultimately accountable for safety, and how that accountability is delegated. Rationale The safety management system depends upon individuals understanding and accepting their delegated responsibility within the organization. Accountability for safety belongs to all levels of management, and the attainment of satisfactory safety performance requires the commitment and participation of all members of the organization. Everybody within an organization should be aware of the consequences of mistakes and strive to avoid them. Management should foster this basic motivation within members of an organization so that everybody accepts their responsibility for safety. Σ Safety priority. Commit the organization to ensure that safety is given the highest priority when considering commercial, operational, environmental, or social pressures. Rationale: The safety management system should clearly address and resist misguided business pressures. Conversely, the safety management system should ensure that safety is not used to support commercial, financial, environmental, etc., decisions inappropriately, which have little real safety significance. If the term' safety' is abused in this way, the safety management system cannot be focused on controlling the real risks. Σ Safety standards and compliance. Commit the organization to complying with all appropriate safety standards and requirements (see Chapter 3 for more information). Rationale. Compliance with safety standards and requirements can form part of a robust safety argument and facilitate the safety assessment process. Σ Externally supplied products and services. Commit the organization to ensure that the safety assurance processes used by its external suppliers satisfy its own safety management standards and safety requirements. Rationale. A safety assessment requires input from all phases of a product or service development. For externally supplied products or services, the external supplier must understand and comply with the organization's safety and safety management system requirements. 12.4.3 Safety Management Principles The following safety management principles (tailored from SRG (1999)) define the scope of a safety management system, provide a framework for the establishment of processes to identify safety shortcomings, and provide assurance that safety levels are being met or improved. These principles must be supported by referring to the applicable procedures that ensure their execution. Σ Safety criteria. Whenever practicable, safety targets should be derived, maintained, and improved for all products and services (see Chapter 4, Chapter 5, and Appendix B for more information). Rationale. If the safety performance of a service or product is to be assessed and monitored, it is necessary to define the safety objectives that need to be met. Σ System safety assessments. All new/modified systems should be subjected to some sort of safety assessment (see Chapter 8 for more information). Rationale. The analysis process is conducted during the development of the system to establish safety requirements. The safety assessment process is used to demonstrate that these requirements are met. Σ Safety case. An organization should assess all existing operations and proposed changes/additions/replacements for their safety significance (see Chapter 9 for more information). For those areas where the probability of the accident occurring may be impacted, formal safety assessments should be conducted. Rationale. Engineering alone cannot guarantee safety. Systems evolve, as do their operational applications. Procedures and maintenance do affect safety. Frequent training can improve effectiveness. Σ Safety records. An organization should identify and record the safety requirements for a service or product, the results of the safety assessment process, and evidence that the safety requirements have been met. These records need to be maintained throughout the life of the service or product. Rationale. The safety assessment documentation should provide evidence to the organization upon which it will base its decision on whether it is safe to use the service or product. Maintenance of these records throughout the life of the service or product provides ongoing assurance that it continues to meet its original safety requirements and that any remaining risks are adequately controlled. Σ Competency. Each department in the organization should ensure that staff remain adequately trained, competent, and qualified for the job they are required to do. Rationale. Staff competence is fundamental to safety. 12.4.4 Safety management plans and procedures Safety management plans and procedures should specify the activities that need to be conducted in order to execute the safety management system policy and principles. Typical latent failures in management include inadequate procedures, poor scheduling and allocation of resources, and neglect of recognized problems. Plans and procedures are needed that clearly stipulate lifecycle milestones as well as responsibility allocation. The following topics should typically be addressed by safety procedures and plans: Σ Lifecycle safety activities. The procedures should stipulate the safety activities that typically need to be conducted during the various phases of the system's life. These procedures could be formulated around either (or both) the safety case approach (see Chapter 10) or the system safety analysis approach (see Chapter 9). Rationale. Clearly defined activities and milestones (both during the development lifecycle as well as operational application) are essential requirements for a proactive approach to safety management. Σ Safety monitoring. An organization should have suitable monitoring arrangements in place so that unacceptable trends in service or product performance can be recognized and subject to remedial action (SRG, 1999). Rationale. Service and product performance can deteriorate, or the environment within which they operate can change. Such changes need to be detected, assessed, and managed. Σ Safety significant events. Studies from a range of industries have shown that there is consistently a much greater number of less serious incidents than those that lead to an injury (David, 2002). Often, it was only a matter of chance that these near misses or non-injury accidents would not harm people. Incidents and accidents should be investigated immediately, and any necessary corrective action should be taken. Rationale. Information on real accidents and incidents, whether or not they actually caused damage, provides the opportunity to learn about actual problems and improve safety. Figure 12.2 illustrates the 'iceberg' of incident statistics, where the large bulk of learning opportunities lie below the surface of obvious accidents. Σ Safety audits. Organizations should routinely carry out safety audits to identify opportunities for improvement, to provide management with assurance of the safety of activities, and to confirm conformance with the safety management system (SRG 1999). Rationale. This should be a routine part of business activity. This is the proactive safety management mechanism by which any potential risks associated with an existing service or product can be identified and controlled. Σ Incident planning. Procedures should be defined to deal with the unfortunate occurrence of incidents and accidents (see bow-tie analysis in Appendix A). Rationale. Recovery procedures will limit loss of life and resources. It will also ensure that the organization knows what to do in the event of an accident investigation/board of inquiry. 12.4.5 Safety instructions/guidance Safety instructions, templates, checklists, guidance material, databases, etc., must evolve within the organization to facilitate business efficiency. These instructions and guidance should empower (not restrict) individuals to accomplish the following: Σ Safety improvement. An organization should have arrangements in place that actively encourage staff to identify system and process inefficiencies and propose solutions. Rationale. This requires an effective means of communicating safety issues and the development of an internal safety culture that encourages every member of staff to focus on the achievement of safety and to report errors and deficiencies without fear of punitive actions against them. Σ Lesson dissemination. An organization should ensure that lessons learned from its safety assessments, hazard logs, safety occurrence investigations, case histories, experience from other organizations, etc., are distributed widely and actioned to minimize the probability of recurrence and to design more error-tolerant and effective systems. Rationale. Few would argue that an effective and widespread learning process is essential to ensure that error management within safety-critical systems is continually informed and improved. Including the results of such lessons in training programs, safety review bulletin, etc., will raise staff awareness levels. These tools should provide a means to eliminate unnecessary work by establishing corporate memory, reducing program risks, and avoiding repeating errors/risks. By ensuring salient lessons are learned throughout the company, the error-management process can be better informed throughout the product lifecycle. 12.5 Discussion Safety management is concerned with having a consistent approach to potential causes of harm and targeting efforts where it will have the most benefit. The safety management system provides the following: Σ A comprehensive corporate approach to safety Σ an effective organization for delivering safety Σ robust systems for assuring systematic safety. To develop an SMS, a company must: Σ Gain top management commitment and involvement Σ ensure that safety policy makes the priority safety explicit Σ initiate steps to build a learning safety culture in the company Σ Build the company's hazard register Σ define the criteria to be used for risk assessment and hazard management Σ Document the safety case/assessments Σ train the staff and management. safety management system has a significant part to play in improving safety performance in an organization – especially if it involves everyone in the company, from totally honest top management dedicated to its success to the most diligent hangar sweepers. Moreover, the adoption of a formal safety management system by operators and service providers makes the safety regulatory function considerably more effective.4 It facilitates the safety monitoring and approval roles of the regulator and makes the task of assessing an organization's corporate safety competence much easier (Profit, 1999, p. 10). The safety management system approach also reflects the general trend in safety regulation to evolve towards a performance-based approach, in other words, specifying what a desired outcome should be and not prescribing in detail how to do it. What an safety management system does is provide a framework for an organization to take responsibility for its own activities rather than rely purely on compliance with ever more detailed, prescriptive safety regulations. By moving beyond mere compliance with regulatory requirements and proactively using best practices, the risk of causing injury to people, damage to property, or harm to the environment should be significantly reduced. Not only does this provide a management framework for controlling risks, but it also enables the regulator to focus resources on the areas of highest risk rather than merely inspecting compliance against predictive safety requirements. Chapter 13 Concluding observations 13.1 Aviation trends Aircraft flight has been transformed from an adventurous activity enjoyed by a select few to a stable mass-market service industry which is largely taken for granted ... until things go wrong. The industry is then dominated by public perception of risk and the social amplification thereof. Accidents resulting in hull loss often result in fatalities and are almost always treated to extensive coverage in the national, if not world-wide, press. The accident rate has been essentially constant for at least 20 years. However, if the accident rate remains constant, and airline traffic grows at the projected rate, then the number of hull loss accidents worldwide would reach almost one per week by the year 2015. This is something that the public will not accept,2 implying a limited growth scenario for airline traffic – unless something is done to reduce the hull loss or fatal accident rate. What is needed is a revised relationship between management and safety. The aircraft industry is set to become more complex, the skies more crowded, and the budgetary pressure will increase. A new impetus must be found in pro-safety activity if the high confidence of the public is to be maintained, let alone improved, through the impending doubling of traffic by 2020 and beyond. It will not be sufficient to increase the reliability of technical systems alone. It is well known that, after controlled flight into terrain (CFIT), fire (and the accompanying toxic fumes) is the most common cause of aircraft fatalities. Furthermore, the accident rate is a function of many factors, which include human performance, weather, design, operation, training, maintenance, and airspace system infrastructure. Regulatory authorities, operators and maintainers need to enforce a proactive approach to safety, whereby the safety management system not only ensures that the intended level of safety remains intact, but also that trends are monitored and used to make improvements before an accident or incident occurs. Trends can be monitored via internal programmes such as FRACAS/DRACAS (see Appendix A) as well as via data-sharing programmes such as Flight Operations Quality Assurance (FOQA), Aviation Safety Action Partnerships (ASAP) and accident databases. 13.2 Safety assessments/safety cases Initially there were some misgivings about moving to the more disciplined safety management approach. The need for safety management plans, safety cases, etc. all gave rise to concerns over the resources that would have to be deployed on such planning, assessment and reporting activities whose value was difficult, initially, to appreciate. Different projects use a variety of safety tools/techniques in numerous combinations. There is much guidance material and standards available on this subject (e.g. JSP318B, ARP4761, etc.). Often these are not agreed upon before contract closure and are used ‘after the fact’ to satisfy safety questions, and not as a useful tool to influence and optimise the design. The real challenge is to recognise that safety management is not a bolt-on extra, but to arrange all activities within the context of a safety management system. For instance: Σ The safety management plan needs to be prepared as part of the project’s throughlife management planning process (Dallimore, 2003). This plan should prevent safety assessment activities from becoming fragmented/disjointed thoughout the system’s life cycle, i.e., from the specification stage through implementation, verification, operation, maintenance and decommissioning (Mauri, 2000). The SMP must co-ordinate and facilitate all safety activities ‘from the cradle to the grave’. Σ The safety assessment/case needs to evolve in a planned and structured manner through life so that it supports all stages of the programme, informs all the key decision makers and provides a clear audit trail to support the safety claims at all stages of the project from concept to disposal (Dallimore, 2003). Σ Ensure consistency of results by integrating and relating all aspects of the safety assessment (e.g., hardware analysis with software analysis; low-level probability studies with the high-level functional failure analysis) (Muari, 2000). Σ Safety assessment activities should not cease upon system certification. There needs to be a proper interchange of information between the aircraft manufacturer and operators, so that: – modes of failure and critical failure rates that occur in service can be checked against the predictions. If either particular failure mode or its effect has not been correctly predicted it is important that the aircraft constructor should know so that he can consider whether the implications are serious. – alterations to checks and maintenance periods can be substantiated by the analysis. – modification can be assessed against the assumptions and limitations of the original design. – a sound minimum equipment list can be maintained and amended according to experience. – actual mean time between failures experienced will probably differ from predicted MTBFs used in fault tree analysis calculations during initial certification. Direct advantages are possible if the fault tree analysis can be updated with actual mean time between failures data. Action can be taken to improve safety or (if the predicted MTBFs were very conservative) maintenance intervals can be increased. Benefits of adopting this approach will include (refer, inter alia, Dallimore, 2003): Σ Early planning helps identify and deal with those safety-related risks that, if not resolved up front, can emerge late in the program and give rise to cost escalation and delay. Σ The as low as reasonably possible approach can assist in setting priorities for investment by indicating which opportunities for designing out a safety risk or adding a safety feature give most benefit. Σ The through-life approach assists judging the value that a proposed investment in safety adds to the provision of the desired operational capability. Σ The audit trail provides a sound, and readily available, basis for defending the various investment and safety decisions when called upon to do so. The point of system safety is to prevent accidents/incidents (the difference between which lies only in the result) before they occur. Safety lessons are there to be learned so as to prevent the next incident becoming an accident. Problems affecting safety must be identified at the earliest possible stage to allow progression of the most cost-effective and efficient solution. Failure to consider and anticipate possible problems at an early stage can be an expensive omission. These risks can be as much of a hazard to the programme as other technical and commercial risks. It is not the identified hazard that is the problem. If a hazard has been identified it can be measured; it can be fixed and controlled. It is the unidentified hazard that is the problem. A hazard not identified is a hazard not managed. If it cannot be identified, it cannot be measured. If it cannot be measured it cannot be controlled. A safety assessment/safety case is therefore not just what has been done – it is also about how it has been done, and why no more is needed to be done. There is no standard, correct and formal way to analyse system safety, there is always the need for human judgement. Engineers have always used judgement for safety issues. Professional judgement continues to be by far the most important part of safety management. Formal safety assessment methods must be used as airborne integrated data system to judgement and not as substitutes for it. Actions and decisions may be challenged by others with hindsight. A decision may have to be defended on the basis of judgement and so the decision process needs to be documented and validated wherever possible. What is required is an ordered approach to consider and document safety. The safety assessment/case provides a way of showing that safety has been considered properly and that decisions have been well founded. The assessment should be systematic but there is no guarantee that the analysis will be 100% effective and complete. The process is therefore an iterative process within the overall life cycle of the system. 13.3 New technologies The application of formal safety management has existed for many years. However, the maturity of safety management in different parts of the industry varies greatly, although there has been some convergence in recent years. Angove (1999) summarises some of the complicating factors: Σ Regulation is fragmented and the requirements for safety are not always consistently imposed or adopted. Σ Systems are made up of a huge diversity of inherited, new and partially mature systems, reflecting a similar diversity of technology. Σ The operational installation and use of equipment and associated procedures (the operational environment) varies greatly. Σ A wide variety of national cultures and attitudes cloud the picture, and different countries have evolved their own approaches to regulation. Σ An increase in the profile of safety, particularly i.t.o. public perception. Σ Litigation and legislation trends in the aftermath of accidents Aviation’s history provides evidence that, whatever the benefits of technological advances, the safety graph dips – or at least waivers – while industry learns how to use the new technology. Example Software automation has not always improved safety as much as expected, generally because of inadequate training and particularly in the ‘need-to-know’ (i.e. keeping the pilot in the loop). Many flight-deck engineering concepts fail to consider the man–machine interface so necessary for safe flight (although this is fast changing under the banner of ‘human factors’). For the near-term future, automated systems cannot be expected to be totally reliable because computers have no intuition, no intelligence and no decisionmaking ability of the kind required to resolve unforeseen situations. When the automatics suddenly start ‘misbehaving’ the technical knowledge limitations and surprise factor of the crew have frequently led to accidents and serious incidents. However, computers do monitor things far better than humans. They also refine performance (e.g. engine or flight path management) more efficiently. There is a clear indication that the sheer complexity of modern systems creates problems for notions of management control (Smith, 1999). Weaknesses in the management of complex technological systems permit predictable and unintentional errors and cause catastrophic loss (Keely, 2000). Given the sheer complexity of modern systems, management faces problems of emergence – where elements of a system interact to create properties that had previously been unforeseen.3 By breaking complex systems down into their component parts (reductionism) to generate solutions, we compound the risk of further failure by neglecting the impact of such interventions on the emergent properties of the system. The intent is to grapple with the unknown and win: to determine a methodology for predicting, more accurately than before, the kind of problems which new technology or new operational practices may bring. 13.4 Safety engineering competence As explored in Chapters 8 and 9, there is a clear need to address the ‘whole system’ when considering safety implications (see Chapter 8). This is due to the inherent complexity of engineering systems today, as well as those of the future, and demands that safety knowledge be distributed throughout the systems’ supply chain, from the safety assessor to the designers, operators and maintainers. The key difference between engineering systems of the past and those of today and the future is that a thorough knowledge of a system held by one individual is unusual because the system will often be an integration of several complex sub-systems. The safety implications of such a system are thus an integrated discipline of computer/mechatronics systems engineering, regulatory requirements and human behaviour. Whilst a profound knowledge in all of these aspects is ambitious, the ability to effect a dialogue between expert parties requires a familiarity with the key potential risks and the interactions between them. This is the philosophy of Safety Engineering. Safety engineering is an engineering discipline requiring specialised professional knowledge and skills in specific principles, criteria and techniques, to allow the identification and control of hazards to acceptable levels (AAP 7001.054, Section 2 Ch 1 para 16-17). It draws upon professional knowledge and skills in the mathematical, physical, and related scientific disciplines, together with the principles and methods of engineering design and analysis, to specify, predict, and evaluate the safety of the system. To apply successfully, consistently and (most of all) efficiently, safety engineering is a skill acquired only after numerous years of practising in the system safety design and analysis areas. Designers often only concentrate on (and then test) normal operation of a system. If the safety assessment is to be used as an effective design tool, then the designer should use it to consider the abnormal situations. The safety assessment should ask how a system will fail, not only how it will work, and then predict the probability of the undesired event occurring. It requires the use of imagination to determine possible sequences of events leading to accidents. In many cases, the safety implications themselves can be as complex as the systems in which they might arise and thus familiarity with the technologies that comprise the modern engineering system is necessary beyond on-the-job, osmotic training. This also means that safety engineers have to work closely with system engineers, operators and maintainers (i.e. the system specialists who know the intricacies of the system) to meet the safety requirements required from the system under consideration. 13.5 Safety culture Safety ownership is often viewed as being the exclusive responsibility of specific departments, yet a good safety culture results only from top-level sponsorship and support. Corporate actions and policies must demonstrate this, not just to the workforce in general, but especially to the safety management teams. Inaction or inappropriate actions by corporate management gives rise to a lack of commitment and erosion in morale. A safety culture needs to be in place if safety in aviation is to improve. Research by Helmreich (1999) identifies three intersecting cultures: 1. National. This can be illustrated by the differences between the US (individualistic) and Asia (collectivistic). In the latter, a ‘high power distance’ culture, the leader is clearly boss. The comparison is strict obedience vs. ‘I’ll do my own thing’. In terms of adherence to rules Taiwan continuing airworthines management exposition out top and the US bottom – with potentially serious safety implications. 2. Professional. Hemreich describes this aspect as the ‘dark side’, as it is reflected in a sense of personnel invulnerability, which is ‘clearly unrealistic. Positive culture, however, is reflected in the pride of work. 3. Organisational. Regards values with respect to errors, openness and adherence (see Chapter 12). Key problems afflict the relationship between corporate management and the safety specialists (Fairfield, 2003). Some examples are: Σ The former view the latter as cost centres, not revenue generators and therefore prime targets during overall expenditure squeezing. Σ The former have difficulty understanding technical issues and the latter have difficulty avoiding the use of technical jargon. Σ Safety shortfalls considered as significant by the former, are so rare as to be regarded as statistically insignificant. This causes considerations of need (or cost benefit of improvement) to seem academic, and are often unquantifiable. The best way to overcome this is to implement a policy of ‘to measure is to manage’. Safety concerns are categorised in accordance with accepted criteria and target and alert levels are set in such a way to enable all stakeholders to remain focused on achieving the safety targets. However, safety objectives can be achieved by a diverse range of means and may suffer from inherent subjectivity (e.g. human factors) and engineering judgement (e.g. service experience). It requires the safety professionals to stand up for themselves and ensure that all statements are made in a manner that will withstand the scrutiny of a legal inquiry after the unfortunate occurrence of a mitigated or unforeseen event occurring. 13.6 Impact on projects Increasingly the procurement of aircraft, equipment and systems for the aeronautical industry is by means of collaborative projects. The arrangements for such projects are negotiated both between the Governments of the participating nations (via a Memorandum of Understanding), and the contractors of the participating countries. Any variations in airworthiness procedure and standards are to be clearly documented. The safety engineer should agree these arrangements to ensure safety responsibilities can be accomplished. Specific issues to consider are listed below. Σ Experience has shown that specifying regulations and standards can help to minimise risk and it can be a very powerful influence on safety provided it is applied intelligently. However, Murphy (1991) advises that it is counterproductive for the contract to simply list a large number of conflicting, sometimes out-of-date and unrealistic documents. This is because of the considerable effort necessary to reconcile these into a common set of practical requirements and deliverables. Σ The safety targets and risk levels need to be clearly defined. Murphy (1991) also correctly emphasises that this is a very important (and arguably most neglected) topic as it is the ‘safety acceptance criteria’ the system is expected to achieve, and hence the standard the safety assessment/safety case will be evaluated against. To successfully conduct the assessment the output should be measurable and achievable in the light of any other contractual constraints. Σ All organisations involved need a common understanding of the applicable terms (for example, see the definition of a hazard in Chapter 5. Many terms have more than one meaning. Be certain that the whole supply chain is working with common definitions. Σ For collaborative projects, safety activities need to be co-ordinated and managed from a top-down total system point of view. – Each organisation involved must understand the system level (see Chapter 6) of their part of the assessment, and how it interfaces with the other levels. Any safety integrity claims of a part of a system (e.g. commercial off the shelf parts, assemblies or subsystems) without considering the whole (e.g. the aircraft) should be viewed with scepticism. – Someone must be appointed with overall responsibility for all aspects of safety, and he/she must (Murphy, 1991) have full visibility of all contractors and subsystems and the authority to initiate and technically control any lower-level analysis from them. 13.7 Final remarks We will continue to see a continuation of the constant quests to protect crew, passengers, maintenance personnel, third parties and the environment from the ever-present risks associated with operating and maintaining complex (and especially high kinetic energy) machinery such as aircraft. Although an accident-free society is an unrealistic dream, it can be tempered by technical excellence in design, maintenance and operation to continually improve on the safety record. Major challenges facing the European aeronautics industry include the following: Σ continuing to reduce cost in every way – cost of design, cost of certification, cost of construction, cost of fuel consumed, cost of in-service support Σ continuing to improve environmental performance – both noise and emissions – despite the major advances already made Σ continuing to increase system capacity and performance Σ continuing to improve safety. Technology will continue to be a major determinant of what the industry can deliver. Not all of the solutions to these issues are likely to be technology related. Technology alone cannot solve all problems caused by technology and when it comes to safety, we need to address the way in which hazards are managed and communicated. All stakeholders (from the decision makers, though to the users and maintainers of the system) need to have better information on the magnitude of the risk, the factors affecting that magnitude, and the consequences of each of the possible mitigating actions. It is essential therefore to make safety assessments/cases, even of the most complex systems, comprehensible to all concerned and not just to the analyst. They must assist the designer and the operator in making decisions. They must make clear what the critical features are and on what special manufacturing techniques, inspection, crew drills and maintenance procedures they are critically dependent. The purpose of the analysis is not only to convince airworthiness authorities that a system is safe, but also to state clearly those aspects on which safety depends. Safety cases/assessments could therefore be especially useful to operators it they can be used in anger, instead of lying in a dust-gathering tomb. Safety and performance, although important, are not the only aspects to be considered. One has to ensure that the design is practical and economical and likely to be reliable in service. It is relatively easy just to multiply the systems to achieve the required level of safety but this in itself may lead to problems of reliability and spares provisioning. In addition, practical operation of the aircraft may demand that it should be able to take off and fly safely, with various defects present. These factors have to be integrated into the overall design of the system and the aircraft. Appendix A Safety assessment tools and techniques Note: This table has been compiled from a variety of sources (ranging from textbooks, publications, and the internet to the personal experiences of friends, colleagues and acquaintances). Each of these tools has its own advantages and disadvantages and the extent to which these can be used during various phases of the product life cycle, and the degree to which it can be applied to safety assessments, vary. Listed in alphabetical order, the tools/techniques most frequently used by the author have been shaded. It is extremely important to note that as the complexity of the tool increases, so does the degree of training required for the user and/or the need for an experienced evaluation team to conduct the evaluation. On the plus side, the data derived from the more complex methodologies may be more supportable. Unfortunately, the primary disadvantage of such tools is that ‘trained subject matter experts’ may have limited experience in the actual operational environment, and, therefore, their evaluations may not be entirely applicable to the certification. This table is intended to be thought-provoking but has all the limitations of generic data. In no circumstances should it be considered complete, applicable to all systems, or wholly objective. Many entries have no advantages/limitations listed, and space is provided for the reader to add data if desired. The author will gladly receive any comments/suggestions/recommendations, which can be sent to systemsafety@hotmail.co.uk. For the latest update on this table (including links to relevant websites), see www.aircraftsystemsafety.com Accident analysis The purpose of the accident analysis is to evaluate the effect of scenarios that develop into credible and incredible accidents. Any accident or incident should be formally investigated to determine the contributors of the unplanned event. Many methods and techniques are applied. Accident sequence evaluation programme (ASEP) This tool is based on the Technique for Human Error Rate Prediction. ASEP comprises pre-accident screening with nominal human reliability analysis, and post-accident screening and nominal human reliability analysis facilities (Kirwan, and Ainsworth, 1992) Action error analysis Action error analysis analyses interactions between machine and humans. It is used to study the consequences of potential human errors in task execution related to directing automated functions. Any automated interface between a human and automated process can be evaluated, such as pilot/cockpit controls, or controller/display, maintainer/equipment interactions. ATLAS ATLAS is a software package for use in support of systems design and analysis work. It combines the elements of graphically based task analysis with the advantages of a database. ATLAS supports a variety of conventional task analysis methods and incorporates more than 60 human performance, workload and human reliability algorithms. (Hamilton and Bierbaum, 1990) Barrier analysis Any system is comprised of energy, should this energy become uncontrolled accidents can result. The barrier analysis method is implemented by identifying energy flow(s) that may be hazardous and then identifying or developing the barriers that must be in place to prevent the unwanted energy flow from damaging equipment, and/or causing system damage. Barrier analysis is an appropriate qualitative tool for systems analysis, safety reviews, and accident analysis. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Bayesian belief networks A BBN is a graphical network that represents probabilistic relationships among events in a network structure. With BBNs, it is possible to articulate expert beliefs about the dependencies between different variables and to propagate consistently the impact of evidence on the probabilities of uncertain outcomes, such as ‘future system reliability’ (Falla, 1997, Ch 4). The BBN on the left uses comparatively little evidence, depending only on the observed reliabilities and defect counts of previous products of the same process, and on the defects discovered in the current product during debugging. The topology of the graph is used to indicate probabilistic relationships among the variables described in the nodes. The BBN on the right includes subjective indicators, like problem complexity and design effort. Thus, this network is meant to be populated with probabilities that are not all derived from statistical inference, but at least in part from expert opinion. BBNs are also sometimes called causal probabilistic networks, probabilistic cause-effect models or probabilistic influence diagrams. Bedford scale Human factors evaluative tool. Bellcore TR332 (now Telcordia) The Bellcore approach is widely used in the telecommunications industry and has been updated to SR-332 (in May 2001). Bellcore’s approach is very similar to that of MIL-HDBK-217 but it’s based primarily on telecommunications data and covers five separate use environments. The approach also assumes an exponential failure distribution and calculates reliability in terms of failures per billion part operating hours, or FITs. Its empirically based models are in three categories: the Method I parts count approach that applies when there is no field failure data available, the Method II modification to Method I to include lab test data and the Method III variation that includes field failure tracking. Method I includes a first year modifier to account for infant mortality. Method II includes a Bayes weighting procedure that covers three approaches depending on the level of previous burn-in the part or unit has undergone. Method III includes a Bayes weighting procedure as well but it is based on three different cases depending on how similar the equipment is to that from which the data was collected. For the most widely used Method I case where the burn-in varies, the steady-state failure rate depends on the basic part steady-state failure rate and the quality, electrical stress and temperature factors Benefits analysis An assessment (either qualitative and/or quantitative) used to determine the potential benefits to be derived from following (or not following) a particular course of action (see cost benefits analysis). Bent pin analysis Connector shorts can cause system malfunctions, anomalous operations, and other risks. Bent pin analysis evaluates the effects should connectors short as a result of bent pins and mating or demating of connectors. Any connector has the potential for bent pins to occur. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Bottom-up analysis approach Also known as the ‘hardware’ method, this starts with the hardware failure modes which can occur, and analyses the effects of these on the sub-system and the system. An example bottom-up approach is the FMEA. Bow tie analysis Uses a methodology known as the hazards and effects management process, which requires hazards to be identified, assessed, controlled and if subsequently they are released, recovery measures to be in place to return the situation to normal if possible. The stages worked through in the bow tie are: Σ Proactive measures: – identification of the hazard. – identification of the threats that could release the hazard. – assessment of the threat controls already in place and the identification of additional controls that may be necessary to manage the threat effectively. – identification of the escalation factors that are conditions that prevent a threat control being effective. – assessment of the escalation controls, which are further measures needed to maintain control of the escalation factor. – identification of the hazardous event, which is the initial release of the hazard that can lead to an accident. Σ Reactive measures: – assessment of the recovery measures that would be appropriate to return the situation to as near to normal as possible. – identification of the escalation factors that are conditions that prevent a recovery measure being effective. – assessment of the escalation controls, which are further measures needed to maintain control of the escalation factor. Brainstorming Uses a team of knowledgeable people to work in an imaginative and non-critical atmosphere to solve problems. Cable failure matrix analysis Less then adequate design of cables can result in faults, failures, and anomalies, which can result in contributory hazards and accidents. Should cables become damaged system malfunctions can occur. Cable failure matrix analysis identifies the risks associated with any failure condition related to cable design, routeing, protection, and securing. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Causal analysis Deductive analysis, which investigates the possible outcome of an undesired event. Uses techniques such as FTA, Software FTA, FMECA. Cause consequence analysis Integration of deductive (e.g. fault tree) and inductive (e.g. event tree) analysis into a single method and notation. Mainly used in nuclear industries, no good examples found in other industries yet. See also consequence analysis. Change analysis Change analysis examines the effects of modifications from a starting point or baseline. Checklists In the past, hazards identification relied on the experience of individual engineers and on previous accidents. Sometimes this knowledge would be embodied in hazard checklists. A checklist is, as its name implies, a list of questions, features or key points against which something is assessed (‘checked’) to determine its acceptability. Checklists can be constructed for many purposes and can be short or long, simple or complex. In fact, checklists are as varied as the systems being designed or evaluated or the tasks to be performed. Checklists incorporate past experiences in convenient lists of ‘do’s’ and don’ts’. The list is more of a prompt to the imagination of the user than a checklist which can guarantee identifying all possible hazards. Some useful checklists include: Σ The air traffic control Electronic Checklist, developed by the Volpe Center and the FAA, provides a checklist of human factors issues that should be considered in the design and evaluation of air traffic control systems and equipment. The checklist points controllers and other operations specialists to questions that they may wish to consider in the evaluation of new systems or subsystems or a new component of an existing system (see http://www.hf.faa.gov/) Σ The Ergonomics Audit Program (ERNAP) is a computerised checklist to help managers design and/or evaluate procedures for aviation maintenance and inspection. ERNAP is simple to use and evaluates existing and proposed tasks and set-ups by applying ergonomic principles. ERNAP allows the auditor to maintain audits for further reference. ERNAP was developed under the auspices of the FAA, and can be downloaded from the Human Factors in Aviation Maintenance and Inspection (HFAMI) website. See http://www.hfskyway.com/jobaids.htm) Σ CRT display checklist, which forms Appendix A to NUREG/CR- 3557. It provides subjective comparisons of methods for displaying screen information but is also used as a design checklist (refer Kirwan and Ainsworth, 1992) Σ Ravden & Johnson Checklist, which is a comprehensive checklist of items that evaluate the usability of human-computer interfaces. It is easy to administer but its 156 questions make it somewhat lengthy. It generates much data on interface factors including visual clarity, consistency, compatibility, feedback, explicitness, functionality, control, error management, help facilities, and the usability of help facilities (Ravden and Johnson, 1989). Σ NUREG-0700: US Nuclear Regulation Commission (NRC) has produced several human factors guidance documents. NUREG- 0700 is a detailed checklist for control room design (or more precisely, design review) in the nuclear power industry. The checklist addresses individual instruments, so using this checklist is a time-consuming process because of its detail. The guidelines, first issued in 1981, were recently revised to take into account the introduction of computer-based, human-computer interface technology (Kirwan and Ainsworth, 1992). Chi-squared method A method for detecting differences between a binomial and a multinomial population. Observations may fall into one or more categories and compare two or more samples Cognitive event tree system (COGENT) Human error reliability assessment. Cognitive work analysis (CWA) Traditional approaches to work analysis tend to emphasise centralised work organisations, whereas turbulent, dynamic environments tend to require more distributed work organisations. The focus of the CWA framework is on identifying the constraints that shape behaviour rather than trying to predict behaviour itself. Rasmussen’s (1986) framework for cognitive work analysis (CWA) provides separate descriptions of different classes of constraints: Work Domain (the functional structure of the work domain in which behaviour takes place); Control Tasks (the generic tasks that are to be accomplished); Strategies (the set of strategies that can be used to carry out those tasks); Social-Organisational (the organisation structure); Worker Competencies (the competencies required of operators to deal with these demands). [http://www.mie.utoronto.ca/labs/cel/research/frameworks/cwa.htm, 5/9/05] Common cause analysis (CCA) Generic term encompassing ZSA, probabilistic risk assessment and common mode analysis (see society of automotive engineers ARP4761) Although most systems employ redundancy techniques (i.e. fail safe design), it will be found on examination that many of them have a ‘single cause’ (e.g. EMI/EMC), or ‘common point’ (e.g. common busbar or common controller), that could cause multiple failures. A common mode failure is a failure that has the potential to fail more than one safety function and to possibly cause an initiating event or other event simultaneously. For instance: Σ Common part failure: three totally independent flying control systems may merge together in a common part – the pilot’s control column. A failure of this common part causes total system failure. Σ Common cause failure: a fire in a compartment might destroy all the channels of a system running through that compartment. Likewise, contaminated hydraulic fluid could cause all the channels of the hydraulic system to fail, or mechanical failures in an electrical loom. Σ Common mode failure: identical software in a dual redundant system will fail when exposed to the same inputs; jamming of a mechanical system (either due to failure or due to FOD); overheating of avionic equipment, etc. Σ Cascade failures: a single failure may overload the remaining channels, thereby increasing the probability of their failure. Or, an initial minor failure (e.g. a deflated tyre) causes a cascade of events (e.g. Concorde). The common cause analysis (consisting of the ZHA, probabilistic risk assessment and the CMA) provides the tools to verify required independence, or to identify specific dependencies. It identifies failures which by-pass or invalidate redundancy/independency assertions. Common mode analysis (CMA) Provides evidence that the failures assumed to be independent are truly independent in the actual implementation. Covers the effect of design, manufacturing and maintenance errors and the effects of common component errors (e.g. considers independence of duplicate systems due to design errors (e.g. S/W), lightning, HIRF, cooling, fire, contamination, etc.). A common mode failure has the potential to fail more than one safety function and to possibly cause an initiating event or other abnormal event simultaneously. Rare in technical systems, but typical in human actions (e.g. maintenance). Comparison-tocriteria The purpose of comparison-to-criteria is to provide a formal and structured format that identifies safety requirements. Comparison-to-criteria is a listing of safety criteria that could be pertinent to any system. This technique can be considered in a requirements cross-check analysis. Applicable safety-related requirements such as OSHA, NFPA, ANSI, are reviewed against an existing system or facility. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Confined space safety The purpose of this analysis technique is to provide a systematic examination of confined space risks. Any confined areas where there may be a hazardous atmosphere, toxic fume, or gas, the lack of oxygen could present risks. Confined space safety should be considered at tank farms, fuel storage areas, manholes, transformer vaults, confined electrical spaces, race-ways. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Consequence analysis Inductive analysis, which takes a given event (usually a failure) as a starting point, and works forward to determine the possible outcome (see also cause consequence analysis). The consequence analysis will determine the relationship between hazards and the accidents to which they lead. The forward looking part of HAZOPS, SWIFT and functional FME(C)A are all consequence analyses. Includes ETA, cause consequence diagrams, etc. Contingency analysis Contingency analysis is a method of minimising risk in the event of an emergency. Potential accidents are identified and the adequacies of emergency measures are evaluated. Contingency analysis should be conducted for any system, procedure, task or operation where there is the potential for harm. Contingency analysis lists the potential accident scenario and the steps taken to minimise the situation. It is an excellent formal training and reference tool. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Continuous safety sampling methodology (CSSM) This is a form of hazard analysis that uses observation (e.g. control charting) and work sampling techniques to Σ determine and maintain a pre-set level of the operator’s physical safety within constraints of cost, time and operational effectiveness. Σ observe the occurrence of conditions that may become hazardous in a given system. These conditions, known as dendritics, may become hazards and could result in an accident or occupational disease. Continuous safety sampling methodology performs a random sampling for the occurrence of these dendritics. The collected data are then used to generate a control chart. Based on the pattern of the control chart, a system ‘under control’ is not disturbed whereas a system ‘out of control’ is investigated for potential conditions becoming hazardous. Appropriate steps are then taken to eliminate or control these conditions to maintain a desired safe system. This tool is used to determine whether activities are within tolerable limits. If outside tolerable limits, corrective action is then derived. (Quintana and Nair, 1997 (DK)) Control rating code Control rating code is a generally applicable system safety-based procedure used to produce consistent safety effectiveness ratings of candidate actions intended to control hazards found during analysis or accident analysis. Its purpose is to control recommendation quality, apply accepted safety principles, and priorities hazard controls. Control rating code can be applied when here are many hazard control options available. The technique can be applied toward any safe operating procedure, or design hazard control. (FAA System Safety Handbook, Chapter 9: Analysis Techniques December 30, 2000) Cost benefit analysis A weighing scale approach to decision making. All the pluses (e.g. cash savings, lives saved) are put on one side of the balance and all the minuses (e.g. costs, disadvantages) are put on the other. Whichever weigh the heavier wins. A frequent mistake is to use non-discounted amounts for calculating costs and benefits. A method like ‘net present value (NPV)’ and ‘economic value added’ is strongly recommended, because all these account for the time value of money. Another frequent problem is that typically the costs are tangible, hard and financial, whilst the benefits are hard and tangible, but also soft and intangible. Care should be taken here against claims that ‘if you cannot measure it, then it does not exist/it has no value’. Critical incident technique This is a method of identifying errors and unsafe conditions that contribute to both potential and actual accidents or incidents within a given population by means of a stratified random sample of participant-observers selected from within the population. Operational personnel can collect information on potential or past errors or unsafe conditions. Hazard controls are then developed to minimise the potential error or unsafe condition. This technique can be universally applied in any operational environment (Tarrents, 1980). Critical path analysis Critical path analysis identifies critical paths in a program evaluation graphical network. Simply it is a graph consisting of symbolism and nomenclature defining tasks and activities. The critical path in a network is the longest time path between the beginning and end events. This technique is applied in support of large system safety programme, when extensive system safety-related tasks are required. Damage modes and effects analysis Evaluates the damage potential as a result of an accident caused by hazards and related failures. Risks can be minimised and their associated hazards eliminated by evaluating damage progression and severity (Tarrents, 1980). Deactivation safety analysis This analysis identifies safety concerns associated with facilities that are decommissioned/closed. The deactivation process involves placing a facility into a safe mode and stable condition that can be monitored if needed. Deactivation may include removal of hazardous materials, chemical contamination, spill cleanup. Decision analysis Decision analysis is a broad term to describe tools for facilitating, understanding or structuring decision-making processes. The essence of decision analysis is to break down a complicated decision into its component part s or elementary qualities, and in particular to separate clearly the subjective and objective aspects of that decision. Decision analysis originates in the field of operations research but has links to economics, mathematics, psychology and human factors. A wide range of tools have been developed which utilise a variety of methods such as influence diagrams, decision trees, voting methods, multi-attribute utility methods and so on. Deductive analysis Analysis which works back from a given event (failure) to identify its causes. It starts from known effects to seek unknown causes. A deductive argument is where the conclusion is implicit in the evidence used to support the argument. Dependence diagrams (DD) Similar to the FTA, but replaces the logic gates by paths to show the relationship of the failures. A dependence diagram analysis is success-oriented, and is conducted from the perspective of which failures must not occur to preclude a defined failure condition. Each block defines, for example, a failure of a part of a system and the conditions related to it and, where needed, the estimated frequency of occurrence. The blocks are arranged in series or parallel to represent ‘and’ or ‘or’ gates respectively. See society of automotive engineers ARP4761 Design appraisal A qualitative appraisal of the integrity and safety of the system design. Can be used to consider a range of issues, such as: Σ what happens if? Σ possibility of maintenance induced failures Σ suitability/compatibility of materials Dynamic workload scale Human factors evaluative tool. Electromagnetic compatibility analysis The analysis is conducted to minimise/prevent accidental or unauthorised operation of safety-critical functions within a system. Adverse electromagnetic environmental effects can occur when there is any electromagnetic field. Electrical disturbances may also be generated within an electrical system from transients accompanying the sudden operations of solenoids, switches, choppers, and other electrical devices, radar, radio transmission, transformers (Tarrents, 1980). Energy analysis The analysis is conducted to minimise/prevent accidental or unauthorised operation of safety-critical functions within a system. Adverse electromagnetic environmental effects can occur when there is any electromagnetic field. Electrical disturbances may also be generated within an electrical system from transients accompanying the sudden operations of solenoids, switches, choppers, and other electrical devices, radar, radio transmission, transformers (Tarrents, 1980). Energy trace analysis This hazard analysis approach addresses all sources of uncontrolled and controlled energy that have the potential to cause an accident. Examples include utility electrical power and aircraft fuel (FAA System Safety Handbook, Chapter 9). Sources of energy causing accidents can be associated with the product or process (e.g., flammability or electrical shock), the resource if different than the product/process (e.g., smoking near flammable fluids), and the items/conditions surrounding the system or resource of concern (e.g., vehicles or taxiing aircraft). A large number of hazardous situations are related to uncontrolled energy associated with the product or the resource being protected (e.g., human error). Some hazards are passive in nature (e.g., sharp edges and corners are a hazard to a maintenance technician working in a confined area). The purpose of energy trace analysis is to ensure that all hazards and their immediate causes are identified. Once the hazards and their causes are identified, they can be used as top events in a fault tree or used to verify the completeness of a fault hazard analysis. Consequently, the energy trace analysis method complements but does not replace other analyses, such as fault trees, sneak circuit analyses, event trees, and FMEAs. Energy trace and barrier analysis Similar to energy analysis and barrier analysis. The analysis can produce a consistent, detailed understanding of the sources and nature of energy flows that can or did produce accidental harm. The technique can be applied to all systems, which contain, make use of, or which store energy in any form or forms, (e.g. potential, kinetic mechanical energy, electrical energy, ionising or non-ionising radiation, chemical, and thermal) (Tarrents, 1980). Energy trace checklist Similar to energy trace and barrier analysis, energy analysis and barrier analysis. The analysis airborne integrated data system in the identification of hazards associated with energetics within a system, by use of a specifically designed checklist. The analysis could be used when conducting evaluation and surveys for hazard identification associated with all forms of energy. The use of a checklist can provide a systematic way of collecting information on many similar exposures (Tarrents, 1980). Environment analysis Human error reliability assessment technique. The environment analysis can be performed concurrently with the user and task analysis. Activities or basic tasks that are identified in the task analysis should be described with respect to the specific environment in which the activities are performed. Environmental risk analysis The analysis is conducted to assess the risk of environmental noncompliance that may result in hazards and associated risks. The analysis is conducted for any system that uses or produces toxic hazardous materials that could cause harm to people and the environment (Tarrents, 1980). Event and causal factor charting Utilises a block diagram to depict cause and effect. The technique is effective for solving complicated problems because it provides a means to organise the data, provides a summary of what is known and unknown about the event, and results in a detailed sequence of facts and activities (Tarrents, 1980). Event tree analysis (ETA) estimated time of arrival is an inductive technique that considers the consequence of an initiating event and the expected frequency of each occurrence. It is a graphical technique that starts from an initial occurrence (e.g. lightning strike or system condition, such as a rupture of a fuel pipe or loss of power supply) and builds upon this by sequencing the possible events. It is illustrated as a tree of possible true/false outcomes against each mitigating mechanism. Event tree analysis starts with a hazard, but instead of working backwards as in the fault tree, it works forward to describe all the possible subsequent events and so identify the event sequences that could lead to a variety of possible consequences. Originally devised to access the protective systems and safety of nuclear reactors, it operates with inductive (i.e. forward) logic by asking the question: ‘What happens if...’ Explosives safety analysis This method enables the safety professional to identify and evaluate explosive hazards associated with facilities or operations. Explosives safety analysis can be used to identify hazards and risks related to any explosive potential, i.e. fuel storage, compressed gases, transformers, batteries (Tarrents, 1980). Extended master plan logic diagram (MPLD) Extended from MPLD to include the additional category of couplings which originate common cause failures (a logic diagram that shows how functional, equipment and component failure combine to cause a system malfunction) These are represented in fault-tree-like structures, except that basic events are not represented as leaf events but are listed in the lower left part of the tree and connected to gates though a sort of matrix (Mauri, 2000). External events analysis The purpose of external events analysis is to focus attention on those adverse events that are outside of the system under study. It is to further hypothesise the range of events that may have an effect on the system being examined. The occurrence of an external event such as an earthquake is evaluated and affects on structures, systems, and components in a facility are analysed (Tarrents, 1980). Facility system safety analysis System safety analysis techniques are applied to facilities and its operations. Facilities are analysed to identify hazards and potential accidents associated with the facility and systems, components, equipment, or structures (Tarrents, 1980). Failure logic analysis for system hierarchies (FLASH) Developed to enable the assessment of a hierarchically described system from the functional level down to the low levels of its hardware and software implementation. Each module of the architecture (i.e. sub-system or basic component) is systematically examined for potential failure modes and how those failure modes relate/propagate to other modules in the system hierarchy (Mauri, 2000). Failure mode and effects analysis (FMEA) and failure modes, effects and criticality analysis (FMECA) A systematic, hardware (i.e. bottom-up) approach of identifying failure modes of a system or item, and determining the effects on a higher level. It answers the question ‘if this part fails, what will be the next result?’ The failure mode and effects analysis is performed at a certain level (system, subsystem, module, part/item, etc.) by postulating the ways the chosen level’s specific implementation may fail. Can be developed to the level of the smallest replaceable item (i.e. piece part FMEA) or functional level (i.e. functional FMEA, which could be the same as an FHA). Piece part failure mode and effects analysis is useful to determine the theoretical failure probability of the part being considered, whilst a function failure mode and effects analysis uses predetermined probabilities as an input. Failure effects leading to the same system condition can be identified and grouped together in a FMES. Does not have to be quantitative. Best suited to mechanical and electrical hardware systems. Although very extensive, the ‘devil is in the details’. It is generated to support the safety assessment, so it is important to understand the expectations and requirements of the failure mode and effects analysis before any work on it commences (e.g. its sole purpose may be to support verification of the fault tree analysis through a comparison of failure mode and effects analysis failure modes with the basic events of the fault tree). Co-ordinate required scope of failure mode and effects analysis with the user requesting it. If the failure rates from a Functional failure mode and effects analysis allow the preliminary system safety assessment targets to be met, then a piece part failure mode and effects analysis may not be necessary. See MIL-STD-1629 and BS 5760 Part 5 and society of automotive engineers ARP4761. For useful software tools, see www.byteworx.com Failure propagation and transformation notation (FPTN) Hierarchical graphical notation that represents system behaviour. It represents a system as a set of interconnected modules; these might represent anything from a complete system to a few lines of program code. The connections between these modules are failure modes, which propagate between them (Mauri, 2000). Fault hazard analysis A system safety technique that is an offshoot from FMEA. Similar to failure mode and effects analysis above, however, failures that could present hazards are evaluated. Hazards and failures are not the same. Hazards are the potential for harm, they are unsafe acts or conditions. When a failure results in an unsafe condition it is considered a hazard. Many hazards contribute to a particular risk. Any electrical, electronics, avionics, or hardware system, sub-system can be analysed to identify failures, malfunctions, anomalies, faults, that can result in hazards (Tarrents, 1980). Fault isolation methodology The method is used to determine and locate faults in large-scale ground-based systems. Examples of specific methods applied are; half-step search, sequential removal/replacement, mass replacement, and lambda search, and point of maximum signal concentration. Determine faults in any large-scale ground-based system that is computer controlled (Tarrents, 1980). Fault tree analysis (FTA) A graphical model (developed in the 1960s) for illustrating: Σ logical relationships between a particular failure condition and the failures or other causes leading to a particular undesired event. Σ the pathways within a system that can lead to a foreseeable, undesirable loss event. The pathways interconnect contributory events and conditions, using standard logic symbols. It is a top-down (deductive) analysis proceeding through successively more detailed (i.e. lower) levels of the design until the risk of occurrence of the top event (the feared event) can be predicted. It is the opposite process to the FMECA: the fault tree analysis goes down to a primary event (i.e. an event which does not need to be broken down any further). The primary events can be hardware failures, human errors, software faults or external factors like the weather. Developed in the 1960s and has since then been readily adopted by a range of engineering disciplines as one of the primary methods of predicting system reliability and availability parameters. fault tree analysis is essentially a systematic qualitative technique to which a quantitative analysis can usually be applied if suitable failure data exists. Even in situations where failure data does not exist, it may still be useful to perform an fault tree analysis due to the insight it yields concerning a system’s potential failure behaviour. fault tree analysis provides valuable information through qualitative analysis but can also be quantified with event probabilities or rates to give an estimate of how often the top event will occur. Computerised fault tree analysis provides good graphic output, quick evaluation of changes, more sophisticated algorithm, but can lead to less understanding by analysts and a temptation to become overly complex. Fire hazards analysis Fire hazards analysis is applied to evaluate the risks associated with fire exposures. There are several fire hazard analysis techniques, i.e. load analysis, hazard inventory, fire spread, scenario method. Any fire risk can be evaluated (Tarrents, 1980) Flow analysis The analysis evaluates confined or unconfined flow of fluids or energy, intentional or unintentional, from one component/subsystem/ system to another. The technique is applicable to all systems which transport or which control the flow of fluids or energy (Tarrents, 1980). Function and task analysis Human error reliability assessment technique. Detailed analysis of the functions to be accomplished by the human/machine/ environment system and the tasks performed by the human to achieve those functions. Σ Function analysis. An analysis of basic functions performed by the ‘system’ (which may be defined as human-machine, humansoftware, human-equipment-environment, etc.). The functional description lists the general categories of functions served by the system. Functions represent general transformations of information and system state that help people achieve their goals, but do not specify particular tasks. Σ Task analysis. Task analysis is one of the most important tools for the user to understand and can vary substantially in its level of detail and completeness. The preliminary task analysis traditionally specifies the jobs, duties, tasks, and actions that a person will be doing. Functional analysis system technique (FAST) This tool is used in the early stages of design to investigate system functions in a hierarchical format and to analyse and structure problems (e.g., in allocation of function). The aim of FAST is to understand how systems work and how cost effective modification can be incorporated. It asks ‘how’ sub-tasks link to tasks higher up the task hierarchy, and ‘why’ the superordinate tasks are dependent on the sub-tasks (Creasy, 1980; Kirwan and Ainsworth, 1992). Functional hazard analysis (FHA) A systematic, comprehensive examination of a system’s functions to identify and classify failure conditions (conditions which the system can cause or contribute to, not only if it malfunctions or fails to function, but also in its normal response to unusual or abnormal external factors) of those functions according to their severity. The functional hazard assessment provides a top-level analysis of the functions performed by the system and the risks presented by these functions following failure or misuse. These hazards produced by the system are categorised according to their level of severity. Potential effects on the aircraft or on crew workload determine each hazard’s associated severity. Generic error modelling system (GEMS) GEMS is an error classification model that is designed to provide insight as to why an operator may move between skill-based or automatic rule based behaviour and rule or knowledge-based diagnosis. Errors are categorised as slips/lapses (frequently skillbased errors) and mistakes (usually knowledge based errors). The result of GEMS is a taxonomy of error types that can be used to identify cognitive determinants in error sensitive environments. GEMS relies on the analyst either having insight to the tasks under scrutiny or the collaboration of a subject matter expert, and an appreciation of the psychological determinants of error. Goals, operators, methods and systems (GOMS) GOMS is a task modelling method to describe how operators interact with their systems. Goals and sub-goals are described in a hierarchy. Operations describe the perceptual, motor and cognitive acts required to complete the tasks. The methods describe the procedures expected to complete the tasks. The selection rules predict which method will be selected by the operator in completing the task in a given environment. GOMS is mainly used in addressing human-computer interaction and considers only sequential tasks. Hazard analysis A generic term describing a whole collection of techniques whose combined strengths have a good chance of revealing and evaluating/analysing hazards. A multi-use technique to identify hazards within any system, subsystem, operation, task or procedure (Tarrents, 1980). Also referred to as a system safety analysis (JAR 25.1309). Includes both top-down techniques oriented to tracing back from potential real-world hazards to the sources of failures which could lead to accidents, and bottom-up techniques which follow through hypothetical component failures to determine their hazardous consequences. (Strictly these are ‘middle-out’ because one also looks at how the component could come to fail.) Hazard and Operability studies (HAZOPs) IEC 61882 DEF STAN 00-58 A team-based structured brainstorming technique for identification of hazards before they arise. hazard and operability starts with a deviation from normal system operation and examines how that deviation might occur and the consequences should such a deviation occur. The purpose is to identify what variations from the intended design values (the ‘design intent’) could occur in the relevant attributes, and then to determine their possible causes and consequences. From their possible consequences, it is seen whether the deviations could cause hazards. The technique was developed by ICI in the 1960s and is well established in the petrochemical sector. Hazard identification study (HAZID) A structured brainstorming technique developed for the marine industry. Considers systems or equipments. Used by the International Maritime Organisation (IMO Paper MSC 69/INF 14 dd 98/2/12) for its safety assessments. Hazard log (HL) A management tool used to track the identification, mitigation and acceptance of risk and also the control of residual risks associated with the operation. Note that hazards are properties of an entire system and may be defined at any system level (see section 6.2). However, it is essential to select the right level so as to ensure consistency in the hazard log. – A common mistake is to select it too low, which results in too many hazards, no system properties, expensive (impossible) to track and over-engineering. – If selected too high, then it is hard to ensure the identification and management of all hazards. Hazardous materials (HAZMAT) list Not an assessment technique, but a list of hazardous materials contained in a product. Health hazard analysis (HHA) Identifies health hazards and recommends measures (e.g. such as ventilation and barriers) to reduce exposure to health hazards. See Mil Std 882C Task 207. Health hazard assessment The method is used to identify health hazards and risks associated within any system, sub-system, operation, task or procedure. The method evaluates routine, planned, or unplanned use and releases of hazardous materials or physical agents. The technique is applicable to all systems which transport, handle, transfer, use, or dispose of hazardous materials or physical agents (Tarrents, 1980). Human error analysis (HEA) A method to evaluate the human interface and error potential within the human/system and to determine human error-related hazards. Contributory hazards are the result of unsafe acts such as errors in design, procedures, and tasks. Many techniques can be applied in this human factors evaluation (Tarrents, 1980). Human error assessment and reduction technique (HEART) HEART is an error quantification process that is quick to use. The process defines a set of generic error probabilities for the types of tasks being examined and identifies the error-producing conditions associated with them. For each of the error-producing conditions the human error probability is multiplied by the error-producing condition multiplier. The tool also provides some guidance on approaches towards error reduction. A human performance modelbased technique utilising some standard probabilities. Data-based method to assess and reduce human error and improve operational performance. Human factors analysis Human factors analysis represents an entire discipline that considers the human engineering aspects of design. There are many methods and techniques to formally and informally consider the human engineering interface of the system. There are special considerations such as ergonomics, bio-machines, anthropometrics. Human factors analysis is appropriate for all situations where the human interfaces with the system and human-related hazards and risks are present. The human is considered a main sub-system (Tarrents, 1980). Human hazard analysis (HHA) Examines the ease of use, the effects of error during use, task distribution, and the adequacy of feedback to the user in terms of the ability to recognise quickly if the desired result of the user’s actions have not been achieved (Flight International, 11–17 Aug 1999, p3). Human reliability analysis The purpose of the human reliability analysis is to assess factors that may impact human reliability in the operation of the system. The analysis is appropriate where reliable human performance is necessary for the success of the human-machine systems (Tarrents, 1980). For more information, see Guide to Practical Human Reliability Assessment, Barry Kirwan, ISBN: 0748401113. Incident reviews These might be for the system itself or for similar systems used elsewhere. Inductive analysis Analysis which works forward from a given event (failure) to determine the possibility outcomes (e.g. see consequence analysis). It starts from known causes to forecast unknown effects. Inductive argument is where the argument is firmly based on the evidence presented, but extrapolates beyond the available evidence. Ishikawa diagrams Also called cause-and-effect or fishbone diagram. Problem of interest (e.g. hazard or accident) is entered at end of main, ‘bone’. All possible causes are then ‘fleshed out’. Job safety analysis This technique is used to assess the various ways a task may be performed so that the most efficient and appropriate way to do a task is selected. Job safety analysis can be applied to evaluate any job, task, human function, or operation. Each job is broken down into tasks, or steps, and hazards associated with each task or step are identified. Controls are then defined to decrease the risk associated with the particular hazards (Tarrents, 1980). Justification of human error data information (JHEDI) JHEDI is derived from the human reliability management system (HMRS) and is a quick form of human reliability analysis that requires little training to apply. The tool consists of a scenario description, task analysis, human error identification, a quantification process, and performance shaping factors and assumptions. JEDHI is a moderate, flexible and auditable tool for use in human reliability analysis. Some expert knowledge of the system under scrutiny is required. Layer of protection analysis (LOPA) Used for SIL determination. Is a relatively new method, developed by the American Institute of Chemical Engineers (CCPS) group in response to the requirements of ISA S84.01 and was formally published in 2001. It effectively combines a number of different techniques into a composite method that is well tailored to assessing process risks and development of hazardous scenarios. As indicated by its name, it involves assessing layers of protection other than just the instrument protective functions. For instance, a contribution toward risk reduction by independent protective layers (IPLs) such as ‘alarms and operators’ or ‘basic process control’ is explicitly defined as a risk reduction factor. The combination of the risk reduction factors for all IPLs provides the total risk reduction possible. It is fundamentally a simplified quantitative method that considers the risk reduction contributed from each IPL typically by order of magnitude risk reduction (i.e., say 0.1 for a DCS, or 0.01 for a relief valve, etc.). (Kirkwood D., Current issues with SIL assessment methods, Functional Safety Professional Network, Technical Advisory Panel, david.kirkwood@rtel.com) Maintenance error decision airport information desk (MEDA) Boeing has invested decades of research in maintenance error. It has developed a widely used maintenance error decision airport information desk (MEDA) which is an attempt to systematise evaluation of events, problems and potential problems by using a repeatable, structured evaluation programme. The company has been encouraging its customers to employ the technique. Management oversight and risk tree (MORT) management and oversight risk tree technique is used to systematically analyse an accident in order to examine and determine detailed information about the process and accident contributors. This is an accident investigation technique that can be applied to analyse any accident (Tarrents, 1980). Man-machine integration design and analysis systems (MIDAS) MIDAS is a silicon graphics software tool designed to airport information desk the application of human factors principles and performance models to the design of complex systems. It is intended for use at the earliest stages of the design process and consequently is likely to reduce some of the costs of simulation and prototyping. MIDAS describes a system’s operating environment and procedures, and incorporates human performance models into the design process. Markov analysis (MA) Similar to the DD and FTA, but it additionally calculates the probability of the system being in various states as a function of time. Here airworthiness is not a simple mathematical calculation, but depends on relative states of parts of the system. Provides a means for analysing reliability/availability of systems whose components exhibit strong dependencies. The Encyclopaedia Britannica defines the Markov process as ‘A sequence of possible dependent random variables (x1, x2, x3,…) – identified by increasing values of a parameter, commonly time – with the property that any prediction of the value xn, knowing the value x1, x2….xn–1, may be based on xn–1 alone. That is, the future value of the variable depends upon the present value and not the sequence of past values’. Master plan logic diagram (MPLD) An outgrowth of the master logic diagram to represent all the physical interrelationships among various plant systems and subsystems in a simple logic diagram. It is used for probabilistic assessments to model and integrate the relationship between all plant functions and equipment (Mauri, 2000). Materials compatibility analysis Provides an assessment of materials utilised within a particular design. Any potential degradation that can occur due to material incompatibility is evaluated. Materials compatibility analysis in universally appropriate throughout most systems (Tarrents, 1980). MIL-HDBK-217 ‘Reliability Prediction of Electronic Equipment ’ – even though this handbook is no longer being kept up to date by the US military, it remains the most widely used approach by both commercial and military analysts. MIL-HDBK-217 has been the mainstay of reliability predictions for about 40 years but it has not been updated since 1995, and there are no plans by the military to update it in the future. For more than ten years Quanterion’s Seymour Morris was department of defense program manager for MIL-HDBK-217. The handbook includes a series of empirical failure rate models developed using historical piece part failure data for a wide array of component types. There are models for virtually all electrical/ electronic parts and a number of electromechanical parts as well. All models predict reliability in terms of failures per million operating hours and assume an exponential distribution (constant failure rate), which allows the addition of failure rates to determine higher assembly reliability. The handbook contains two prediction approaches, the parts stress technique and the parts count technique, and covers 14 separate operational environments, such as ground fixed, airborne inhabited, etc. Σ As the names imply, the parts stress technique requires knowledge of the stress levels on each part to determine its failure rate. Σ The parts count technique assumes average stress levels as a means of providing an early design estimate of the failure rate. Typical factors used in determining a part’s failure rate include a temperature factor (pT), power factor (pp), power stress factor (pS), quality factor (pQ) and environmental factor (pp) in addition to the base failure rate lambda_b. Monte-Carlo analysis (as used by federal aviation administration for fuel tank safety assessments) Analytical method to determine flammability exposure time of a fuel tank. The percentage fleet flammability exposure result can be used to determine if the fuel tanks exist in a flammable state for a long period of time, thereby requiring more rigorous analysis in the SSA. Spreadsheet that simulates uncertain parameters by randomly selecting values from distribution tables. The calculation is performed repetitively and averaged to approximate real conditions. NPRD-95 The nonelectronic parts reliability data (NPRD-95) databook is a widely used data book published by the Reliability Analysis Center that provides a compendium of historical field failure rate data on a wide array of mechanical assemblies. The document provides detailed failure rate data on over 25,000 parts for numerous part categories grouped by environment and quality level. Because the data does not include time-to-failure, the document is forced to report average failure rates to account for both defects and wearout. Cumulatively, the database represents approximately 2.5 trillion part hours and 387,000 failures accumulated from the early 1970s through 1994. The environments addressed include the same ones covered by MIL-HDBK-217; however, data is often very limited for some environments and specific part types. For these cases, it then becomes necessary to use the ‘rolled up’ estimates provided, which make use of all data available for a broader class of parts and environments. Although the data book approach is generally thought to be less desirable, it remains an economical means of estimating ‘ballpark’ reliability for mechanical components. NSWC-94/L07 Handbook of Reliability Prediction Procedures for Mechanical Equipment developed by the Naval Surface Warfare Center – Carderock Division. This handbook presents a unique approach for prediction of mechanical component reliability by presenting failure rate models for fundamental classes of mechanical components. Examples of the specific mechanical devices addressed by the document include belts, springs, bearings, seals, brakes, slider-crank mechanisms and clutches. Failure rate models include factors that are known to impact the reliability of the components. For example, the most common failure modes for springs are fracture due to fatigue and excessive load stress relaxation. The reliability of a spring will therefore depend on the material, design characteristics and the operating environment. NSWC-94/L07 models attempt to predict spring reliability based on these input characteristics. Occupational health hazard analysis (OHHA) Identifies health hazards and recommends provisions such as ventilation, barriers, protective clothing, etc. Operability analysis The aim of carrying out operability analysis is to highlight any issues that have a bearing on the operability of a system/equipment. An operability analysis should be designed for operation in the simplest and easiest way possible. Carrying out an operability analysis involves the following: Σ task analysis Σ Workload analysis Σ human reliability analysis Σ taking due account of the prevailing environmental conditions. Effort invested in the operability analysis will vary with the criticality of the equipment, its interfaces and interactions with other equipment. Therefore the scope of operability assessments can be restricted to a single task or cover a range of tasks. Methods include: Σ Anthropometrical studies can be used to provide known physical data on the population to assess workplace layout and architecture. Σ Rapid prototype modelling permits varied configurations to be tested over comparatively short timescales. This technique permits feedback from subject matter experts to be incorporated into the model, and assessed promptly, before possible inclusion into the design. Σ Task analysis involves a study of the workforce (operators) to ascertain what is required to achieve the system goals. This allows comparison between the task demands and the operators’ capabilities. Σ Workload analysis is an analysis of the demand placed on the operator by the task requirements. Σ Human reliability analysis recognises the critical area where human error may affect performance. Σ Operational scenario analysis is an analysis that the activities required to be undertaken, can be successfully completed using the manpower and facilities provided for the purpose. Operating and support hazard analysis (OSHA) The analysis is performed to identify and evaluate hazards/risks associated with the environment, personnel, procedures, and equipment involved throughout the operation of a system (Tarrents, 1980). Evaluates hazardous operating, maintenance and support tasks by systematically evaluating each phase of operation and support. Can be divided into two separate analyses: Σ The operating hazard analysis Σ The support hazard analysis. Pareto analysis A ranking technique based only on past data that identifies the most important items among many. Uses the 80-20 rule, which states that about 80% of the problems are caused by about 20% of the causes. Particular risk assessment (PRA) A form of CCA. Technology or circumstance dependent analysis which considers common events or influences that are outside the system(s) concerned (e.g. fire, lighting) which may violate failure independence claims. Some of these risks may also be the subject of specific airworthiness requirements. probabilistic risk assessment examines common events that are external to the systems concerned, but which may violate independence requirements (e.g. uncontained engine rotor failure; fire; bird strike; lightning; HIRF; human factors, etc.). (e.g. damage may result in multiple systems failing; incorrect pilot response could lead to a hazardous flying condition). Each risk is then examined to assess any simultaneous or cascading effects of each risk. Qualitative assessment A collective term for the various methods of assessing causes, severities, and likelihood of potential failure conditions. Typical types of analysis include design appraisal, installation appraisal, FMEA, FTA, DD, reliability block diagrams, etc. Quantitative assessment A collective term for the various analyses (such as failure modes and effects, fault tree, or dependence diagram) which also includes numerical probability information. The probabilities of primary failures can be determined from failure rate data and exposure times, using failure rates derived from service experience on identical or similar items, or acceptable industry standards. The conventional mathematics of probability can then be used to calculate the estimated probability of each failure condition as a function of the estimated probabilities of its identified contributory failures or other events. Often used for hazardous or catastrophic failure conditions of systems that are complex, that have insufficient service experience to help substantiate their safety, or that have attributes that differ significantly from those of conventional systems. Quantitative probability terms are usually expressed in terms of acceptable numerical probability ranges for each flight hour, based on a flight of mean duration for the aeroplane type (however, for a function which is used only during a specific flight operation, e.g., take-off, landing, etc., the acceptable probability should be based on, and expressed in terms of, the flight operation’s actual duration). RDF 2000 This is the latest and most comprehensive of the European methodologies developed by CNET. It has not yet received much attention in the US but it could evolve into the new international standard should MIL-HDBK-217 continue to become outdated. Like the PRISM approach, it also addresses thermal cycling and dormant system modelling. RDF 2000 is the new version of the CNET UTEC80810 reliability prediction standard that covers most of the same components as MIL-HDBK-217. The models take into account power on/off cycling as well as temperature cycling and are very complex with predictions for integrated circuits requiring information on equipment outside ambient and print circuit ambient temperatures, type of technology, number of transistors, year of manufacture, junction temperature, working time ratio, storage time ratio, thermal expansion characteristics, number of thermal cycles, thermal amplitude of variation, application of the device, as well as per transistor, technology related and package related base failure rates. Reliability analysis A full review of the reliability of an aircraft part or component, making use of past data to determine the reliability of a component or maintenance technique. Reliability block diagram A graphical means of representing which set of correctly working components may combine to provide the system function. Constructed of blocks and connections representing devices in provision of a function. Risk-based decision analysis An efficient approach to making rational and defensible decisions in complex situations (Tarrents, 1980). Root cause analysis This method identifies causal factors to accident or near-miss incidents. The root causes are the underlying contributing causes for observed deficiencies that should be documented in the findings of an investigation (Tarrents, 1980). Root causes are the most basic causes of an event that meet the following conditions: Σ they can be reasonably identified Σ management has the ability to fix or influence them. Typically, root causes are the absence, neglect, or deficiencies of management systems that control human actions and equipment performance. Root cause analysis provides a means to determine how and why something occurred. Understanding the accident scenario is not enough. Scenarios tell us what happened, not why it happened. Events in accident scenarios are generally only symptoms of underlying problems in the administrative controls that are supposed to keep those events from occurring. Understanding only the scenario addresses the outward symptoms, but not the underlying problems. More investigation of the underlying problems is needed to find and correct those that will contribute to future accidents. Safety review Assesses a system, identify facility conditions, or evaluate operator procedures for hazards in design, the operations, or the associated maintenance. Periodic inspections of a system, operation, procedure, or process are a valuable way to determine their safety integrity. A safety review might be conducted after a significant or catastrophic event has occurred (Tarrents, 1980). Scenario analysis Scenario analysis identifies and corrects hazardous situations by postulating accident scenarios where credible and physically logical scenarios provide a conduit for brainstorming or to test a theory where actual implementation could have catastrophic results. Where system features are novel, subsequently, no historical data is available for guidance or comparison, a scenario analysis may provide insight (Tarrents, 1980). SHEL model An illustration of the interrelationships between the three types of system resource and their environment Σ S = software (i.e. rules, regulations, SOPs, customs, habits, etc. Σ H = hardware (i.e. physical assets) Σ E = environment (i.e. physical, political, social, economic) Σ L = liveware (i.e. people). The usual, interfaces: Σ L–H interface: the interaction between man and the machine (i.e. ergonomics) is probably the cause of most catastrophic accidents. Σ L–S interface: considers the interaction of human characteristics with the requirements of the rules, procedures, etc. Σ L–E interface: considers how the human can cope in extreme conditions. Model can be extended to be 3D: Σ H–H interface (e.g. plug and play devices) Σ S–S interface (e.g. consistency of company operating procedures) Σ L–L interface (e.g. command and control). Single function diagram (SFD) Shows schematically how a specific function is normally produced. Single-point failure analysis This technique is to identify those failures that would produce a catastrophic event in items of injury or monetary loss if they were to occur by themselves. This approach is applicable to hardware systems, software systems, and formalised human operator systems (Tarrents, 1980). Sneak analysis (or sneak circuit analysis) Looks for unintended paths (flows) within an electrical system. A sneak circuit is an unexpected path or logic flow within a system which, under certain conditions, can initiate an undesired function or inhibit a desired function. The path may consist of hardware, software, operator actions, or combinations of these elements. Sneak circuits are not the result of hardware failure but are latent conditions, inadvertently designed into the system, coded into the software program, or triggered by human error. The traditional approach to sneak circuit analysis is manually to dissect the schematic drawings and transform them into structures called network trees. Sneak clues are then applied to these trees. SNA can be performed using the sneak circuit analysis tool (SCAT), a PC-based software package, and CapFast, an electrical circuit design and schematic editing tool. SCAT integrates with the schematic design package, CapFast. Original version was Sneak Circuit Analysis, devised after Mercury Redstone rocket launch accident (1961). See DEF STAN 00-41 and Mil-Std-1543. Software failure modes and effects analysis This technique identifies software-related design deficiencies through analysis of process flow- charting. It also identifies areas for verification/validation and test evaluation (Tarrents, 1980). Software fault tree analysis This technique is employed to identify the root cause(s) of a ‘top’ undesired event. To assure adequate protection of safety critical functions by inhibiting interlocks, and/or hardware (Tarrents, 1980). Software hazard analysis The purpose of this technique is to identify, evaluate, and eliminate or mitigate software hazards by means of a structured analytical approach that is integrated into the software development process. Software hazard analysis and resolution in design (SHARD) Very hazard and operability like, but with different keywords (i.e. early, late, omission, commission and value). Developed by the University of Yor k. Static source code analysis The process by which software developers check their code for problems and inconsistencies before compiling. Organisations can automate the source code analysis process by implementing a tool that automatically analyses the entire program, generates charts and reports that graphically present the analysis results, and recommends potential resolutions to identified problems. Static analysis tools scan the source code and automatically detect errors that typically pass through compilers and become latent problems, including the following: Σ syntax Σ unreachable code Σ unconditional branches into loops Σ undeclared variables Σ uninitialised variables Σ parameter type mismatches Σ uncalled functions and procedures Σ variables used before initialisation Σ non-usage of function results Σ possible array bound errors Σ misuse of pointers. Statistical distributions When carrying out the tasks assigned to it, the ‘output’ of a system can be expressed as a statistical distribution which describes the probabilities that the system output will reach or exceed any particular values. Structural safety analysis This method is used to validate mechanical structures. Inadequate structural assessment results in increased risk due to potential for latent design problems (Tarrents, 1980). Structured what if technique (SWIFT) High level structured brainstorming technique that originated from the process/manufacturing industry. As the name implies, this process is based around a series of structured and well-defined questions aimed at brainstorming possible failure mechanisms for the system at an early stage of the design. Considers complete systems, subsystems and processes. Has many similarities to HAZOPS, in that it is team-based brainstorming and uses prompts (e.g. checklists) to explore the behaviour of a system and identify hazards. Instead of guide words, SWIFT uses a series of questions which usually, but not always start ‘what if ...’. For example: What if Σ a specific item of equipment fails? Σ the operator fails to carry out the correct procedure? Σ the level control fails to operate? Σ a fire occurs in a particular part of the plant? Σ a flood occurs? Σ the maintainer tried to work without isolating the power supply? (Defence Procurement Management Guide, DPMG/TEC/320 Iss1 (Sept98)) Task analysis Task analysis is a fundamental human factors method and underlies many other techniques. A small selection of known tools include: Σ applied cognitive task analysis (ACTA) Σ ATLAS Σ functional analysis system technique (FAST) Σ goals, operators, methods and systems (GOMS) Σ Micro Saint (software program) Σ repertory grid analysis. Task analysis is a method to evaluate a task performed by one or more personnel from a safety standpoint in order to identify undetected hazards, develop notes/cautions/ warnings for integration in order into procedures, and receive feedback from operating personnel (Tarrents, 1980). Test safety analysis Test safety analysis ensures a safe environment during the conduct of systems and prototype testing. It also provides safety lessons to be incorporated into the design, as application. This approach is especially applicable to the development of new systems, and particularly in the engineering/development phase (Tarrents, 1980). Tests Often analysis alone cannot accurately predict precise effects or probability of failures., so it becomes essential to conduct actual tests (i.e. on rigs or in situ). Essential in the following circumstances Σ with circuits which use integrating and differentiating functions or other processing which may be sensitive to changes in time constants. Σ in control system where it is often necessary to have crossconnections between channels in order to achieve synchronisation or load sharing or cross-monitoring. The IEEE gold book IEEE STD 493-1997, IEEE Recommended Practice for the Design of Reliable Industrial and Commercial Power Systems, provides data on commercial power distribution systems. Provides data concerning equipment reliability used in industrial and commercial power distribution systems. Reliability data for different types of equipment are provided along with other aspects of reliability analysis for power distribution systems, such as basic concepts of reliability analysis, probability methods, fundamentals of power system reliability evaluation, economic evaluation of reliability, and cost of power outage data. The handbook was updated in 1997; however, the most recent reliability data reflected in the document is only through 1989. The sequentially timed events plot investigation system (STEP) This method is used to define systems; analyse system operations to discover, assess, and find problems; find and assess options to eliminate or control problems; monitor future performance; and investigate accidents (Tarrents, 1980). Top-down analysis approach Starts by identifying the failure condition to be investigated and then proceeds to derive those failure modes (and combinations of failure modes) which can produce it. Built on the assumption that evaluation can be best served by examining the system as a whole (its goals, objectives, operating environment, etc.), and examining the individual sub-systems or components (Garland, et al., 1999). An example top-down approach is the functional hazard analysis (FHA). Weibull analysis Most reliability analysis uses an exponential time to failure (TTF) distribution, which says that the instantaneous rate of failure is constant over time, and the item is as likely to fail at one moment as another (i.e. it is ‘memoryless’ – that is, the item is not more likely to fail the next moment simply because it has operated for a long time). This is not good enough when considering the effect of ageing, when the failure rates are increasing. The question is: how often should this inspection be performed? One very useful distribution for modelling TTF in the presence of ageing is the Weibull distribution, which has the advantages of: 1. being very flexible to fit a large number of field data samples, and 2. collapsing to the exponential TTF distribution when the field data is fairly flat over time, and 3. being a theoretical ‘limiting distribution’ (which is somewhat beyond the scope of this brief). In Weibull analysis, the practitioner attempts to make predictions about the life of all products in the population by ‘fitting’ a statistical distribution to life data from a representative sample of units. The parameterised distribution for the data set can then be used to estimate important life characteristics of the product such as reliability or probability of failure at a specific time, the mean life for the product and failure rate. Life data analysis requires the practitioner to: Σ gather life data for the product Σ select a lifetime distribution that will fit the data and model the life of the product Σ estimate the parameters that will fit the distribution to the data Σ generate plots and results that estimate the life characteristics, like reliability or mean life, of the product. What-if analysis What-if analysis methodology identifies hazards, hazardous situations, or specific accident events that could produce an undesirable consequence. It is a simple method of applying logic in a deterministic manner (Tarrents, 1980). A problem-solving approach that uses loosely structured questioning to (i) suggest upsets that may result in accidents or system performance problems and (ii) make sure the proper safeguards against those problems are in place. Typical qualitative probability terms are: a. Probable failure conditions are those anticipated to occur one or more times during the entire operational life of each aeroplane. b. Improbable failure conditions are divided into two categories as follows: (i) Remote. Unlikely to occur to each aeroplane during its total life but which may occur several times when considering the total operational life of a number of aeroplanes of the type. (ii) Extremely remote. Unlikely to occur when considering the total operational life of all aeroplanes of the type, but nevertheless has to be considered as being possible. c. Extremely improbable failure conditions are those so unlikely that they are not anticipated to occur during the entire operational life of all aeroplanes of one type. Zonal safety analysis (ZSA)/Zonal hazard analysis (ZHA) common cause analysis technique which specifically considers physical proximity of different technologies. Theoretical and visual examination of each physical zone to ensure that interference and interactions with adjacent systems do not violate the independence requirements. Used to: Σ determine compliance with the installation rules Σ identify any potential cascade failures due to system interaction Σ identify any potential areas for maintenance errors Σ identify potential areas for system malfunction due to environmental factors. This technique is used to look at the complex interactions that can occur between high-energy systems and is specifically concerned with their physical position in relation to each other. The zonal hazard analysis techniques are also used to assess the effects of the proliferation of hazards into adjacent physical areas or compartments. They can be used to identify the routes by which the hazards may spread and in so doing, solutions can be developed to control and mitigate the effects of the hazard. See society of automotive engineers ARP5754 p38 Appendix B Safety criteria B.1 Introduction Regulations have different definitions for the various categories of failures and/or hazards. In order to guide the safety assessment process, it is necessary firstly to define the criteria used to evaluate the various failures and hazards present and judge the acceptability of their occurrence. It has been said that: ‘You cannot manage what you cannot measure’. We therefore need to define the exact terminology and allocate a measure of performance. These definitions are fundamental keys to understanding the data presented, as the resultant ‘safety acceptance criteria’ form the baseline standards against which the system is then evaluated during the safety assessment. The broad range of accidents/hazards (see Chapter 6), their associated risks, and the particular circumstances of each potential accident situation means that it may not be practicable to have one single set of criteria covering all contingencies. However, irrespective of which criteria are chosen, they must be substantiated and agreed by the relevant regulatory authority. To measure is to know, but first we need to define the measuring stick. Hence the production of the safety criteria report. The criteria should be formulated so as to provide effective safety measures, be readily understood in terms of both concept and application, and be flexible to provide scope for user contribution. The aim of this Appendix is to summarise some of the commonly used safety criteria that may be useful in evaluating the safety of a system. This chapter must be read with an understanding of the contents of Chapters 4 and 5. It is for the safety assessor (with regulatory authority concurrence) to select the optimum criteria (or combination of criteria) from applicable regulations to apply to the specific system level (see Fig. 8.1) under consideration. Most importantly, the chosen criteria must be applied consistently throughtout the complete system safety assessment/safety case. If this is not done then efficient risk comparison will be compromised, and the integration of lower-level safety assessments is bound to be exceedingly complicated. B.2 international civil aviation organization accepted safety criteria B.2.1 Background With reference to Chapter 5 section 5.2, the international civil aviation organization Airworthiness Manual (Appendix H to Chapter 4, page IIA-4h-I) states the following for large civil aircraft: Where it is necessary to use numerical assessments the values given below may be used in providing a common point of reference: Σ ‘Frequent’ may be interpreted as a probability of occurrence greater than ten power minus three per hour of flight for the expected mean flight time of the type of aeroplane involved. Σ ‘Reasonably probable’ may be interpreted as a probability of occurrence in the range of ten power minus three to ten power minus five per hour of flight for the expected mean flight time of the type of aeroplane involved. Σ ‘Remote’ may be interpreted as a probability of occurrence in the range of ten power minus five to ten power minus seven per hour of flight for the expected mean flight time of the type of aeroplane involved. Σ ‘Extremely remote’ may be interpreted as a probability of occurrence in the range of ten power minus seven to ten power minus nine per hour of flight for the expected mean flight time of the type of aeroplane involved. Σ ‘Extremely improbable’ may be interpreted as a probability of occurrence of less than ten power minus nine per hour of flight for the expected mean flight time of the type of aeroplane involved. The numerical values are goals rather than precise values, and judgment should be used in their application. The probability should be established taking into account the appropriate time of risk. Such statistical methods should be used to complement engineering judgement and should not be regarded as a substitute. Critical combinations of failures should be investigated and may be accepted on the basis of assessed numerical probability values where these values can be substantiated, and a suitable analysis technique has been employed. When the failure of a device can remain undetected in normal operation, the frequency with which the device is checked will directly influence the probability that such a failure is present on any particular occasion. When using quantitative analyses to help determine compliance with FAR/CS 25.1309(b), these descriptions of the probability terms have become commonly accepted as airborne integrated data system to engineering judgement. They are expressed in terms of acceptable ranges for the average probability per flight hour.The JAA/EASA and federal aviation administration allocate these numerical goals to failure conditions of aircraft systems as follows (refer FAR/JAR/CS25.1309): The aeroplane systems and associated components, considered separately and in relation to other systems, must be designed so that – (1) Any Catastrophic failure condition: Σ is Extremely Improbable; and Σ does not result from a single failure; and (2) Any Hazardous failure condition is Extremely Remote; and (3) Any Major failure condition is Remote. Note that this is the goal-based approach as discussed in Chapter 5. B.2.2 Application These criteria are applied as per Tables B.1 to B.6. Σ Failures affecting airworthiness can be defined according to the severity categories in Table B.1. Σ In accordance with ACJ25.1309 and joint aviation authority Notice for Proposed Amendment (NPA) 25F-281, each failure is allocated a qualitative safety objectives (i.e. minimum probability of occurrence) based on the worst potential consequence of the hazard as per Table B.2. Σ For qualitative assessments the following assertions/claims in Table B.3 (if properly substantiated) may satisfy the qualitative objectives. Σ For quantitative assessments the limits for probability of hazard occurrence in Table B.4 are commonly accepted as airborne integrated data system to engineering judgement. Σ ACJ25.1309 provides an indication of the level of effort that is needed to satisfy these objectives and these are summarised in Table B.5. Σ Table B.5 can be illustrated as the flowchart in Fig. B.1. For software induced hazards, Table B.6 (refer RTCA-DO178B) allocates a development assurance level2 (DAL) as an objective to each hazard’s severity category. Proof of the level of development assurance may lead to qualitative occurrence claim level as indicated. Failure severity catetories Failure definition No safety effect – Failure condition which would not affec aeroplane safety in any manner. At most a nuisance. Minor – Failure condition that would not significantly reduce the aeroplane safety and which involve crew actions that are well within their capabilities. Slight reduction in safety margins or functional capabilties. Slight increase in crew workload, routine flight path changes. Some inconvenience to occupants. May require oerating limitations or emergency procedures. Major – Failure that would reduce the capability of the aircraft or the ability of the crew to cope with adverse operating conditions. Significant reduction in safety margins or functional capabilities. Significant increase in crew workload impairing crew efficienc. Discomfort to occupants, possibly including injuries. Would require operating limitations or emergency procedures. Critical – Failure conditions that would significantly reduce the capability of the airplane or the ability of the crew to cope with adverse operating conditions. Large reduction in safety margins or functional capabilites. Physical distress or higher workload such that the flight crew cannot be relied upon to perform their tasks accurately or completely. Serious or fatal injury to a relatively small number or the occupants. Catastrophic – Failure conditions which would prevent continued safe flight and landing. Normally with hull loss. Multiple deaths usually with loss of the aircraft. Qualitative Safety Objectives No Safety Effect – Frequent – Conditions anticipated to occur several times. Minor – Probable - Conditions anticipated to occur one or more times during the entire operational life of each aeroplane. Major - Remote – Conditions unlikely to occur to each a irplane during its entire life but which may occur several times when considering the total operational life of a number of aeroplanes of this type. Critical – Extremely Remote – Conditions unlike to occur when considering the total operational life of all airplanes of the type, but nevertheless have to be considered as being possible. Catastrophic – Extremely Improbable – Conditions so unlikey to occur that they are not anticipated to occur during the entire operational life cycle of all aeroplanes of the type. Quantitative safety objectives Frequent – NO requirements Probable – between 10 power minus five and ten power minus 3. Remote – between ten power minus seven and ten power minus six Extremely remote – between ten power minus nine and ten power minus seven Extremely Improbable – less than ten power minus nine Depth of analysis required to meet safety target Frequent – None required. Probable – Design and installation appraisal to verify independence function and physical separation from airworthiness-related components. Verify that failures of the system will not contribute to more severe failure conditions if combined with other systems or functionn. If still minor, then no further action required to be 25.1309 compliant. Remote – If the complexity of the system is low, and the system is similar in its relevant attributes to those use in other aeroplanes and the effects of failures would be the same, then design and installation appraisals, and satisfactory service history of the equipment being analyzed, or of similar design, will usually be acceptable for showing compliance with. If similar cannot be justified, but the system is conventional in its relevant attributes, then compliance may be shown by means of a qualitative assessment. This also applies to system of high complexity, provided that there is reasonable confidence that the failure condition is not worse than major. For complex systems which include functional redundancy, a qualitative failure mode and effects analysis or fault tree may be necessary to determine that redundancy actually exists and to show that the failure modes of the equipment do not have any airworthiness-related effects on other functions. Extremely Remote/Extremely Improbable – except as specified, a detailed safety analysis will be necessary for each hazardous and catastrophic failure condition identified by the functional hazard assessment. The analysis will usually be a combination of quality assessmen t of the design. Probability levels that are related to catastrophic failure conditions should not be assessed only on a numerical basis unless this basis can be substantiated beyond reasonable doubt. For very simple and conventional installations with low complexity and similarity in relevant attributes, it may be possible to assess a catastrophic failure condition as being extremely improbable, on the basis of experienced engineering judgement, without using all the formal procedures, listed. The basis for the assessment will be degree of redundancy, the established independence and isolation of the channels and reliability record of the technology involved. Satisfactory service experience on similar systems commonly used in many aeroplanes may be sufficient when a close similarity is established in respec of both the system design and operating conditions. As discussed in paragraphs (1) and (2) compliance for a system or part thereof that is not complex may sometimes be shown by design and installation appraisals and evidence of satisfactoryservice experience on other aeroplanes using the same or other systems that are similar in their relevant attributes. Software development assurance levels No Safety Effect – no requirement DAL – Occurrence claim level is frequent. Minor – Require DAL level D – Occurrence claim level is Reasonably probable. Major – Require DAL Level C - Occurrence claim level is remote. Critical – Require DAL level B - Occurrence claim level is extremely remote. Catastrophic – Require DAL level A - Occurrence claim level is Extremely Improbable. B.3 UK Ministry of Defence safety criteria B.3.1 Background In the UK military aviation industry, two distinct sets of safety criteria are applied: Accident/health and safety criteria (refer to the Health and Safety at Work Act), which considers the risk (i.e. probability and severity) of a potential accident. These criteria are derived from the risk-based approach discussed in Chapter 4, which is based on an accident sequence model, a simple example is illustrated by Fig. B.2. As can be seen, the probability of the accident is dependent on the probability of the system hazard and the probability of the intermediate events (e.g., friendly fire or human error). The severity of the accident is dependent on the extent of injuries involved or damage caused. Airworthiness criteria, where safety assessments for airborne equipment are represented in terms reflecting aviation-specific standards and requirements. The term ‘airworthiness’ is used here within the context of the aircraft’s ability to continue safe flight and landing. Regulations such as JSP553 (and CS25.1309) use ‘failure’ severity categories rather than ‘accident’ severity categories because there is reall only one accident being considered (i.e., the aircraft crashes). It also tends to embed directly the probability of the accident happening, given that the hazard has happened, into the hazard severity definition (in other words, it considers the ability to continue safe flight and landing following the occurrence of a hazardous situation). Whilst there is no direct relationship between these two sets of criteria, by virtue of considering and mitigating aircraft system hazards, the system safety assessment will naturally contain some OH&S considerations. B.3.2 Accident criteria For the variety of systems6 and operational conditions within the MOD’s remit, DEF STAN 00-56 Part 1 Para 7.3.2 categorizes accident severity in accordance with the impact on personnel as defined in Table B.7. The DEF STAN 00-56 approach assumes an accident sequence model similar to that shown in Fig. B.3. The hazard is that state of the system being considered which causes/permits/exacerbates the risk of the accident arising. The probability of the accident is dependent on the probability of the system hazard and the probability of the intermediate events (considered to be external to the system, but are necessary conditions for the accident to occur). In accordance with DEF STAN 00-56 Part 1 para 7.3.2(d), the accident probability of occurrence shall be categorized during risk estimation in accordance with the definitions in Table B.8. Table B.7 Accident severity categories Negligible – At most a single minor injury or minor occupational illness Marginal - A single severe injury or occupational illness, and/or multiple minor injuries or minor occupational illness. Critical – A single death and/or multiple severe injuries or severe occupational illness. Catastrophic – Multiple deaths. DEF STAN 00-56 (Part 1 section 7.3.2) does allow for these definitions to be modified if not appropriate for the system being considered. The UK MOD base their acceptance of hazards on a risk classification scheme, which is based on the combination of the severity, probability and time of exposure for each particular hazard. For the purposes of the accident risk classification scheme, accidents are considered single events (Table B.9). These classifications can be combined to determine a hazard risk index (HRI), which is a numerical risk factor that can be used to prioritise the need for corrective action or resolution. The HRI matrix in Table B.10 is an example showing how the hazard severity and the hazard probability categories combine to yield the HRI. Table B.8 Accident probability categories Accident Probability = Frequent. Occurrence, during operational life considering all instances of the system = Likely to be continually experienced. Quantitative probability per operating hour = less than ten power minus two. Accident Probability = Probable. Occurrence during operational life, considering all instances of the system = Likely to occur often. Quantitative probability per operating hour = less than ten power minus four. Accident Probability = Occasional. Occurrence during operational life, considering all instances of the system = Likely to occur several times. Quantitative probability per operating hour = less than ten power minus six. Accident Probability = Remote. Occurrence during operational life, considering all instances of the system = Likely to occur at some time. Quantitative probability per operating hour = less than ten power minus eight. Accident Probability = Improbable. Occurrence during operational life, considering all instances of the system = Unlikely, but may exceptionally occur. Quantitative probability per operating hour = less than ten power minus ten. Accident Probability = Incredible. Occurrence during operational life, considering all instances of the system = Extremely unlikely that the event will occur at all, given the assumptions recorded about the domain of the system. Quantitative probability per operating hour = less than ten power minus twelve. Refer DEF STAN 00-56 Part 1 section 7.3.2. Note that the term ‘operating hour’ does not necessarily correlate with ‘flight hours’ (as discussed in Table 5.1 and section 1.5.4). Within the risk-based approach, operating hours could include the hours during maintenance (e.g. for hazards presented to ground crew). Or, from another perspective, a fleet of ten aircraft flying in formation for two operating hours will accumulate 20 flying hours. Table B.9 Risk classification Frequent – Catastrophic = A Frequent – Critical = A Frequent – Marginal = A Frequent – Negligible = B Probable – Catastrophic = A Probable – Critical – A Probable – Marginal – B Probable – Negligible – C Occasional - Catastrophic – A Occasional – Critical – B Occasional – Marginal – C Occasional – Negligible – C Remote - Catastrophic – B Remote – Critical – C Remote – Marginal – C Remote – Negligible – D Improbable – Catastrophic – C Improbable – Critical – C Improbable – Marginal – D Improbable - Negligible - - D Incredible - Catastrophic – C Incredible – Critical – D Incredible – Marginal – D Incredible - negligible – D Source Data: DEF STAN 00-56 Part 1 page 26 Table 5. Can be tailored if agreed by the accepting authority and the independent safety auditor (ISA). Sometimes it may be that different safety criteria are applied to individual risk groups (e.g. safety of passengers vs. safety of armament personnel), refer DEF STAN 00-56 (Part 1 Section 7.3.2.b). Class A: these risks are intolerable and shall be removed by the use of safety features. Class B: these risks are undesirable, and shall only be accepted when risk reduction is impracticable. Class C: these risks are tolerable with the endorsement of the Project Safety Review Committee. May need to show that risk is ALARP. Class D: these risks are tolerable with the endorsement of normal project reviews. No further action needed. Table B.10 Example hazard risk index matrix Frequent – Catastrophic = 1, Frequent – Critical = 3 Frequent – Marginal = 7 Frequent – Negligibel = 13 Probable – Catastrophic = 2 Probable – Critical = 5 Probable – marginal = 9 Probable – negligible = 16 Occasional – Catastrophic = 4 Occasional – Critical = 6 Occasional – marginal = 11 Occasional – negligible – 18 Remote – Catastrophic – 8 Remote – Critical – 10 Remote – Marginal – 14 Remote - Negligible – 19 Impobable - Catastrophic – 12 Improbable – Critical – 15 Improbable – marginal – 17 Improbable – Negligible – 20 See BAe Safety System (Doc No. AWN/GEN/996, dd March 98). With: Σ HRI 1 to 5: high risk. Unacceptable. Design changes or other action required. Σ HRI 6 to 11: moderate risk. Acceptable with customer/safety management review. Justification required. Σ HRI 12 to 20: low risk. Acceptable after safety working group review. B.3.3 Airworthiness criteria In accordance with JSP553 (Iss 1 Para 1.38), for peacetime flying, the design standard of a UK military aircraft type may be considered airworthy where the conditions of either (a) and (b) or (a) and (c) below are met as appropriate: (a) For all military aircraft types, their associated equipment and software, the aircraft designer has satisfactorily demonstrated, in a safety case, the airworthiness of the design. This demonstration may include design analysis, application of specific standards (such as DEF STAN 00-970) and procedures, historical evidence of successful use of particular design features, system tests, and ground and air tests to arrive at an overall assessment of airworthiness. (b) The cumulative probability of loss of an aircraft due to technical fault, and the cumulative probability of the aircraft (inclusive of its systems, structures and stores) which could result in the death of any aircrew or passengers, should bothbe assessed to be of the order of one in a million per flying hour (probability of occurrence ten power minus six per flying hour) when operated within the conditions used for the airworthiness demonstration. (c) Aircraft derived from civil passenger aircraft and used by the MoD in the passenger carrying airliner role should meet a higher standard of safety. Such aircraft may be considered airworthy if the cumulative probability of loss of an aircraft due to a technical fault, and the cumulative probability of a technical failure of the aircraft (inclusive of all its systems, structures and stores) which could result in the death of any aircrew or passengers are both assessed to be in the order of not more than one in ten million per flying hour (probability of occurrence ten to the power minus seven per flying hour) when operated within the conditions used for the airworthiness demonstration. Note: Practicably, this data is seldom available to fully populate all the accident models and combine them to achieve a prediction of their combined probability (i.e. some kind of loss-model). For large transport aircraft, the civil aviation authorities have similar targets7 as for (c) above and provide the following assumptions to assist the assessment process (refer AMC25.1309): Σ Historical evidence indicated that the probability of a serious accident due to operational and airframe-related causes was approximately one per million hours of flight. Furthermore, about 10 per cent of the total were attributed to failure conditions caused by the aeroplane’s systems. It seems reasonable that serious accidents caused by systems should not be allowed a higher probability than this in new aeroplane designs. Σ It is thus reasonable to expect that the probability of a serious accident from all such failure conditions be not greater than one per ten million flight hours or ten to the power minus seven per flight hour for a newly designed aeroplane. Σ It is arbitrarily assumed that there are about 100 potential failure conditions in an aeroplane which would prevent continued safe flight and landing. The target allowable risk of ten to the power minus seven was thus apportioned equally among these conditions, resulting in a risk allocation of not greater than ten to the power minus nine to each. The upper-risk limit for ‘failure conditions which would prevent continued safe flight and landing’ would be ten to the power minus nine for each hour of flight, establishing an approximate probability value for the term ‘Extremely improbable’. Failure conditions having less severe effects could be relatively more likely to occur (see Table B.1). The tolerance for safety risks affecting civil aviation during peacetime is likely to be very different from the tolerance of safety risks to military aircraft during conflict (operational safety criteria may be less severe). Following DEF STAN 00-56 (Part 1 Para 7.3.2.c), some systems have a defensive role whereby inaction under hostile circumstances may constitute a hazard. Safety targets for such systems shall address the requirements to reduce, to a tolerable level, the risk resulting from inaction under hostile circumstances. Example Loss/malfunction of a missile approach warning system (MAWS) may not affect the airworthiness of the aircraft, and could be classified as a minor failure condition (using the goal-based approach) and may be probable in occurrence. However, in accordance with DEF STAN 00-56, this loss could cause loss of the platform as no warning would result in no evasive or protective action, and should thus be classified as a catastrophic failure condition, which should be extremely improbable in occurrence. Where there is a conflict between the practicable realisation of safety targets for action and inaction within the system’s operational role, a reasonable balance of risk reduction shall be established and agreed between the design authority, the independent safety auditor and the ministry of Defence’s programme manager. B.4 Air traffic management risk matrix During 1999, the European Commission ARIBA project attempted to build an accident risk tolerability matrix for air traffic operations on UK Health and Safety Executive lines. The main reason for this was due to the fact that UK industry safety assessments usually use the HSE studies and guidelines about ‘tolerable’ and ‘acceptable risk’, with the following (simplified) HSE definitions: Σ as low as reasonably possible principle. The principle that no risk in the tolerability region can be accepted unless reduced ‘As Low As Reasonably Practicable’. Σ Broadly acceptable risk. A risk which is generally acceptable without further reduction. Σ Intolerable risk. A risk which cannot be accepted and must be reduced. Σ Tolerability region. A region of risk which is neither high enough to be unacceptable nor low enough to be broadly acceptable. Risks in this region must be reduced ALARP. The acceptable means of compliance 25.1309 guidance then allows failure conditions with the combinations of severity and frequency shown in Table B.11 (Brooker, 2004, Appendix A). ARIBA then produced a matrix (Table B.12) indicating how the as low as reasonably possible concept might be integrated into this framework. The three regions indicate the management decision-making and action required: 1. The intolerable region shows risk which cannot be accepted and must be reduced. 2. In the as low as reasonably possible region, specific safety management measures should be defined (e.g. safety monitoring, safety improvement projects, etc.) as long as such is reasonably practicable. 3. Tolerable risks may be managed through normal procedures. Table B.12 Possible aviation accident risk tolerability matrix Severity of accident – hundred of fatalities. Frequency of accident = one a year in civil aviation. ALARP. Severity of accident – many fatalities. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – single fatality. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – major injury. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – minur injury. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – no injury. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – hundreds of fatalities. Frequency of accident = more than one a year in civil aviation. ALARP. Severity of accident – many fatalities. Frequency of accident = one a year in civil aviation. ALARP. Severity of accident – single fatality. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – major injury. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – minur injury. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – no injury. Frequency of accident = one a year in civil aviation. Tolerable. Severity of accident – hundreds of fatalities. Frequency of accident = one a year per large airline. Intolerable. Severity of accident – many fatalities. Frequency of accident = one a year per large airline. ALARP. Severity of accident – single fatality. Frequency of accident = one a year per large airline. ALARP. Severity of accident – major injury. Frequency of accident = one a year per large airline. Tolerable. Severity of accident – minur injury. Frequency of accident = one a year per large airline. Tolerable. Severity of accident – no injury. Frequency of accident = one a year per large airline. Tolerable. Severity of accident – hundreds of fatalities. Frequency of accident = more than one a year per large airline. Intolerable. Severity of accident – many fatalities. Frequency of accident = more than one a year per large airline. Intolerable. Severity of accident – single fatality. Frequency of accident = more than one a year per large airline. ALARP. Severity of accident – major injury. Frequency of accident = more than one a year per large airline. ALARP. Severity of accident – minur injury. Frequency of accident = more than one a year per large airline. Tolerable. Severity of accident – no injury. Frequency of accident = more than one a year per large airline. Tolerable. Severity of accident – hundreds of fatalities. Frequency of accident = one a year per aircraft. Intolerable. Severity of accident – many fatalities. Frequency of accident = one a year per aircraft. Intolerable. Severity of accident – single fatality. Frequency of accident = one a year per aircraft. Intolerable. Severity of accident – major injury. Frequency of accident = one a year per aircraft. ALARP. Severity of accident – minur injury. Frequency of accident = one a year per aircraft. ALARP. Severity of accident – no injury. Frequency of accident = one a year per aircraft. Tolerable. B.5 MIL-STD-882D criteria The MIL-STD approach is to make decisions regarding resolution of identified hazards based on the assessment of the risk involved. It requires the identification of the risk category by combining the ‘mishap’ severity with the ‘mishap’ probability. In this case, ‘mishap’ is the same as an accident. B.5.1 Mishap severity Mishap severity categories are defined to provide qualitative measures of the worst credible mishap (i.e. accident) resulting from personnel error; environmental conditions; design inadequacies; procedural deficiencies; or system, subsystem or component failure/malfunction as shown in Table B.13. Table B.13 Suggested mishap severity categories Catastrophic. Category I, Could result in death, permanent total disability, loss exceeding $1M, or irreversible severe environmental damage that violates law or regulation. Critical. Category II, Could result in permanent partial disability, injuries or occupational illness that may result in hospitalization of at least three personnel, loss exceeding $200K but less than $1M, or reversible environmental damage causing a violation of law or regulation. Marginal, Category III. Could result in injury or occupational illness resulting in one or more lost work days(s), loss exceeding $10K but less than $200K, or mitigatible environmental damage without violation of law or regulation where restoration activities can be accomplished. Negligible, Category IV. Could result in injury or illness not resulting in a lost work day, loss exceeding $2K but less than $10K, or minimal environmental damage not violating law or regulation. These severity categories provide guidance to a wide variety of programmes (not only aviation). MIL-STD-882D does allow adaptation to a particular programme if agreed by the approval authority. B.5.2 Mishap probability MIL-STD-882D (para A.4.4.3.2.2) states that ‘Mishap probability is the probability that a mishap will occur during the planned life expectancy of the system. It can be described in potential occurrences per unit time, events, population, items, or activity’. Suggested mishap probability levels are shown in Table B.14. Table B.14 Suggested mishap probability levels Frequent – Level A. Likely to occur often in the life of an item with a probability of occurrence greater than ten to power minus one. Fleet or inventory continuously experienced. Probable – Level B. Will occur several times in the life of an item with a probability of occurrence less than ten to the power minus one but greater than ten to the power minus two in that life. Fleet or inventory will occur frequently. Occasionall. Level C. Likely to occur some time in the life of an item with a probability of occurrence less than ten to the powe minus two, but greater than ten to the power minus three in that life. Fllet or inventory will occur several times. Remote; Level D. Unlikey to occur in the lfe of the item with a probability of occurrence less than ten to the power minus three but greater than ten to the power minus six in that life. Fleet or inventory unlikely, but can reasonably be expected to occur. Improbable. Level E. So unlikely, it can be assumed occurrence may not be experienced with a probability of occurrence less than ten to the power minus six in that life. Fleet or inventory unlikely to occur, but possible. MIL-STD 882D (para A.4.4.3.2.3) provides the matrix in Table B.15 for mishap risk assessment. B.6 Other useful criteria It is up to the asssessor to propose the most appropriate safety criteria, which will be used to judge acceptability of the system, and to agree these criteria with the appropriate authority. The following subsections provide examples of other safety criteria which may be usefully tailored to the unique circumstances of the system under consideration. B.6.1 Impact on the mission Safety and reliability are not synonymous because not all failures are hazardous. However, these failures could significantly impact the mission. Tables B.16 and B.17 provide criteria which can be tailored to specific circumstances. B.6.2 Risk to environment and assets Tables B.18 and B.19 provide criteria that may be useful to assess the risk that the hazard poses to the environment, assets, company reputation, etc. B.7 Safety critical system components B.7.1 Background Whilst we are on the subject of severity classification, it may be useful to clarify the use of the term ‘safety critical’, which is often used as the basis for design guidance, continued airworthiness, and maintenance. To this purpose, the following information is summarised from a draft federal aviation administration memorandum (ANM-03-117-10), which provides the criteria for identifying flight-critical system components as applied to large aircraft. First, we need some definitions: Σ A ‘component’ is (ANM-03-117-10, page 2) any software or equipment that would normally be part-number controlled at the aircraft level and is applicable to all aircraft systems and associated non-structural components, including the interfaces with structural components, and items consumed by the systems, such as lubrication, fuel, and hydraulic fluid. These part numbers are typically shown on the system or aeroplane-level installation drawings. Σ A ‘failure’ means (ANM-03-117-10, page 2) failure to function as intended, i.e., a loss of function or a malfunction. Failures of sub-components, safety features, or consumable items associated with a part-number-controlled component are considered within the context of the higher-level component failure effect. The failures to be considered are based on the most severe aeroplane-level effect that cannot be reasonably ruled out by knowledgeable persons. B.7.2 federal aviation administration policy The federal aviation administration considers (ANM-03-117-10, page 3) a component to be safety critical when it has one or more of the following attributes: Σ Its single failure results in a hazardous or catastrophic failure condition (see Table B.1 above). Although the design and certification processes normally strive to eliminate single failures that could result in catastrophic events, the federal aviation administration policy is intended to also cover the continued airworthiness process where potentially catastrophic single failures may be discovered. Common cause or cascading failures are considered single failures. When specific regulations allow exceptions for potentially catastrophic single failures, such as uncontained engine failures and flight control jams, those regulations shall take precedence. Σ When a combination of two failures results in a hazardous failure condition, or a combination of three failures results in a catastrophic failure condition, every component in the combination is a flight-critical system component regardless of its individual hazard classification. There may be cases where a combination of four (or more) failures warrants additional review and validation. Σ All components contributing to a significant latent failure condition are considered flight critical. The identification of safety-critical features of the aircraft should ensure that future alterations, maintenance, repairs, or changes to operational procedures can be made with cognisance of those safety features. Appendix C GSN safety argument C.1 Introduction Any convincing argument (refer to Ch. 8 para 4 and Ch. 9 para 3.1) or report requires three elements: 1. a distinct objective(s) or goal, 2. supporting evidence, 3. a clearly discernible ‘thread’ or argument, which communicates the relationship between the evidence and objectives. A safety assessment (or safety case report) is no different. A ‘mass of evidence’ is generated during system development and certification (e.g., stress analysis, electrical load analysis, fatigue test, flight tests, performance verification, FHA, ZHA, FTA, regulatory compliance checklists, etc.) as well as during service experience (e.g., user confidence, training, etc.). This mass of evidence provides substantiation for our confidence in the safety of the system. The challenge is to tie all this evidence to our safety objective via a logical, systematic, complete safety argument. This argument can be provided in textual format but is likely to be cumbersome, and, for complex arguments, the ‘devil may get lost in the details.’ How many times have we read a wordy safety report which, on the surface, seems impressive but is a little confusing and leaves you with the question: ‘But what have we missed?’ It is easier with pictures – especially if the picture ‘carries’ the reader through the argument with sufficient, judiciously placed ‘stepping stones’ (i.e., sub-goals and subarguments down to an inevitable solution). Goal structuring notation (GSN) is a graphical notation method for developing complex arguments that explicitly represent the individual elements of any safety argument, i.e., requirements, claims, evidence, and context and (perhaps more significantly) the relationships that exist between these elements, i.e., how individual requirements are supported by specific claims, how claims are supported by evidence and the assumed context that is defined for the argument. The purpose of GSN is to provide that ‘discernible thread’ (i.e. linking the objectives with the evidence) by means of a logical argument. The fundamental approach of GSN is to show how: Σ goals are broken into sub-goals Σ and eventually supported by evidence (solutions) Σ whilst making clear the strategies adopted, Σ the rationale for the approach (assumptions and justifications) Σ the context in which the goals are stated. C.3 GSN process Most logical arguments are naturally hierarchical so that each argument can be broken down hierarchically into claims, arguments, sub-claims, sub-arguments, and eventually evidence. This suits the application of GSN, which illustrates how goals are broken into sub-goals, and eventually supported by evidence (solutions) whilst making clear the strategies adopted, the rationale for the approach (assumptions, justifications) and the context in which goals are stated. The GSN argument thus follows the following process: Σ claim/objective (i.e. what we want to show) Σ argument/premise (i.e. why we believe, subject to any assumptions/justifications, the claims met), based on Σ evidence/solutions (e.g. tests, analyses, etc.). The GSN Safety Argument can be formulated by following the process summarised in Fig. C.2 (tailored from the six-step method by Kelly). C.3.1 Step 1 The top goal is the seed from which the argument can develop and is the ultimate aim of the system safety assessment or safety case. The following guidelines apply: Σ Goals should be phrased as positive propositions, i.e., statements which can be said to be either true or false. Kelly (1998) advises that goals are best expressed in a <noun-phrase><verb-phrase>’ format (i.e. noun-phrase identifies the subject of the goals, and the verb-phrase defines the predicate over the subject) (e.g. ‘The sky is blue’) Σ Be careful of oversimplification (e.g. ‘System X is safe’ vs. ‘System X is acceptably safe within context Y’ (refer to section 2.1). C.3.2 Step 1a (Step 3a) Having presented a goal, make clear the basis on which that goal is stated. This is done by inserting context information, assumptions, Justifications and/or models which ensure that the goal is unambiguous. C.3.3 Step 2 Work out how to substantiate the goals (i.e. what reasons are there for saying the goal is ‘true’. This may require that you break the argument down into a number of smaller goals. Two options are available: if the argument is implicit, go to Step 3 (i.e. straight from goal to sub-goal) or, if an explicit argument is required, then insert it between the goal and the sub-goal. Kelly (1998) advises that strategies are best expressed in the noun-phrase form: ‘Argument by … <approach>’ (e.g. ‘argument by consideration of historical data’) C.3.4 Step 2a As per Step 1a (i.e. ask yourself what information is required in order to expand/fulfil the strategy outlined). C.3.5 Step 3 Having identified an approach, it is necessary to lay out the goals that fulfil that approach (i.e. by going into sub-goals). Here it is important not to lose the argument by making too big a leap. As soon as the question ‘why’ is raised, then consideration should be given to going ‘up’ a level to provide another ‘stepping stone’ to the argument. Kelly (1998) advises concentrating on the breadth of the argument first, before getting wrapped-up in the depth of it. C.3.6 Step 4 Eventually, faced with a goal that does not need further expansion/refinement/ explanation, add (or reference) the solution. Ideally solutions should be noun-phrases (e.g. ‘software tests result XYZ’) (Kelly 1998), but is it often useful to refer to reports/assessments where the solutions can be found (e.g. an functional hazard assessment need not be taken from tabular format into individual GSN arguments for each functional failure mode). C.3.7 Step 4a Declare (or reference) any assumptions needed in the development of the solution. A solution might be ‘not applicable’, in which case a ‘justification’ will be required. C.4 Discussion Argument without supporting evidence is unfounded, and therefore unconvincing. Evidence without argument is unexplained – it can be unclear that (or how) safety objectives have been satisfied (Kelly and Weaver, 1994). GSN is most useful wherever: Σ there is most uncertainty about the argument (i.e. key claims and evidence) Σ the argument is currently confused or is over-complex Σ there is disagreement about the argument Σ the consequences of having a wrong argument are high. Advantages of using a GSN Safety Argument include: Σ improved comprehension of existing arguments Σ useful way to define safety assessment/case strategy Σ easy to read – even for a novice Σ presents logical argument to get from a goal to its logical solution (forces a logical argument) Σ identifies holes in an argument Σ positively identifies assumptions Σ removes ambiguity (i.e. measurable goals have to be defined) Σ assists in managing programme risk (i.e. solution planning and prioritising) Σ easy to audit Σ prevents duplication of solutions Σ prevents unnecessary work (e.g. if not required by a goal) Σ defines scope of work, so assists in planning and budgeting Σ arguments can be reused in another project. However, GSN is not without its limitations: Σ It takes a lot of effort to develop the arguments. Needs experience and skill to do it efficiently. Σ Can easily go into too much complicated detail (e.g. sometimes it is more efficient to make the solution a separate compliance matrix rather than trying to argue compliance via GSN). Σ Arguments are always subjective, so every person will compile a GSN differently. A lot of time can be spent agreeing an argument instead of getting on with the required solutions. So, it may be more efficient to restrain GSN to a top-level argument only and not to repeat each finding which exists in tabular format (e.g. such as in a functional hazard assessment). Σ GSN is not as user friendly in hard copy format, because a complex and large GSN needs hyperlinks to facilitate ease of use. The high-level safety argument must be developed as early as possible as it provides a clear picture of the methodology by which safety will be substantiated. If agreed with the applicable authority, this argument scopes all future safety-related activities to generating lower-level arguments and solutions. If the high-level argument shows (with justification) that a particular solution is not applicable, then no further action is required for that solution. Possible approaches to the inclusion of GSN argument in a document such as a safety assessment/safety case include: Σ in full as an appendix/annex to the report Σ as a chapter/paragraph in a report, which guides the reader through a potentially confusing and intimidating report Σ as an ‘executive summary’ at the beginning of the report Σ as a separate, stand-alone, index document (i.e. to link separate documents together). Abbreviations and acronyms AC Advisory Circular ACJ Advisory Circular Joint ACS Aircraft Systems automatic direction finder Automatic Direction Finder ADRP Airworthiness Design Requirements and Procedures (UK MoD) ADU Air Data Unit AECMA Aircraft European Manufacturers Association AFCC Air Force Command Council as low as reasonably possible As Low As Reasonably Practicable ALC Air Logistics Command acceptable means of compliance Advisory Material Circular AMJ Advisory Material Joint APD Air Publications Depot auxiliary power unit Auxiliary Power Unit ARIBA ATM system safety criticality Raises Issues in Balancing Actors responsibility aeronautical radio incorporated Aeronautical Radio Inc. ARP Aerospace Recommended Practices ASIP Airframe Structural Integrity Program airline transport association Air Transport Association of America air traffic control Air Traffic Control ATM Air Traffic Management ATS Air Transport System AWO All Weather Operations BCAR British Civil Aviation Regulation BS British Standard C of G Centre of Gravity civil aviation authority Civil Aviation Authorities civil aviation authority Civil Aviation Administration CAF Chief of the Air Force calibrated air speed OPS Chief of Air Staff Operations common cause analysis Common Cause Analysis critical design review Critical Design Review CEO Chief Executive Officer CFDS Chaff and Flare Dispensing System CFE Customer Furnished Equipment CFIT Controlled Flight Into Terrain CFT Certificate for Flight Trials confidential human factors incident reporting programme Confidential Hazard & Incident Reporting Program CIDS Critical Item Development Specification (C-Spec) configuration management Configuration Management configuration maintenance and procedures Configuration Management Plan CofC Certificate of Conformance commercial off the shelf Commercial Off The Shelf commercial off the shelf Consumed Off The Shelf CS Certification Specification CSANDF Chief of the South African National Defence Force computer software configuration item Computer Software Configuration Items cockpit voice recorder Cockpit Voice Recorder CWAP Caution and Warning Advisory Panel DA Design Authority DAA Design Approval Authority DAFA Director Air Force Acquisition DAL Development Assurance Level DD Dependence Diagram DDP Declaration of Design and Performance DEF STAN Defence Standard distance measuring equipment Distance Measuring Equipment department of defense Department of Defence DRACAS Data Reporting, Analysis and Corrective Action System DSI Director System Integration EA Engineering Authority ECCM Electronic Counter Counter Measures EDA Excess Defence Articles EFIS Electronic Flight Instrumentation System EIDS Engine Instruments Display System emergency locator transmitter Emergency Locator Transmitter EMC Electromagnetic Compatibility electromagnetic interference Electromagnetic Interference ENSIP Engine Structural Integrity Program ESDU Electronic Safety Disarm Unit estimated time of arrival Event Tree Analysis european organization for civil aviation equipment European Organisation for Civil Aviation Equipment EW Electronic Warfare F Frequency (i.e. the average rate at which an event will occur) federal aviation administration Federal Aviation Administration FAD First Aircraft Delivery FADEC Full Authority Digital Engine Control FAR Federal Aviation Regulations FCS Flight Control System flight data recorder Flight Data Recorder FOD Foreign Object Damage flight data recorder Final Design Review functional hazard assessment Functional Hazard Assessment FL Flight Level FMC Flight Material Certificate failure mode and effects analysis Failure Modes and Effects Analysis failure mode and effect criticality analysis Failure Modes, Effect and Criticality Analysis flight management system Flight Management System FRACAS Failure Reporting, Analysis and Corrective Action System fault tree analysis Fault Tree Analysis FTRR Flight test readiness Review GCU Generator Control Unit GM Guidance material GPS Global Positioning System GSN Goal Structuring Notation H/W Hardware hazard and operability study Hazard and Operability Study human factors High Frequency high intensity infrared fields High Intensity Radio Frequency HSA Hard Systems Approach HSC Health and Safety Commission HSE Health and Safety Executive HSWA Health and Safety at Work Act HUMS Health & Usage Monitoring System international civil aviation organization International Civil Aviation Organisation ICD Interface Control Document IEC International Electrotechnical Commission indentify friend or foe Identification Friend or Foe instrument flight rules Instrument Flight Rules instrument landing system Instrument Landing System instrument landing system Integrated Logistic Support instrument meteorological conditions Instrument Meteorological Conditions INS Inertial Navigation System integrated product team Integrated Project Team ISP Integrated Support Plan ISSA Interim System Safety Assessment joint aviation authority Joint Aviation Authorities JAR Joint Aviation Regulations LOC Loss of Control line replaceable units Line Replaceable Unit LSP Logistic Support Plan MA Markov Analysis MAA Military Airworthiness Authority MAWS Missile Approach and Warning System MCDU Multi-functional Control and Display Unit minimum equipment list Minimum Equipment List MIL-STD Military Standard master minimum equipment list Master Minimum Equipment List MoD Ministry of Defence MRI Master Record Index mean time between failures Mean Time Between Failures NAA National Approval Authority NPA Notice for Proposed Amendment notice of proposed rulemaking Notice of Proposed Rule Making original equipment manufacturer Original Equipment Manufacturer OHSA Occupational Health and Safety Act OPS Operations P Probability of occurrence p Probability per unit time (usually per hour) PCCB Project Configuration Control Board preliminary design review Preliminary Design Review PE Professional Engineer PECP Project Engineering Change Proposal PIDS Prime Item Development Specification (B-Spec) probabilistic risk assessment Particular Risk Analysis preliminary system safety assessment Preliminary System Safety Assessment Q Probability of event not occurring QA Quality Assurance QB Qualification Board QMG Quality Management Group R Reliability (i.e. the probability of success) ROTE Release for Operational Test and Evaluation radio technical commission for aeronautics Radio Technical Commission for Aeronautics radio technical commission for aeronautics Requirements and Technical Concepts for Aviation RTS Release to Service S/W Software SAAF South African Air Force society of automotive engineers Society of Automotive Engineers (Aerospace Division) SAPT South African Project Team SAS Software Accomplishment Summary SDRL Supplier Data Requirements List Sec Def Secretary of Defence SELCAL Selective Calling SIL Safety Integrity Level SMP Safety Management Plan safety management system Safety Management System SOP Standard Operating Procedures statement of work Statement of Work SQA Software Quality Assurance SRS Software Requirement Specification system safety analysis System Safety Assessment SSAP System Safety Assessment process t Elapsed time T Fixed period of time TACAN Tactical Air Navigation System traffic control and air navigation system Traffic Collision Avoidance System TRU Transformer Rectifier Unit TWA Transworld Airlines UK United Kingdom URS User Requirement Statement USA United States of America USAF United States Air Force VDD Version Description Document visual flight rules Visual Flight Rules very high frequency Very High Frequency vhf ominidirectional range very high frequency Omni-directional radio ranging ZHA Zonal Hazard Assessment Definitions Accident An unplanned event or series of events resulting in death, injury, occupational illness to people, or damage to the environment. Accident severity category Qualitative description of worst-case credible consequences of hazard. Airworthiness The condition of an item (aircraft, aircraft system, or part) in which that item operates in a safe manner to accomplish its intended function (SAE ARP4754 p. 75). The ability of an air system to operate without significant hazard to aircrew, ground crew, passengers (where relevant) or to the general public or friendly military personnel over which such airborne systems are flown. The concept of airworthiness defines the condition of an air system/subsystem, and supplies the basis for the judgement of its suitability for flight operations in its intended role and application, in that it has been designed and manufactured, and is managed, maintained and operated, to approved standards and limitations, by competent and approved individuals, who are acting as members of approved organisations, and whose work is authorised, certified as correct, and accepted on behalf of the Air Force (SAAF). as low as reasonably possible As low as reasonably practicable defines the region in which the risk taken is acceptable only if justified by the benefits, and where the cost of further risk reduction would exceed the benefits. Analysis Generally implies a more specific, more detailed investigation. The terms ‘analysis’ and ‘assessment’ have broad definitions and the two terms are to some extent interchangeable. However, the term analysis generally implies a more specific, more detailed evaluation, while the term assessment may be a more general or broader evaluation but may include one or more types of analysis. In practice, the meaning comes from the specific application, e.g., fault tree analysis, Markov analysis, preliminary system safety assessment, etc. (AMC to CS25.1309). Annunciated Warning or indication of failure is given to the flight crew in sufficient time to react to the failure (AMC to CS25.1309). Annunciation Any form of visual or aural presentation designed to draw the attention of the flight or ground crew to an abnormal system operating condition (AMC to CS25.1309). Assessment An assessment is a more general, or broader, evaluation and may include one or more types of analysis. The terms ‘analysis’ and ‘assessment’ has broad definitions and the two terms are to some extent interchangeable. However, the term analysis generally implies a more specific, more detailed evaluation, while the term assessment may be a more general or broader evaluation but may include one or more types of analysis. In practice, the meaning comes from the specific application, e.g., fault tree analysis, Markov analysis, preliminary system safety assessment, etc. (AMC to CS25.1309). Average probability per flight hour A representation of the number of times the subject failure condition is predicted to occur during the entire operating life of all aeroplanes of the type divided by the anticipated total operating hours of all aeroplanes of that type (AMC to CS25.1309). (Note: the average probability per flight hour is normally calculated as the probability of a failure condition occurring during a typical flight of mean duration divided by that mean duration.) Candidate certification maintenance requirements (CCMR) A periodic maintenance or flight crew check may be used in a safety analysis to help demonstrate compliance with JAR 25.1309(b) for hazardous and catastrophic failure conditions. Where such checks cannot be accepted as basic servicing or airmanship they become candidate certification maintenance requirements (CCMRs) (AMC to CS25.1309). (Note: acceptable means of compliance 25.19 defines a method by which certification maintenance requirements (CMRs) are identified from the candidates. A certification maintenance requirements becomes a required periodic maintenance check identified as an operating limitation of the type certificate for the aeroplane.) Certification Certification means the legal recognition by the certification authority that a product, service, organisation or person complies with the applicable requirements. Such certification comprises the activity of technically checking the product, service, organisation or person and the formal recognition of compliance with the applicable requirements by issue of a certificate, licence, approval or other documents as required by national laws and procedures. In particular, certification of a product involves: i The process of assessing the design of a product to ensure that it complies with a set of standards applicable to that type of product so as to demonstrate an acceptable level of safety. ii The process of assessing an individual product to ensure that it conforms with the certified type design. iii The issue of any certificate required by national laws to declare that compliance or conformity has been found with standards in accordance with items (i) or (ii) above (ARP4754). Certification is the end result of a qualification process. It is the act whereby the design, manufacture and engineering quality of a product is endorsed. It entails the legal recognition by the certification authority that a product, service, organisation, or person complies with the requirements. Check An examination (e.g., an inspection or test) to determine the physical integrity and/or functional capability of an item (AMC to CS25.1309). Complex A system is complex when its operation, failure modes, or failure effects are difficult to comprehend without the airport information desk of analytical methods (AMC to CS25.1309). Applicable to systems whose architecture and logic are difficult to comprehend without the airport information desk of analytical tools (e.g. FMEAs, FTAs, RBDs, etc.) and whose safety cannot be shown solely by tests. Component A ‘component’ is any software or equipment that would normally be part-number-controlled at the aircraft level and are applicable to all aircraft systems and associated non-structural components including the interfaces with structural components and items consumed by the systems, such as lubrication, fuel, and hydraulic fluid. These part numbers are typically shown on the system or airplanelevel installation drawings (ANM-03-117-10, p. 2). Conclusion A judgement or statement arrived at by any reasoning process (Oxford English Dictionary 1991). Condition An existing or potential state such as exposure to harm, toxicity, energy source, activity, etc. Contributing factors Other conditions (whether normal states/events or coincident failures) which must hold for a given event to lead to a hazard or accident. Conventional A system is considered to be conventional if its functionality, the technological means used to implement its functionality, and its intended usage are all the same as, or closely similar to, that of previously approved systems that are commonly used (AMC to CS25.1309). Cut sets Ways in which failure can occur. Used in fault tree analysis. Design The whole intellectual process of converting a requirement into a set of manufacturing drawings. Design appraisal This is a qualitative appraisal of the integrity and safety of the system design (AMC to CS25.1309). Design rigour Extent (quantity and quality) of effort during the design process. Detected Failure is detected by the function or system and some mitigating action is automatically implemented by the function or system, which may involve annunciation of the failure (AMC to CS25.1309). Development assurance All those planned and systematic actions used to substantiate, to an adequate level of confidence, that errors in requirements, design, and implementation have been identified and corrected such that the system satisfies the applicable certification basis (AMC to CS25.1309). Development assurance level A (qualitative) specification or description of how much reliance will be placed on a particular system function. Used to prescribe the level of rigour which must be applied when developing the function. Display Any form of visual presentation of system operation information to the flight or ground crew (AMC to CS25.1309). Diversion A diversion is the landing of an aircraft at an airport other than the airport of origin or destination (AMC to CS25.1309). Erroneous indication A display where a difference of scale exists between the actual and displayed values (AMC to CS25.1309). Error An omission or incorrect action by a crew member or maintenance personnel, or a mistake in requirements, design, or implementation (AMC to CS25.1309). An act that through ignorance, deficiency, or accident departs from or fails to also be categorised as primary or contributory. Primary errors are those committed by personnel immediately and directly involved with the accident. Contributory errors result from actions on the part of personnel whose duties preceded and affected the situation during which the results of the error became apparent. The difference between a computed, observed, or measured value or condition and true, specified, or theoretically correct value or condition. A mistake in engineering, requirement specification, or design, implementation, or operation which could result in a failure, and/or contributory hazard. There are four types of human errors: 1. Omission 2. Commission 3. Sequence 4. timing. Event An occurrence which has its origin distinct from the aeroplane, such as atmospheric conditions (e.g. gusts, temperature variations, icing and lightning strikes), runway conditions, conditions of communication, navigation, and surveillance services, bird-strike, cabin and baggage fires. The term is not intended to cover sabotage (AMC to CS25.1309). Explosion proof The item is designed to withstand an internal explosion; designed to vent explosive gases below ignition temperature. Fail-operational A characteristic in design which permits continued operation in spite of the occurrence of a discrete malfunction. Fail-safe A characteristic of a system whereby any malfunction affecting the system safety will cause the system to revert to a state that is known to be within acceptable risk parameters. Fail-soft Pertaining to a system that continues to provide partial operational capability in the event of a certain malfunction. Failure An occurrence which affects the operation of a component, part, or element such that it can no longer function as intended (this includes both loss of function and malfunction). (Note: errors may cause failures, but are not considered to be failures (ACJ25.1309). The inability of a system, sub-system, component, or part to perform its required function within specified limits, under specified conditions for a specified duration. The termination of the ability of an item to perform its intended function(s), i.e., a loss of function or a malfunction. Failures of sub-components, safety features, or consumable items associated with a part-number-controlled component are considered within the context of the higher-level component failure effect. The failures to be considered are based on the most severe aeroplane-level effect that cannot be reasonably ruled out by knowledgeable persons (ANM-03-117-10, p. 2). Failure condition A condition having an effect on the aeroplane and/or its occupants, either direct or consequential, which is caused or contributed to by one or more failures or errors, considering flight phase and relevant adverse operational or environmental conditions, or external events (AMC to CS25.1309). Failure mode The way in which the failure of an item occurs. False indication A display where logical difference exists between actual and displayed conditions (AMC to CS25.1309). Formal qualification The process that allows the determination of whether a configuration item complies with the requirements allocated to it. Formal qualification review Formal evaluation by top management of the status and adequacy of the quality system in relation to quality policy and objectives. Formal verification The process of evaluating the products of a given phase using formal mathematical proofs to ensure correctness and consistency with respect to the products and standards provided as input to that phase. Fracture mechanics In materials science fracture mechanics applies the physics of stress and strain, in particular the theories of elasticity and plasticity, to the microscopic crystallographic defects found in real materials in order to predict the macroscopic mechanical failure of bodies. In modern materials science, it is an important tool in improving the mechanical performance of materials and components. Frequency The expected number of occasions of the event per unit time (usually year, or hour, or product lifetime). In reliability analysis this is also known as ‘failure rate’. Hazard Any real or potential condition that can cause injury, illness, or death to personnel, damage to or loss of a system, equipment or property, or damage to the environment (MIL-STD-882D). A set of conditions in the operation of a product with the potential for initiating or contributing to events that could result in personal injury, damage to property or harm to the environment. Hazard analysis (HA) A generic term describing a whole collection of techniques whose combined strengths have a good chance of revealing most of the hazards. The techniques chosen depend upon the industry, stage of the project, the information available and the complexity and criticality of the equipment. Hazardous material Any substance that, due to its chemical, physical, or biological nature, causes safety, public health, or environmental concerns that would require an elevated level of effort to manage (MIL-STD-882D). Highly integrated Refers to systems that perform or contribute to multiple aircraft level functions. Inadvertent An inadvertent action may be performed by the pilot who did not mean to do it (unintended but demanded operation). This term is normally used to consider the consequences of an unintended action by a crew member (ground, flight or cargo crew) (AMC to CS25.1309). Incident A near miss accident with minor consequences that could have resulted in greater loss. An unplanned event that could have resulted in an accident, or did result in minor damage, and which indicates the existence of, though may not define, a hazard or hazardous condition. Sometimes called a mishap. Initiating events Initiating events; initiator; the contributory hazard; unsafe act and/ or unsafe condition that initiated the adverse event flow, which resulted in the hazardous event under evaluation; also see root cause. Inspection A static technique that relies on visual examination of development products to detect deviations, violations or other problems. Installation appraisal This is a qualitative appraisal of the integrity and safety of the installation. Any deviations from normal, industry-accepted installation practices, such as clearances or tolerances, should be evaluated, especially when appraising modifications made after entry into service (AMC to CS25.1309). Item Any level of hardware assembly (system, sub-system, equipment, component, etc.) including associated software or firmware. Latent Present and capable of becoming, though not now visible or active. Latent failure A failure is latent until it is made known to the flight crew or maintenance personnel. A significant latent failure is one which would in combination with one or more specific failures or events result in a hazardous or catastrophic failure condition (AMC to CS25.1309). Life cycle All phases of the system’s life including design, research, development, testing and evaluation, production, deployment (inventory), operations and support, and disposal (MIL-STD-882D). Likelihood Likelihood defines in quantitative or qualitative terms, the estimated probability of the specific hazardous event under study. Likelihood is one element of associated risk (the other being severity). Fault trees and other models can be constructed and individual hazard probabilities are estimated, and likelihood can be calculated via Boolean Logic. Maintainability The ability of an item to be retained in or restored to a specified condition when maintenance is performed by personnel having specified skill levels, using prescribed procedures, resources and equipment at each prescribed level of maintenance and repair. Malfunction Failure to operate in the normal or usual manner. Any anomaly which results in system deviation. Mean time between failures (MTBF) Indicates mean life of repairable items. As the reciprocal of failure rate, mean time between failures is an alternative for describing the random failure portion of the bath-tub curve. Mean time to failure (MTTF) Indicates mean life of non-repairable items. Methodology A particular procedure or set of procedures. Mishap An unplanned event or series of events resulting in death, injury, occupational illness, damage to or loss of equipment or property, or damage to the environment (MIL-STD-882D). Mishap risk An expression of the impact and possibility of a mishap in terms of potential mishap severity and probability of occurrence (MIL-STD-882D). Mitigation Any downstream circumstances or functions that reduce the probability of a malfunction escalating to an accident. Objective evidence Information that can be proved true, based on facts obtained through observation, measurement, test or other means. Optimum safety The associated risks that have been identified have been accepted provided that all identified controls are implemented and enforced. Pareto principle In 1906, Italian economist Vilfredo Pareto created a mathematical formula to describe the unequal distribution of wealth in his country, observing that twenty per cent of the people owned eighty per cent of the wealth. In the late 1940s, Dr Joseph M. Juran inaccurately attributed the 80/20 rule to Pareto, calling it Pareto’s Principle. While it may be misnamed, Pareto’s Principle or Pareto’s Law as it is sometimes called, can be a very effective tool to help effective management. As a result, Dr Juran’s observation of the ‘vital few and trivial many’, the principle that 20 per cent of something always are responsible for 80 per cent of the results, became known as Pareto’s Principle or the 80/20 Rule. Partitioning Partitioning is a technique for providing isolation between functionally independent software components to contain and/or isolate faults and potentially reduce the effort of the software verification process. Path sets Routes to success. Phase Defined segment of work. (Note: a phase does not imply the use of any specific life-cycle model, nor does it imply a period of time in the development of a product.) Practice Recommended methods, rules, and designs for voluntary compliance. Premise A previous statement or proposition from which another is inferred or follows as a conclusion (Oxford English Dictionary 1991). Probability The probability of the event occurring in a given time period or the conditional probability of it occurring given that a previous event has occurred. Process Set of interrelated resources and activities, which transform inputs into outputs. Product Any system in the form of an integrated platform, facility, sub-system, equipment, software, or service, which is either to be provided to a customer or taken into service by the company as an entity, or being developed for such purposes. Product liability Generic term used to describe the onus on a producer or others to make restitution for loss related to personal injury, property damage or other harm caused by a product. Product safety The risks of hazards to operators, the public, property and the environment, when used for their intended purpose. Or in any reasonably foreseeable way, including disposal of the product. Product service history Historical data generated by activities at the interface between the supplier and the customer and by supplier internal activities to meet the customer needs regarding the quality, reliability and safety trends of the product or service. Qualification Qualification is the systematic process during the design of an aircraft or airborne system, of demonstrating conformance to the design objective/requirement and a set of specific and predetermined airworthiness regulations for a specific type and category of aircraft. These regulations, (such as FARs, DEF STAN 00-970, etc.), are determined by the relevant airworthiness authority. The qualification process is satisfied as soon as it is objectively proven that the laid down regulations and requirements for that specific aircraft type and category have been satisfied so as to ensure continuous airworthiness. Qualification process Process of demonstrating whether an entity is capable of fulfilling specified requirements. Qualitative Those analytical processes that assess system and aeroplane safety in an objective, non-numerical manner (AMC to CS25.1309). Quantitative Those analytical processes that apply mathematical methods to assess system and aeroplane safety (AMC to CJ25.1309). Random failure Failure that results from a variety of degradation mechanisms in the hardware. Failure rates arising from these are assumed to be constant over the useful life of the item and can be quantified with reasonable accuracy. Redundancy Multiple independent methods incorporated to accomplish a given function, each one of them is sufficient to accomplish the function or flight operation (AMC to CS25.1309). Reliability The probability ratio that a system or product will perform in a satisfactory manner under stated conditions for a stated period of time, assuming it was in proper condition at the mission beginning. Requirements Statements describing essential, necessary or desired attributes. Requirements specification Specification that sets forth the requirements for a system or system component. Risk An expression of the possibility and impact of an event in terms of hazard severity and hazard probability. In other words, it is the combined effect of the probability of occurrence of an undesirable event, and the severity of that event. Safe life The safe life design technique is employed in critical systems which are either very difficult to repair or may cause severe damage to life and property. These systems are designed to work for years without requirement of any repairs. Safety Freedom from those conditions that can cause death, injury, occupational illness or damage to or loss of property, or damage to the environment. It is the state in which risk is lower than the boundary risk. The boundary risk is the upper limit of acceptable risk. It is specific for a technical purpose or state (SAE ARP 4754, p. 80). Freedom from those conditions that can cause death, injury, occupational illness, damage to or loss of equipment or property, or damage to the environment (MILSTD- 882D). Safety assessment The safety assessment is a structured body of evidence that provides a convincing and valid argument that a system is adequately safe for a given application in a given environment. It is a collection of documents that, taken together, provide objective evidence that all reasonable steps were taken to ensure product safety. It may also provide data that the customer finds helpful throughout the life of the product. Note that the safety assessment is applicable to one specific point in time only and is a deliverable to the system manager/owner. Safety case The Health and Safety Commission defines a safety case as: ‘a suite of documents providing a written demonstration that risks have been reduced as low as is reasonably practicable. It is intended to be a living dossier which underpins every safety-related decision made by the licensee.’ Note that it is a living dossier, which implies that it needs continuous management to ensure its currency and validity. This implies that it falls within the remit of the system owner/manager. The safety case(s) of an organisation are subordinate to the corporate safety management system but are used to interact with the safety management system (i.e. the safety management system is the facilitator of a live safety case; the safety case, in turn, can receive inputs from a safety assessments. Safety critical A term applied to a condition, event, operation, process or item which is essential to safe system operation or use (e.g. safety critical function, safety critical path, safety critical item, etc.). All interactions, elements, components, subsystems, functions, processes, interfaces, within the system that can affect a predetermined level of risk. Safety critical computer software module Those computer software modules whose errors can result in a hazardous or catastrophic or critical severity. Safety critical item An item whose failure can cause hazards of catastrophic or critical severity. Safety incident Any unplanned event or series of events, other than an actual accident, which have the potential to cause death, injury, or occupational illness to people; or otherwise cause damage to the environment. Safety integrity level (SIL) The likelihood of a safety related system satisfactorily performing the required safety functions under all the stated conditions within a stated period of time. An indication of the required level of protection against failure (degree to which a component must be free from flaws). Safety management The application of engineering and management principles and techniques in order to optimise all aspects of safety within constraints of operational effectiveness, time and cost. It is a systematic and explicit approach to managing safety. A methodology that drives safety as a measurable design parameter (ensuring that an acceptable level of safety is designed into the product) and provides a form of measure of that achievement. Safety management system A ‘safety management system’ is an explicit element of the corporate management responsibility which sets out a company’s safety policy and defines how it intends to manage safety as an integral part of its overall business. The safety management system is a management tool for executing safety throughout the life cycle of a project. Safety monitoring Safety monitoring, as related to digital systems, is a means of protecting against specific failure conditions by directly monitoring a function for failures that could contribute to the failure condition. Monitoring functions may be implemented in hardware, software, or a combination of both. Through the use of monitoring techniques, the software level of the monitored function may be reduced to the level associated with the loss of its related function. To allow this level of reduction, there are four important attributes of the monitor that should be determined. 1. Software level. Safety monitoring software is assigned the software level associated with the most severe failure condition category for the monitored function. 2. System fault coverage. Assessment of the system fault coverage of a monitor ensures that the monitor’s design and implementation are such that the faults which it is intended to detect will be detected under all necessary conditions. 3. Independence of function and monitor. The monitor and protective mechanism are not rendered inoperative by the same functional failure condition that causes the hazard. 4. Hardware integrity. The monitor hardware integrity will need to be commensurate with the hazard. A configuration which requires high-integrity monitor software but proposes low-integrity monitor hardware would be unacceptable. Severity An expression of consequence used in the assessment of a specific hazard. Severity category Qualitative description of worst case credible consequences of hazard. Sub-system An element of a system that, in itself, may constitute a system. A grouping of items satisfying a logical group of functions within a particular system (MIL-STD-882D). System A combination of components, parts, and elements which are interconnected to perform one or more functions (AMC to 25.1309). An integrated composite of people, products, and processes that provide a capability to satisfy a stated need or objective (MIL-STD-882D). System safety The application of engineering and management principles, criteria, and techniques to achieve acceptable mishap risk, within the constraints of operational effectiveness and suitability, time, and cost, throughout all phases of the system life cycle (MIL-STD-882D). A standardised management and engineering discipline that integrates the consideration of man, machine, and environment in planning, designing, testing, operating, procedures, and acquisition projects. System safety is the systematic process of the identification and resolution of hazards during the life cycle of an aircraft or airborne system. The resolutions of identified hazards are basically through three means: 1. the elimination (normally by design) of an identified hazard 2. the control of a hazard during testing or operational usage of an aircraft, or airborne system 3. the acceptance of a hazard without any elimination or control action where the hazard criticality and probability is sufficiently low to be able to accept the risk. System safety analysis The analysis of a complex system by means of methods, techniques, and/or processes, to comprehensively evaluate safety-related risks that are associated with the system under study. System safety engineer An engineer qualified by appropriate credentials: training, education, registration, certification, and/or experience to perform system safety engineering should have an appropriate background and credentials directly related to system safety in order to practise in the field, i.e., CSP, PE, training, education, and actual experience. System safety engineering An engineering discipline requiring specialised professional knowledge and skills in applying scientific and engineering principles, criteria, and techniques to identify and eliminate, or reduce safety-related risks (refer MILSTD-882D et al.). System safety manager A person responsible for managing a system safety programme. System safety working group A formally charted group of persons representing organisations associated with the system under study, organised to assist management in achieving the system safety objectives. Systematic failures A failure caused by errors in the specification, design, construction operation or maintenance which cause the item to fail under some particular combination of inputs or conditions. All software failures are systematic failures. Systems approach A step-by-step procedure for solving problems; a decision-making process which moves from the general to the specific; an iterative process. Technical airworthiness A concept, the application of which defines the condition of an aircraft, and supplies the basis for judgement of the suitability for flight of that aircraft in that it has been designed, constructed, operated and maintained to approved standards by competent individuals, who are acting as members of an authorised organisation and whose work is both certified as correct and accepted on behalf of the regulatory authority. Type certificate The type certificate is considered to include the type design, the operating limitations, the type certificate data sheet, the applicable requirements with which the authority records compliance, and any other conditions or limitations prescribed for the product in the appropriate regulatory code (JAR21.4). Unanunciated No warning or indication is given to the flight crew, or warning/ indication gives insufficient time for the flight crew to react to the failure (AMC to CS25.1309). Uncommanded This term is used to consider the consequences of a system/equipment failure resulting in the ‘unintended and undemanded’ operation (AMC to CS25.1309). Undetected Failure is not detected by the function or system or no timely mitigating action is possible, and the failure is not annunciated (AMC to CS25.1309). Waiver A written authorisation by the engineering authority to accept an item (or limited quantity of), which during manufacture, or after submission for inspection or acceptance, is found to depart from specified requirements, but nevertheless is considered suitable for use ‘as is’ or after repair by an approved method. Also known as a concession.