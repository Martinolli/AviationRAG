Title: Managing The Risks Of Organizational Accidents – Chapters 7 – 8 - 9 - 10 Author(s): James Reason Category: Management, Risks Tags: Risks, Organization, Accidents, Aircraft Chapter 7 A Practical Guide to Error Management What is Error Management? Error management (EM) has two components: error reduction and error containment. Error reduction comprises measures designed to limit the occurrence of errors. Since this will never be wholly successful, we also need error containment measures designed to limit the adverse consequences of those errors that still occur. At this general level, EM is indistinguishable from quality management or, indeed, from good management of any kind. Error management includes: • Measures to minimize the error liability of the individual or team. • Measures to reduce the error vulnerability of particular tasks or task elements. • Measures to discover, assess, and then eliminate error-producing (and violation-producing) factors within the workplace. • Measures to diagnose organizational factors that create error-producing factors within the individual, the team, the task, or the workplace. • Measures to enhance error detection. • Measures to increase the error tolerance of the workplace or system. • Measures to make latent conditions more visible to those who operate and manage the system. • Measures to improve the organization's intrinsic resistance to human fallibility. Ancient but often Misguided Practices There is nothing new in the idea of error management. The rope's end and the whip were probably among its first instruments. However, the implementation of a principled and comprehensive program is very rare. Most attempts at error management are piecemeal rather than planned, reactive rather than proactive, and event-driven rather than principle-driven. They also largely ignore the substantial developments that have occurred in the behavioral sciences over the last 20-30 years in understanding the nature, varieties, and causes of human error.1 Some of the problems associated with existing forms of EM include the following: • They 'firefight' the last error rather than anticipating and preventing the next one. • They focus on active failures rather than latent conditions • They focus on the personal rather than the situational contributions to error. • They rely heavily on exhortations and disciplinary sanctions. • They employ blame-laden and essentially meaningless terms such as 'carelessness,' 'bad attitude,' and 'irresponsibility'--even in Total Quality Management (TQM).2 • They do not distinguish adequately between random and systematic error-causing factors. • They are generally not informed by current human factors knowledge regarding error and accident causation. Errors are Consequences, not Causes At the time of writing, I had before me a newspaper headline that read, 'Human error is blamed for the crash.' This is a source of confusion rather than clarification. In aviation and elsewhere, human error is one of a long-established list of 'causes' used by the press and accident investigators. But human error is a consequence, not a cause. Errors, as we have seen in earlier chapters, are shaped and provoked by upstream workplace and organizational factors. Identifying an error is merely the beginning of the search for causes, not the end. The error, just as much as the disaster that may follow it, is something that requires an explanation. Only by understanding the context that provoked the error can we hope to limit its recurrence. So why are people so ready to accept human error as an explanation rather than as something that needs explaining? The answer is deeply rooted in human nature. Psychologists call it the fundamental attribution error. When we see or hear of someone performing badly, we attribute this to some enduring aspect of the individual's personality. We say that he or she is careless, silly, stupid, incompetent, reckless or thoughtless. But if you were to ask the person in question why they are behaving in this fashion, they would almost certainly point to the local situation and say they had no choice- circumstances forced them to do it that way. The reality, of course, lies somewhere in between. The Blame Cycle Why are we so ready to blame people rather than situations? Part of the answer lies in the illusion of free will. It is this that makes the fundamental attribution error so basic to human nature. People, especially in Western cultures, place great value on the belief that they are free agents, the captains of their own fate. They can even become physically or mentally ill when deprived of this sense of personal autonomy. Placing so much value, as we do, on this sense of individual freedom, we naturally assume that other people are similarly the controllers of their own destinies. They are also seen as free agents, able to choose between right and wrong and between correct and error-prone paths of action. When people are given accident reports to read and asked to judge which causal factors were the most avoidable, they almost invariably identify the human actions. They are seen as far less constrained or fixed than any of the situational or organizational contributions. It is this, together with the illusion of free will, that drives the blame cycle shown in Figure 7.1. Because people are regarded as free agents, their errors are seen as being, at least in part, voluntary actions. Although deliberate wrongdoing attracts warnings, sanctions, threats, and exhortations not to do it again, these have little or no effect on the error-producing factors, and so errors continue to be involved in incidents and accidents. Now, the bosses are doubly aggrieved. People have been warned and punished, but they persist in making errors. Now, they seem to be deliberately flouting the management's authority, and those who commit subsequent errors are given even stronger warnings and suffer heavier sanctions. And so the cycle goes on. Of course, people can behave carelessly and stupidly. We all do at some time or another. But a stupid or careless act does not necessarily make a stupid or careless person. Everyone is capable of a wide range of actions-sometimes inspired, sometimes foolish-but mostly somewhere in between. One of the basic principles of error management is that the best people can sometimes make the worst errors. So, how do we break free of the blame cycle? We must recognize the following basic facts about human nature and error. • Human actions are almost always constrained by factors beyond an individual's immediate control. • People cannot easily avoid those actions that they did not intend to perform in the first place. • Errors have multiple causes: personal, task-related, situational, and organizational factors. • Within a skilled, experienced, and largely well-intentioned workforce, situations are more amenable to improvement than people. People or Situations? Human behavior is governed by the interplay between psychological and situational factors. Free will is an illusion because our range of actions is always limited by the local circumstances. This applies to errors as to all other human actions. Such claims raise a crucial question for all those in the business of minimizing potentially dangerous errors: which is the easiest to remedy, the person or the situation? Common sense and general practice would suggest that it is the person. After all, people can be retrained, disciplined, advised, or warned in ways that will make them behave more appropriately in the future, so it is widely believed. This view is especially prevalent in professions that take pride in their willing acceptance of personal responsibility, such as pilots, engineers, and the like. Situations, in contrast, appear as givens: we seem to be stuck with them. But is this actually the case? Not really. The balance of scientific opinion clearly favors the situational rather than the personal approach to error management. There are many reasons for this. • Human fallibility can be moderated up to a point, but it can never be eliminated entirely. It is a fixed part of the human condition, partly because errors, in many contexts, serve a useful function (for example, trial-and-error learning in knowledge-based situations). • Different error types have different psychological mechanisms, occur in different parts of the organization, and require different methods of management. • Safety-critical errors happen at all levels of the system, not just at the sharp end. • Measures that involve sanctions, threats, fear, appeals, and the like have very limited effectiveness. And, in many cases, they can do more harm to morale, self-respect, and a sense of justice than good. • Errors are a product of a chain of causes in which the precipitating psychological factors -momentary inattention, misjudgment, forgetfulness, and preoccupation- are often the last and least manageable links in the chain. • The evidence from a large number of accident inquiries indicates that bad events are more often the result of error-prone situations and error-prone activities than they are of error-prone people. Such people do, of course, exist, but they seldom remain at the hazardous sharp end for very long. Quite often, they get promoted to management. An Overview of the Error Management Tool Box It is not within the scope of this book to make a definitive review of all the available techniques that relate either directly or indirectly to error management in the broadest sense of· the term. We will not, for example, deal with the many measures that have evolved over the years and are now widely used. These include selection, training, licensing, and certification, skill checks, human resource management, quality monitoring and auditing, technical safety audits, unsafe act auditing, hazard management systems, procedures, checklists, rules and regulations, administrative controls (for example, permit-to-work systems), TQM and the like. Nor will we consider those instruments that are largely the province of reliability- engineers and technical specialists. These would include probabilistic safety assessment (P5A), other event and fault tree analyses, human reliability analysis (HRA), human error analysis (HEA), hazard and operability studies (HAZOPs), failure modes and effects analysis (FMEA), and similar measures. It is not that these techniques are regarded as unimportant; on the contrary, they form an essential part of the safety manager's tool. However, they have already been discussed at length in a number of excellent recent books.6 Our focus here will be on those techniques of EM that relate directly to the issues discussed throughout this book, and most particularly upon those measures that follow the principles of safety assessment set out in Chapter 6. It is hoped that what the resulting selection lacks in scope will make up for novelty and practical utility. Comprehensive error management can and should be directed at several different levels of the organization, individual and team, the task, the workplace, and the organizational processes. Many organizations already target most of their EM resources at the individual, as indicated earlier. For this reason, and because it is more in keeping with the book's emphasis on contextual rather than personal factors, we will not discuss individual measures any further. Nor will we deal with team-related measures, which have been covered very widely elsewhere. Over the last 15 years, the major airlines have been training their flight crews and, more importantly, their maintenance engineers in Crew Resource Management (or Cockpit Resource Management). This technique has proved to be very successful in improving flight deck performance, particularly with regard to better sharing of situational awareness, improved communications, and enhanced leadership skills. However, cockpit resource management techniques, their strengths, and pitfalls have already been the subject of a number of recent books and so will not be pursued further here. That leaves only three levels for consideration: the task, the workplace, and the organization. A technique for identifying error-prone steps in maintenance tasks was described in Chapter 5 and will not be repeated here. This brings us to the main areas for consideration: reactive and proactive tools designed to reveal and correct error-producing factors at both the workplace and the organizational levels. The measures to be discussed are summarized in Table 7.1. In the descriptions that follow, Tripod-Delta will be discussed in more detail than the other proactive process-measuring instruments. There are a number of reasons for this: • Tripod-Delta is the 'ancestor' of both Review and MESH. As such, it embodies all the principles that underlie these later techniques. • Tripod-Delta has now been in use for several years, so we know more about its strengths and weaknesses than is the case for the other techniques. • Tripod-Delta has been tested on a wide variety of continents, cultures, and operations, from North America and Northern Europe through the Middle East, Africa, and Southeast Asia to Australia. It has also been applied successfully in a maritime setting. Tripod-Delta Tripod-Delta was created for the oil exploration and production operations of software hardware environment liveware Internationale Petroleum Maatschappij (now software hardware environment liveware International Exploration and Production BV) by a research team from the Universities of Leiden and Manchester.8 The Tripod project began in 1988. The technique was developed in various software hardware environment liveware operating companies from 1989-1992. The first version was issued Shell-wide in 1993, and the present revised version, known as Tripod- -Delta (to distinguish it from its close relative, Tripod-Beta)-was released in 1996. Tripod-Delta has three elements: • A coherent safety philosophy that leads to the setting of attainable safety goals. • An integrated way of thinking about the processes that disrupt safe operations. • A set of instruments for measuring these disruptive processes- termed General Failure Types (GFTs)-that does not depend upon incident or accident statistics (that is, outcome measures). During Tripod-Delta's development, Shell's principal safety metric was the LTIF, the number of lost-time injuries per million manhours. The reduction of lost-time injuries (LTIs) was the main focus for the Tripod programme. But Tripod-Delta did not seek to reduce LTIs directly. It operated at one level removed by addressing the General Failure Types, the situational and organizational factors that provoked LTIs. The underlying philosophy can be summarized as follows: • Safety management is essentially an organizational control problem. • The trick is to know what is controllable and what is not. • Unsafe acts, LTIs, and accidents are born from the union of two sets of parents: General Failure Types (see below for a full description) and local triggering factors. • But only one of these parents is knowable in advance and thus potentially correctable before it causes harm, namely, GFTs or the latent conditions associated with specific organizational processes. • LTIs are like mosquitoes. It is pointless trying to deal with them one by one. Others appear in their place. The only long-term solution is to drain the swamps in which they breed- that is, the General Failure Types. • Effective safety management depends upon the regular measurement and selective remediation of the GFTs. The Tripod-Delta instruments are designed to guide this process. Tripod takes its name from the three-part structure illustrated in Figure 7.2. This figure also summarizes the main elements of the Tripod philosophy. The bottom right-hand 'foot' of Figure 7.2 represents the traditional concern of safety management: the performance of unsafe acts in hazardous circumstances. On occasion, these penetrate the defenses to produce bad outcomes. In the past, most remedial measures had been directed at these lower two 'feet.' Strenuous efforts were made to train and motivate people to work safely. The defenses were regularly monitored and improved. And events were investigated in order to prevent their recurrence. Tripod-Delta adds a third and most important dimension- the measurement and control of the GFTs. The GFTs are identified in part from the recurrent latent conditions associated with past events (indicated by the arrow linking events to GFTs). The GFTs, in turn, create the conditions that promote or exacerbate unsafe acts (indicated by the arrow linking GFTs with unsafe acts). The nature, measurement, and control of the GFTs are discussed below. After observing operations in a number of operating companies and studying their accident records, 11 GFTs were chosen as best reflecting those workplace and organizational factors most likely to contribute to unsafe acts and, hence, create LTIs. They are listed below: • Hardware. This relates to the quality and availability of tools and equipment. Its principal components would include policies and responsibilities for purchase, quality of stock system, quality of supply, theft and loss of equipment, short-term renting, compli- ance to specifications, age of equipment, non-standard use of equipment and so on. • Design. Design becomes a GFT when it leads directly to the commission of errors and violations. There are three main classes of problem: a failure on the part of the designer to provide external guidance (the knowledge gulf); designed objects are often opaque with regard to their inner workings or to the range of safe actions (the execution gulf); and the failure of designed items to provide feedback to the user (the evaluation gulf). • Maintenance management. This GFT is concerned with management rather than the execution of maintenance activities (which are covered by other GFTs). Was the work planned safely? Did maintenance work or an associated stoppage cause a hazard? Was maintenance carried out in a timely fashion? • Procedures. This relates to the quality, accuracy, relevance, availability, and workability of procedures. • Error-enforcing conditions. These are conditions relating either to the workplace or to the individual that can lead to unsafe acts. They break down into two broad (and, to a degree, overlapping) categories: error-producing conditions and violation-promoting conditions. Error-enforcing conditions are influenced by many of the 'upstream' GFTs, as shown in Figure 7.3. • Housekeeping. This constitutes a GFT when problems have been present for a long time and when various levels of the organization have been aware of them but nothing has been done to correct them. Its 'upstream' influences include: inadequate in- vestment, insufficient personnel, poor incentives, poor definition of responsibility, poor hardware. • Incompatible goals. Goal conflicts can occur at any of three levels: - individual goal conflicts caused by preoccupation or domestic concerns group goal conflicts, when the informal norms of a work group are incompatible with the safety goals of the organ- ization - conflicts at the organizational level in which there is incom- patibility between safety and productivity goals. • Communications. Communication problems fall into three categories: - system failures in which the necessary channels of communication do not exist, or are not functioning, or are not regularly used - message failures in which the channels exist but the necessary information is not transmitted - reception failures in which the channels exist, the right message is sent, but it is either misinterpreted by the recipient or arrives too late. • Organization. This concerns organizational deficiencies that blur safety responsibilities and allow warning signs to be overlooked. The three main components are organizational structure,- organizational responsibilities, and the management of contractor safety. • Training. Problems include failure to understand training requirements, downgrading training relative to operations, obstruction of training, insufficient assessment of results, poor mixes of experienced and inexperienced personnel, poor task analyses, inadequate definition of competence requirements, and so on. • Defences. These comprise detection, warning, personnel protection, recovery, containment, escape, and rescue failures. The Tripod-Delta assessments for any particular type of operation (for example, drilling, seismic, engineering, road transport, shipping, and the like) are derived from checklists based upon specific indicators or symptoms of the presence and degree of each GFT. These indicators are obtained directly from task specialists, that is, from those involved in the day-to-day management and operation of each particular activity. Those on the spot have vital (and usually untapped) knowledge about what is safe and what is dangerous in a particular type of work. Tripod-Delta is tailored specifically for their use, thus ensuring that they see the information provided as relevant to their job. This means that the instruments are built and owned by those actually carrying out the core business. Each indicator relates to a tangible item- something that can be observed directly within the facility or found within a filing system- and requires a simple 'yes/no' response. Below are listed some indicator items relating to design on an offshore platform: • Was this platform originally designed to be unmanned? • Are shutoff valves fitted at a height of more than two meters? • Is standard (company) coding used for the pipes? • Are there locations on this platform where the deck and the walkways differ in height? • Have there been more than two unscheduled maintenance jobs over the past week? • Are there any bad smells from the low-pressure vent system? The indicators are created by task specialists working in syndicates. The Tripod-Delta software, which is implemented on a PC, stores the indicator databases (one database for each GFT for each type of operation). It constructs the measuring instrument for each testing occasion by selecting 20 indicators for each GFT and then produces a 220-item checklist to be completed by a member of the workforce (for example, a drilling supervisor). Only a small proportion of items recur from one checklist to the next. The checklist can be presented either on the computer screen or in hard copy. Once completed, the data are analyzed by the software, which generates a Failure State Profile bar chart showing the relative cause for concern for each of the 11 GFTs (see Figure 7.4). In particular, it will identify those two or three GFTs most in need of immediate attention. The software will also analyze trends over time and archive the data from many sites. Tripod-Delta testing sessions usually take place on a quarterly basis, though this can be adjusted to suit the pace of change in a particular work site. The key to using Tripod-Delta is to manage the profiles rather than individual indicators. Because indicators are merely symptoms, fixing them individually rarely 'cures' the underlying 'disease.' Line management's task is to review the Failure of State Profiles for their respective areas and to plan and implement remedial actions for the two to three 'worst' GFTs obtained on any testing occasion. In this way, safety management involves the continuous proactive improvement of the underlying causal processes instead of reactive 'fixes' for preventing the last accident. It should also be noted that only one of the GFTs' defenses is specifically related to safety. The remainder relates to general processes underpinning both quality and safety. As such, a Failure State Profile forms an important part of any line manager's toolbox. Safety is not an 'add-on'. There is a close synergy between Tripod-Delta and the kinds of safety management systems that have been produced by oil exploration and production companies in response to the Cullen Report on the Piper Alpha disaster.9 A safety management system provides the administrative structures necessary to drive good safety practices. It focuses on the technical and managerial factors associated with hazards. It is top-down and management-led. It is prescriptive and normative- that is, it states how things ought to be. It is comprehensive, embracing all hazards and their management requirements. Tripod-Delta, on the other hand, focuses on the organizational and human factors affecting safe working practices. It is bottom-up in its operation. It is created in the workplace using the craft knowledge of task specialists. It is descriptive- that is, it tells how the world actually is rather than how it ought to be. It is deliberately non-comprehen- sive. It samples a limited number of possible dimensions of safety and health. In short, each process supplements and augments the other. The two are intended to work in parallel. Review and MESH Although they are applied to different domains, it is appropriate to deal with Review and MESH together since they are both guided by the same underlying principles and both use the same method of assessment. Both instruments are the natural descendants of Tripod-Delta, although they use ratings rather than indicators to measure the adverse impact of latent conditions. Both were developed at the University of Manchester in collaboration with British Rail Research and British Airways Engineering, respectively, in the early 1990s and have been used for some years. As indicated in Table 7.1, the Review addresses the local and organizational factors affecting human performance in railway operations, while MESH (Managing Engineering Safety Health) assesses the same kinds of issues in the context of aircraft maintenance. The Tripod-Delta indicators have many obvious merits. They are directly observable and so not easily influenced by the respondent's whims and biases. They have both content validity and face validity. That is, they are directly linked to individual GFTs in a particular operational context and can be seen by all specialist users as relevant to the local task issues (since the indicators were generated by people just like themselves). But they are not without their problems. Creating a database of indicators is a lengthy, expensive, and labor-intensive process. Although it is assumed that each item in a related collection of indicators is equally 'diagnostic' of the associated GFT, that is not always the case in practice. Some items can be more discriminating than others; others can suffer from 'floor and ceiling' effects. That is, they would always be answered with a 'yes' or a 'no' and so convey little information. For these and other reasons, both Review and MESH employ subjective five-point ratings rather than objective 'yes/no' indicators. Many engineers view numbers that cannot easily be given to two decimal places with dark suspicion. What they do not always appreciate, however, is that such impressionistic ratings exploit a natural human talent. We are very good at judging the relative frequencies of particular kinds of events using an ordinal measurement scale rather than an interval or ratio scale. We encode frequency-of-occurrence data automatically, without conscious effort. When these ordinal estimates of the frequencies of events are compared with reality (when that is known), the correlation coefficients are often in the region of +0.9 or better. Clearly defined problems are events like any other and can be estimated in the same way. In both Review and MESH, task special- ists-people who actually do the 'hands-on' work or first-line managers are asked to estimate on a scale from 1 (hardly at all) to 5 (very often indeed) how frequently particular kinds of workplace or organizational problems have adversely affected their work over a short, clearly defined time period, or in relation to a few specific jobs. Another matter that concerns those with a background in the 'hard' sciences is the conviction that such ratings are prey to bias. Malcontents, they think, will use the scales as an excuse to complain, while those at the opposite end of the spectrum will say that everything is for the best in the best of all possible worlds, and neither will report the reality. Of course, such people do exist, but the behavioral sciences have been coping with the subjectivity problem for over a century. One answer is to pool data from a large number of people to create an average picture at any one time. In this way, biased views of either kind cancel one another out. Such a method also helps to preserve the anonymity of the respondents (see Chapter 9 for a further discussion of what is needed to achieve a 'minimal blame' or 'just' reporting culture). The review assesses 16 Railway Problem Factors (RPFs) selected on the basis of extensive field studies. These assessments are made on a regular basis (the intervals vary according to the location) by supervisors in differing activities and locations via a computer program. The assessors are asked to rate the degree to which each RPF has constituted a problem in their area of work over the last accounting period. The program analyses and archives the data by both location and task. The RPFs are listed below: • Tools and equipment • Materials • Supervision • Working environment • Staff attitudes • Housekeeping • Contractors • Design • Staff communication • Departmental communication • Staffing and rostering • Training • Planning • Rules • Management • Maintenance. While railway operations are strung out along thousands of miles of track, aircraft engineering activities are usually located in a centralized cluster of workshops, hangars, and offices. This was certainly the case for British Airways Engineering at Heathrow, where MESH was developed. As a result, MESH could be configured in a more varied way than Review. Assessments were made of both local and organizational factors, each by different types of personnel. Whereas local factors varied from one workplace to the next (in-line hangars, base maintenance bays, workshops, airworthiness offices, and so on), the same eight organizational factors were measured throughout the organization: • Organizational structure • People management • Provision and quality of tools and equipment • Training and selection • Commercial and operational pressures • Planning and scheduling • Maintenance of buildings and equipment • Communication. The organizational factors ratings are made monthly, and sometimes quarterly, by technical managers- people at the interface between the system as a whole and their particular workplaces. By contrast, assessments of local factors are made by between 20-30 percent of the 'hands-on' workforce in any given location. Assessors are selected randomly, and each makes regular assessments, usually on a weekly basis but sometimes at longer intervals. The assessments are made directly onto a computer. Each assessor logs on anonymously, giving only his or her grade, trade, and location. At each assessment, the MESH program asks the individual to rate the degree to which each local factor has been a problem with regard to a limited number of specified jobs or within a given work period (for example, a particular day). The assessments are made by moving a mouse-driven cursor along a rating scale. Listed below are the set of local factors created for line maintenance: • Knowledge, skills and experience • Morale • Tools, equipment, and parts • Support (from other sectors) • Fatigue • Pressure • Time of day • The environment • Computers • Paperwork, manuals, and procedures • Inconvenience • Personnel safety features. In both Review and MESH (as for Tripod-Delta), these assessments are summarized as bar diagrams, in MESH, for example, each as- sessor sees first the bar diagram for his or her ratings and then the average ratings for that workplace over the past four weeks. As in Tripod-Delta, the purpose of these profiles is to identify two or three factors most in need of remediation and to track their changes over time. It should be emphasized that the success of techniques like Tripod-Delta, Review and MESH depends crucially upon the assessors seeing management act on their ratings. The management needs to keep the workforce informed of remedial progress at all times. If nothing is seen to be done, there is no incentive to give assessments and the system dies. Human Error Assessment and Reduction Technique (HEART) HEART was developed by Jeremy Williams, a British ergonomist with experience in many hazardous technologies. Its detailed application has been described elsewhere. In this section, we will outline its basic features, describe how error-producing conditions can be ranked according to their relative influences, and introduce a new aspect - a principled means of assessing the impact of various violation-producing factors (VPCs) upon a person's likelihood of failing to comply with safe operating procedures. HEART provides a set of generic task types with their associated nominal error probabilities. They are listed in Table 7.2. The starting point for the HEART analysis is to match the activity to be evaluated to one of the generic tasks listed in Table 7.2. The next and most important step (and the one that gives HEART its pre-eminence in this area) is to consult the list of error-producing conditions (EPCs) and to decide which condition(s) are likely to impact the performance of the activity in question. Having done this, the nominal error probability is multiplied by a suitably judged proportion of the appropriate EPC factor. When people are asked to make absolute probability estimates of a particular kind of error type, their judgments may vary by order of magnitude from person to person. However, an extensive survey of the human factors literature has revealed that the effects of various kinds of manipulation upon error rates show a high degree of consistency across a wide variety of experimental situations. The principal error-producing conditions (EPCs) are listed below in rank order of effect. The numbers in parentheses indicate the amount by which the nominal error probability must be multiplied to reflect the influence of each factor: • Unfamiliarity with a situation that is potentially important but which is either novel or occurs only infrequently (x 17) • Shortage of time for error detection and correction (x 11) • Low signal-to-noise ratio-when really poor (x 10) • Suppression of feature information that is too accessible (x 9) • Absence or poverty of spatial and functional information (x 8) • Mismatch between designer's and user's model of the system (x 8) • No obvious means of reversing an unintended action (x 8) • A channel capacity overload, particularly one caused by the simultaneous presentation of non-redundant information (x 6) • A need to unlearn a technique and apply one that requires the application of an opposing philosophy (x 6) • The need to transfer specific knowledge from one task to another without loss (x 5.5) • Ambiguity in the required performance standards (x 5) • A mismatch between real and perceived risk (x 4) • Poor, ambiguous, or ill-matched system feedback (x 4) • No clear, direct, and timely confirmation of an intended action from the portion of the system over which control is to be exerted (x 3) • Operator inexperience (for example, a newly qualified technician) (x 3) • An impoverished quality of information conveyed by procedures and person-to-person interaction (x 3) • Little or no independent checking or testing of output (x 3) • A conflict between immediate and long-term objectives (x 2.5) • No diversity of information input for veracity checks (x 2.5) • A mismatch between the educational achievement level of an individual and the requirements of the task (x 2) • An incentive to use other, more dangerous procedures (x 2) • Little opportunity to exercise mind and body outside the immediate confines of the job (x 1.8) • Unreliable instrumentation that is recognized as such (x 1.6) • A need for absolute judgments which are beyond the capabilities or experience of an operator (x 1.6) • Unclear allocation of function and responsibility (x 1.6) • No obvious way to keep track of progress during the task (x 1.4) • A danger that physical capacities will be exceeded (x 1.4) • Little or no intrinsic meaning in a task (x 1.4) • High-level emotional stress (x 1.3) • Ill-health, especially fever (x 1.2) • Low workforce morale (x 1.2) • Inconsistency of meaning of displays and procedures (x 1.15) • Prolonged inactivity or repetitious cycling (x 1.1 for the first half-hour, and x 1.05 for each hour thereafter) • Disruption of normal work-sleep cycles (x 1.1) • Task-pacing caused by the intervention of others (x 1.06) • Additional team members over and above those necessary to perform tasks normally and satisfactorily (x 1.03 per additional person) • Age of personnel performing perceptual tasks (x 1.02). Over the past two years, Jerry Williams has compiled a similar list for violation-producing conditions (VPCs), again using an extensive trawl of the psychological and human factors literature.13 As before, he created a limited list of generic violation behaviors, each with their associated nominal probabilities for females-unlike errors; there is a large gender effect in non-compliance. For males, the nominal probabilities need to be multiplied by 1.4. The generic situations and their nominal values for females are shown in Table 7.3. To date, Williams has established the relative impact of eight VPCs. These, together with their weighting factors, are: • Perceived low likelihood of detection (x 10) • Inconvenience (x 7) • Apparent authority or status to violate, disregard, or override advice, requests, procedures or instructions (x 3) • Copying behaviour (x 2.1) • No disapproving authority figure present (x 2) • Perceived requirement to obey 'authority figure' (x 1.8) • Gender (x 1.4 for males) • Group pressure (x 1.07 per individual encouraging deviation- up to a maximum of five people). Together, these lists of EPCs and VPCs constitute the best available account of the factors promoting errors and violations within the workplace. The fact that they can be ranked reliably- so that we can assess the relative effects of the different factors- represents a major advance and an extremely valuable addition to the safety-concerned manager's toolbox. The Influence Diagram Approach (IDA) The Influence Diagram Approach (IDA) has two important aspects. First, it provides the tools for modeling qualitatively the influences existing at various organizational levels upon adverse outcomes- either in assessing their contributions to past events or in considering the likelihood of some future event. Second, this qualitative model can also be used to generate quantitative measures of the influences of various technical, human, and organizational factors on the risks faced by a particular hazardous technology. Once again, these quantitative measures can be derived either reactively in regard to a particular accident or proactively to gauge the probability of some possible adverse event occurring in the future. As such, this approach can play a very informative role in both accident investigation and in the preparation of Formal Safety Analyses. In principle, it is applicable to any hazardous system, though the example of its application given below will be drawn from the maritime domain. While the particular ways in which bad outcomes can occur may not be easily foreseeable, the varieties of bad outcomes themselves are mostly known for each hazardous domain. In shipping, for example, the ways in which disaster can strike a vessel-or the failure modes-are not likely to extend much beyond fires, collisions, groundings, founderings, piracy, and acts of war. The Influence Diagram Approach charts the influences upon a particular failure mode-we will take the example of a vessel grounding on a river bar - at each of a number of levels: • the influencing factor level: this includes the unsafe acts or technological failures immediately responsible for the event, • the performance-influencing factor (PIF) level - these are the immediate workplace conditions that shape the occurrence of human or technical failures, • the implementation level- these is the underlying organizational factors that create the PIFs, • the policy level comprises the policy and regulatory factors that determine organizational processes occurring at the implementation level. Figure 7.5 shows an Influence Diagram relating to the case of a vessel grounding on a river bar. We have combined the PIF and the influencing factor levels for simplicity- such contractions are allowable within the Influence Diagram methodology. It will also be noticed that the Influence Diagram is entirely consistent with the theoretical framework for the development of organizational accidents set out in earlier chapters of this book. In practical terms, the IDA is performed in stages. First, the influence Diagram is developed using a structured discussion with a group of experts- in this case, marine specialists, though comparable experts would be required for other domains-who have a detailed knowledge of both the operational realities and the broader policy issues. Having identified the factors involved at each level and their respective influences upon the failure mode in question, the next step involves quantifying the influences. Normally, the business of assigning numbers to human failure probabilities is little more than an art form, but the steps involved in quantifying the elements of the Influence Diagram-and hence deriving the 'top event' likelihood- are relatively simple, workable, elegant and based upon the best available information. The process begins with one of the 'bottom influences' (leaf nodes)-that is, a factor that has no external influences shown as impinging upon it. In the case of the simplified example presented in Figure 7.4, this includes all the factors at the implementation level, excluding staffing levels, and the single factor shown at the policy level (normally, there are many more). As in the case of creating the qualitative Influence Diagram, the quantification of the individual and combined influences is an iterative process, requiring inputs from the expert assessment team. In order to explain how these calculations are made, we will start with a small part of the influence diagram -the risk perception of the Master and the two factors shown as influencing it: operational experience and standing orders. In the first step, the assessment team evaluates the evidence that feedback from operational experience will influence the Master's risk perception either positively or negatively. To help them make this judgment, they are given a graded indicator scale that specifies the nature of the evidence to be taken into account. At the ideal extreme of the scale there could be the statement: 'Results of grounding incidents and near misses are regularly fed back to ships' masters.' The other end of the scale could describe the worst case as follows: 'No feedback from operational experience is available to masters.' The assessment team will need to be guided by an experienced facilitator. Let us suppose that the evidence before them indicates that feedback is ineffective to improve the Master's perception of risk. Thus, in answer to the question 'What is the weight of evidence for the availability of feedback from operational experience as an airport information desk to enhancing the Master's risk perception?' the assessment team comes up with an influence ratio (a balance of likelihoods) of 0.20 (good): 0.80 (poor). Let us also suppose that when the same question is asked in relation to standing orders, the team judges the ratio to be 0.20 (used): 0.80 (not used). The next stage is to combine these two influences to derive the likelihood that the Master's risk perception will be accurate or inaccurate. The individual steps involved are shown in Table 7.4. The steps involves in calculating the unconditional probability of a factor having two or more 'upstream' influences are as follows: • List the possible combinations of influencing factors. • The assessment team is asked to evaluate the weight of evidence that the accuracy of the Master's perception will be high or low for each of these combinations of influences. In Table 7.4 it can be seen that the combined effect of poor feedback and unclear standing orders is judged to degrade the accuracy of risk perception more strongly than either factor considered in isolation. • Each of the conditional assessments is modified by a joint weighting factor obtained from the product of the appropriate individual factor probabilities. • The overall unconditional probability is derived by multiplying each of the conditional probabilities by the appropriate weighting and then summing over all of these obtained values. The same basic steps are applied to all the other influences shown in Figure 7.4. The overall probability of grounding at a river bar is obtained by summing the unconditional probabilities of each of three immediate influencing factors: the risk perception of the master, the risk perception of the crew, and the likelihood of profit being placed before safety. Although laborious for the assessment team, the Influence Diagram Approach is capable of yielding qualitative analyses and quantitative risk estimates that are based upon sound theoretical principles and state-of-the-art knowledge engineering techniques. Maintenance Error Decision airport information desk (MEDA) Maintenance Error Decision airport information desk (MEDA) is a tool devised by Boeing in collaboration with the federal aviation administration and Galaxy Scientific Corporation to investigate maintenance errors.15 Although designed for an aviation context, its basic principles apply to any safety-critical maintenance activities. The investigation takes place at two levels: • Line investigation. maintenance error decision aid begins with a paper-based investigation that gives line-level maintenance personnel a standardized way to investigate maintenance errors, their origins, and their consequences. It offers front-line engineers a principled means of detecting and removing error-provoking factors at both the workplace and the organizational levels. • Organizational trend analysis. maintenance error decision aid also provides a means for computerized trend analysis for the maintenance organization. maintenance error decision aid is divided into five sections. Sections 1-3 deal with the question 'What happened?'. Section 4 addresses how and why the error occurred. Section Sa pinpoints failed defenses, and Section 5b outlines potential solutions. • Section 1 gathers information about the airline, aircraft type, engine type, time of day, and the like. • Section 2 describes the nature of the event (for example, flight delay, cancellation, gate return, inflight shutdown and so on). • Section 3 classifies the nature of the lapse. These are broken down into the following categories: improper installation, improper servicing, improper or incomplete repair, improper fault isolation, inspection or testing, foreign object damage, surrounding equipment damage, personal injury. An error is defined as 'the condition resulting from a person's actions, when there is general agreement that the actions should have been other than what they were'. • Section 4 takes the form of a contributing factors checklist that must be completed for each of the errors identified in Section 3. These include information, equipment, tools or parts, aircraft design and configuration, job or task, qualifications and skills, individual performance, environment and facilities, organizational environment, supervision, and communication. • Section 5a asks whether any current procedures, processes, and policies within the system should have prevented the incident but which did not. • Section 5b asks what corrective measures have been taken or should have been taken at the local level to prevent the recurrence of the incident. Boeing has distributed maintenance error decision aid free to all their airline customers. One of its primary aims is to provide a common language to increase communication and cooperation between operators, regulators, manufacturers, and the maintenance workforce. A PC-based version has recently been developed for Singapore Airlines Engineering Company, which is used in conjunction with MESH. Tripod-Beta Tripod-Beta is a PC-based tool created within software hardware environment liveware Exploration and Production to conduct an incident analysis in parallel with an event investigation.16 Interaction between these two activities provides the investigators with guidance as to the relevance of their fact-gathering and also highlights avenues of investigation leading to the identification of contributing latent conditions- General Failure Types in Tripod parlance. Tripod-Beta was born out of a marriage between Tripod theory (discussed earlier in this chapter) and Shell's response to the post-Cullen Report safety case requirements. In Shell, this took the form of the Hazard and Effects Management Process (HEMP). This is designed to provide a structured approach to the analysis of health, safety and environmental hazards throughout the lifecycle of an installation. It is based on four processes: identify, assess, control and recover. Tripod-Beta was designed to be compatible with both Tripod-Delta and HEMP. Figure 7.6 shows the basic elements of the Tripod-Beta analytical toolkit. As indicated in Chapter 1, an event involves hazards coming into damaging contact with targets (people, assets, environment) as the result of a defensive failure. Starting with the end result, a loss of some kind, the analyst works backwards to determine the nature of the failed defence(s) and the hazard. A short piece of descriptive text is written into each box or node to describe the actual occurrence. The next step is to establish why a particular defence failed. This may be due to either active or latent failures (the boxes below the latent failure nodes with question marks are there to identify which of the 11 GFTs were implicated-see page 132, this chapter). In the case of an active failure, there are likely to be preconditions within the workplace. These, like active failures, are identified by short pieces of descriptive text. Preconditions are likely to be the product of identifiable latent failures. Once again, they are described and the associated GFTs identified. The procedure continues until all the identifiable factors have been identified and described. As the analyst progresses, he or she will link the causally related nodes and boxes by lines on the computer screen. Thus, the Tripod-Delta software provides the means to assemble the facts learned in the investigation and manipulate them on-screen to produce a graphic picture of the event and its causes- an incident tree. The program also checks whether the logic of the tree structure (its labeling and connections) conforms to the Tripod theory and HEMP. Once anomalies have been resolved, a draft accident report is automatically generated for final editing on a word-processing package. Summary of the Main Principles of Error Management 1 The best people can sometimes make the worst errors. 2 Short lived mental states- preoccupation, distraction, forgetfulness, and inattention, which are the last and the least manageable parts of an error sequence. 3 We cannot change the human condition. People will always make errors and commit violations. But we can change the conditions under which they work to make these unsafe acts less likely. 4 Blaming people for their errors-though emotionally satisfying- will have little or no effect on their future fallibility .. 5 Errors are largely unintentional. It is very difficult for management to control what people did not intend to do in the first place. 6 Errors arise from informational problems. They are best tackled by improving the available information - either in the person's head or in the workplace. 7 Violations, however, are social and motivational problems. They are best addressed by changing people's norms, beliefs, attitudes, and culture, on the one hand, and by improving the credibility, applicability, availability, and accuracy of the procedures, on the other. 8 Violations act in two ways. First, they make it more likely that the violators will commit subsequent errors, and second, it is also more likely that these errors will have damaging consequences. Chapter 8 The Regulator's Unhappy Lot Regulators in the Frame The regulators' lot- like the policeman's- is not a happy one. Not only are they rarely loved by those they regulate, but they are now ever more likely to be blamed for organizational accidents. Over the past 30 years, the search for the causes of a major catastrophe has spread steadily outwards in scope and backward in time to uncover increasingly more remote contributions. Prominently and frequently featured in this extended causal 'fallout', are the decisions and actions of the regulatory authority. This chapter examines the difficult and complex role of the regulator in limiting the occurrence of organizational accidents. First, we will look briefly at five tragic events in which regulatory failures- or, at least, shortcomings in the regulatory process- were implicated. We do this in order to gain a better understanding of how regulators (or the regulatory process) can contribute to the breakdown of complex well-defended technologies. The five case studies are: • the Challenger spacecraft explosion (28 January 1986) • the King's Cross Underground fire (18 November 1987) • the Piper Alpha platform explosion (6 July 1988) • the Fokker-28 crash at Dryden, Ontario (10 March 1989) • the Piper Chieftain crash at Young, New South Wales (11 June 1993). Regulated Accidents Challenger: When Deviance Becomes the Norm The Presidential Commission, investigating the loss of the Challenger space shuttle and its seven astronauts, concluded that the explosion 157 was caused by a faulty seal, or O-ring, on one of the rocket boosters. This allowed flames to escape and ignite an external fuel tank. The Commission's report was published in June 1987. Since then, the Challenger accident has been the subject of an intensive study by Diane Vaughan, a Boston College sociologist. This work has recently been published and represents one of the most detailed and compelling analyses yet made of an organizational accident. It also challenges the widely held view that the accident happened because national aeronautics and space administration and its prime contractor, Morton Thiokol, did not do their jobs properly: The impression conveyed by the Presidential Commission report and press accounts is that 'flawed decisions' were made by the middle managers present at the dramatic eve-of-Iaunch teleconference. The received wisdom was that these managers, driven by production pressures and political concerns about the agency, turned a blind eye to the O-ring defects, violated safety rules, and went ahead with the launch in order to meet the program deadlines. These analyses focused on individual failures. As Diane Vaughan expressed it; they , ... conveyed an imagery of evil managers, so that the incident appeared to be an anomaly: a peculiarity of the individuals who were in responsible decision making positions at the time.' Diane Vaughan's conclusions were quite the reverse. She argued that the accident happened because those involved in the launch decision did precisely what they were supposed to do. It can truly be said that the Challenger launch decision was rule-based. But the cultural understandings, rules, procedures and nonns that had always worked in the past did not work this time. It was not amorally calculating managers violating rules that was responsible for the tragedy. It was conformity. Moreover, she found that the Challenger tragedy was not an anomaly peculiar to NASA. It was shaped by factors common to many other organizations. What drove national aeronautics and space administration inexorably towards the tragic outcome was the insidious erosion of the standards against which they regulated themselves. While the Presidential Commission was shocked by NASA's frequent use of the phrase 'acceptable risk', Vaughan's analysis reveals that flying with' acceptable risks' was an integral part of NASA's culture, indeed, of any aviation culture. In fact, the 'acceptable risks' on the space shuttle filled six volumes. At NASA, problems were the norm. The word 'anomaly' was part of everyday talk .... The whole shuttle system operated on the assumption that deviation could be controlled but not eliminated. In short, national aeronautics and space administration had created a closed culture that, in Diane Vaughan’s words, 'normalized deviance'. What seemed to the outside world like reckless risk-taking was to national aeronautics and space administration managers a reasonable and sensible way of doing their jobs. The worrying fact about this and other organizational accidents is that it is very difficult to identify exactly when things started to go wrong since each step and each decision proceeded naturally from the last. No fundamental decision was made at national aeronautics and space administration to do evil. Rather, a series of seemingly harmless decisions incrementally moved the space agency toward a catastrophic outcome. Three safety units oversaw the Shuttle operations. The most significant of these was NASA's internal body, the Safety, Reliability, and Quality Assurance Program (SR & QA). The Presidential Commission identified three principal failures in the SR & Q's monitoring and surveillance responsibilities: 1 They failed to discover and rectify the confusion that surrounded the assignment of criticality categories to Shuttle components according to the seriousness of their failure consequences. The solid rocket booster joint was originally listed as CI-R. The 'Cl' was the highest criticality rating, indicating 'loss of life or vehicle', but the 'R' meant that there were redundancies or backups to its possible failure. Later, the criticality rating was changed to the more serious Cl designation. However, after the accident, it was discovered that many internal documents still listed the joint as CI-R, leading some national aeronautics and space administration managers (and even some SR & QA staff) to believe there was defensive redundancy. 2 The SR & QA staff failed to compile and disseminate trend data relating to in-flight O-ring erosion. Between the tenth and the twenty-fifth mission (the Challenger flight), more than half the flights experienced O-ring problems. Had these data been circulated, national aeronautics and space administration administrators would have had essential information regarding the history and scope of the joint weaknesses. 3 The Commission found three problem-reporting failures. First, the SR & QA did not establish an adequate means for reporting Shuttle problems up the national aeronautics and space administration hierarchy. Second, they failed to create a precise set of requirements for reporting inflight anomalies. Third, they failed to detect violations of problem- reporting requirements. In addition, no member of the SR & QA staff was present at the teleconference between Marshall Space Flight Center and Morton Thiokol engineers on the night before the launch. On this occasion, Morton Thiokol engineers expressed considerable disquiet about launching after such an unusually cold night. Instead of being asked to prove that the launch was safe, however, the Morton Thiokol engineers were challenged to prove that it was unsafe. The absence of any SR & Q representatives from this critical decision-making meant that another opportunity to express safety concerns was missed. The reason for their absence? No one thought to invite them. A theme that links all of the regulatory problems to be discussed here is the lack of resources. Between 1970 and the Challenger disaster, national aeronautics and space administration reduced its safety and quality control personnel by 71 percent. The SR & QA staff at Marshall Space Center, with oversight responsibility for the rocket booster project, had been cut from around 130 to 84. Overall, safety, reliability, and quality staff comprised about 2 percent of NASA's 22,000 staff. Both the director and deputy director of the SR & QA unit had other duties, so each spent around 10 and 25 percent of their time on the Shuttle program, leaving them very little time for safety issues. King's Cross Underground Station: A Blinkered View of the Law Just after the evening rush hour, a lighted match or cigarette passed through a crack in a wooden escalator and set light to an accumulated heap of greasy fluff and waste beneath.4 Twenty minutes later, flames shot up the escalator shaft and hit the ceiling of the ticket hall, killing 31 people. Desmond Fennell, the Queen's Counsel heading the subsequent Inquiry, devoted a chapter of his report to the role of the Railway Inspectorate, the regulator. He begins by asserting that the Railway Inspectorate took an overly restricted view of their responsibilities. In my view, the Railway Inspectorate was mistaken in its interpretation of the law in believing if London Underground discharged its duty to have due regard to the safety of operations, it had discharged all its statutory duties for the health and safety of passengers... Even making allowances for the Railway Inspectorate's misunderstanding of their responsibilities [under the law], it is my view that the level of resources and degree of vigor they applied to enforcement ... was insufficient. It was in this climate that poor housekeeping and potentially dangerous conditions in underground stations were allowed to persist. In the years preceding the disaster, fires on the London Underground had been commonplace, though usually of a very minor nature- cardboard cartons and the like. They were given the comforting label of 'smoulderings.' Confident that these apparently trivial events were being adequately covered by the routine inspections of the London Fire Brigade, the Railway Inspectorate decided, three years before the King's Cross disaster, that it no longer wanted to receive copies of such reports from the Fire Brigade. As Fennell put it, the Chief Inspector ' ... [now] conceded that this was an unfortunate decision'. Although the Inspectorate believed the primary responsibility for fire hazards lay with the London Fire Brigade, it had responded to two escalator fires in 1973 by writing to the Chief Operating Manager of the (then) London Transport Executive, suggesting a drive to clear away accumulations of inflammable rubbish under escalators. However, no mention was made of risk to passengers. Neither the Chief Inspector nor his staff had' ... ever conceived the possibility of an escalator fire rapidly developing and endangering life'. Moreover, he could be excused for this viewpoint on two grounds. First, there was no obvious precedent- the 'flashover' that had wreaked such havoc in the ticket hall was a relatively new phenomenon. Second, as in other railway operations, regulators- not unreasonably- saw the main risks to passengers as being associated with moving trains rather than static stations. Fennell concluded his account of the Railway Inspectorate's role in the disaster by observing that it had made insufficient use of its powers and had not devoted enough attention to London Underground' ... to create the tension necessary to ensure safety. He was also critical of the Inspectorate's 'cozy' relationship with the London Underground and its lack of vigor in checking up on the implementation of agreed safety improvements. We will return to the issue of regulatory style at a later point in this chapter. Piper Alpha: Superficial Inspections and Incomplete Sampling The Public Inquiry into the Piper Alpha disaster (see Chapter 5 for a brief description of the event) was headed by Lord Cullen, a senior Scottish judge.6 His report was published in two volumes. The first dealt with the background, nature, and aftermath of the disaster. A chapter is devoted to the regulatory deficiencies -in particular, the stark contrast between the findings of the Department of Energy's inspections and what was revealed by the Inquiry. The second volume looked to the future and outlined a new safety regime that has subsequently revolutionized the regulation and management of safety in offshore installations. The Piper Alpha platform received three visits from an inspector in the Department of Energy's Safety Directorate in the year preceding the disaster. The first was a routine inspection that identified a number of minor points requiring no immediate follow-up. The second followed a fatal accident and identified weaknesses in shift handovers and the permit-to-work system. The third took place 10 days before the disaster and concentrated on areas in which construction work was in progress. It was the report on this last visit that contrasted so starkly with the Inquiry findings. In particular, it accepted that Occidental had 'tidied up' the weaknesses in the permit-to-work scheme and shift handovers, when deficiencies in these areas were subsequently found to be major contributors to the disaster. A checklist existed to facilitate the evaluation of permit-to-work systems, but the inspector did not have it, nor was he experienced in the use of such procedures. He failed to inspect Occidental's Operating Procedures manual in connection with the permit-to-work system because he felt he did not have the time to carry out a full audit, requiring two to three days. In the event, he was only able to devote 10 hours to his inspection. The inspector was also unaware of the practice of switching the diesel fire pumps to manual mode during shifts in which diving took place. Nor did he inspect the deluge system or establish the frequency with which lifeboat drills were carried out. In short, he failed to spot many of the factors that contributed to the disaster a few days later. When these discrepancies were pointed out to the Director of Safety at the Inquiry, he said, with considerable justification, 'I think that within the context of carrying out an inspection and the very wide-ranging Inquiry that is going on here, there is a total difference in approach? The Director of Safety described the nature of inspections as follows: [An inspection is] essentially a sampling exercise. The inspector supervises and audits the state of the equipment and management procedures. He talks to personnel and seeks to obtain an overall picture of how well the installation is being operated, maintained, and managed. An inspector must exercise his professional judgment in determining the scope and depth of the inspection and is selected, trained, and supervised by line management to this end. He is not given a fixed list of procedures, equipment and items which he must tick off in the form of a check list. This could create considerable difficulties given the variety of operations, working procedures, and installations involved. In addition, it would lead operators to anticipate those areas in which an inspector always checks. Given the natural constraints of the inspection process, there are many who would have considerable sympathy with this approach. Lord Cullen, however, did not. He gave the following judgment on the regulatory process. Even after making allowance for the fact that the inspections were based on sampling, it was clear to me that they were superficial to the point of being of little use as a test of safety on the platform. They did not reveal a number of clear cut and readily ascertainable deficiencies. While the effectiveness of the inspections has been affected by persistent undermanning and inadequate guidance, the evidence led me to question, in a fundamental sense, whether the type of inspection prac- tised by the Department of Energy could be an effective means of assessing or monitoring the management of safety by operators. Lord Cullen then devoted a large part of the second volume of the Inquiry report to recommending a new mode of regulation, which has subsequently been implemented. This is discussed later in this chapter. The Fokker-28 Crash at Dryden: The Safety Net That Failed An Air Ontario Fokker-28, with 65 passengers and four crew, took off from Dryden Municipal Airport just after midday on 10 March 1989. After takeoff, the aircraft failed to gain altitude and crashed just under one kilometer beyond the end of the runway. Twenty-one passengers and three crew members died. Freezing rain had been forecast and there was a heavy snowstorm in progress at the time of takeoff. The aircraft had not been de-iced and the immediate cause of the crash was surface contamination on the wings. On the face of it, this might appear to be an open-and-shut case of pilot error since the captain was obviously mistaken in deciding to proceed with the takeoff under those conditions. However, the Inquiry conducted by Commissioner Virgil Moshansky, an Alberta judge, took two years, received 34 000 pages of testimony and heard 166 witnesses to come to the following conclusion The accident at Dryden ... was not the result of one cause, but of a combination of several related factors. Had the system operated effectively, each of these factors might have been identified and corrected before it took on significance ... the accident was the result of a failure in the air transportation system [as a whole]. Featuring prominently among these causal factors were the short- comings identified at many different levels of the regulator, Transport Canada. This agency was not in a happy position at the time of the crash, being caught in a classic double bind. Deregulation had resulted in a greatly increased workload. At the same time, spending cuts imposed by the Canadian government meant that the number of people available and qualified to carry out these extended responsibilities was reduced significantly. Transport Canada was also trapped within a bureaucratic nightmare: for example, approval of budget increases required separate cases to be submitted to 11 levels of the appropriate Ministry. Safety initiatives had been consistently down-graded or denied for several years. The result of these constraints was that the air carrier inspection and monitoring safety net failed. Several factors relating to Transport Canada were identified by the Moshansky Inquiry as contributing to the Dryden crash. These included: • lack of guidance to air carriers on the need for de-icing, • no audit of F-28 operations, • no approved Airplane Operating Manual or Flight Operations Manual, • no Minimum Equipment List six months after the aircraft had entered service, • incomplete, conflicting and ambiguous air navigation orders (the operating regulations), • Dispatcher training and certification entirely in the hands of the air carriers-the dispatcher responsible for the F-28 flights on that day was underqualified, prepared an incorrect flight release, did not notify the crew of freezing rain at Dryden, and failed to advise the crew that an overflight of Dryden was indicated. The Inquiry Report summarized Transport Canada's involvement in the accident sequence as follows: Transport Canada, as the regulator, had a duty to prevent serious operational deficiencies in the F-28 program... Had the regulator been more diligent in scrutinizing the F-28 implementation at Air Ontario, many of the operational deficiencies that had a bearing on the crash of Flight 1363 could have been avoided. There can be little doubt that these regulatory failings arose from deep-rooted systemic failures rather than from the inadequate performance of individuals at the 'sharp end'. The source problems lay high up in Transport Canada and in the financial climate imposed by the government of the time. The operational regulators who testified at the Inquiry were very forthright in their condemnation of both the existing regulations and the 'chronic inaction' of Transport Canada's senior management. The Inquiry also acknowledged that, for the most part, Transport Canada was staffed by 'competent and dedicated persons who are sincerely doing their best to ensure a safe air transportation system for the public, at times under trying and frustrating circumstances'. Let us give Commissioner Moshansky the last word on the Dryden tragedy:12 'After more than two years of intensive investigation and public hearings, I believe that this accident did not just happen by chance -it was allowed to happen.' In this instance, the failure of the regulatory safety net was but one part, albeit a significant one, of the malaise afflicting the entire Canadian air transport system. Piper Chieftain crash at Young, NSW: Conflicting Goals At about 1918 hours on 11 June 1993, a small commuter aircraft flying out of Sydney, with five passengers and two pilots onboard, struck trees on a small hill on the approach to the aerodrome at Young, New South Wales (NSW), and crashed. Six people were killed by the impact, of the subsequent fire, and one survivor died in hospital the next morning. By the standards of aviation accidents, this was a relatively small disaster, but it was to have far-reaching consequences- particularly for the regulator, the Australian Civil Aviation Authority (ACAA)P The investigation was conducted by the Bureau of Air Safety Investigation (BASI), an agency that had recently adopted the accident causation model outlined in Chapter 1 as a framework for uncovering organizational and systemic contributions.14 In addition to identifying failed or absent defenses, active failures, and local error-provoking factors, the model also directs the investigator to consider organizational factors and latent failures. Among these, a number of regulatory contributions were identified by the BASI accident report. These included the following factors: • Conflicting goals. The activities of the ACAA's Safety and Standards Division appeared to be biased towards promoting the viability of the operator (Monarch Air, a small commuter airline in commercial difficulties) rather than serving the safety needs of the traveling public. • Poor division of responsibilities. Both responsibilities and crucial information were shared between the ACAA: Head Office in Canberra and the local district offices-that included the flight operations inspectors who actually carried out the required surveillance. This meant that at a regional level, no single person had a comprehensive overview of the safety health of Monarch'. • Poor planning. The district office responsible for Monarch neither formulated nor undertook an effective programme of operational surveillance of Monarch. Because the assigned flight operations inspector had a heavy workload, his approach was reactive rather than proactive. His checking on Monarch was, therefore, largely confined to document inspections. BASI estimated that, for the number of hours the aircraft had flown in the year prior to the crash, there should have been a minimum of 14 hours and an average of 17 hours en route surveillance. In reality, there was none. This meant that the regulator was unaware of the actual standard of Monarch's inflight operations. One indicator that could have been significant in Monarch's case was its financial circumstances, but no mechanism existed for the ACAA to consider any possible connection between an organization's financial state and its ability to fly safely. • Inadequate resources. The local flight operations inspector was overloaded and under resourced. He was responsible for the surveillance of around 40 companies holding Air Operators Certificates (AOC). In addition, he was required to take over his manager's duties for lengthy periods while still supervising the air operator certificate holders assigned to him. BASI assessed his workload as being unachievable in full. • Ineffective communications. Effective regulation requires a good understanding of the supervised organization, but communications between Monarch and the ACAA district office were carried out at a largely formal level, with no indication of a close working relationship between the two parties. There was little personal contact between the assigned inspector and the individuals responsible for Monarch flight operations. • Poor control. The investigators found no evidence of effective control of the Monarch surveillance programme by the ACAA district office. Furthermore, the assigned inspector lacked adequate support and supervision from his line managers. • Poor operating procedures. The Safety Regulation and Standards Division lacked the procedures to ensure that Monarch continued to meet the standards required of an air operator certificate holder. Although deficiencies in Monarch's flight operations had been identified a few months before the accident, the ACAA appeared to be reluctant to take decisive action to improve Monarch's operating standards. In the absence of a coordinated monitoring strategy, several meetings with Monarch had failed to achieve the significant improvements required. The Monarch investigation received a good deal of attention from the Australian media, and widespread public concern was expressed about the safety of small commuter airlines and the effectiveness of the ACAA's surveillance and monitoring program. In May 1994, this concern continuing airworthines management exposition to a head when the Opposition Transport spokesman in the Federal Parliament made strong allegations about safety violations by Seaview Air, a small charter airline operating between Lord Howe Island and the mainland. The source of these charges was the former Chief Pilot of Seaview Air. They were vigorously denied by the company's proprietor as being' ... unfounded, scurrilous, defamatory and entirely incorrect'. In this, Seaview's owner was strongly supported on the same day by the ACAA’s director of air safety. In October 1994, an aeronautical Commander owned by Seaview Air crashed into the sea between Newcastle, NSW, and Lord Howe Island, causing the deaths of eight passengers and the pilot. Just after the crash, the ACAA 's director of air safety regulation was quoted by the Sydney Morning Herald as claiming, 'We have been very actively looking at how Seaview has been conducting their operation, and we consider their operation quite satisfactory.' The same article went on to reveal that, in January 1994, the ACAA had warned Seaview Air that the airline risked being grounded unless 'serious operational deficiencies' were remedied. The ACAAs letter described these problems as 'a major cause for concern from a safety and regulatory compliance point of view'. These deficiencies included: • Overloading of the aircraft being 'the norm for some time.' • Flying without a backup navigation aid. • Advertising the airline as a regular passenger service when in fact it was a charter service. • Training deficiencies in the case of some pilots. • A pilot flying without a flight plan. The editorial in the Sydney Morning Herald on 7 October 1994, six days after the Seaview tragedy, reported that the ACAA had altered its earlier position and conceded that it had indeed told Seaview that it was violating the safety rules, but these violations had been satisfactorily rectified. The same editorial also observed that, when the matter had been raised in the Federal Parliament in May, Mr Brereton, the Minister for Transport, had merely referred the Opposition questions to a parliamentary committee. The editorial commented, 'This was essentially a housekeeping response when, in the light of subsequent events, a more direct shake-up might have been required.' The shake-up was not long in coming. One week after the Seaview disaster, Mr Brereton sacked the ACAA head of safety. Twelve days after the crash, the Federal police were called in to investigate claims of corruption in the ACAA stemming from the Seaview accident. In the same week, Mr. Brereton announced that the Federal Cabinet had agreed to establish and fund a new independent Air Safety Agency, replacing the ACA.A Department of Air Safety Regulation.17 He felt that this was the most appropriate way of addressing 'the inherent conflict between the ACAAs commercial and policing functions'. He also announced that government funding for safety regulation was to be increased by over 20 percent. Meanwhile, Mr Brereton struggled to resist calls for his resignation over his handling of the ACAA (the Opposition called him 'Pontius Pilate in the snow' when it was confirmed that he had continued his skiing holiday after hearing of the Seaview tragedy). In the following year, the Civil Aviation Authority was disbanded and replaced by two agencies: the Civil Aviation Safety Authority and Air Services Australia, the former to regulate air safety and the latter to cover air traffic control and similar aspects of the old ACAN's business. How this arrangement will fare after the recent change of government has yet to be discovered. Deregulation of the aviation industry in Australia had produced a commuter sector, or third-tier of airlines, comprising a large number of single-pilot, single-plane operators ferrying up to a dozen passengers in and out of Australia's regional centers. Most of them operated under the relatively lax rules governing charter operations. Unlike the United States, where new operators must establish that they have sufficient resources to maintain their aircraft, it was the case in Australia that 'those who could beg, borrow or steal a decent enough plane can become their very own airline magnate'.18 This might have been a manageable situation had the ACAA the resources and the overriding aim of maintaining aviation safety, but it was an enterprise agency that was financed, in part, by those it regulated. As a result, the ACAA was quite naturally concerned with sustaining the commercial viability of its client operators in the commuter sector. This goal conflict eventually- and probably inevitably- brought about its demise. US Regulators under Fire The following headline appeared in the New York Times in November, 1995: 'FAA's Lax Inspection Setup Heightens Dangers in the Skies.' The following feature article by Adam Bryant chronicled some of the problems besetting the Federal Aviation Administration (FAA), one of the world's largest and most prestigious regulatory bodies. An investigation by the New York Times based on government documents and interviews with inspectors, agency officials, and industry identified what the Times regarded as two major shortcomings in the FANs' inspection system.1 First, inspectors did not appear to be held accountable for not discovering or pursuing deficiencies that were later implicated in fatal accidents. The FAN's failure to deal aggressively with lax oversight was identified by the National Transport Safety Board (NTSB), the US air accident investigator, as a factor in three fatal airline crashes as well as an accident involving one of the FAA's own aircraft. Second, a significant number of inspectors still lacked training on the planes and equipment they were required to oversee. For example, many inspectors said they were assigned to overseeing a fleet of Boeing 737s without ever receiving training on the aircraft. In the year 1994-95, only a handful of the 143 inspectors who oversaw pilots flying a widely used turbo- prop plane, the ATR, were fully qualified to fly that type of plane themselves. This lack of regulator training was cited by the national transportation safety board as a factor in three airline accidents since 1983. In response to these criticisms, agency officials pointed to the nation's air accident rate-among the lowest in the world-as proof that the system worked. But the critics countered with the assertion that resting on a safety record measured by the low rate of bad outcomes is a mistake akin to regarding a bald tyre as safe before it blows out. Senator William S. Cohen, Chairman of the Congressional committee investigating the FAA's inspection system was troubled by the FAA's unwillingness to acknowledge the holes in its safety net. 'Deny, defend and deflect,' he said, 'That had been the attitude of the FAA.' Like many comparable agencies in other countries, the federal aviation administration in- spectorate is not overresourced. It has 2500 safety inspectors to oversee 7298 airline aircraft, 184 434 general aviation aircraft, 682 959 active pilots, 4817 repair stations, 656 pilot training schools and 192 main- tenance schools. Each year, an inspector will perform more than 400 inspections, anyone of them taking between five minutes to five weeks. Their average salary is $57 500. As elsewhere, the deregulation of the airline industry in 1978 has not made the regulators' work any easier. Previously, they felt that they could trust workers in the industry to do a good job. Now, however, with one in four small airlines going out of business each year and new ones taking their place, there are strong commercial pressures to cut safety corners. As one experienced inspector put it, 'You have mechanics that falsify documents and overlook things that are obvious to them because they are afraid of reprisals from their bosses. The trust is gone'. The federal aviation administration is not the only US regulatory agency under attack. On 4 March 1996, Time featured a major article on the Nuclear Regulatory Commission (NRC), the body responsible for overseeing the safety of 110 commercial reactors in the United States. Its headline ran 'Two gutsy engineers in Connecticut have caught the Nuclear Regulatory Commission at a dangerous game: routinely waiving safety rules to let plants keep costs down and stay online'.20 The problem related to the use of a deep body of water, called the spent-fuel pOOVl Every 18 months, the type of nuclear power plant in question was shut down so that the fuel rods can be replaced. The old rods-radioactive and very hot-were moved into racks within the pool. Because the Federal Government has not created a storage site for high-level radioactive waste, these pools have become de facto nuclear dumps with many filled to near-capacity. If the system failed, the pool could turn into a bubbling cauldron billowing radioactive steam. Furthermore, if the pool were to be drained by an earthquake, technical failure or human error, there would be a meltdown of multiple cores occurring outside the containment area, releasing mas- sive amounts of radioactive material and rendering large tracts of the United States uninhabitable. To control these risks, Federal regulations required that older plants, without up-to-date cooling systems, move only one-third of the rods into the pool. In the plant in question, one of its senior engineers discovered that all the hot fuel was being dumped into the pool at once. It also turned out that this practice had been going on for at least 20 years. Instead of allowing the required 250-hour cooling down period, the fuel was being moved just 65 hours after shut- down. By this device, the plant reduced the downtime for each refuelling by two weeks, saving around seven million dollars for the replacement power costs. Thus began a three-year struggle to put matters right. For 18 months· the engineer's supervisors denied that the problem existed and re- fused to report it to the NRC. Eventually, the management called in outside consultants to prove the engineer wrong, but they ended up by agreeing with him. Finally, the engineer took the issue directly to the NRC and discovered that the nuclear regulators had known about this practice for at least 10 years without acting to halt it. The NRC argued that the practice was widespread and was safe so long as the plant's cooling system was designed to handle the heat load-although this was not the case for the plant in question. It was also discovered that plants in three other states had similar fuel-pool problems. In a written response to faxed questions from Time, the executive director of the NRC stated that 'The responsibility for safety rests with the industry .... The NRC is essentially an auditing agency'. With just four inspectors for every three plants, ' ... we have to focus on the issues with the greatest safety significance. We can miss things'. As elsewhere, the NRC is under attack from all sides. Delaware Senator Joseph Biden, who is pushing to create an independent nuclear safety board outside the NRC, described the NRC's regulatory style as that of 'the fox guarding the hen house'. Critics of the NRC also point out that the US nuclear industry can veto the appointment of commission nominees whom it regards as too hostile, while regulators 'enjoy a revolving door to good jobs' at the nuclear companies. The executive director of a whistle-blower support group called 'We the People' commented, 'It all comes down to money .... When a safety issue is too expensive for the industry, the NRC pen- cils it away.' Meanwhile, the nuclear industry itself has complained that many NRC rules boost costs without improving safety because 'the regulatory system hasn't kept pace with advances in technology'.22 Like its counterparts in other countries, this regulatory body finds itself between a rock and a hard place. Damned if They Do and Damned if They Don't Regulatory bodies worldwide seem to be hopelessly trapped in a mesh of double binds. Consider the following: • Workload has increased as resources have been slashed. • Regulators are regularly accused of lax oversight and overly collusive relationships with their clients, while the clients themselves often regard the regulators as intrusive, obstructive, threatening, rigid, out-of-date, ignorant, and generally unsympathetic to their commercial pressures. • Accident inquiries find regulators guilty of not being fully acquainted with all the details of their clients' operations and of missing important contributing factors, but the only means they have of obtaining this information is from the operators themselves or from periodic inspections and follow-ups. After an accident, these omissions take on a sinister significance, but for regulators, armed only with foresight, they are but one of many possible contributions to a future accident. As stated earlier, warnings are only truly warnings if we know what kind of an event the organization will suffer. • Front-line regulators are generally technical specialists, yet major accidents arise from the unforeseen-and often unforeseeable interactions of human and organizational factors whose role is only now being acknowledged by health and safety legislators, and then in the most general terms. In short, regulators are in an impossible position. They are being asked to prevent organizational accidents in high-technology domains when the etiology of these rare and complex events is still little understood. This book has tried to make it plain. But it was not always the case. Some of the most dramatic reductions in accident rates-usually involving individuals facing clearly defined hazards in particular situations-have been brought about by introducing safety-related legislation combined with effective regulation and enforcement. We will look briefly at some of these success stories in order to redress the balance. Legislation and Regulation: Some Major Successes After one of General Grant's battles in the American Civil War, a Washington politician, unhappy at the high losses (around 19000) suffered by the Union forces, accused Grant of killing more of his soldiers than the railroads. Although, at first sight, this appears to be little more than political rhetoric, a closer look at the number of employees killed in nineteenth-century railroad operations indicates that this was hardly an exaggeration. In the year 1892, for example, there were 821 415 railroad employees in the United States, of whom 30 821 were injured or killed as the result of work accidents. Of these events, 35 per cent were associated with the operation of coupling freight cars or wagons. Prior to that, between 1877 and 1887, 38 per cent of all railworker accidents involved coupling.23 Until Congress passed the Safety Appliance Act in 1893, the task of coupling involved guiding a link tube into a coupler pocket and then inserting a pin into a hole on the tube to hold the link in place. To do this, the crew members had to go between moving cars during coup- ling and were frequently injured and somet~mes killed. In 1873, Eli J. Janney-a dry goods clerk and former Confederate Army officer-patented a knuckle-style coupler that allowed railworkers to couple and uncouple cars without having to go be- tween them to guide the link and set the pin. Though the market was soon flooded with thousands of patented couplers, Janney's design was clearly the best. In 1888, the Master Car Builders Association adopted the Janney coupler as its standard. Congress, satisfied that the device could do the job safely, passed the Safety Appliance Act in 1893, making the Janney coupler a legal requirement throughout the railroad industry. The impact was staggering. The accident rate fell dramatically during the 10 subsequent years, and by 1902 (only two years after the end of the Act's seven-year grace period), coupling accidents represented a mere 4 percent of all employee accidents. In absolute terms, coupler-related accidents dropped from nearly 11 000 in 1892 to just over 2000 in 1902, even though the number of railroad employees had steadily increased during that decade-from 873 602 in 1893 to 1189315 in 1902. Interestingly, there was no reduction in the proportion of railroad employees involved in accidents. This remained steady at around 3-4 percent of the total workforce in any one year. Other conspicuously successful safety laws have involved the introduction of speed limits on the roads. The imposition of the 30 mph limit on British roads in the 1930s brought about the largest single reduction in road casualties. The 55 mph nationwide speed limit (previously 70 mph) introduced in the United States in 1974 in response to the oil crisis reduced the fatal crash rate by 34 per cent. The subsequent raising of this speed limit to 65 mph on rural Interstate highways in 1987 led to an increase in crash fatalities of 16 percent. In Britain, the wearing of seat belts by drivers and front-seat passengers became a legal requirement on 31 January 1983. A comparison between February-December 1982 and February-December 1983 revealed a 23 percent drop in fatalities and a 26 percent reduction in serious injuries. A subsequent study found a fall-off in serious injuries (including fatalities) of 23 percent for car drivers and 30 percent for front-seat passengers individuals directly affected by the law. We could, of course, continue to catalogue the beneficial effects of other safety laws, such as the wearing of crash helmets by motorcyc- lists-and the adverse effects of repealing these laws in certain states in the USA- but the point has been made. Legislation targeted at restricting particular types of unsafe acts has had a very large impact. But these are all individual accidents in which the people at risk, the hazards and the dangerous situations are well known. Unfortunately, the same degree of specificity-or even understanding-does not exist for organizational accidents. So, the question remains: how can regulators function more effectively to limit the occurrence of these catastrophic yet infrequent events? To address this question, we first need to consider the nature of the regulatory process and then look at how key events, such as the UK's Health and Safety at Work Act (1974) and the Cullen Report on Piper Alpha, have changed the way in which safety laws are framed and enforced. Autonomy and Dependence as Constraints on the Regulatory Process Regulatory authorities are agents of social control. The organizational literature is rich in theoretical accounts of this control process, but the one favored here is that presented by Diane Vaughan in her detailed analysis of the Challenger accident.24 What makes Vaughan's notions of organizational autonomy and interdependence particularly relevant to our present concerns is that they were developed to elucidate the regulatory problems that can, and did, contribute to an organizational accident. The essence of her argument is that the regulatory process-discovery, monitoring, investigation and sanctioning - is inevitably constrained by the interorganizational relations existing between the regulatory body and the regulated company. These, in turn, lead to relationships based more upon bargaining and compromise than threats and sanctions. The fact that both the regulator and the regulated are autonomous, existing as separate and independent entities, poses special problems for the regulator. All organizations are, to varying degrees, insulated from the outside world by their physical structure, culture and measures for limiting the leakage of sensitive facts. Information passing outwards gets filtered. Moreover, organizations tend to be highly selective in their transactions with external organizations, and especially with regulators. Regulators, for their part, attempt to penetrate the boundaries of the regulated organizations by requesting certain kinds of informa- tion and by making periodic site visits. But these strategies can only provide isolated glimpses of the organization's activities. Size, com- plexity, the peculiarities of organizational jargon (as at NASA, for example), the rapid development of technology and, on occasions, deliberate obfuscation all combine to make it difficult for the regulator to gain a comprehensive and in-depth view of the way in which an organization really conducts its business. And, being themselves members of an autonomous organization with its own agenda, individual regulators confronting these difficulties are likely to give their immediate supervisors the impression that they know more about the regulated organization than is actually the case. To confess that they cannot penetrate to the heart of their assigned organization's dealings is to admit that they lack the necessary investigative skills, or are not doing the job diligently enough, or both. Regulators, too, have careers. In an effort to work around these obstacles, regulators tend to become dependent upon the regulated organizations to help them acquire and interpret information. Such interdependence can undermine the regulatory process in various ways. The regulator’s knowledge of the nature and severity of a safety problem can be manipulated by what the regulated organization chooses to communicate and how this material is presented. Regulators, being human beings, tend to establish personal relationships with the regulated- they get to like the people they oversee and come to sympathize with their problems on a personal level-and this sometimes compromises their ability to identify, report or sanction violations. Bad relations consume limited resources, take up valuable time and are unpleasant and often counterproductive-particularly when the internal sources of information dry up. As a result, both the regulator and the regulated generally try to avoid adversarial encounters, favoring negotiation and bargaining over conflict and confrontation. Diane Vaughan summarizes the situation as follows: Situations of mutual dependence are likely to engender continual negotiation and bargaining rather than adversarial tactics, as each party tries to control the other's use of resources and conserve its own. To interpret the consequences of this negotiation and bargaining (e.g., the 'slap-on-the-wrist' sanction or no sanction at all) as regulatory 'failures' is to miss the point. Compromise is an enforcement pattern systematically generated by the structure of inter-organizational regulatory relations. The Move Towards Self-regulation The past two or three decades have seen a marked change in the way safety legislation is framed in many industrialized countries. Putting it very simply, there has been a shift away from laws that specify the means by which safe working should be achieved to laws that focus on the attainment of certain safety goals. Instead of rules that pre- scribe the precise steps to be taken by individuals or organizations, leaving little or no discretion for deviation, the current trend is to- wards rules that emphasize the required outcomes of safety management, allowing considerable freedom on the part of the operators of hazardous technologies to identify the means by which these ends will be achieved. Achieving safety goals assumes two basic organizational functions. First, it requires reducing the probability of accident occurrence by identifying and eliminating the conditions that cause accidents. Second, it involves reducing the harmful consequences caused by the accidents that still occur-by establishing accident mitigation and emergency response procedures, by reducing those causal factors likely to increase the injury and damage and by moving potential victims and assets out of the exposure zones. Both of these functions are strongly influenced by what the technology or the society regards as safe enough. There are a number of different approaches: • The feasibility or as low as reasonably possible approach (risks to be kept as low as reasonably practicable)-safety exists at the point beyond which it is neither technologically nor commercially feasible for the organization to do more. • The comparative risk approach -safety is determined on the basis of comparisons to other risks that the society voluntarily accepts (for example, choice of air travel, road transport, smoking and so on). • The de minimis approach-safety exists when the risks are regarded as trivial, commonly taken as 10-6 or better. • The zero-risk approach-safety exists only when there is no risk of an accident with harmful consequences. As the Boston law professor, Michael Baram, has pointed out: Irrespective of the concept invoked to define what safety is at a particular point in time, as a society progresses, it demands a higher degree of safety. Thus, safety is a target moving continuously towards zero risk, except for interruptions during times of economic distress or high unemployment.27 In the brief account of the move towards self-regulation given below, we will focus on the British scene, though comparable developments have occurred elsewhere-see, for example, the Emergency Planning and Community Right to Know Act, passed by the US Congress in 1986, and the Clean Air Act Amendments, enacted by Congress in 1990. Throughout the first half of this century, safety legislation in Brit- ain was firmly rooted in the 'Factories Act Model'. 28 This was characterized by its fragmentation and its lack of workforce involvement. Since the mid-nineteenth century, the tendency had been for the passage of highly focused Acts of Parliament involving the de- tailed prescriptive regulation of specific work activities. The lack of workforce involvement was a legacy of the Factories Act tradition that imposed duties on employers, while workers' participation in the safety effort was seen mainly as a matter of discipline to be enforced by the employers. By the late 1960s this unwieldy legislative edifice, with one set of industry-related rules piled upon another, was beginning to crumble. A rapidly rising rate of industrial accidents, together with sweeping changes in technology and materials, created a widespread demand for new health and safety legislation. The government established a Commission of Inquiry under Lord Robens, hitherto the Chairman of the National Coal Board. The recommendations of the Robens Report later became enacted-with remarkably little govern- mental tinkering-as the Health and Safety at Work Act (HSW Act) in 1974. It was this Act that initiated the move towards self-regulation. The Robens Committee identified a number of major defects in the existing statutory system for promoting health and safety at work. The first, and the most fundamental, of these was that there was too much law. This had the effect of 'conditioning people to think of health and safety at work as in the first and most important instance a matter of detailed rules imposed by external agencies'.29 The second defect was that much of the existing law was unsatisfactory. It was unintelligible, obsolete in parts and largely concerned with physical circumstances rather than with 'the attitudes, capacities and performance of people and the efficiency of the organizational systems within which they work'.30 The third problem was the fragmentation of administrative jurisdictions and the absence of a comprehensive official provision for health and safety at work. The Report stated: We need a more effectively self-regulating system ... It calls for better systems of safety organization, for more management initiatives, and for more involvement of work people themselves. The objectives of future policy must therefore include not only increasing the effective- ness of the state's contribution to safety and health at work but also, and more importantly, creating conditions for more effective self-regulation.31 The second chapter of the Robens Report also made the following pertinent observation: Promotion of health and safety at work is an essential function of good management. ... Good intentions at the board level are useless if managers further down the chain and closer to what happens on the shop floor remain preoccupied exclusively with production problems.32 The HSW Act set in motion the progressive replacement of the existing health and safety legislation by a system of regulations and approved codes of practice. It imposed on an employer a duty 'to ensure, so far as is reasonably practicable, the health, safety and welfare at work of all his employees' and 'to conduct his undertaking in such a way as to ensure, so far as is reasonably practicable, that persons not in his employment who may be affected thereby are not thereby exposed to risks to their health and safety.' (ss. 2 and 3). It created two new bodies: the Health and Safety Commission (HSC), with responsibility for achieving the general purposes of the Act, and the Health and Safety Executive (HSE), charged with the enforcement of health and safety policy. Excluded from the province of the HSE at that time were offshore operations, consumer and food safety and transport other than that of hazardous goods. In 1976 the Act was extended to cover workers in the offshore oil and gas industry, but the responsibility for regulation was placed with the Department of Energy (that continuing airworthines management exposition into existence in 1974) in view of their specialist knowledge and experience of this relatively new area of operations. To summarize: Unlike the Factories Act, the HSW Act did not go into any great detail with regard to particular accident-producers (for example, machinery, hoists and lifts, gas holders and the like). Instead, it provided broad guidelines for the duties of employers, employees, suppliers and users. It also established the processes for creating future regulations and codes of practice and set out the frameworks for a policy-making body (the HSC) and a regulatory body (the HSE). Between 1971 and 1980 there was a clear downward trend in overall numbers for both fatal and non-fatal accidents. The trend was particularly marked after 1973 and from 1978 onwards when the picture was one of more or less continuous decline in the number of industrial accidents. It should be noted, however, that this was also part of a long-term decline in industrial accidents that had been in progress since the tum of the century. But from 1980-81 onwards, the UK industrial accident rate levelled out and then began to increase. More people were being seriously injured in the manufacturing and construction industries at the end of the 1981-85 period than in the beginning. The causes of this upturn are still obscure. In 1984 the HSE issued the Control of Industrial Major Hazards (CIMAH) Regulations covering all onshore major hazard installations. This required that the operator should provide the HSE with a written report on the safety of the installation, commonly known as the Safety Case. These regulations had their origins in the Flixborough disaster in 1974 and were confined to the European Commission's (EC's) Directive on Major Accident Hazards, commonly termed the Seve so Directive. Their requirements included: • The demonstration of safe operation. • The notification of major accidents. • A written report (the Safety Case). • Updating of the report following any significant changes. • An obligation to supply the HSE with further information. • Preparation of an on-site emergency plan. • Provision of relevant information to the public. • An emergency plan produced by the local authority. On receipt of a Safety Case, the HSE assembled a team of varied specialists to establish that all the required information had been provided and to identify any matters of immediate concern. Any such matters were then pursued by letter or by site visits. Once satisfied that the Safety Case complied with the requirements, the HSE then used it as a basis for their subsequent inspection strategy. It should be emphasized that this was not a licensing or approval process that could be seen as transferring some of the responsibility for health and safety to the HSE. This responsibility lay squarely with the operator, and considerable importance was attached by the HSE to management and organizational issues. One of the most important sources of UK law since the HSW Act has been the Control of Substances Hazardous to Health (COSHH) Regulations, implemented in 1988 in response to an EC Directive on Hazardous Agents. This sets out the guidelines for measures necessary to protect the health not only of employees but of others. The COSHH Regulations affected virtually every workplace. They also adopted a strategy that was in marked contrast to the HSW Act- namely that, among other things, employers were required to be aware of the properties of over 5000 substances and to identify those which have relevance to their particular workplaces. They also laid down a 'rolling procedure' for the identification, assessment and control of risk. This entailed the following steps: • Making a list of all hazardous substances, identifying points of use and carrying out risk assessments-using outside experts, if necessary. • Controlling exposure to substances by means of ventilation, improved facilities for washing, and using less hazardous substances where possible. • Testing of respiratory equipment, ventilation and the like, and keeping careful records of these examinations. • Monitoring exposure through the use of valid occupational hygiene techniques. • Carrying out health surveillance of employees exposed to risk. • Training employees in the nature of the risks and the precautions to be taken. The next major development followed directly from the recommendations set out in the second volume of the Cullen Report on the Piper Alpha disaster, published in November 1990. Building on the recent legislation, Lord Cullen proposed a new regulatory mode for offshore operations. He described this as follows: I am satisfied that operators of installations ... should be required to carry out a formal safety assessment of major hazards, the purpose of which would be to demonstrate that the potential major hazards of the installation and the risks to the personnel thereon have been identified and appropriate controls provided. This is to assure the operators that their operations are safe. However it is also a legitimate expectation of the workforce and the public that operators should be required to demonstrate this to the regulatory body. The presentation of the formal safety assessment should take the form of a Safety Case, which would be updated at regular intervals and on the occurrence of a major change of circumstances. 33 The Cullen Report recommended that the regulation of offshore safety should be carried out by 'a discrete division of the HSE which is exclusively devoted to offshore safety'. It also proposed that the HSE should employ a specialist inspectorate that had 'a clear ident- ity and strong influence in the HSE', and that it should be headed by a chief executive who reported directly to the Director General. The chief executive should also be a member of the HSE's senior management board. This model of Formal Safety Assessments (FSAs) presented as Safety Cases subsequently spread to other industries. For example, prompted in part by the Estonia disaster in 1994, the Marine Safety Agency (MSA), in September 1995, created a well funded research programme to investigate the formal safety assessment of shipping. An FSA was seen by the MSA as comprising five steps: • Identification of hazards. • Assessment of risks associated with those hazards. • Consideration of alternative ways of managing those risks. • Cost-benefit assessment of alternative risk management options. • Decisions on which options to select. One further development is worth a brief mention- the upsurge within Britain of an 'ideology of deregulation'. In 1992 the Prime Minister made a speech urging UK regulators to take a more relaxed approach to enforcing the 'excessive detail' of EU regulations. Three months later, the Secretary of State for Trade and Industry set up two inquiries into 'red tape'. One was to look into opportunities for lessening the 'burden' of EU regulations, and the second was tasked with reviewing, simplifying and, where possible, abolishing some 7000 regulations on business. Within two months, the HSE announced its plans to review over 400 health and safety regulations with the intention of lessening their burden upon small businesses. Meanwhile, Parliament passed the Contracting Out and Deregulation Bill, granting the Secretary of State for Employment new powers to repeal safety legislation. Running in parallel with these moves towards deregulation, there has been-predictably- a steady erosion of the HSC/HSE's budget. This amounted to reductions of 2.6 per cent and 5 per cent in successive years, a fall of some £15 million. Equally predictably, an all-party Parliamentary committee has recently criticized the HSE for con- ducting too few inspections and, most particularly, for failing to meet its targets with regard to examining outstanding Safety Cases called for under the CIMAH regulations relating to hazardous installations. Although by no means a definitive review, enough has probably been said to cover the main steps in the move towards self-regulation-and deregulation-in the United Kingdom. It is now time to consider the implications of these changes for the regulatory authorities. Will these measures be effective in limiting the occurrence of organizational accidents? Has it made the regulator's job any easier? The Pluses and Minuses of the Move to Self-regulation Any measure that shifts the onus for maintaining safe work practices onto the organizations directly concerned has to represent an enormous 'plus' in the struggle to limit the occurrence of organizational accidents. Also, there is the related demand upon them to take a close, continuing, and proactive interest in all the varied factors affecting the safety of their installations. Most technological operations, even very complex ones, are relatively simple in comparison to the task of maintaining safe working conditions. As noted in Chapter 4, there are not enough trees in the rainforests to carry out all the procedures necessary to guarantee safe operations. Safety, as we have seen, is a 'dynamic non-event' that depends crucially upon a clear understanding of the interactions between many different underlying processes. The long-term safety benefits of being forced to grapple with these enormously difficult and still unresolved sociotechnical issues are undoubtedly greater than any number of purely technical 'fixes'. The process is more valuable than the product. And, as the Cullen Report notes, many operators have found the exercise of producing a Safety Case valuable: 'Often it would be the first time that a report had been made of the major hazard aspects of the organization. Many stated that the exercise had led them to make chainless in their approach and improvements to systems and procedures.'3 While there can be little doubt that this legislative drive towards self-regulation has definitely got its heart in the right place when it comes to reducing the likelihood of organizational accidents, its benefits for the regulatory bodies are less certain. Let us examine some of the problems. Traditionally, regulators worked to ensure compliance with safety rules laid down by some legislative authority. No matter how fragmented, obsolescent, or externalized these rules were, they nonetheless represented an agreed standard, at least at that time-against which to determine whether or not a particular work practice or hazardous installation was in violation of this or that regulation. In the new self-determining climate, regulators are still required to look out for deviations, but of quite a different kind. Now they have to inspect for departures from a Safety Case that is expressed in far more general terms, that can vary widely from organization to organization (according to their means), and for which they must take some direct responsibility since it would not exist as a frame of reference had the regulatory authority not approved it in the first place. As we have seen, spotting, monitoring, and sanctioning violations were difficult enough in the past, but now the responsibility placed upon the regulator is indeed great. Not only do they have to police compliance with a variety of Safety Cases, but they also need a very clear idea of what constitutes an adequate Safety Case, and this is by no means an easy task in our present limited state of knowledge. In order to judge the adequacy of a Safety Case in something other than a cursory checklist fashion, regulators are now required to have a comprehensive-one could almost say god-like-appreciation of all the many factors contributing to both individual and organizational accidents. While the physical origins of the former were largely enshrined in earlier legislation, the various ways in which human, technical, and organizational factors can combine to produce the latter are still not fully known and are perhaps ultimately unknowable- since each major organizational accident seems to throw up a fresh set of surprises. This additional evaluative burden is not lightened by the fact that most regulatory staff possess expertise in technical and operational matters rather than in human and organizational factors. While the 80:20 rule (80 human/organizational to 20 technical) may apply to accident causation, it certainly does not reflect the balance of expertise in most regulatory authorities. Even the largest of them have only a sprinkling of individuals with formal training in these 'softer' disciplines- and probably rightly so, since they have yet to deliver the goods with regard to agreed theory and practice. So, even if the causally appropriate 80:20 expertise ratio were achieved, it would not necessarily solve the problem. The situation for the regulatory authority would become even more difficult should one of its overseen organizations suffer a major accident. The subsequent investigation could turn up one of two things: either that the organization's performance was in compliance with its Safety Case or that the accident was due in part to violations of the Safety Case. The former could be judged as stemming from shortcomings in the regulator's evaluation process-the Safety Case should not have been approved in the first place-while the latter is likely to be viewed as a failure of regulatory surveillance. Damned if they do and damned if they don't! A Possible Model for the Regulatory Process Regulators are uniquely placed to function as one of the most effective defenses against organizational accidents. They are located close to the boundaries of the regulated system, but they are not of it. This grants them the perspective to identify unsatisfactory practices and poor equipment that the organization has grown accustomed to or works around. Regulators are specialists in the technology in question. Indeed, many of them have been recruited from the regulated industries. Regulators have been trained and are highly experienced in identifying technical inadequacies and formal systemic weaknesses. They also possess the investigative access and sanctions necessary to enforce their decisions. What regulators often lack, however, are the tools, training, and resources to confront all the important human and organizational factors and to monitor the insidious accumulation of latent conditions that can subsequently be combined to penetrate the system's defenses. The same also applies to the policy-makers. The past decade has seen isolated flurries of post-disaster legislation when what was needed were the statutory and regulatory structures necessary to forestall future accidents rather than trying to prevent the previous ones. This penultimate section draws heavily on the principles and techniques presented in Chapters 6 and 7 in order to sketch out a model for a regulatory process that, it is believed, could be effective in limiting the occurrence of organizational accidents. The regulatory model outlined below is shaped by three concerns: • How can regulators deploy their limited resources in the most cost-effective and targeted manner? • How can regulators bring about the organizational reforms necessary to achieve,and then sustain optimum levels of 'safety health' on the part of the complex well-defended organizations that they oversee? • Since the absolute criteria for safe operation are rarely known in advance, how can we design a regulatory process that will enable the policy-maker, the regulator, and the regulated all to be integral parts of an effective learning cycle? Figure 8.1 summarizes the basic elements of the regulatory process as they might appear to front-line inspectors. Site visits generate various indicators, such as instances of non-compliance or deviations from safe working practices. These are the risk achievement worth data of the regulatory process. They could include, for example, badly maintained or unsuitable items of equipment, a lack of briefing or drills, a poor permit-to-work system, or an unsatisfactory shift handover arrangement. Each instance generates two kinds of action items: those on the regulated organization to put them right (the dark octagons), and those on the regulator to monitor their rectification and, if this is not done satisfactorily, to impose some sanction (the white octagons). Figure 8.2 shows the next stage of model development. Here, the basic elements of regulation have been extended to identify the upstream organizational and managerial (0 & M) determinants of the local instances. The precise nature of these 0 & M factors will vary from industry to industry, but the assessed factors will always be less than the total number of possible factors (as discussed in Chapter 7). In this case, only seven 0 & M factors (represented by the dark rectangles) are considered in the analysis. If, for example, the regulated organization were a small airline, the 0 & M factors could include crew factors, operational management, maintenance management, safety management, organizational structure, and commercial and operational pressures. It is presumed that each instance is the product of different contributions from each of the 0 & M factors. The circular nodes in the lower half of the figure are points at which the regulator is required to make an assessment of the impact of each 0 & M factor on that particular instance. The regulator rates the relative contributions of each 0 & M factor to each instance on a five-point scale, where 1 = very little influence and 5 = very considerable influence. The associated white rectangles are action items on the regulator to monitor and, if necessary, to assist with any reforms. These ratings are then summed over all the instances in that particular collection (either from a single inspection or several) to generate an Organizational Factor Profile (see Figure 8.3). The purpose of the profile is to indicate to both the regulator and the regulated which of the various upstream factors is most in need of urgent reform. As discussed in Chapter 7, we cannot expect any organization to deal effectively with all of its problems at once. Later inspections will yield further instances that, in turn, will generate more organizational factor profiles. A succession of these profiles will allow both the regulator and the regulated to track the progress of remedial efforts. Finally, Figure 8.4 attempts to show how the regulatory process modelled here could form part of a wider learning cycle, involving legislators, regulators, and hazardous technologies. The systemic improvements generated by these focused organizational reforms will, it is hoped, come to represent new standards of health and safety at work. This could be incorporated into new legislation, which, in turn, would change the regulator's inspection and surveillance criteria- and so on. It will be seen that the local repairs required of regulated organizations become relegated to a subordinate loop. The main emphasis throughout is upon the continuing reform of the source factors influencing a system's intrinsic resistance to its operational hazards. The philosophy underlying this model is identical to that described for Tripod-Delta (see Chapter 7). Local instances, such as unsafe acts, are merely symptoms of the underlying organizational and managerial pathology. Removing the instances one by one will not improve the organization's safety and health. That can only be achieved by curing the source problems. As noted in Chapter 6, maintaining safety, like life, is 'one damn thing after another.' It is expected that while each targeted 0 & M factor is being reformed, others will be deteriorating. Nevertheless, the model provides a principled way of tracking these changes and of addressing the latest 'big problems' as each is revealed by the prevailing pattern of local indicators. The Regulator Deserves a Better Deal Judging by the failures presented in the first part of the chapter, regulatory authorities are not always able to prevent organizational accidents. But a closer examination shows that they, like other people on the front line of the safety war, are more often victims than villains. Yes, they make mistakes. But why should they be any different from the rest of the human race? And, like other people, they do not make their errors in isolation. As this chapter has tried to show, the majority of regulatory shortcomings have their origins far upstream from the individual inspector. Given the current trend of searching for increasingly more remote contributions to organizational accidents, it is inevitable that the regulator's alleged deficiencies should be judged by those with 20:20 hindsight as making significant contributions to a major disaster. The regulators' position vis a vis the affected organization means that they are likely to attract blame from all directions. Standing as they do on the organizational borders of all hazardous technologies, their sphere of responsibility is bound to be implicated in a wide variety of contributing factors. However, if regulators are to be other than convenient scapegoats, they will have to be provided with the legislation, the resources, and the tools to do their jobs effectively. They are potentially one of the most important defenses against organizational accidents. Societies, just like the operators of hazardous systems, put production before protection. As we have seen, safety legislation is enacted in the aftermath of disasters, not before them. There is little or no political kudos to be gained from bringing about a non-event, although in the long run, meeting this challenge successfully is likely to be much more rewarding. Every society gets the disasters it deserves. Let's hope that, in the next millennium, the regulators are seen to deserve something better than has so far been the case. Then, perhaps, we will all be safer. Chapter 9 Engineering a Safety Culture The Scope of the Chapter Few phrases occur more frequently in discussions about hazardous technologies than safety culture. Few things are so sought after and yet so little understood. However, should it be thought that the current preoccupation with safety culture is just another passing fad, consider the following facts. Commercial aviation is an industry that possesses an unusual degree of uniformity worldwide. Airlines across the globe fly many of the same types of aircraft in comparable conditions. Flight crews, air traffic controllers, and maintenance engineers are trained and licensed to very similar standards. Yet, in 1995, the risks to passengers (the probability of becoming involved in an accident with at least one fatality) varied by a factor of 42 across the world's air carriers, from a 1 in 260,000 chance of death or injury in the worst cases to a 1 in 11,000,000 probability in the best cases. While factors such as national and company resources will play their part, there can be little doubt that differences in safety culture are likely to contribute the lion's share to this enormous variation. We first encountered the term 'safety culture' in Chapter 2 when making the distinction between pathological, bureaucratic, and generative organizations. It cropped up again in Chapter 6 in regard to the motive forces that drive an organization towards a state of maximum resistance to its operational hazards. The present chapter focuses mainly on three questions: What is an organizational culture? What are the main ingredients of a safety culture? And, most importantly, how can it be engineered? The term 'engineered' is deliberate. But it is not meant in the traditional sense of developing more sophisticated gadgetry. Rather, we will be discussing the application of social engineering. This book has sought to argue that the most effective solutions to human performance problems are more the province of the technical manager (and the regulator) than the psychologist since they are concerned with the conditions under which people work rather than the human condition itself. The main message of this chapter is that the same general principle also applies to the acquisition of an effective safety culture (hereafter, the phrase 'safety culture' will be taken to mean an effective safety culture). Whereas national cultures arise largely out of shared values, organizational cultures are shaped mainly by shared practices (a claim that is developed in the next section). And it is these practices that will be the focus of this chapter. Many people say that a safety culture can only be achieved through some awesome transformation akin to a religious experience. This chapter takes the opposite view, arguing that a safety culture can be socially engineered by identifying and fabricating its essential components and then assembling them into a working whole. It is undoubtedly true that a bad organizational accident can achieve some dramatic conversions to the 'safety faith,' but these are all too often short-lived. A safety culture is not something that springs up readymade from the organizational equivalent of a near-death experience; rather, it emerges gradually from the persistent and successful application of practical and down-to-earth measures. There is nothing mystical about it. Acquiring a safety culture is a process of collective learning, like any other. Nor is it a single entity. It is made up of a number of interacting elements, or ways of doing, thinking, and managing, that have enhanced safety and health as their natural byproduct. What is an Organizational Culture? To those with a 'hard' engineering background, many attempts to describe the nature of organizational culture must seem to have the definitional precision of a cloud. There is no standard definition, but here is one that captures most of the essentials with the minimum of fuss: Shared values (what is important) and beliefs (how things work) that interact with an organization's structures and control systems to produce behavioral norms (the way we do things around here). Until the 1980s, 'culture' was a term applied more to nationalities than to organizations. 'Organizational culture' became an essential part of 'management speak' largely as the result of two widely read books: Corporate Culture (by Terrence Deal and Allan Kennedy)3 and In Search of Excellence (by Thomas Peters and Robert Waterman), both published in 1982. The latter book introduced the notion of cultural strength and observed, 'Without exception, the dominance and coherence of culture proved to be an essential quality of the excellent companies.' Fifteen years later, there are some who might doubt this assertion (particularly those laid off from the 'excellent' companies), but few would argue with the idea that a strong culture is one in which all levels of the organization share the same goals and values. To quote Peters and Waterman again, 'In these [strong culture] companies, people way down the line know what they are supposed to do in most situations because the handful of guiding values is crystal clear'. Organizational theorists have described a number of negative or dysfunctional cultures. One such 'bad' culture is characterized by what psychologists have termed learned helplessness, describing a condition in which people learn that attempts to change their situation are fruitless so that they simply give up trying: 'The energy and will to resolve problems and attain goals drains away.' Another counter-productive organizational strategy is anxiety-avoidance. When such an organization discovers a technique for reducing its collective anxiety, it is likely to be repeated over and over again regardless of its actual effectiveness. The reason is that the learner will not willingly test the situation to determine whether the cause of the anxiety is still operating. Thus, all rituals, patterns of thinking or feeling, and behaviors that may originally have been motivated by a need to avoid a painful, anxiety-provoking situation are going to be repeated, even if the causes of the original pain are no longer acting, because the avoidance of anxiety is, itself, positively reinforcing. Both learned helplessness, and repetitive anxiety avoidance is likely to assist in driving the blame cycle described in Chapter 7. People feel helpless in the face of ever-present dangers and while the familiar reactions to incidents and events, such as 'write another procedure' and 'blame and train,' may not actually make the system more resistant to future organizational accidents, they at least serve the anxiety-reducing function of being seen to do something-and blaming those at the sharp end deflects blame from the organization as a whole. There is a controversy among social scientists as to whether a culture is something an organization 'has' or whether it is something an organization 'is.' The former view emphasizes management's power to change culture through the introduction of new measures and practices, while the latter sees culture as a global property that emerges out of the values, beliefs, and ideologies of the organization's entire membership. The former approach is favored by managers and management consultants, while the latter is preferred by academics and social scientists. This chapter stands with the managers and agrees with the organizational anthropologist Geert Hofstede when he wrote: On the basis of [our] research project, we propose that practices are features an organization has. Because of the important role of practices in organizational cultures, the ['has' approach] can be considered as somewhat manageable. Changing the collective values of adult people in an intended direction is extremely difficult, if not impossible. Values do change, but not according to someone's master plan. Collective practices, however, depend on organizational characteristics like structures and systems and can be influenced in less predictable ways by changing these. Although the idea of a safety culture has existed since 1980, it was given an authoritative boost by the International Atomic Energy Agency when they published a report in 1988 elaborating on the concept in detail. They defined safety culture as: ' ... that assembly of characteristics and attitudes in organizations and individuals which establishes that, as an overriding priority, nuclear plant safety issues receive the attention warranted by their significance.1° Unfortunately, this is something of a 'motherhood' statement specifying an ideal but not the means to achieve it. A more useful definition, which is worth quoting in full, has been given by the UK's Health and Safety Commission in 1993: The safety culture of an organization is the product of individual and group values, attitudes, competencies, and patterns of behavior that determine the commitment to, and the style and proficiency of, an organization's health and safety programs. Organizations with a positive safety culture are characterized by communications founded on mutual trust, shared perceptions of the importance of safety, and confidence in the efficacy of preventive measures. While remaining in sympathy with this definition, this chapter emphasizes the critical importance of an effective safety information system- the principal basis of an informed culture. It must be stressed again that our primary concern in this book is not with traditional health and safety measures that are directed, for the most part, at the prevention of individual work accidents. Our focus is on the limitation of organizational accidents, and it is this that has shaped the arguments set out below. The Components of a Safety Culture The main elements of a safety culture and their various interactions are previewed below. Each sUbcomponent will be discussed more fully in succeeding sections. • As indicated in Chapter 6, an ideal safety culture is the engine that continues to propel the system towards the goal of maximum safety and health, regardless of the leadership's personality or current commercial concerns. Such an ideal is hard to achieve in the real world, but it is nonetheless a goal worth striving for. • The power of this engine relies heavily upon continuing respect for the many entities that can penetrate and breach the defenses. In short, its power is derived from not forgetting to be afraid. • In the absence of bad outcomes, the best way-perhaps the only way to sustain a state of intelligent and respectful wariness is to gather the right kinds of data. This means creating a safety information system that collects, analyses, and disseminates information from incidents and near-misses, as well as from regular proactive checks on the system's vital signs (see Chapter 7). All of these activities can be said to make up an informed culture in which those who manage and operate the system have current knowledge about the human, technical, organizational, and environmental factors that determine the safety of the system as a whole. In most important respects, an informed culture is a safety culture. • Any safety information system depends crucially on the willing participation of the workforce, the people in direct contact with the hazards. To achieve this, it is necessary to engineer a reporting culture- an organizational climate in which people are prepared to report their errors and near-misses. • An effective reporting culture depends, in turn, on how the organization handles blame and punishment. A 'no-blame' culture is neither feasible nor desirable. A small proportion of unsafe human acts are egregious (for example, substance abuse, reckless non-compliance, sabotage, and so on) and warrant sanctions, severe ones in some cases. A blanket amnesty on all unsafe acts would lack credibility in the eyes of the workforce. More importantly, it would be seen as an opposing nature justice. What is needed is a just culture, an atmosphere of trust in which people are encouraged, even rewarded, for providing essential safety-related information, but in which they are also clear about where the line must be drawn between acceptable and unacceptable behavior. • The evidence shows that high-reliability organizations-domain leaders in health, safety, and environmental issues possess the ability to reconfigure themselves in the face of high-tempo operations or certain kinds of danger. A flexible culture takes a number of forms, but in many cases, it involves shifting from the conventional hierarchical mode to a flatter professional structure, where control passes to task experts on the spot and then reverts back to the traditional bureaucratic mode once the emergency has passed. Such adaptability is an essential · feature of the crisis-prepared organization and, as before, depends crucially on respect this case, respect for the skills, experience, and abilities of the workforce and, most particularly, the first-line supervisors. However, respect must be earned, and this requires a major training investment on the part of the organization. • Finally, an organization must possess a learning culture willingness and the competence to draw the right conclusions from its safety information system, and the will to implement major reforms when their need is indicated. The preceding bullet points have identified four critical subcomponents of a safety culture: a reporting culture, a just culture, a flexible culture and a learning culture. Together they interact to create an informed culture which, for our purposes, equates with the term 'safety culture' as it applies to the limitation of organizational acci- dents. Engineering a Reporting Culture On the face of it, persuading people to file critical incidents and near-miss reports is not an easy task, particularly when it may entail divulging their own errors. Human reactions to making mistakes take various forms, but frank confession does not usually come high on the list. Even when such personal issues do not arise, potential informants cannot always see the value in making reports, especially if they are skeptical about the likelihood of management acting upon the information. Is it worth the extra work when no good is likely to come of it? Moreover, even when people are persuaded that writing a sufficiently detailed account is justified and that some action will be taken, there remains the overriding problem of trust. Will I get my colleagues into trouble? Will I get into trouble? There are some powerful disincentives to participating in a reporting scheme: extra work, skepticism, perhaps a natural desire to forget that the incident ever happened, and above all, a lack of trust and, with it, the fear of reprisals. Nonetheless, many highly effective reporting programs do exist. What can we learn from them? How have they engineered their success? In what follows, we will briefly look at the 'social engineering' details of two successful aviation reporting programs, one operating at a national level and the other within a single airline. These are NASA's Aviation Safety Reporting System (ASRS) and the British Airways Safety Information System (BASIS). In this, we will rely heavily on the work of two people, Sheryl Chappell of national aeronautics and space administration and Captain Mike O'Leary of British Airways12-each, each of whom has been closely involved in the design and management of these programs. Our purpose here is not to consider the programs themselves in any detail but to abstract from them the 'best practices' for achieving a reporting culture. Throughout, we will concentrate on the issue of how valid reporting may be promoted. Although we are dealing exclusively with aviation reporting schemes, the basic 'engineering' principles can be applied in any domain. Indeed, reporting programs in other domains-particularly in medicine, are partly modeled on pioneering aviation schemes such as critical incident reporting. Examination of these successful programs indicates that five factors are important in determining both the quantity and the quality of incident reports. Some are essential in creating a climate of trust; others are needed to motivate people to file reports. The factors are: • Indemnity against disciplinary proceedings as far as practicable. • Confidentiality or de-identification. • The separation of the agency or department collecting and analyzing the reports from those bodies with the authority to institute disciplinary proceedings and impose sanctions. • Rapid, useful, accessible, and intelligible feedback to the reporting community. • Ease of making the report. The first three items are designed to foster a feeling of trust. O'Leary and Chappell explain the need: For any incident reporting program to be effective in uncovering the failures that contribute to an incident, it is paramount to earn the trust of the reporters. This is even more important when there is a candid disclosure of the reporter's own errors. Without such trust, the report will be selective and will probably gloss over information on pivotal human factors. In the worst case, in which the potential reporters have no trust in the safety organization- there may be no report at all. Trust may not come quickly. Individuals may be hesitant to report until the reporting system has proved that it is sensitive to reporters' concerns. Trust is the most important foundation of a successful reporting program, and it must be actively protected, even after many years of successful operation. A single case of a reporter being disclosed as the result of a report could undermine trust and stop the flow of useful reports.13 The rationale for any reporting system is a recurrent theme throughout this book is that valid feedback on the local and organizational factors promoting errors and incidents is far more important than assigning blame to individuals. To this end, it is essential to protect informants and their colleagues as far as possible from disciplinary actions taken on the basis of their reports. But there will be limits upon this indemnity. These limits are defined most clearly by the Waiver of Disciplinary Action issued in relation to NASA's Aviation Safety Reporting System. Below is an excerpt from the federal aviation administration Advisory Circular (AC No. 00-46C) describing how the immunity concept applies to pilots making incident reports. The filing of a report with national aeronautics and space administration concerning an incident or occurrence involving a violation of the Act of the Federal Aviation Regulations is considered by the federal aviation administration to be indicative of a constructive attitude. Such an attitude will tend to prevent future violations. Accordingly, although a finding of a violation may be made, neither a civil penalty nor a certificate suspension will be imposed if: • The violation was inadvertent and not deliberate; • The violation did not involve a criminal offense, or accident or ... a lack of qualification or competency; • The person has not been found in any prior federal aviation administration enforcement action to have committed a violation of the Federal Aviation Act, or of any regulation promulgated under the Act for a period of 5 years prior to the date of the occurrence; and • The person proves that, within 10 days after the violation, he or she completed and delivered or mailed a written report of the incident or occurrence to national aeronautics and space administration under ASRS. This formula appears to work. The aviation safety report system reporting rate was high, even at the outset. In the beginning, it averaged approximately 400 reports per month. It now runs at around 650 reports per week and more than 2000 reports per month. In 1995, aviation safety report system received over 30,000 reports. BASIS has been extended over the years to cover a wide variety of reporting schemes. All flight crew are required to report safety-related events using Air Safety Reports (ASRs). aviation safety report system are not anonymous. To encourage the filing of ASRs, British Airways Flight Crew Order No. 608 states: It is not normally the policy of British Airways to institute disciplinary proceedings in response to the reporting of any incident affecting air safety. Only in rare circumstances where an employee has taken action or risks which, in the Company's opinion, no reasonably prudent employee with his/her training and experience would have taken will British Airways consider initiating such disciplinary action. Again, the formula seems to work. Its success is suggested by two statistics. First, the airport surveillance radar filing rate more than trebled between its inception in 1990 and 1995. Second, the combined number of reports assigned to the severe and high-risk categories has decreased by two-thirds between the first six months of 1993 and the first half year of 1995.16 Another important component of BASIS is the British Airways Confidential Human Factors Reporting Programme, instituted in 1992. While the aviation safety report system provided good technical and procedural information, an information channel that was more sensitive to human factors issues was needed. Each pilot filing an airport surveillance radar is now invited to complete a confidential human factors questionnaire relating to the incident. The return of the questionnaire is voluntary. The following assurance was given by the senior manager in charge of BA's Safety Services on the front page of the initial version: I give an absolute assurance that any information you provide will be treated confidentially by Safety Services and that this questionnaire will be destroyed immediately after the data is processed. This program is accessible only to my Unit. In its first year of operation, the human factors reporting program received 550 usable responses. The issues raised in the reports are communicated to management on a regular basis, but great care is taken to separate the important safety issues from the incidents in order to preserve the anonymity of the reports. Another important input to BASIS comes from the Special Event Search and Master Analysis (SESMA). This bypasses the need for human reporting by monitoring directly the flight data recorders (FDRs) of BA's various aircraft fleets while at the same time guaranteeing the flight crews complete anonymity. The flight data recorder for each flight is scanned for events that are considered to lie outside safe norms. All events are stored in a BASIS database, and the more serious ones are discussed at a monthly meeting of technical managers and the pilots' union representatives. If the incident is considered to be sufficiently serious, the union representative is required to discuss the matter with the flight crew involved while still withholding their identities from the management. When a report is received by NASA's aviation safety report system staff, it is processed in the following manner, with great care being taken to preserve the anonymity of the reporter. • An initial analysis screens out reports involving accidents, criminal behavior, or those classified as 'no safety content.' • The report is coded, and the reporter is de-identified. At this stage, the reporter is also contacted by telephone to confirm receipt and de-identification. • After a quality check, the information is entered into the aviation safety report system database, and the original report is destroyed. The most obvious way of ensuring confidentiality is to have the reports filed anonymously. But, as O'Leary and Chappell point out, this is not always possible or even desirable.19 The main problems with total anonymity are as follows: • Analysts cannot contact the informant to resolve questions. • It is more likely that some managers will dismiss anonymous reports as the work of disaffected troublemakers. • In small companies, it is almost impossible to guarantee anonymity. O'Leary and Chappell conclude that removing identities from reports at a later stage, described above for ASRS, is probably the most workable means of maintaining confidentiality. At a national level, complete de-identification means repeating not only people's names but also the date, the time, the flight number, and the airline name. The criteria for de-identification must be known and understood by all potential reporters. Another important measure for engendering trust is to separate the organization, receiving the reports from both the regulatory body and from the employing company. As in the case of ASRS, the system analysts should ideally have no legal or operational authority over the potential reporters. Reporting systems run by disinterested third parties such as universities also help to earn the trust of porters. If, like BASIS, the reporting system is internal to a company, the receiving department should be perceived as being completely independent of operational management, thus giving the necessary assurance of confidentiality. Apart from a lack (or loss) of trust, few things will stifle incident reporting more than the perceived absence of any useful outcome. Both the aviation safety report system and BASIS place great emphasis on the rapid feedback of meaningful information to their respective communities. If an aviation safety report system report describes a continuing hazardous situation - for example, a defective navigation aid, a confusing procedure, or an incorrect chart alerting message is sent out immediately to the appropriate authorities so that they can investigate the problem and take the necessary medical action. (As mentioned earlier, aviation safety report system has no legal or operational authority of its own.) Some 1700 alert bulletins and notices have been issued by the aviation safety report system team since the program began in 1976. In 1994, there was a 65 percent response rate to alert bulletins and FYI notices. The information assembled in the aviation safety report system database is made available to the aviation and research communities in a variety of ways. First, targeted searches can be carried out at the request of companies, agencies, or researchers. The information is also disseminated via a newsletter, Callback, whose extended readership is estimated at over 100,000, and other aviation safety report system reports. Such newsletters describe safety issues and highlight improvements that have been made as a result of incident reporting. This serves the double function of both informing the reporters and congratulating them on their collective contribution to aviation safety. British Airways Safety Services also disseminates their BASIS information in a variety of ways. In addition to unit reports and journal articles, they issue Flywise, an 18-20 page monthly bulletin that includes trend analyses relating to selected events and brief accounts of incidents broken down by fleets. Each incident is assigned both a risk category (based on a risk matrix-see Figure 9.1) and one of the following action categories: • Active investigation actions to prevent recurrence are not fully understood. • Action required-preventive measures have been identified but not yet implemented. • Action monitored-preventive measures have been implemented, and their effects are being monitored. • Report monitored action taken without need for further investigation by Safety Services. Rates of occurrence are being monitored. The last factor to be considered here is ease of reporting. The format, length, and content of the reporting form or questionnaire are extremely important, as is the context in which respondents are expected to make their report. Privacy and a labor-free return mechanism are all important incentives, or, to put it the other way around, their absence could be a deterrent. O'Leary and Chappell make the following observations regarding the design of the reporting form: If a form is long and requires a great deal of time to complete, reporters are less likely to make an effort. If the form is too short, it is difficult to obtain all the necessary information about the incident. In general, the more specific the questions, the easier it is to complete the questionnaire; however, the information provided will be limited by the choice of the questions. More open questions about the reporter's perceptions, judgments, decisions, and actions are not subject to this limitation and give the reporter a greater chance to tell the full story. This method is more effective in gathering all the information about an incident, but it takes longer and usually requires more analytic resources within the reporting system.20 A certain amount of trial-and-error learning may be necessary before an organization hits upon a format that is best suited both to its purpose and to its potential respondents. In this regard, we can learn from the experience of British Airways Safety Services with their confidential human factors questionnaire. In its initial form, it asked a limited number of very specific questions. Some of the questions sought to establish whether either a slip or a mistake had occurred (see Chapter 4 for the technical descriptions of these unsafe acts) and, if so, what the contributing factors were. The latter was listed below each item and required 'yes/no' responses: in the case of action slips and lapses, they included tiredness, time pressure, lack of stimulation, and flight deck ergonomics; in the case of mistakes, they included misleading manuals, misleading displays, insufficient training, and crew cooperation. Even though one of the questions asked what went right, several respondents complained about the negative flavor of the questions overall, and it was realized that this could well deter some potential respondents from completing the questionnaire at all. In addition, the BA analysts were unhappy with the validity of some of the data since the technical distinctions between slips, lapses, and mistakes were not well understood by the flight crew respondents. As a result, BA Safety Services launched a new jargon-free questionnaire in 1995. This asked open-ended questions covering a range of factors from local flight deck influences to the effectiveness of training. O'Leary gives two questions as examples of this new approach: • How did you and the crew initially respond to the event, and how did you establish what technical and personal issues were involved? • Was all the relevant flight, FMS, and system information clearly available, and were all the controls and selectors useful and ambiguous? If not, how could these be improved? As O'Leary observes, this style of questioning moves the analytic workload from the reporter to the human factors analysts. Although the change has increased demand on the limited resources available to process the data, it has made the questionnaire sensitive to a variety of issues not previously covered. In addition, the reliability of the analysis has improved dramatically: 'Previously, some 3500 pilots and engineers may report events idiosyncratically. Now, we use a team of only a dozen volunteer flight crew analysts'. With the multiple-choice format of the previous form of the human factors questionnaire, it was relatively simple to convert 'yes/no' responses directly into bar charts. With the second, more open-ended version, the BA analysts had to develop an agreed classification structure in order to identify the issues with significance for human factors. They were interested in two major categories: crew performance and the influences upon that behavior. Crew behavior was subdivided into error descriptors (action slip or mistake), crew team skills, and task specifics, such as automatic or manual handling. Influences, too, were divided into three groups: organizational factors (training, procedures, commercial pressure, and the like), environmental factors (airport facilities, weather conditions, and the like), and personal factors (automation, complacency, morale, and the like). Future developments are directed at establishing causal links between the various factors shift from addressing the 'what?' question to tackling the 'why?' question. The aim here is to identify 'resident pathogens' that may contribute to a variety of different problems on the flight deck. Figure 9.2 gives an idea of what this development might yield in the way of causal analysis. The figure summarizes a fictional incident involving a rejected takeoff. Here, operational stress was created by a busy airfield and communicating with the company to establish load-sheet figures while taxiing out. The climate on the flight deck was poor. The co-pilot felt overloaded but was not able to communicate this to the captain. He focused on one task at a time and did not cross-check what the captain was doing. As a result, he omitted the 'before takeoff' checks. Takeoff clearance was given as the aircraft approached the runway, and as takeoff power was set, the configuration warning horn indicated that no flap had been selected, and the takeoff was aborted. Finally, in this context of engineering a reporting culture, it is worth asking whether there is any scientific evidence to support the efficacy of near-miss accident reporting. In one Swedish study relating primarily to individual accidents, the implementation of an incident reporting scheme resulted in an increase in the number of remedial suggestions from the workforce but no significant reduction in the accident rate. In a follow-up study, participants received training in how to recognize and interpret critical incidents. This resulted in a 56 percent reduction in the severity of injuries but no drop in the accident frequency rate. The main message of these findings is that potential respondents need to be very clear about what constitutes an incident. In some situations, this is not always intuitively obvious. Engineering a Just Culture A wholly just culture is almost certainly an unattainable ideal. How- ever, an organization in which the majority of its members share the belief that justice will usually be dispensed is within the bounds of possibility. Two things are clear at the outset. First, it would be quite unacceptable to punish all errors and unsafe acts regardless of their origins and circumstances. Second, it would be equally unacceptable to give a blanket immunity from sanctions to all actions that could, or did, contribute to organizational accidents. While this book has strongly emphasized the situational and systemic factors leading to the catastrophic breakdown of hazardous technologies, it would be naive not to recognize that, on some relatively rare occasions, accidents can happen as the result of the unreasonably reckless, negligent, or even malevolent behavior of particular individuals. The difficulty lies in discriminating between these few truly 'bad behaviors' and the vast majority of unsafe acts to which the attribution of blame is neither appropriate nor useful. A prerequisite for engineering a just culture is an agreed set of principles for drawing the line between acceptable and unacceptable actions. To this end, we will start by outlining some of the psychological and legal issues that must be taken into account when making this judgement. Figure 9.3 sets the scene. All human actions involve three core elements: • An intention that specifies an immediate goal and-where these goal-related actions are not wholly automatic or habitual-the behaviour necessary to achieve it. • The actions triggered by this intention-which mayor may not conform to the action plan. • The consequences of these actions-which mayor may not achieve the desired objective. The actions can be either successful or unsuccessful in this respect. The feedforward arrows shown in Figure 9.3 indicate that, in for-mulating an intention, actions are selected in the belief that they will achieve the goal (or at least provide useful feedback to ensure the success of future actions), but this belief is not always justified. The feedback arrows complete the loop by providing information about the success or otherwise of the preceding actions and their outcomes. Human actions are embedded in a context that contains both the immediate physical environment and the purpose of the behavioural sequence of which a particular action forms a part. The historical context is shown symbolically in Figure 9.3 by the three preceding action frames in the background. Both of these issues have a close bearing on individual responsibility. In the case of hazardous technologies it is inevitable that all physical situations will contain an element of risk. But is also likely that individual actors will have been-or should have been-trained to foresee and to minimize these risks. This brings us back to the distinction made in Chapter 4 between successful and unsuccessful behaviour on the one hand, and correct and incorrect behaviour on the other. Although success is determined solely by whether the planned actions achieve their immediate objectives, success does not necessarily mean correctness. Successful actions may be incorrect. That is, they could achieve their local purpose and yet be either reckless or negligent. In the law, a person who acts recklessly is one who takes a deliberate and unjustifiable risk (that is, one that is foreseeable and where a bad outcome is likely though not certain). However, as Smith and Hogan point out: The operator of aircraft, the surgeon performing an operation and the promoter of a tightrope act in the circus must all foresee that their acts might cause death; but we should not describe them as reckless, unless the risk taken was unjustifiable. Whether the risk is justifiable depends on the social value of the activity involved, as well as on the probability of the occurrence of the foreseen evil. Negligence, on the other hand, involves bringing about a consequence that a 'reasonable and prudent' person would have foreseen and avoided. One can also be negligent with regard to a circumstance: 'A person acts negligently with respect to a circumstance when a reasonable man would have been aware of the existence of the circumstance and, because of its existence would have avoided acting in that manner.' In the latter case whether the person failed to foresee the bad outcome and was unaware of the circumstance is irrelevant. For example, X picks up a gun, believing it to be un- loaded, points it at Y and pulls the trigger. If any reasonable person would have realized that the gun might possibly be loaded, and thus avoided acting in this way, then X was negligent with regard to circumstance. If the gun was loaded and kills Y, then X was negligent with regard to consequence. In a court of law, it is not necessary for the prosecution to prove anything at all about the person's state of mind at the time of the act. It is enough to establish that particular actions were carried out in certain circumstances. Negligence is- historically a civil rather than a criminal law concept and has a much lower level of culpability than recklessness. Those involved in the operation of hazardous technologies are often perceived as carrying an additional burden of responsibility by virtue of their training and of the great risks associated with human failure. For example, in the case of Alidair v. Taylor in 1978, Lord Denning ruled that: There are activities in which the degree of professional skill which must be required is so high, and the potential consequences of the smallest departure of that high standard are so serious, that one failure to perform in accordance with those standards is enough to justify dismissaI.28 This 'hang them all' judgement is unsatisfactory in many respects. It ignores the ubiquity of error as well as the situational factors that promote it. Nor is it sensitive to the varieties of human failure and their differing psychological origins. Pushing this judgement to an absurd conclusion, it could be claimed that, since all pilots, control room operators and others with safety-critical jobs in hazardous technologies are fallible, they will all, at some time or another, inevitably fall short of Lord Denning's 'high standards' and so should all be sacked. Even wise and distinguished judges do not get it right all of the time. A much sounder guideline is Neil Johnston's substitution test. 29 This is in keeping with the principle that the best people can make the worst errors. When faced with an accident or serious incident in which the unsafe acts of a particular person were implicated, we should perform the following mental test. Substitute the individual concerned for someone else coming from the same domain of activity and possessing comparable qualifications and experience. Then ask the following question: 'In the light of how events unfolded and were perceived by those involved in real time, is it likely that this new individual would have behaved any differently?' If the answer is 'probably not' then, as Johnston put it, ' ... apportioning blame has no material role to play, other than to obscure systemic deficiencies and to blame one of the victims'. A useful addition to the substitution test is to ask of the individual's peers: 'Given the circumstances that prevailed at that time, could you be sure that you would not have committed the same or similar type of unsafe act?' If the answer again is 'probably not', then blame is inappropriate. So much for the background. We will now turn to the task of grading unsafe acts according to their blameworthiness. In this, as in jurisprudence, a crucial discriminator is the nature of the intention. A crime has two key elements: the mens rea, or 'guilty mind' and the actus reus, or 'guilty act'. Both are necessary for its commission. Except in very specific instances (as, for example, in the case of negligence), the act without the mental element is not a crime. While, for the most part, we are not concerned here with criminal behavior, we will adopt the 'mental element' principle as a basic guideline. But hereafter we will approach the issue more from a psychological perspective than from a legal one. Figure 9.4 sketches out the bare essentials of a decision tree for discriminating the culpability of an unsafe act. It is assumed that the actions under scrutiny have contributed either to an accident or to a serious incident in which a bad outcome was only just averted. In an organizational accident, there are likely to be a number of different unsafe acts, and the decision tree is intended to be applied separately to each of them. Our concern here is with individual unsafe acts committed by either a single person or by different people at various points in the accident sequence. The key questions relate to intention. If both the actions and the consequences were intended, then we are likely to be in the realm of criminal behaviour and that is probably beyond the scope of the organization to deal with internally. Unintended actions define slips and lapses-in general, the least blameworthy of errors-while unintended consequences cover mistakes and violations. The decision tree usually treats the various error types in the same way, except with regard to the violations question. For mistakes, the question reads as shown in Figure 9.4, but for slips and lapses, the question relates to what the person was doing when the slip or lapse occurred. If the individual was knowingly engaged in violating safe operating procedures at that time, then the resulting error is more culpable since it should have been realized that violating increases both the likelihood of making an error and the chances of bad consequences resulting (see Chapter 4). The 'unauthorized substance' question seeks to establish whether or not the individual was under the influence of alcohol or drugs known to impair performance at the time the unsafe act was committed. Since the ingestion of unauthorized substances is usually a voluntary act, their involvement would indicate a high level of culpability. But the matter is not entirely straightforward. In 1975, during a descent towards Nairobi, the co-pilot of a Boeing 747 misheard an air traffic control instruction. Instead of 'seven five zero zero', he heard 'five zero zero zero' and set the autopilot to level out at 5000 feet. Unfortunately, that would have placed the aircraft in a tunneling mode since it was around 300 feet below the unusually high airfield. When the aircraft broke cloud, the flight crew saw the ground a little more than 200 feet below them. Prompt action by the captain prevented this from being the first major disaster involving a Boeing 'jumbo' jet. It later transpired that the copilot had picked up a large tapeworm on a holiday in India and was dosing himself with un- authorized drugs that had, among their side-effects, drowsiness and nausea. Taking unauthorized medication as the result of a medical condition, while clearly reprehensible, is less blameworthy than taking drugs or alcohol for 'recreational purposes' and, as such, in Figure 9.4, it has been assigned to the category of 'substance abuse with mitigation'. The degree of mitigation will, of course, depend upon the local circumstances. Except when non-compliance has become a largely automatic way of working (as sometimes happens in the case of routine short-cuts), violations involve a conscious decision on the part of the perpetrator to break or bend the rules. However, while the actions may be deliberate, the possible bad consequences are not in contrast to sabotage in which both the act and the consequences are intended. Most violations will be non-malevolent in terms of intent, so the degree to which they are blameworthy will depend largely on the quality and availability of the relevant procedures. These, as discussed in Chapter 4, are not always appropriate for the particular situation. Where this is judged to be the case- perhaps by a 'jury' of the perpetrator's peers -the problem lies more with the system than with the indi- vidual. However, when good procedures were readily accessible but deliberately violated, the question must arise as to whether the behaviour was reckless in the legal sense of the term. Such actions are clearly more culpable than 'necessary' violations-that is, non-com- pliant actions necessary to get the job done when the relevant procedures are either wrong, inappropriate or unworkable. It seems appropriate to apply Johnston's substitution test once the issues of possible substance abuse and deliberate non-compliance have been settled, although something like it clearly has a part to play in judging the culpability of system-induced violations (as indicated by the dotted arrow in Figure 9.4). The issue is reasonably straightforward. Could (or has) some well-motivated, equally competent and comparably qualified individual make (or made) the same kind of error under those or very similar circumstances? If the answer given by a 'jury' of peers is 'yes', then the error is probably blameless. If the answer is 'no', then we have to consider whether there were any system-induced deficiencies in the person's training, selection or experience. If such latent conditions are not identified, then the possibility of a negligent error must be considered. If they are found, it is likely that the unsafe act was a largely blameless system-induced error. Such a category would apply to the technician whose miswiring of a signal box significantly contributed to the Clapham Junction rail disaster (see Chapter 5). His actions would not pass the substitution test, since he was largely self-taught and had acquired his bad work practices in the absence of adequate training and supervision. But, as the Inquiry established, the underlying problems were those of the system rather than the individual, who was hardworking and highly motivated to do a good job. In legal jargon, the last major question at the top right-hand corner of Figure 9.4 could be rephrased as 'Any previous?'. People vary widely and consistently in their liability to everyday slips and lapses. For example, some individuals are considerably more absentminded than others. If the person in question has a previous history of unsafe acts, it does not necessarily bear upon the culpability of the error committed on this particular occasion, but it does indicate the necessity for corrective training or even career counselling along the lines of 'Don't you think you would be doing everyone a favour if you considered taking on some other job within the company?'. This is the way that management acquires some of its most distinguished members. Absentmindedness has nothing whatsoever to do with ability or intelligence, but it is not a particularly helpful trait in a pilot or control room operator. So where should the line be drawn on Figure 9.4 between accept- able and unacceptable behaviour? The most obvious point would be between the two substance abuse categories. Both malevolent damage and the dangerous use of alcohol or drugs are wholly unacceptable and should receive very severe sanctions, possibly administered by the courts rather than the organization. Between 'substance abuse with mitigation' and 'possible negligent error' lies a grey area in which careful judgement must be exercised. The remaining categories should be thought of as blameless-unless they involve aggravating factors not considered here. Experience suggests that the majority of unsafe acts-perhaps 90 percent or more-fall into this blameless category. What should happen to the small proportion of individuals whose unsafe acts are justly considered culpable? It is not within the competence of this chapter to advise on the nature of the sanctions. Although this is a matter for the organizations concerned, we can say something about the value-or otherwise punishments. Unfortunately, a large amount of psychological research concerned with the issues of reward and punishment has involved the white rat, and is not especially relevant. Figure 9.5 summarizes in a very simplified way what psychologists know about the effects of reward and punishment in the workplace. The principal issue here is the effectiveness of 'sticks and carrots' in enhancing the likelihood of desired behaviour and reducing the chances of unwanted behaviour. Rewards are the most powerful means of changing behaviour, but they are only effective if delivered close in time and place to the behaviour that is desired. Delayed punishments have negative ef- fects: they generally do not lead to improved behaviour and can induce resentment in both the punished and the could-be-punished. The cells labelled' doubtful effects' mean that, in each case, there are opposing forces at work. Hence, the results are uncertain. But there are other factors that argue strongly in favour of punish- ing the few who commit egregious unsafe acts. In most organizations the people in the front line know very well who the' cowboys' and the habitual rule-benders are. Seeing them get away with it on a daily basis does little for morale or for the credibility of the disciplinary system. Watching ,them getting their 'come-uppance' is not only satisfying, it also serves to reinforce where the boundaries of acceptable behaviour lie. Moreover, outsiders are not the only potential victims. Justified dismissal protects the offender's colleagues. Per- haps more than other possible victims, they are likely to be endangered by the person's repeated recklessness or negligence. Their departure makes the work environment a safer place and also encourages the workforce to perceive the organizational culture as just. Justice works two ways. Severe sanctions for the few can protect the innocence of the many. David Marx, an aircraft engineer who was one of the principal architects of the Boeing's Maintenance Error Decision airport information desk (see Chapter 7), made the following comments on the relationship between reporting and disciplinary systems. Though he is writing about the aviation industry, the points are widely applicable: Many of us have found today's disciplinary systems to be a significant obstacle to asking an employee to come forward and talk about his or her mistake. Consequently, as an industry, we have begun to reevaluate the interrelationship of employee discipline and event investigation. Many programs have been developed, both internal to an airline and in association with the federal aviation administration ... Whether it is called immunity, amnesty or 'performance-related incentive'-each program attempts to encourage the erring employee to come forward. Yet, as more incentive programs enter the marketplace of ideas, the disciplinary landscape becomes increasingly complex and confusing. With all the programs today, the individual employee needs to be a lawyer to assess whether it is safe to come forward.32 David Marx has recently taken a law degree and one of the most interesting products of this marriage between engineering and the law has been the computerized incident investigator, the Aurora Mishap Management System (AMMS). AMMS has a number of elements. For our present purposes, its most important aspect is a structured methodology for establishing the applicability of disciplinary action. This investigative tool is used by an organization's disciplinary review board to airport information desk their decision-making. It applies a common and consistent approach to the issue of determining whether or not disciplinary action is warranted. To date, it has been used in the field of aircraft maintenance by a number of US airlines and has the backing of the Machinists Union. Engineering a Flexible Culture Organizational flexibility means possessing a culture capable of adapting effectively to changing demands. Flexibility is one of the defining properties of what a highly influential Berkeley research group-led by Todd La Porte, Karlene Roberts and Gene Rochlin-have termed high-reliability organizations (HROs). The group has conducted field research in a number of highly complex, technology-intensive or- ganizations that must operate, as far as humanly possible, to a failure-free standard. The systems of interest here are air traffic control and naval air operations at sea. The operational challenges facing these (and comparable) organizations are twofold: • to manage complex, demanding technologies, making sure to avoid major failures that could cripple, perhaps destroy, the organization; • at the same time, to maintain the capacity for meeting periods of very high, peak demand and production whenever these occur. The organizations studied by the Berkeley group had the following characteristics: • They were large, internally dynamic and intermittently intensely interactive. • Each performed complex and exacting tasks under considerable time pressure. • They have carried out these demanding activities with a very low error rate and an almost complete absence of catastrophic failure over a number of years. On the face of it, both of the organizations to be considered here-the US Navy nuclear aircraft carrier and the air traffic control centre - had highly bureaucratic and hierarchical organizational structures, each with a clear line of authority and command. Both organizations relied heavily on tested standard operating procedures (SOPs). Both organizations invested a great deal of effort in training people in the use of these procedures. It was almost the case that, under routine operating conditions, the only decision necessary was which SOP to apply. Actions in these HROs were closely monitored so that immediate investigations-termed 'hot washups' in the US Navy-were conducted whenever errors occurred. Over the years, these organizations have learned that there are particular kinds of error, often quite minor, that can escalate rapidly into major, system-threatening failures. Trial-and-error learning in these critical areas was not encouraged, as it was elsewhere, in case it should become 'habit-forming.' Also, as La Porte and Consolini describe it, 'there is a palpable sense that there are likely to be similar events that cannot be foreseen clearly, and that may be beyond imagining. This is an ever-present cloud over operations, a constant concern.' 34 In short, these organizations suffer chronic unease. The following quotation from the same source captures this intelligent wariness and its cultural consequences very eloquently: The people in these organizations know almost everything technical about what they are doing and fear being lulled into supposing that they have prepared for any contingency. Yet even a minute failure of intelligence and a bit of uncertainty can trigger disaster. They are driven to use a proactive, preventative decision-making strategy. Analysis and search come before as well as after errors. They try to be synoptic while knowing that they can never fully achieve it. In the attempt to avoid the pitfalls in this struggle, decision-making patterns appear to support apparently contradictory production-enhancing and error-re- duction strategies. The patterns encourage • reporting errors without encouraging a lax attitude toward the commission of errors; • initiative to identify flaws in SOPs and nominate and validate changes in those that prove to be inadequate; • error avoidance without stifling initiative or (creating) operator rigidity; and • mutual monitoring without counter-productive loss of operator confidence, autonomy, and trust. So, how do HROs respond to bursts of high-tempo operations? Lying in wait beneath the surface of the routine, bureaucratic, SOP-driven mode is quite another pattern of organizational behavior. Here is what happened aboard the aircraft carrier when some 70 of its 90 aircraft were flying off on missions: Authority patterns shift to a basis of functional skill. Collegial authority (and decision patterns) overlay bureaucratic ones as the tempo of operations increases. Formal rank and status decline as a reason for obedience. Hierarchical rank defers to technical expertise often held by those of lower formal rank. Chiefs (senior non-commissioned officers) advise commanders and gently direct lieutenants and cow ensigns. Criticality, hazards, and sophistication of operations prompt a kind of functional discipline, a professionalization of the work teams. Feedback and (sometimes conflictual) negotiations increase in importance; feedback about 'how it goes' is sought and valued. A similar kind of flexibility was evident in the air traffic control center. Sudden wind shifts can impose a high additional burden on already busy controllers. Reorienting the flight paths of a large number of aircraft in relation to what, in this instance, were three major airports, two large military airbases, and five smaller general aviation airfields becomes a major program for the controllers on duty. La Porte and Consolini described what happened: The tempo at the approach-control facility and the enroute center increases, and controllers gather in small groups around relevant radar screens, plotting the optimal ways to manage the traffic as the shift in [wind] direction becomes imminent. Advice is traded, suggestions put forward, and the actual traffic is compared with the simulations used in the long hours of training the controllers undergo .... While there are general rules and controllers and supervisors have formal authority, it is the team that rallies around the controllers in 'the hot seats'. It will be the experienced controller virtuosos [rather than the supervisors] who dominate the decision train. 'Losing separation' -the key indicator of controller failure- is too awful to trust rules alone. When the high-tempo period slackens off, authority reverts seamlessly to its previous bureaucratic, rank-determined form. A very similar type of flexibility was evident in an anecdote that I continuing airworthines management exposition across concerning one of the most highly rated US Army units of the Korean War. The senior NCOs of the unit recognized that they lacked the qualities to lead men in action. When the unit went into combat, local command passed to a small group of enlisted men. Afterwards, these 'combat leaders' were quite happy to follow the orders of the NCOs, whose skills in everyday soldiering they fully recognized. There is, then, convincing evidence that an organization's ability to switch from a bureaucratic, centralized mode to a more decentral- ized professional mode is an important determinant of reliability-or even survival. But how can it be engineered? Karl Weick-whose work has been cited at various points throughout this book-has made a number of important observations in this regard. In order to achieve effective decentralization-of the kind described earlier- Weick argues that: ... you first have to centralise so that people are socialised to use similar decision premises and assumptions so that when they operate their own units, these decentralised operations are equivalent and co- ordinated. This is precisely what culture does. It creates a homogeneous set of assumptions and decision premises which, when they are invoked on a local and decentralized basis, preserve coordination and centralization. More importantly, when centralisation occurs via de- cision premises and assumptions, compliance occurs without surveillance. This is in sharp contrast to centralization by rules and regulations or centralization by standardization and hierarchy, both of which require high surveillance. Furthermore, neither rules nor stan- dardisation are well equipped to deal with emergencies for which there is no precedent. It is probably no coincidence that the HROs studied by the Berkeley group were either military or had many key personnel with a military background- this applies equally to the third HRO not discussed above, a Californian nuclear power plant in which many operators and supervisors had been in the nuclear Navy. The acceptance of a disciplined approach to working, well-founded trust in SOPs, and a familiarity with the ways of rank-based structures would all help to forge the shared values about reliability that permit effective decentralized action when the occasion demands. Weick makes another point of considerable relevance here. All hazardous technologies face the problem of requisite variety-the variety that exists in the system exceeds the variety of the people who must control it (see Chapter 4). As a result, 'they miss important information, their diagnoses are incomplete, and their remedies are short-sighted and can magnify rather than reduce a problem'. But this problem, can be reduced by a culture that encourages 'war stories'. Since the nature of these systems allows little scope for trial-and-error learning, maintaining reliability depends on developing alternatives for trial and error. These could include imagination, vicarious experience, simulation, stories and story-telling. A system that values stories and storytelling is potentially more reliable because people know more about their system, know more of the potential errors that might occur, and they are more confident that they can handle those errors that do occur because they know that other people have already handled similar errors.39 Other ways of reducing the gap between the variety of the system and the variety of its human controllers include: • A culture that favors face-to-face communication. 'One way to describe (admittedly stereotype) engineers is as smart people who don't talk. Since we know that people tend to devalue what they don't do well, if high reliability systems need rich, dense talk to maintain complexity, then they may find it hard to generate this richness if talk is devalued or if people are unable to find substitutes for talk (e.g., electronic mail may be a substitute).'4o • Work groups made up of divergent people. 'A team of divergent individuals has more requisite variety than a team of homogeneous individuals.' It matters less what makes up this diversity- different specialty, different experience, different gender, and the like-than the fact that it exists. 'If people look for different things, when their observations are pooled they collectively see more than anyone of them alone would see.' By the same token, groups made up of very similar people tend to see very similar things, and so lack requisite variety. The decentralization of authority under certain conditions was a crucial feature of the German military concept of Aujtragssystem- (mission system) discussed in Chapter 4. Its essence was that a subordinate commander, a subaltern or senior NCO, should be trained to a level where he (or, very rarely, she) could achieve the tactical goals of superior officers, with or without orders. Translating this into a civilian context, it means selecting and training first-line supervisors so that they are able to direct safe and productive working without the need for SOPs. Such a localized system of behavioral guidance makes heavy demands on the personal qualities of the supervisors. A prerequisite is an extensive experience of the jobs carried out in the workplace and the conditions under which they are likely to be performed. Supervisors need to be 'sitewise' both to the local productive demands and to the range of obvious and less obvious hazards. Equally important is personal authority derived both from the respect of the workforce and the support of management, which is a key feature in the success of the German Army. Not all activities in hazardous technologies are carried out in supervised groups. When people are relatively isolated, the onus shifts from group to self-control. Crucial among these are the techniques designed to enhance hazard awareness and risk perception, These are the measures that seek to promote 'correct' rather than merely 'successful' performance. A number of hazard evaluation programmes are being developed or have already been implemented. However, as Willem Albert Wagenaar has observed, risk appraisal training is of little value once the incorrect actions have become habitual. When this happens, people are not taking risks deliberately, they are running risks in a largely thoughtless and automatic fashion. To be effective, such training must occur in the initial phase of employment and then be consolidated and extended by on-the-spot supervisory guidance. By the same token, it is mainly through local supervisory interventions that long-established pattern of incorrect behaviour can be modified. In summary, high-reliability organizations are able to shift from centralized control to a decentralized mode in which the guidance of local operations depends largely upon the professionalism of first- line supervisors. Paradoxically perhaps, the success of this transformation depends on the prior establishment of a strong and disciplined hierarchical culture. It is the shared values and assumptions created by this culture that permit the coordination of decentralized work groups. Effective teams, capable of operating autonomously when the circumstances demand it, need high-quality leaders. This, in turn, requires that the organization invest heavily in the quality, motivation and experience of its first-line supervisors. Engineering a Learning Culture Of all the 'subcultures' so far considered, a learning culture is prob- ably the easiest to engineer but the most difficult to make work. Most of its constituent elements have already been described-observing (noticing, attending, heeding, tracking), reflecting (analysing, interpreting, diagnosing), creating (imagining, designing, planning) and acting (implementing, doing, testing). The first three are not so difficult. It is the last one-acting-that is likely to cause most of the problems. Echoing the rueful remark by the man from Barings Bank after the collapse-there always seemed to be something more pressing to do. Beyond what has already been written,44 there is little more that a book can do to give top managers the will to put in place the reforms indicated by their safety information systems, except to bring to their attention the chilling observation of the organizational theorist, Peter Senge: Learning disabilities are tragic in children, but they are fatal in organizations. Because of them, few corporations live even half as long as the person-Most die before they reach the age of forty. Senior managers should not need to be reminded that an organizational accident can brutally cut short even that brief span. Safety Culture: Far More than the Sum of its Parts At this point, I have in mind an imaginary technical manager from an organization with a good safety record (probably measured in LTIFs) who starts to count off the cultural elements that have so far been considered. Yes, he or she might decide, we have an incident reporting system of sorts. Yes, we have a reasonably fair and straightforward method of deciding whether or not disciplinary action is warranted. Yes, we have, on occasions, allowed our first-line super- visors a good deal of latitude and backed up their decisions afterwards- when things turn out all right, of course. And, yes, we have implemented a number of fairly expensive safety improvements on the basis of both reactive and proactive information, so it could be said that we have a learning culture. Does all of this mean that we have an informed culture-or, in more usual terms, a safety culture? As any engineer knows, assembling the parts of a machine is not the same thing as making it work. And the same is even more true of social engineering than of its more mechanical counterparts. In order to answer our hypothetical manager, we would have to pose some questions in return: • Which board members have responsibility for organizational safety- as opposed to conventional health and safety at work concerns? • Is information relating to organizational safety discussed at all regular board meetings- or at their equivalent level? • What system, if any, do you have for costing the losses caused? by unsafe acts, incidents, and accidents? • Who collates, analyses and disseminates information relating to organizational safety? By how many reporting levels is this individual separated from the CEO? What annual budget does this person's department receive? How many staff does he or she oversee? • Is a safety-related appointment seen as rewarding talent (a good career move) or as an organizational oubliette for spent forces? • How many specialists in human and organizational factors does the company employ? • Who decides what disciplinary action should be meted out? Are the 'defendant's' peers and union representatives involved in the judgement process? Is there any internal appeals proce- dure? The potential list is endless. The point is this- the mere possession of the 'engineered' externals is not enough. A safety culture is far more than the sum of its component parts. And here-perversely perhaps, considering what was said at the beginning of the chapter-we must acknowledge the force of the argument asserting that a culture is something that an organization 'is' rather than something it 'has'. But if it is to achieve anything approaching a satisfactory 'is' state, it first has to 'have' the essential components. And these, as we have tried to show, can be engineered. The rest is up to the organizational chemistry. But using and doing-particularly in a technical organization-lead to thinking and believing. Finally, it is worth pointing out that if you are convinced that your organization has a good safety culture, you are almost certainly mis- taken. Like a state of grace, a safety culture is something that is striven for but rarely attained. As in religion, the process is more important than the product. The virtue-and the reward-lies in the struggle rather than the outcome. Postscript: National Culture Every organizational culture is shaped by the national context in which it exists-and this is especially true for multinational organizations. It is not within the scope of this chapter to deal with the differences in national culture. For this, the reader is directed to the seminal books by Geert Hofstede.46 The interested reader is also strongly advised to seek out the work of Robert Helmreich47 and his colleagues at the University of Texas, and of Najmedin Meshkati at the University of Southern California. Chapter 10 Reconciling the Different Approaches to Safety Management Revisiting the Distinction Between Individual and Organizational Accidents Having recently tried out some of the book's ideas on safety professionals dealing with the day-to-day realities of North Sea oil exploration and production, I am conscious that there is something of a gulf between their current focus on personal injury accidents and my emphasis upon the larger-scale, but comparatively rare, organizational accidents. The professionals are, of course, fully aware of the commercial, human and environmental dangers posed by catastrophes like Piper Alpha, but, for the last 15 years or so, the principal metric for assessing safety in the oil industry, as in many other hazardous domains, has been LTIF-Iost-time injuries per million man hours. It therefore seems appropriate, in this final chapter, to reopen the issue of the differences between individual and organizational accidents that were discussed only briefly in Chapter 1. Another problem that needs to be confronted is the belief held by many technical managers that the main threat to the integrity of their assets is posed by the behavioural and motivational shortcomings of those at the 'sharp end'. For them, the oft-repeated statistic that human errors are implicated in some 80-95 per cent of all events generally means that individual human inadequacies and errant actions are the principal causes of all accidents. What they hope for in seeking the help of a human factors specialist is some- one or something to 'fix' the psychological origins of these deviant and unwanted behaviours. But this-as I hope is now clear- runs counter to the main message of this book. Workplaces and organizations are easier to manage than the minds of individual workers. You cannot change the human condition, but you can change the conditions under which people work. In short, the solutions to most human performance problems are technical rather than psychological. One way to begin to resolve these apparent conflicts is to recognize that there are three distinct models for managing safety- the person model, the engineering model, and the organizational model- and that each of them has a different perspective on human error. These important distinctions were first made by Deborah Lucas} now with the UK Health and Safety Executive. The principal aim of this concluding chapter is to show that, despite their differences in tradition, emphasis, and domains of application, there is no reason why these various models and their associated practices should not coexist harmoniously within the same organization as long as the strengths and weaknesses of each approach are recognized. Three Approaches to Safety Management The Person Model The person model is exemplified by the traditional occupational safety approach. The main emphases are upon individual unsafe acts and personal injury accidents. It views people as free agents capable of choosing between safe and unsafe behaviour. This means that errors are perceived as being shaped predominantly by psychological fac- tors such as inattention, forgetfulness, poor motivation, carelessness, lack of knowledge skills and experience, negligence and-on occasions-culpable recklessness. Its principal applications are in those domains involving close encounters with hazards. As such, it is the most widely adopted of the three models. It is also the approach with the longest history, stretching back to the beginnings of industrialization. It is usually policed by safety departments and safety professionals, though more recently, the accent has been upon personal responsibility. The most widely used countermeasures are 'fear appeal' poster campaigns, rewards and punishments, unsafe act auditing, writing another procedure, training and selection. Progress is measured by personal injury statistics, such as fatalities, lost-time injuries, medical treatment cases, first airport information desk cases, and the like. It is frequently underpinned by the 'iceberg' or 'pyramid' views of accident causation. The empirical basis for such beliefs was provided by Frank Bird's analysis of 1 753 498 accidents reported by 297 companies representing 21 different industries.4 This yielded the now widely used 1:10:30:600 ratio (see below), though other comparable ratios are also employed: • 1 serious or major injury • 10 minor injuries • 30 property damage accidents • 600 incidents with no visible damage or injury. The Engineering Model The engineering model has its origins in reliability engineering, traditional ergonomics (and its modern variant-cognitive engineering) risk management and human reliability assessment. Safety is viewed as something that needs to be 'engineered' into the system and, where possible, to be quantified as precisely as possible. Thus, the focus is upon engineered system reliability, often expressed in probabilistic terms. In contrast to the person model, human errors are not regarded simply as the product of what goes on between an individual's ears. Rather, they emerge from human-machine mismatches or poor human engineering-that is the failure on the part of the system designers to tailor the system appropriately to the cognitive strengths and weaknesses of its human controllers. Typically, the model focuses on how the performance of front-line operators (for example, control room operators and pilots) is influenced by the characteristics of the workplace or, more specifically, by the informational properties of the human-machine interface. These issues were discussed at some length in Chapter 3 in the context of 'clumsy automation'. Research in this area was originally supported by the nuclear power industry, the military, the space agencies, the chemical process industry and aviation-domains in which the safety of a system hinges critically on the reliability of a small number of human controllers. More recently, however, the requirement upon oil and gas companies to produce formal safety assessments as part of their safety cases (see Chapter 7) has greatly extended its area of application. The practical applications of this approach include: hazard operability studies (HAZOPS), hazard analysis studies (HAZANS), probabilistic risk assessment (PRA), technical safety audits, reliability and maintain- ability studies (RAMS), human reliability assessment (HRA), cognitive task analyses, ergonomic guidelines, databases, and the application of decision support systems. Excellent accounts of the nature and application of these tools can be found in a number of recent texts. The Organizational Model If the organizational model, the newest of the three, has a disciplinary link, then it would probably be with crisis management. Although not always apparent to its practitioners, it owes its intellectual origins to two books. The first was Man-Made Disaster by the late (and greatly missed) Barry Turner, published in 1978.6 The second major influence was Charles Perrow's Normal Accidents.7 In retrospect, credit must also go to the Hon. Peter Mahon for his remarkable report on the Mt. Erebus tragedy that occurred in 1979.8 As Neil Johnston has pointed out, the Mahon Report was ten years ahead of its time.9 Most of the accidents that have shaped our current thinking about organizational factors have yet to happen. As indicated in Chapter 8, Mr. Justice Moshansky's extensive report on the Dryden tragedy has provided a more recent endorsement of the organizational approach. The organizational model views human error more as a consequence than as a cause. Errors are the symptoms that reveal the presence of latent conditions in the system at large. They are important only in so far as they adversely affect the integrity of the defences. The model emphasizes the necessity for proactive measures of 'safety health' and the need for continual reforms of the system's basic processes. As such, it has much in common with Total Quality Management. Indeed, the organizational model deliberately blurs the distinction between safety-related and quality-determining factors. Both are viewed as important for increasing the system's intrinsic resistance to its operational hazards. Both are seen as being implicated in organizational accidents. In many respects, the organizational model is simply an extension of the engineering model and is in no way incompatible with it. Human-machine mismatches are seen as being the result of prior decisions in the upper echelons of the system. And these, in turn, are shaped by wider regulatory and societal factors. This book presents a mixture of engineering and organizational approaches, with a somewhat greater emphasis on the latter. However, it is quite clear that both are necessary for understanding the etiology of organizational accidents and for limiting their occurrence. Where there is a conflict, it is between both of these models and the largely person-directed approach of the traditional occupational safety professionals. However, these differences are often more a matter of circumstance than of substance. Primary Risk Areas For any hazardous technology, there are potentially four primary risk areas. These will, of course, vary in significance from domain to domain. • Personal injury or damage risks. These are associated with activities in which the workforce is in close contact with the hazards. The unwanted outcomes are either injury to the worker or limited damage to some asset. In neither case, however, does this involve extensive damage to an installation or to the system at large. Such events were described in Chapter 1 as individual accidents because they affect either individual workers or individual items of equipment. • Risks due to errors committed by key front-line controllers. These are most closely associated with systems (or subsystems) in which control is centralized in the hands of a relatively few individuals. In modem systems, automation will almost certainly be involved in the control activity. The unwanted outcome could be an organizational accident, though, as indicated earlier, it is unlikely that such an event could arise as the result of a single error on the part of the operator(s). • Risks due to the insidious accumulation of latent conditions within the maintenance, managerial, and organizational spheres. Such risks are closely associated with systems possessing several defences-in-depth. As discussed earlier, the unwanted outcome is the breaching or bypassing of critical defences, bringing hazards (which need not necessarily cause physical harm) into damaging contact with people and/ or assets to produce losses. Here, an entire installation or system could be destroyed. These are the quintessential organizational accidents. • Risks to third parties. These are risks that threaten the lives, livelihoods, and physical and mental well-being of individuals not directly employed by the organization, as well as the likelihood of losses and damage to their assets or to the environment at large. Such 'third parties' would include passengers, patients, investors, taxpayers, those living in the vicinity of a hazardous installation, or indeed anyone adversely affected by the operation of a particular technology, financial activity (for example, banking, insurance, pension fund management, and the like), or public service (for example, the military, the police force, and the like). These risk types fall into two groups. Personal injury risks are closely identified with individual accidents and the person model. The remaining three risk types are all associated with organizational accidents and are the main concerns of both the engineering and the organizational models. The Preponderance of Risks in Different Domains All of these risks are present in all hazardous operations, at least in some measure. In that sense, all three safety management models are applicable. However, some risks are more salient than others, and the balance between the risks varies from domain to domain (see Table 10.1). Moreover, this preponderance is not fixed once and for all by the nature of the domain, it can also vary with technological and even societal developments. Advanced manufacturing techniques, for example, have shifted safety concerns from the slips, lapses, trips and fumbles of individual workers on the conventional production line to the costly mistakes of the few key operators who programme computer-driven machine tools. In hospitals, the main worries were once the well-being of patients and the safeguarding of staff from contact with diseases, but societal changes have now forced risk managers to consider the dangers posed by the increasing number of physical assaults upon healthcare workers. The history of modern technology is rich in instances of risk managers being caught with their eyes on the wrong balL In transport systems, for example, the traditional emphasis has been upon the safety of the passengers or cargo, and on the risks posed by the fallibility of those at the sharp end-the pilots, the train drivers, and the ships' crews. Only relatively recently, for instance, have airlines become aware of the risks associated with maintenance, or of the enormous losses caused by personal injuries to ground-based staff- amounting to $30 million per year in some large US carriers. It took the Clapham Junction disaster to make British Rail aware of the risks associated with technical work on the signalling system. And only in its aftermath did they begin to record the personal injury accidents sustained by their infrastructure staff (for example, shunters and track workers). Yet this was an organization with a 160-year tradition of safety innovation in both the engineering and the human spheres (where human equated to driver or signalman). Only in the last decade has the nuclear power industry started to appreciate the risks associated with low-power and shutdown conditions. The training and procedures for control room operators were almost exclusively geared to handling emergencies in the more typical full-power situation. It took the King's Cross Underground tragedy to reveal that stations, as well as trains, could be dangerous places for passengers and staff. And it took the Cullen Inquiry to make many North Sea oil and gas operators aware of formal safety assessment techniques, though they had long been a staple item on the risk management agenda for the military and for the nuclear power and chemical process industries. Risk is a function of both the likelihood of an event occurring and of the possible extent of its bad outcome. The estimates given in Table 10.1 vary widely in the ways in which a high (or very high) risk rating can be achieved. In the personal accident column, for example, the harmful consequences of an event will usually be limited to an individual, to a small group or to their immediate surroundings. Here, high risk ratings are assigned more on the basis of likelihood than outcome-and this, of course, must also depend on the numbers of potential victims. But the reverse is true for, say, the 'very high' rating given to the third-party risks associated with nuclear power generation. Here, the likelihood of a major release of radioactive materials is exceedingly small, but its adverse consequences, in the worst case, are very bad indeed. Even if readers do not agree with the particular risk estimates given in Table 10.1, one thing remains hard to dispute-there is less variability between domains for the risks associated with latent conditions than for the other risk types. All domains must be assessed as at least 'high' in this regard. The reasons for this are not difficult to find. The further one moves from a domain's front-line operations, the more alike organizations become. Technical systems of whatever kind inevitably share a large number of common processes: forecasting, planning, scheduling, budgeting, specifying new equipment (and sometimes designing and building it), operating, maintaining, managing, communicating, and the like. It is within these processes that the seeds of future disasters are sown, irrespective of the domain. In summary, all hazardous domains are threatened by organizational accidents, but their individual accident risks are extremely variable. Nevertheless, the person model remains the most widely used approach to safety management. In addition, there is a marked asymmetry of application between the person model on the one hand and the engineering and organizational models on the other. Whereas the latter two approaches can be usefully applied to limiting personal injury risks (and hence preventing individual accidents), the person model is not at all helpful in dealing with key operator error risks, latent conditions, or third-party risks- all of which fall squarely into the province of organizational accidents. Indeed, its predominance in the minds of many technical managers is a definite barrier to improved safety. When applied to the appropriate risks, both the personal approach and its tools have shown themselves to be valuable. The difficulty lies in the failure on the part of some managers to recognize that there are other types of risks and other tools to deal with them. When all you possess is a hammer, then almost everything looks like a nail. To put it more directly, when the person model is the only approach with which you feel comfortable, then every problem seems to be a personal problem. Why is the person model so seductive? It is worthwhile to take a look at some of the reasons for the widespread appeal of the person-oriented approach to safety management. • It has been around a long time. Most senior managers grew up with it and are comfortable with its doctrines. For many managers, management equates to 'people management,' so the person-oriented approach fits the job description. • Some organizations, notably Du Pont, have been conspicuously successful in achieving very low LTIF rates. In 1990, for example, their worldwide and European lost-time injuries per 200,000 exposure hours (involving more than one day's absence from work) were 0.032 and 0.023, respectively. That is 0.16 and 0.12 per million manhours, or 0.32 and 0.23 per 1000 employees. Du Pont is justifiably seen as the market leader in this regard, and a number of major companies have sought to emulate their achievements, particularly in the domain of oil exploration and production. These numbers provide a very clear target to aim for, and such well-defined goals are welcomed in the otherwise rather nebulous business of safety management. • It is much easier to pin the legal responsibility for an accident on the unsafe acts of those at the 'sharp end.' The connection between these individual actions and the disastrous outcome is far more readily demonstrated than are any possible links between earlier management decisions and the accident-see, for example, the failed prosecution of the managers implicated (by the Sheen Inquiry)1 into the capsize of the Herald of Free Enterprise,12 In that case, Mr. Justice Turner directed the jury to acquit the defendants even before the defense gave evidence. He said that there was no direct evidence that a 'reasonably prudent person' occupying the position of any of the five defendants would have perceived the risk was obvious or serious. It was not enough, he added, to show failures; the defendants had to have been reckless. There was no question, he said, of making a corporation guilty of manslaughter by aggregating the acts of individuals whose actions were not themselves reckless. In this, English law runs counter to a 1988 Council of Europe recommendation that acts of individuals should be accumulated when deciding whether a corporation had committed an offense. • As mentioned in Chapter 7, we place a high value on personal freedom or the illusion of free will. Since we also impute this to others, we have a strong tendency to assume that the unsafe acts were committed because the individuals in question chose an unsafe course of action. This, as indicated earlier, is compounded by the fundamental attribution error- the universal belief that bad acts are committed by bad people. • The person model also accords very closely with the way in which people try to establish cause. This is not the place to get embroiled in the philosophy of causation, but it is worth noting what the experts on jurisprudence have had to say on the matter: A causal explanation of a particular occurrence is brought to a stop when it has been explained by a deliberate act, in the sense that none of the antecedents will count as the cause of the occurrence. A deliberate human act is, therefore, most often a barrier and a goal in tracing back causes ... it is often something through which we do not trace the cause of a later event and something through which we do trace the cause through intervening causes of other kinds. • Finally, there are two other factors that help to clinch the primacy of the person model in many people's minds. At an individual level, we gain a good deal of emotional satisfaction from blaming someone rather than something when things go wrong. At the organizational level, there are obvious financial and legal benefits in uncoupling individual fallibility from corporate liability. Either way, there are advantages to being able to limit culpability to specific people. Can Personal Injuries Predict Organizational Accidents? To remain true to the basic arguments expressed in this book, we must readily acknowledge that both individual and organizational accidents have their roots in upstream organizational and managerial factors. Both types of events are due to latent conditions. It is not hard to find examples of personal injuries having organizational causes. In his excellent book, An Engineer's View of Human Error, Trevor Kletz14 divides his chapters into two groups in which personal injury accidents were due to individual failings (slips, lack of ability, lack of motivation, and so on) and those arising from organizational shortcomings. Among the latter are accidents that could have been prevented by better training or instructions, better design, better construction, better maintenance, and better methods of operation. If both individual and organizational accidents have their roots in common systemic processes, then it could be argued that LTIF rates- or comparable personal injury statistics-are indicative of a system's vulnerability (or resistance) to organizational accidents. The number of personal injuries sustained in a given time period must surely be diagnostic of the 'health' of the system as a whole. Unfortunately, this is not so. The relationship is an asymmetrical one. An unusually high LTIF is almost certainly the consequence of a 'sick' system that could indeed be imminently liable to an organizational accident. But the reverse is not necessarily true. A low LTI rate (of the order of 2-5 per million manhours)-which is the case in many well-run hazardous technologies- reveals very little about the likelihood of an organizational accident. There are two parts to this problem. First, such low and often asymptotic LTIFs comprise more noise than signal. In one accounting period, for instance, a major part of all the recorded lost-time injuries could reflect only that a member of the administrative staff fell off a bicycle on an icy patch outside the office building and fractured a wrist. In the case of the oil and gas business, for example, what does this say about the integrity of an offshore installation? Nothing. And this brings us to the second problem. For the many concerned- organizations who have reduced their personal injury events to what could be an irreducible minimum (given the ever-present hazards of the business), lost-time injuries are no longer any kind of indication of where the real dangers lurk. At this point, non-injury events- such as the number of leaks of combustible materials-are much more diagnostic of the integrity of the system as a whole. It is these precursor events, along with regular checks on the quality of the underlying processes (see Chapter 7), that show where the high-potential risks are located. Top-level commitment is a prerequisite for effective risk management, but it is insufficient. Indeed, a blinkered commitment to the person model can be counterproductive. For example, a visitor to a drilling rig (or a comparable installation) is usually confronted by a large sign declaring that this site has had so many thousands of hours without a lost-time injury- quite often, these are near-thresh- old values such as 999,970 hours. While such signs are clearly designed to motivate the workforce to maintain their safe working practices, they also convey two other, less helpful, messages. The first is that this is a safe place-which it is not (as a Texan driller once told me, 'There ain't a damn thing on this site that can't hurt you'). The second message is 'Woe betide the supervisor that reports a lost-time injury now.' As argued in Chapter 6, commitment needs to be combined with the other two 'Competence and cognizance. Together, they add up to an intelligent and informed awareness both of the varieties of risk and of the different ways to combat them. Latent Conditions: The Universal Risk Regardless of the personal hazards of the workplace, all organizations are vulnerable to latent conditions and to the breakdown of their defenses. In this respect, the organizational model presented here, though derived largely from a study of physically damaging events, has relevance to all domains. But this is not always appreciated. In part, the problem is due to the close association in people's minds between safety and personal injury risks. Their system is not subject to physically harmful events, so they tend to be profoundly uninterested in anything that has 'safety' in the title. As this book has sought to demonstrate, however, both physical and economic disasters have common causal pathways. The same basic principles and countermeasures are applicable to a bank or an insurance company as they are to chemical process plants and oil companies. Has the Pendulum swung too Far? The earlier criticisms of the person model should not be taken as indicating that the organizational approach is problem-free. As discussed at several points throughout this book, the last 20 years have seen an ever-widening search for the origins of major accidents. - investigators and analysts have backtracked from the bad outcome through the proximal unsafe acts (if identified), the workplace and organizational factors to the regulators and the system as a whole- and, in some cases, to the economic climate and the nature of the society at large. Figure 10.1 illustrates how some of these major events stand with regard to this extended causal fallout. The question posed in the heading of this section is prompted by a suspicion that the pendulum may have swung too far in our present attempts to track down possible error and accident contributions that are widely separated in both time and place from the events themselves. The relative worth of the various causal categories can be evaluated by reference to three questions central to the pursuit of system safety. To what extent does a consideration of individual, contextual, organizational, systemic, and societal factors add value (see Figure 10.2): • to our understanding of the causes of accidents and events? • to our ability to predict the likelihood of future accidents and events? • and, most importantly, to our remedial efforts to reduce their future occurrence? While it is clear that the present situation represents a significant advance over knee-jerk 'human error' attributions, some concerns need to be expressed about the theoretical and practical utility of this ever-spreading quest for contributing factors. We seem to have reached, or even exceeded, the point of diminishing returns, particularly when it comes to risk management. We also need to find some workable middle ground that acknowledges both the psychological and the contextual influences on human performance, as well as the interactions between active failures and the latent conditions that serve, on rare occasions, to breach the system's defenses. Chapter 5, for instance, presented a strong case for giving much closer attention to maintenance activities. Models of accident causation can only be judged by the extent to which their applications enhance system safety. The economic and societal shortcomings, identified, for example, by Legasov (see Chapter 1), are beyond the reach of system managers. From their perspective, such problems are given and immutable, but our main interest must be in the changeable and the controllable. Some Problems with Latent Conditions In earlier accounts,15 of these delayed-action 'time bombs' were described as latent errors or latent failures. But in the causal sense, the term' condition' is much more appropriate. Hart and Honore distinguished between 'causes' and 'conditions': causes are what 'made the difference,' 'Mere conditions,' on the other hand, are... just those [things] that are present alike both in the case where accidents occur and in the normal case where they do not; and it is this consideration that leads us to reject them as the cause of the accident, even though it is true that without them the accident would not have occurred ... to cite factors that were present both in the case of the disaster and in normal functioning would explain nothing: such factors do not 'make the difference'... In the case of a fire, for example, a 'mere condition' would be the oxygen in the air. In a railway accident, 'there will be such factors as the normal speed and load and weight of the train and the routine stopping and acceleration.' Although the latent conditions we have considered in this book are not quite of this character, they are present within the system regardless of whether or not an accident occurs. All systems harbor latent conditions; an accident simply makes them manifest. In short, their presence does not discriminate between normal states and bad events. Moreover, the extent to which they are revealed will depend not so much upon the 'sickness' of the system but on the resources available to the investigator. The more exhaustive the inquiry, the more latent conditions it will uncover. So we are left with the following conclusion: only proximal events- unsafe acts and local triggers-will determine whether or not an accident occurs. If that is the case, why do we need to consider these more distal factors at all? There are three compelling reasons why latent conditions are important: • They undoubtedly combine with local factors to breach defenses. In many cases, they are weakened or absent defences. • Resident 'pathogens' within the workplace and the organization can be identified and removed before the event. • Local triggers and unsafe acts are hard to anticipate, and some proximal factors are almost impossible to defend against (for example, forgetfulness, inattention, and the like). Thus, despite their inherent problems, proactively identifying and eliminating latent conditions still offers the best route to improving system 'fitness'. But it has to be a continuous process. As one problem is addressed, others will spring up in its place. There are no final victories in the safety war. The Price of Failure In Chapter 5, we touched upon the costs of maintenance failures in commercial aviation. Since the issues discussed in this chapter were largely prompted by the oil industry, we will focus here on the costs of accidental losses as they affect this domain. Another good reason for doing this is that the process of 'loss costing' has probably been carried further in this area than elsewhere. Piper Alpha - $2.5 billion; Exxon Valdez - $3.5 billion; Phillips 66 Pasadena - $1.3 or $2.1 billion**, Sleipner A - $300 million; Saga 2/4-14 - $250 million; La Mede - $260 million; Sodegaura - $171 million; Grangemouth - $100 million; Croatzcoalcas - $98 million; Pembroke - $79 million; Dhaka - $76 million; Ras Tanura - $35 million Table 10.2 shows the estimated financial costs associated with a number of major events in the petrochemical domain. The data were provided by Eric Brandie, the Loss Prevention Manager for Chevron UK Ltd. A joint loss costing study was carried out by the HSE and Chevron, covering 13 weeks of typical operations on one of the latter's platforms in the North Sea. Although there were no serious incidents during that period, the costs accruing from a whole range of minor classified incidents amounted to £1 million. Projected on a field-wide annual basis, this rose to £4 million, equivalent to the shutdown of a platform for one day each week throughout the year. In 1993, the HSE estimated that for every £1 of costs recoverable through insurance, another £S to £SO are added to the final bill through a wide variety of other financial losses. Like an iceberg, for every visible pound (recovered from the insurers), there are up to 50 times that sum below the surface in indirect costs. These include: • product and material damage • plant damage • building damage • tool and equipment damage • legal costs • expenditure on emergency supplies • clearing site • production delays • overtime working • investigation time • supervisors' time diverted • cost of panels of inquiry • clerical effort. These should also be added intangibles such as damage to the company's reputation (probably reflected in the share price), loss of business, recruitment difficulties, and a general lowering of morale. Accidents not only cost lives, but they are also economically disastrous. Very few organizations can sustain these levels of financial loss. The real question, of course, is not what safety costs us but what it saves. This book describes how organizational accidents arise and outlines practical measures for reducing the likelihood of their occurrence. But such accidents can afflict even the best-run systems. It is, therefore, not enough simply to plan for their prevention; it is also essential to plan for post-accident business recovery in order to minimize these huge losses. Dan Stover18 gives us an example of what an effective loss recovery procedure can achieve. The consequences of the massive Bishopsgate bombing in the City of London in 1993 were shared by many organizations. Among these, the Saudi International Bank was open for business as usual the following Monday. This bank had in place a well-conceived loss recovery program that had been tested by the bombing in the nearby St Mary Axe. The total cost of this bombing was estimated at £1.5 billion. Finally, a sobering thought: four out of five organizations suffering from a major disaster without recovery procedures never reopened for business. Furthermore, 80 percent of disaster recovery plans do not work the first time.19 Recent studies20 have shown that crisis-prepared organizations plan for at least five different types of crises, and the crisis plans are closely linked to business recovery plans. In addition, such organizations have a flexible and adaptive structure and are low on both rationalizations and denial. As Denis Smith of Durham University Business School put it, 'Any denial of the mainstream nature of crisis management is a manifestation of a crisis-prone culture and, as such, becomes a suitable case for treatment.' The Last Word In this chapter, we have sought to reconcile three different approaches to safety management: the person model directed at reducing personal injury events, the engineering model focusing on the human-machine interface and system reliability, and the organizational model that deals with the integrity of defenses and the broader systemic factors. Each has its own metrics and countermeasures. To the extent that any domain is exposed to the risks of both individual and organizational accidents, all of these models have their part to play in the overall safety management program. Such conflicts arise mainly from the predominance of the person model in situations that demand a closer consideration of technical and systemic factors. Whereas the engineering and organizational models can be usefully applied to the reduction of individual accidents, the person model alone (and especially the mindset that goes with it) has very limited value in domains where the risks are mainly derived from the insidious accumulation of latent conditions and their rare conjunctions with local triggers to defeat the multi-layered defenses. This does not mean, of course, that we should ignore the personal injury risks or the behavior of individuals and teams. However, it does mean that risk managers should be aware of the broader systemic origins of organizational accidents and of the variety of techniques now available to thwart their development. Effective risk management requires the application of different counter-measures targeted at different levels of the system at the same time- and all the time. It takes only one organizational accident to put an end to all worries about the bottom line.