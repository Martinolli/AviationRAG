Title: Aircraft System Safety – Military and Civil Aeronautical Applications – Chapters 7 – 8 - 9 Author(s): Duane Kritzenger Category: System Safety Tags: Airworthiness, Aircraft, Safety, Chapter 7 The Fail-safe Dimension 7.1 Introduction There are many reasons why systems may fail. Murphy (1991) groups some of these reasons as follows: Σ Failures due to components. These include failures of circuit breakers, capacitors, connectors, wiring, valves, pumps, etc. Σ Failures due to performance and functional limitations. For instance, the accuracy of both the transmitters and receivers of an instrument landing system (ILS) on an aircraft. See also Section 6.4.2. Σ System failures due to operator error: For instance, when the pilot does not select the correct autopilot mode to initiate a ‘Go-around’ manoeuvre. Σ Failures due to design deficiencies. Software errors provide a typical example of this failure category, where failure will be repeatable under the exact same conditions. See also Section 6.5. Σ Failures due to production deficiencies. Actual manufacturing tolerances may be greater than anticipated. Σ Failures due to interference. Electromagnetic vulnerability of electrical parts of a system may lead to inadvertent or incorrect operation. Σ Failures due to maintenance deficiencies. For instance, blocked pitot-static ports on an aircraft. Σ Failures due to environmental deficiencies. Excessive environmental conditions (e.g. vibration, temperature, etc.) could cause premature failures. 7.2 Defences against failures The first line of defence against hazardous failure conditions is avoidance, in which design and management techniques should be applied to minimise the likelihood of faults arising from random or systemic causes (see Section 6.5). The second line of defence is based on the provision of fault tolerance as a means of dynamic protection during system operation. Possible approaches include: Σ fault masking, where the system or component is designed to survive potential failures with full functionality graceful degradation (sometimes referred to as fail-soft), where the system or component is designed so that in the event of a failure its operation will be maintained but with some loss of functionality and Σ fail-safe, where in the event of a failure, the system or component automatically reverts to one of a small set of states known to be safe and thereafter operates in a highly restricted mode. This may involve complete loss of functionality or reverting to back-up/redundant features. The high levels of functional safety needed from essential systems are usually achieved by some form of fail-safe design. The fail-safe design concept considers the effects of failures and combinations of failure in defining a ‘safe’ design. The application of the fail-safe concept is probably the most important discipline involved in the design of systems and operations. It has evolved over many years.1 The definition first appeared in the dictionary in the mid-1950s after the final reports on the Comet disasters were published. The Collins Dictionary (2003) defines fail-safe as: Σ ‘designed to return to a safe condition in the event of a failure or malfunction’ Σ ‘(in the case of a nuclear weapon) capable of being deactivated in the event of a failure or accident’ Σ ‘unlikely to fail; foolproof’ Σ ‘to return to a safe condition in the event of a failure or malfunction’. The fail-safe concept implies the acceptance of the notion that there is no single element or process in any part of a system that can ever have a sufficient level of reliability to be relied upon without some manner of alternative backup or protection. The CS/JAR/FAR25.1309 airworthiness standards are based on the fail-safe design concept. The following basic objectives pertaining to failures apply (AMJ25.1309, 2000): Σ ‘In any system or subsystem, the failure of any single element, component, or connection during any one flight should be assumed, regardless of its probability. Such single failures should not prevent continued safe flight and landing or significantly reduce the airplane's capability or the crew's ability to cope with the resulting failure conditions. Σ Subsequent failures during the same flight, whether detected or latent, and combinations thereof, should also be assumed unless their joint probability with the first failure is shown to be Extremely Improbable.’ 7.3 Fail-safe principles The fail-safe design concept uses the following design principles or techniques to ensure a ‘safe’ design (refer, inter alia3, AMJ25.1309): Σ Designed integrity and quality, including life limits, to ensure intended functional reliability by minimizing the occurrence and/or the effects of failures. Examples Σ Automatic retraction of spoilers/speed brakes in an emergency full-throttle climb. Σ Using safe-life, fatigue, and fracture mechanics principles to schedule preventative and/or corrective maintenance actions. Σ Redundancy or backup systems enable continued function after any single (or other defined number of) failure(s). It also enables the performance of an intended function even though a fault has occurred. Redundancy can also be used for diagnostics to detect faults. Redundancy is one way to improve the functional reliability of a system. If critical elements can be duplicated, the functional reliability of the system can be improved but with penalties of increased complexity, weight, space, power consumption, and maintenance (i.e., preventative and corrective). Standby redundancy often involves switching over to additional units, which may or may not be identical to the ones that have failed. It is usually preferred to active redundancy if the associated disadvantages do not exclude it, since a more remarkable reliability improvement can be expected if the standby units are operated less of the time. The disadvantages of standby redundancy are that the additional switching process has its own (possibly unacceptable) unreliability, the delay involved in switching over from a failed unit may be safety-critical, and it does increase the risk of dormant failures. There are three forms of active redundancy: 1. Full active redundancy is when any one of the two or three (or more) parallel units can satisfy the required function. 2. Partial active redundancy is when, for example, two or three parallel units must continue to operate in order to satisfy the required function. 3. Conditional active redundancy involves a ‘voting system’ and is used in applications such as digital/analog data processing when there is no simple way of identifying a failure. Mauri (2000, p 21) advises that these are four ways in which redundancy can be employed: 1. Hardware redundancy, e.g., one or more hardware components or ‘channels’ 2. software redundancy, e.g., different software versions doing the same task 3. time redundancy, i.e., enough time to initiate a safe recovery 4. information redundancy, e.g., data is coded in such a way that a certain number of bit errors can be detected or recovered. Σ Isolation (especially electrical, physical, and/or spatial separation/segregation) and independence of systems, components, and elements ensure that the failure of one does not cause the failure of another. Examples Σ Ensuring that fluid carrying hoses (and especially their connections) are not routed above sensitive electronics Σ Ensuring that redundant hydraulic systems are not vulnerable to a common cause of hydraulic fluid loss (e.g. common reservoir). Σ Proven reliability ensures that multiple, independent failures are unlikely to occur during the same flight. Example Σ Continuation of flight after failure of one or more engines, hydraulic systems, flight control systems, etc. Σ Failure warning or indication will provide detection of a condition before it can lead to a dangerous scenario. Examples Σ Failure flag showing false indication on a cockpit display Σ Applying ‘leak before burst’ criteria to pressurized pipes/vessels/containers. Σ Functional verification is the capability for testing or checking the component’s condition. Examples Using BIT (Built-in testing) for software-driven avionics. Σ Flight crew procedures for use after failure detection enable continued safe flight and landing by specifying crew corrective action. Example The ability to override any malfunction of an automatic system such as an autopilot. Not only should the design facilitate the timely use of these recovery procedures, but also, the cockpit management should seek out errors through constant cross monitoring between crew members. Σ Checkability provides the capability to check/monitor a system’s/component’s condition. Example Inspection windows to see if the landing gear is down and locked (or using miniature cameras installed for this purpose). Σ Failure containment Limits the safety impact of a failure. Example Containing the effects which can result from a tire burst or a turbine rotor becoming detached. Σ Designed failure path and damage tolerance controls and directs the effects of a failure in a way that limits its safety impact. Examples Σ Crack propagation containment or through the use of alternative load paths Σ Appropriately positioned structural sacrificial fuses Σ Recovery after tire burst during take-off/landing. Σ Fault tolerance preserves the delivery of the expected (or a minimum) service despite the presence of errors caused by faults within the system itself (Avizienis, 1996). Example A failed generator disconnects from a power supply bus, either automatically or manually, without loss of critical services. For instance, freezing a runaway actuator or trim system drive before dangerous control surface movement can be applied. Σ Error tolerance considers probable human error in the operation, maintenance, and fabrication of the airplane Σ Margins or factors of safety account for foreseeable but uncertain or undefined adverse conditions. The concept of margins of safety is central to the larger concept of aviation itself. Nothing is pushed or designed right to the edge. Example Airframes are designed to take far more than the normal amount of abuse. Each bolt, for instance, is selected with a safety factor to cover possible material flaws, it is inserted into a fitting which is itself designed with a huge margin to cover possible fit problems and the fitting is part of an airframe design which was based on assumed flight loads which are likely never to occur. Σ Failure-survivability, ensures that a failure does not result in a significant loss of performance. This is usually achieved by some form of separate back-up or multiple redundancy. Examples Σ standby instruments in the cockpit Σ Designing the electrical generation system to keep functioning after the failure of one or more generators. 7.4 Applying fail-safe principles The use of only one of these principles or techniques is seldom adequate. A combination of two or more is usually needed to provide a fail-safe design. Effective application of the fail-safe concept can result in the highest level of safety. However, its effectiveness is determined by the quality of the architectural design, the limits imposed by external influences and crew fail-safety. These three factors are now discussed. 7.4.1 Quality of the architectural design The quality of the architectural design ensures redundancy, warning indications, etc. However, it is a fallacy to assume that survivability is assured just because redundancy is provided. Although most essential and critical systems employ some sort of redundant technique, closer scrutiny soon makes it apparent that many of these systems have a ‘single element’ (or ‘common point’), the failure of which will cause multiple channel failures. Examples Σ It is not much use having two engines if a false indication leads to the wrong one being shut down after an engine failure. Σ Having two sets of independent instruments but positioning them in such a manner as to prohibit frequent comparison of their displayed results. Σ Having a dual redundant system but ignoring any potential common failure modes (e.g., common software or common bus failure). Howard (2000 para 4.37) advises that fail-safe architecture may be considered as comprising three main categories: 1. Primary/integral redundancy. This is often applied when it is the only practical fail-safe method possible. Examples Σ Dual ignition on piston engines was adopted in 1912 to counter the then notoriously bad reliability of spark plugs and magnetos Σ dual tires on nose-wheel landing gear Σ triplicate hydraulic systems (i.e. utility-, booster- and auxiliary hydraulic systems) Σ self-checking systems, such as CBIT (continuous built-in testing) in digital systems. 2. Secondary redundancy. Secondary redundancy architecture covers the range of design implementations and reconfigurations that can be referred to or implemented after the failure of the primary system. Examples Σ Standby instruments (such as the standby attitude indicator and the magnetic compass) Σ mechanically lowering the landing gear following total hydraulic failure (e.g., due to loss of fluid). 3. Damage protection redundancy. This addresses failures that can cause hazardously cascading failure conditions (see Section 6.6.3) after a root cause failure. Solutions involve aspects such as designing pressure vessels to leak before they burst, using energy-absorbing designs to contain high kinetic energy parts, and providing survival equipment (e.g., oxygen systems and fire extinguishers), etc. Example Air France Concorde crashed due to a burst tire rupturing the fuel tanks. 7.4.2 External influences Any limits imposed by external influences, such as common-mode failures in redundant systems must be considered. Examples Σ Freezing of pitot-static ports robbing the aircraft of airspeed and altitude information. Σ Duplicated system with each channel having an mean time between failures of 5000 hours. System failure probability should thus be 1 x 10^–8 per hour, but if a common mode failure (e.g. HIRF) could find its way into the system at a rate of 1 x 10^–5 per hour, then total failure probability reduces to (1 x 10^–8 ) + (1 x 10^–5) = 1 x 10^–5 per hour. Due to the nature of cyclical loading on airframe structures and rotating parts in gas turbine engines, they are prone to fatigue. Mismanagement of fatigue in service may lead to the development of catastrophic events, which can occur without prior warning. Whilst system safety standards (such as FAR/JAR 25.1309) encourage redundant designs to achieve the fail-safe design concept, this becomes impractical for some critical structures and all critical engine rotating parts. However, a fail-safe design can still be effectively achieved by applying factors of safety during design, applying life limits in service, and ensuring responsible management of accumulated fatigue. 7.4.3 Crew operations Fail-safety in crew operations is a subject which has become more prominent in the last decade (refer INT/POL/25/14), under terms such as ‘human factors engineering’. The fundamental principle is that if the design is vulnerable to human error, then an accident is bound to happen. Example: Swissair MD-11 disaster, Nova Scotia, 1998 (229 fatalities) Post-accident analysis showed that all onboard electrical power was lost at FL100 (FL = flight level). The crew diverted to an unfamiliar airport at night with smoke in the cockpit and with oxygen masks on. In this high-workload environment, a significant contributing factor to the crash was allocated to the ‘disorientation of the flight crew and loss of control’ due to standby instrument location: Σ In the MD-11, the small standby attitude, airspeed, and altitude instruments were located at the bottom of the center instrument panel, above the power levels. Σ A retractable compass was installed at the top of the windshield to the left of its center pillar. A considerable vertical scan was required to complete an instrument crosscheck, thereby risking Coriolis illusions from large up and down head or eye movements (ICAO Safety Advisory Number 1 (2002), and Avionics Magazine (March 2002), p. 35) The problem has become exacerbated due to the increasing use of cockpit automation – especially for systems that do not keep the crew in the feedback loop and thus reduce their situational awareness. Researchers from the University of Newcastle upon Tyne and the University of York have discovered that modern aircraft have computerized control systems that may over-tax the mental capability of pilots. The team says that although the air accident rate has been constantly decreasing over the last few decades, common cockpit designs are too complicated for pilots to make emergency decisions. The scientists found that during emergencies, pilots are overloaded with technical information that they are unable to process quickly enough. This could mean that wrong decisions are made at crucial moments. They are urging aircraft designers to achieve higher levels of safety by taking into account the psychological characteristics of pilots as well as their physical capabilities. 7.5 Summary Fail-safe architecture stands squarely on the shoulders of basic reliability and system integrity: Σ For hazards related to the loss of a function, the reliability expected above can usually be achieved only through redundancy Σ For hazards related to the incorrect or misleading provision of a function, reliability, and integrity must usually be sought through independent monitoring or comparison of redundant units Σ For hazards related to the provision of a function when not desired, interlocks or other appropriate fail-safe mechanisms are normally used. Fail-safety is effectively a large package of different techniques used for surviving failure and hence giving high functional integrity levels. No other design discipline makes a bigger contribution to safety than the correct application of the fail-safe design concept. Howard advises (2000, p. 517) that all accidents can be attributed to fail-safety implementations either breaking down, not having been adequately provided or are due to extremely remote multiple coincident failures. Any design/safety analysis should consider the application of the fail-safe design concept. Special attention should be given to ensuring the effective use of design techniques that would prevent single failures or other events from damaging or otherwise adversely affecting more than one redundant system channel or more than one system performing operationally similar functions. When considering such common-cause failures or other events consequential or cascading effects should be considered in deciding whether they would be inevitable or reasonably likely. Chapter 8 The System Safety Assessment 8.1 History Lloyd and Tye (1995) recall that the airworthiness requirements (e.g. BCAR and FAR) of the mid-20th century ‘were devised to suit the circumstances. Separate sets of requirements were stated for each type of system and they dealt with the engineering detail intended to secure sufficient reliability’. Where the system was such that its failure could result in a serious hazard, the degree of redundancy (i.e. multiplication of the primary systems or provision of emergency systems) was stipulated. Compliance was generally shown by some sort of an FMEA.1 For simple, self-contained systems this approach had its merits. However, systems rapidly became more complex. Complex systems2 have a considerable amount of interfaces and cross/interconnections between the electrical, avionic, hydraulic and mechanical systems.3 In addition, there are essential interfaces with the pilot, maintenance personnel and flight performance of the aircraft. The aircraft designer is thus faced not only with the analysis of each individual system independently, but also needs to consider how these systems act in concert with other systems. Airworthiness Authorities could therefore not continue to issue detailed engineering requirements for each new application. Firstly, this would lead to a mountain of regulatory requirements and, secondly, this approach would inhibit innovation by leading designers into sub-optimum solutions. It therefore became necessary to have some basic objective requirement (see Section 5.2) related to an acceptable level of safety, which could be applied to the safety certification and release to service (RTS) of any system or function. It was the auto-land system of the 1960s which first precipitated this new approach (Lloyd and Tye, 1995; Cherry, 1995) which, for civil transport aircraft, resulted in regulations such as JAR 25.1309 and its supporting Advisory Circular (AC25.1309, undated). Only in the event of specific concerns are supplementary detailed requirements developed for particular types of systems or hazards. Examples of these additional requirements are: Σ Special Federal Aviation Regulation (SFAR) 88 was developed to counter the fuel tank ignition concerns following the in-flight explosion of TWA800 in 1996. Σ Advisory Circular (AC) 20-138 was issued to approve the airworthiness of global positioning system (GPS) navigation equipment for use as a visual flight rules and instrument flight rules supplemental navigation system (AC20-138, undated). This new approach required that, for safety certification, the designers conduct a thorough assessment of potential failures and evaluate the degree of hazard inherent in the effect of failures. With complex critical systems and functions, the designer has to consider not only the effect of single failures but also the effects of possible multiple failures, particularly if some of these failures are passive (see Chapter 6). The designers need to show that there is an inverse relationship (see Chapter 5) between the probability of occurrence and the degree of hazard inherent in its effect. The designers also need to consider whether the design is such that it can unnecessarily lead to errors during manufacture, maintenance, or operation by the crew. Furthermore, the designer needs to consider the environment to which the systems would be exposed, which could involve large variations in atmospheric temperature, pressure, acceleration (e.g., due to gusts), vibration, and other hostile events such as lightning strikes and icing. The vehicle to report this demonstration for the purposes of safety certification and release to service (RTS) became known as the safety assessment system (SSA). A system safety assessment can, therefore, be defined as a structured body of evidence that provides a convincing and valid argument that a system is adequately safe for a given application in a given environment. It is a collection of documents that, taken together, provides objective evidence that a system, if used in accordance with the listed recommendations and limitations, can be certified as being ‘safe enough’ to be released into service’ 8.2 Aims and objectives of a system safety assessment 8.2.1 System safety aim The aim of the system safety assessment program is to ensure that (refer, inter alia, MIL-STD-882C para 4): Σ Safety is designed into the system in a timely and cost-effective manner. Σ Hazards associated with each aircraft sub-system are identified, tracked, evaluated and eliminated or the associated risk reduced to an acceptable level. Σ Historical safety data, including lessons learned from other systems, are considered and used. Σ Minimum risk is sought in accepting and using new technology, materials, or designs and new production, test and operational techniques. Σ Actions taken to eliminate hazards or reduce risk to an acceptable level are documented. Σ Retrofit actions required to improve safety are minimized through the timely inclusion of safety features. Σ Changes in design, configuration, or mission requirements are accomplished in a manner that limits the risk from any hazard to an agreed acceptable level. Σ Procedural and training requirements to support and maintain safety assumptions and assertions are identified. Σ Design criteria for inadequate or overly restrictive requirements regarding safety are reviewed and new design criteria supported by study, analysis or test data are recommended. Σ The program team are made aware of system safety and how the design can be used to mitigate risks. Σ Unwarranted complexity and novelty for novelty’s sake are avoided. 8.2.2 System safety objectives The system safety assessment’s objectives are to: Σ demonstrate that there is an inverse relationship between the probability of occurrence and the degree of hazard inherent in its effect Σ demonstrate that the design is such that it cannot lead unnecessarily to errors during manufacture, maintenance or operation by the crew Σ demonstrate that the systems are suitable for the environment that the systems will be exposed to. The latter could involve large variations in atmospheric temperature, pressure, acceleration (e.g. due to gusts), vibration, and other hostile events such as lightning strikes and icing. 8.2.3 System Safety Design Requirements Safety is built in, not added on (see also fail-safe in Chapter 7). Safety requirements should thus be integrated in the design and development of life-cycle, i.e., starting at concept generation. The general system safety design requirements include (refer, inter alia, AMC25.1309 and MIL-STD-882C: Σ No single component failure, or single failure combined with a latent failure, shall result in a catastrophic event. Σ For single component failures having a ‘life dependent failure characteristic’ (e.g. structural members) which can result in system loss, a failure rate and component life must be established by means of accepted engineering mathematical models and methods, or accrued from actual tests (e.g. fatigue tests) or service experience. Σ The elimination of identified hazards or reduce associated risk through design, including material selection or substitution. When hazardous materials must be used, those with least hazard risk throughout the life cycle of the system must be used. Σ Hazardous substances, components and operations must be isolated from other activities, areas, personnel and incompatible materials. Σ Equipment must be located so that access during operations, servicing, maintenance, repair or adjustment minimizes personnel exposure to hazards such as burns, noise, electric shock, electromagnetic radiation, cutting edges, sharp points or toxic atmospheres. Σ The severity and probability of any failure resulting from excessive environmental conditions such as temperatures, pressure, acceleration and vibration must be minimized. Σ Warning information (instructions and warning/caution markings) to alert crew of unsafe system operating conditions must be provided. Σ Design must minimize the severity and probability of human error in the operation or support of the system: incorporate fixed, automatic or other safety devices (with periodic functional checks); provide warning devices; develop procedures and training. Σ Alternative approaches to minimize risk from hazards that cannot be eliminated must be considered. Such approaches include interlocks, redundancy, fail-safe design, system protection, fire suppression and protective clothing, equipment, devices and procedures. Σ The power sources, controls and critical components of redundant sub-systems must be protected by physical and electrical separation and shielding. Σ When alternative design approaches cannot eliminate the hazard, safety devices (e.g. alerts) and warning/caution notes must be provided. Σ The severity of personnel injury or damage to equipment in the event of an accident (i.e. increase survivability) must be minimized. Σ Software-controlled or monitored functions must be designed to minimize the probability of accidents or safety incidents. 8.3 The system and its relationship to safety Before we can conduct a system safety assessment we first need to understand what we mean by the word ‘system’. A system is an assemblage of interrelated elements comprising a unified whole. From the Latin and Greek, the term ‘system’ means to combine, to set up, to place together. A sub-system is a system which is part of another system. When conducting a system safety assessment, the first step should therefore be to decide the level at which the safety assessment is aimed and scope the assessment accordingly. DEF STAN 00-35 and society of automotive engineers ARP4754 (Section 1.3) defines a system as ‘a combination of subsystems and/or items organized to perform a specified function or functions’ with: Σ a subsystem is ‘a group of assemblies, designed together to form a major part of a system, complete in its own right performing a specific function or functions’ Σ a unit or assembly is ‘any part which is less than a subsystem, but whose performance can be independently assessed in terms of the overall performance of the subsystem’, and finally Σ a part or component is ‘any item incapable of useful function at a lower level of assembly’. Safety is a system property. The safety of the whole cannot be argued from the claimed safety of the individual system elements alone. Any safety integrity claims for a part of a system (e.g. COTS5 parts, assemblies or subsystems) without considering the whole (e.g. the aircraft) should be viewed with skepticism. The collection of component claims and supporting arguments and evidence do not make a complete safety assessment for the component/part. Rather, they form a partially constructed safety assessment with arguments (e.g. identified hazards, failure modes and their probability of occurrence) in ‘ready-to-use’ form. A system safety assessment must therefore consider not only all the elements within an individual system, but also the safety-related systems making up the total combination of the required functionality. These partial safety assessments can be thought of as safety assessment modules. It is important to understand the claims being made in these modules, the context assumed, and the evidence presented (Kelly, 2003, p. 105). Only then can you intelligently apply this information within the context of your own system safety assessment. System safety is more than the sum of the parts. In most situations, safety is achieved via the integration of a number of systems/sub-systems/ components, which rely on a variety of technologies (be they mechanical, hydraulic, pneumatic, electrical, electronic, programmable electronic, etc.), which is then put into an environment where it has to function safely as an operational system. It is this environment (i.e. physical installation as well as operational application) which highlights a deficiency in the DEF STAN 00-35 definition of a system, which does not clearly differentiate the various system levels available. The South African Air Force (SAAF) make use of the illustration in Fig. 8.1 to distinguish between the different system levels for their logistic support strategy. This illustration can be applied to the system safety assessment as it helps us define what the system under consideration is. For instance: Σ a component (Level 2) or sub-system (Level 3) does not possess safety as a property. Safety is a property of the product system (e.g. the aircraft) in its environment Σ the integrating engineering authority must ensure that all components (including software) and sub-systems are fit for purpose (i.e. System Level 4), and the system safety analysis is a useful design tool for the purpose (with the added benefit of showing the authorities how the regulatory requirements, such as FAR25.1309, are met) Σ The operators (and maintainers) need to ensure (via the Level 5/6 Safety Case6 –see Chapter 9) that they operate (and maintain) the product in a manner that does not degrade the original design integrity or cause any undue occupational hazards to the personnel exposed to the product. The integration of various parts/sub-systems into an operable system is hardly ever simply one of acquiring commercial off the shelf packages of technology. The hurdles with integrating various pieces of commercial off the shelf equipment into a safety assessment are as follows: Σ In the drive to ‘divide and conquer’ the assessor must be careful to ensure that the interaction between the systems is considered. The total is more than the sum of its parts. Seemingly established technologies may nonetheless be ‘new’ in terms of the issues and problems that are presented in particular circumstances. Σ Non-conforming safety criteria; if there is a common understanding of exactly what the definitions of terms such as improbable, probable, unlikely, minor, major, hazardous, catastrophic, etc., mean, the effort to integrate the safety argument into a system safety assessment will be greatly reduced and auditability will be improved. Σ Failure to consider interactions between sub-system safety assessments could lead to either disproportionate effort, duplication of effort, or nugatory effort, allocated across the overall safety assessment development. An underlying cause of these problems is a poor understanding of the overall structure of the system safety analysis argument and how the various arguments link together. The loss of efficiency can be prevented by clearly defining the scope and boundaries of each sub-system’s safety assessment. A cooperative relationship within the organization and with sub-contractors is required. A target-driven top-down approach (i.e., using measurable safety objectives) will assist in ensuring that proportional levels of effort are put into each part’s safety assessment. Explicit planning (at an early stage of the life cycle) and managing the safety assessment argument can alleviate these problems. If the system safety assessment is not managed from the top-level system down to all its components, then the hurdles discussed above are bound to occur. The old adage is true: if you fail to plan, you plan to fail. 8.4 Planning the safety assessment The content of a safety assessment varies considerably depending on factors such as the complexity of the system, how critical the system is to flight safety, what volume of experience is available the type of system being used, and the novelty and complexity of the technologies being used. Suppose the safety assessment is to ensure that the developed products are ‘safe enough’ to be taken into use (or deployed). In that case, the safety assessment should be planned and managed to provide the necessary assurance that all relevant hazards and failure conditions have been identified and that all significant combinations of hazards and failures that could cause those conditions to have been considered.8 Furthermore, the safety assessment must be comprehensible to all parties concerned, not just the analyst. The assessment must assist the designer and management in making decisions. It must make clear what the critical features of each system are and upon which special manufacturing techniques, inspection, testing, crew drills, and maintenance practices are critically dependent (Lloyd and Tye, 1995, p. 19). A strategy is therefore needed to facilitate the planning of a system safety assessment. Not only does it ensure that we do not run into the hurdles discussed above, but it also assists a third party, such as the customer, the certification authorities (e.g. CAA), your fellow designers, or even your boss, in reading and understanding the methodology used to argue and validate/prove safety. How do we compile a safety assessment strategy? The following basic elements have to be considered: Σ The stakeholders. We need to identify the problem owners/decision makers who have a stake in the assessment. These will include the project manager, the client, the regulatory authorities be they military (e.g. MoD) or civilian (e.g. CAA), the independent safety auditors and the internal departments/teams and subcontractors. The success of the safety assessment requires a close working relationship and mutual understanding between all these stakeholders Σ The safety/risk criteria. The safety/risk criteria establish the top-level system safety requirements, or objectives. Regulatory authorities may have different definitions for the various categories of hazards/accidents. To be able objectively to distinguish and evaluate the various hazards present, it is important to define the exact terminology and to allocate a measure of performance. This is an important (and arguably most neglected) topic as it is the ‘safety acceptance’ criteria the system is expected to achieve, and hence the measure (or standard) the assessment will compare the system against. For more details on safety criteria, see Appendix B. Σ The system level. Define the systems’ level at which safety is to be assessed. The importance of this step is explained in Section 8.3 above. A safety assessment by a supplier of a component (e.g. a flare dispenser) will vastly differ in scope and approach to a safety assessment for a product (e.g. an aircraft) or user system (e.g. a facility). Σ The system description. The system will need to be defined in terms of its physical and functional operation and interfaces. The description will include the systems (e.g. equipment) included; the functions these perform – including any modes of operation; its operating environment/envelope; the interface with other systems and where the functional and physical boundary lies between them; and, in the event of a modification, any deletions from the existing system.10 If the boundaries are not clear to everyone involved in the assessment, some vital part may be overlooked. Furthermore, boundaries help with responsibility allocation, especially when products from sub-contractors are integrated into a more complex system. It is said that a picture is worth a thousand words. For the sake of brevity – and to facilitate rapid comprehension by both the reader and the assessor – it is useful to include diagrams (e.g. electrical diagrams, functional block diagrams,11 etc.) illustrating the functional and physical interrelationships of the systems under consideration. These may be far more informative than a long narrative. Σ The argument. Define the safety argument, i.e., how are you going to prove that the system is acceptably safe? Traditionally, a safety plan or in the preliminary safety assessment which explains and validates the safety assessment tools and techniques (Appendix A) used in the safety assessment process. A new technique available for graphically portraying the safety argument is goal structured notation (GSN), where goals are broken into sub-goals, and eventually supported by evidence (solutions) whilst making clear the strategies adopted, the rationale for the approach (assumptions, justifications) and the context in which goals are stated. See Appendix C for more information. Although some regulatory authorities have preferences for certain hazard identification techniques, it is up to the designer to use the most appropriate methods all integrated into a logical argument. This choice of tools/technique must, therefore, be substantiated and adequate so as to ensure that nothing falls between the cracks. Σ The program plan. We then need to decide which safety activities will be conducted, when, and by whom. Safety activities are undertaken throughout the life of a system, but it is vital that the right ones are done at the right time. If this is not done, then there are two possible undesirable outcomes (Rhys, 2002): Σ Introducing an unsafe system into service (i.e., excessive safety risk) Σ major delays, cancellations, or cost overruns if safety problems are discovered late (i.e., excessive project risk). If the system safety assessment is to determine the safety requirements and influence the design, then the safety program plan needs to be integrated into the development process. 8.5 Safety during the development process The aim of the development process is to produce a system that is fit for purpose and meets the contractual requirements. Many design drivers have to be satisfied. Safety is one of those design drivers. The safety assessment process is thus an inherent part of the development process. At the conceptual stages of design, designers have great freedom, and the cost of design and design changes are minimal. As the design matures, design freedom decreases, and the subsequent costs associated with design changes increase. Figure 8.2 illustrates that the ability to influence a system’s characteristics diminishes rapidly as the system proceeds from one phase of its life cycle to the next. This illustrates that problems experienced downstream are symptoms of neglecting upstream. The required analysis must be conducted as early as possible in the development process because of the influence that it may have on system architecture. However, confirmation may not always be feasible until implementation is complete. As stated in Section 8.2.3 above, safety is built in, not added on. In order to understand how we achieve a ‘safe’ design we must briefly consider the development process. The V-diagram in Fig. 8.3 presents a simplified illustration of the design process. The left branch represents the assessment of the design as it progresses towards low-level components. The right branch illustrates how these components are systemically integrated into sub-systems and systems, whilst continuously verifying integrity at each level. Safety must be an integral part of this process if we are to design effectively. Hence the basic safety activities are: Σ Determining the safety requirements (i.e., qualitative and/or quantitative safety objectives commensurate with the particular hazard), starting with the appropriate system level and flowing those requirements down to the required sub-system/unit/component level. This is the purpose of the PSSAs. Σ Quantitatively and/or qualitatively assessing the proposed design and taking action where the design is inadequate with respect to safety requirements. Σ Gathering evidence for the safety assessment showing that an acceptable process has been followed to ensure that an acceptable level of safety is integrated into the delivered system. This evidence is gradually added to each re-issue of the preliminary system safety assessment until the final system safety analysis can be issued. This relationship between the safety assessment process and the system development process for aircraft is illustrated in society of automotive engineers ARP475414, of which a tailored version can be seen in Fig. 8.4. We can thus see that the safety assessment process includes requirements generation and verification, which supports the development activities. Just like other development activities, it is iterative in nature. However, it does progress along with the development process: Σ It begins with the concept design and derives the safety requirements for it. Σ It then progresses into requirements verification which provides the evidence (justification) to support the modification activities leading up to certification. Σ As the design evolves, changes are made that require re-assessment, which might influence the design again. Hazard and safety analysis help evaluate design trade-offs. Σ The safety process finally produces the safety assessment, which substantiates the developed products as safe enough to be taken into use and/or verifies that the design meets the safety requirements (see also Fig. 8.8 on page 125 for an illustration of typical system safety analysis activities against the product life cycle) 8.6 Modelling the safety assessment process How do we portray the safety assessment process in a manner that safety novices (e.g. program managers) can understand? There are various frameworks available with the Open University’s (refer T840 Block 6 p. 26) hard systems approach (HSA) being particularly useful as a framework for modelling the safety assessment process. HSA is a problem-solving tool that considers both quantitative and qualitative issues within the framework of the defined system. It is useful for problems which can be well defined, fairly limited in extent and with agreed objectives (defined future state, such as a system which meets its safety targets). The HSA process is iterative in nature and is summarized in Fig. 8.5 The hard systems approach can be tailored to the safety assessment process as shown in Fig. 8.6. Step 1: identify and involve the problem owner/decision maker The owners/decision makers may include the project manager, the client and the accepting authorities (be they military (e.g. MoD) or civil (e.g. CAA)). These stakeholders decide the criteria in Step 4 chosen for the assessments, as well as provide the resources to implement any required solution (Step 9). Step 2: define the aim of the assessment The problem statement could be user-defined or extracted from regulations such as JAR25.1309 para (b): The airplane systems … must be designed so that the occurrence of any failure condition which would prevent the continued safe flight and landing of the airplane is extremely improbable, and the occurrence of any failure condition which could reduce the capability of the airplane or the ability of the crew to cope with adverse operating conditions is improbable. Step 3: describe the system The system will need to be defined in terms of the function it performs, the equipment included, its environment, and where the boundary lies between it and other systems. If the boundaries are not clear to everyone involved in the assessment, some vital part may be overlooked (Vicente, 1999, p. 7). For more information, see Section 8.4. Step 4: define the safety criteria (measure of performance) In order to guide the safety assessment process, it is necessary firstly to define the criteria used to judge the acceptability of hazards. These definitions are fundamental to understanding the data presented, as the resultant ‘safety acceptance criteria’ form the baseline standards against which the system is then evaluated in the final system safety assessment (SSA) report. For more information, see Chapter 5 and Appendix B. Step 5: identify safety objectives and constraints Identify and classify the hazardous conditions attributable to: Σ system functions (and combinations of functions). The most popular tool in the early stages of the design is the functional hazard assessment15 (see Appendix A). As the design matures, the assessment is then supplemented by other failure predictive assessment techniques such as the zonal hazard assessment. All failure modes are classified according to their severity (e.g. minor, major, hazardous and catastrophic) depending on the safety criteria chosen. Each failure mode classification is allocated a numerical or a qualitative safety objective, which is agreed with the applicable airworthiness authority. Example An example quantitative objective statement: ‘Flap system failure could present a potentially catastrophic situation and shall have a probability of occurrence of less that 10–9 per hour of flight.’ An example qualitative statement: Software error in the altitude display could present a catastrophic situation and shall have a Development Assurance Level A.’ Safety Constraints may be imposed by regulatory requirements and/or regulatory guidance. Example AC25-11 states that ‘display of weather radar in the cockpit is a non-essential function; however, presentation of hazardously misleading information must be improbable’. Σ Item characteristics (e.g., hazardous materials or working practices) if the aim in Step 2 required these to be addressed as well. These hazards could be identified via hazard identification tools/techniques such as the particular risk analysis, HAZOP, etc. (see Appendix A). Each hazard must then be allocated with either a numerical or a qualitative safety objective, which is agreed with the applicable airworthiness authority. Example using DEF STAN 00-56 criteria When using the DEF STAN 00-56 criteria (refer Appendix B), each hazard shall be allocated to the most credible possible accident it can cause. The severity of this potential accident is then classified as shown in Table B.8. In order for the risk to be no more than a Risk Class C, the same tables can then be used to determine the acceptable probability of the accident occurring. The probability of the accident is constrained by the events in the accident sequence (i.e the model, see Step 7), so by determining the probability of all the events in the accident chain, the assessor will be able to allocate a probability target to the hazard. Example goal statement The probability of fire in the fuel tanks is catastrophic and must be extremely improbable in occurrence. Example constraint No single failure condition may cause an ignition source in the fuel tank, no matter how low the probability (refer to SFAR88). Step 6: generation of routes to objectives We now need to decide how we are going to prove the accomplishment of safety objectives identified in Step 5: Σ For the FHA, the level of detail needed for the various safety assessment activities is dependent on the aircraft-level condition classification, the degree of integration, and the complexity of the system implementation. The joint aviation authority provides useful guidance in this regard; see the decision tree in Fig. B.1. Σ For all risk-based criteria, the assessor will need to agree with the relevant authorities on which hazards need to be broken into full accident sequences (see Fig. 4.1). Step 7: modeling By this stage, the design has commenced in earnest, and the system architecture is being formalized in models (e.g. test benches, scale models or paper designs). In turn, the safety assessment process needs to build qualitative and quantitative models in order to analyze the probability of an undesired event/failure occurring. Once any potential hazardous effects have been identified, the task is therefore to determine the system conditions which will produce these effects. An analysis has to be performed to identify all failure conditions, and combination of conditions, which would lead to any effect listed in the airworthiness objectives. This is done by considering the reliability of the equipment; the design of the system; the precautions taken in installation, operation and maintenance procedures; as well as the intended operational exposure. During this phase the failure analysis is conducted by means of three main approaches: 1. The ‘top-down’ methods start by identifying the failure condition to be investigated, and then proceeds to derive those failure modes which can produce the condition (e.g. safety requirements from the base events of the fault tree analysis lead to the requirements for FMEA, life testing, etc.). 2. The ‘bottom-up’ methods start with the hardware failures (e.g. identified via piece-part FMEAs) which can occur and analyses the effects of these on the system (e.g. the fault tree analysis basic events get their failure rates from the FMES). 3. Taking account of combinations of failures and dependent failure conditions from both internal and external causes (e.g. via the CCA). As a result of this, design changes may well be required, which means the analysis becomes of an iterative nature until the required objectives are met. Any significant failure conditions not previously identified in the safety assessment need to be picked up and fed back in the safety management process. Step 8: evaluation In this step the achieved probability (qualitative or quantitative) is compared against the objective as defined in Step 5: Σ For the goal-based approach, all deviations will require dispensation from the approval authority. Σ For the risk-based approach, we may find that some accidents have a higher risk classification than we originally desired. Appropriate action is defined in Section 4.5. Step 9: choice of routes to objectives Ideally, the design (i.e., the selection of components, the system architecture, and the means of integration) should now be implemented in such a way as to ensure that the safety objectives are accomplished. The best route(s) to achieving the objectives must be chosen. Qualitative objectives and constraints will come into play here through the influence of the different stakeholders (especially the decision-makers who control the purse strings). A trade-off may be required in terms of a cost-benefitanalysis to ensure that certain hazards are indeed ALARP. Step 10: implementation Implementation is action orientated and represents the detailed work necessary to actually complete a design that should meet all required objectives. Ideally, from the point of view of a smooth development process, requirements should be validated before design implementation commences. However, in practice, particularly for complex and integrated systems, the necessary visibility of the whole set of consequences that flow from the requirements may not be obtainable until the implemented system is available and can be tested in its operational context. In consequence, validation is normally a staged process contributing through the development cycle. At each stage the validation activity provides increasing confidence in the correctness and completeness of the requirements. 8.7 Conducting a safety assessment There is no one correct way of conducting a safety assessment. It all depends on the system complexity and on the safety assessment approach utilized (see Chapter 2). That does not mean to say that the assessment has to be analyzed from a single approach only for, more often than not, a combined approach is far more feasible to identify and analyze the range of possible hazards (see Chapter 6). The following section will broadly contrast/compare the manner in which the goal-based approach (Chapter 5) and the risk-based approach (Chapter 4) are applied during a system safety assessment. 8.7.1 Goal-based safety assessment Identify failures/hazards A good starting point is the functional hazard assessment (FHA), starting from the highest system level possible. The functional hazard assessment considers functional interaction and provides a methodology to evaluate the system’s functions and the design of sub-systems performing those functions. Note that functional division may cut across systems (and therefore organization boundaries). Multiple systems may contribute to the performance of a particular safety function. Similarly, systems may contribute to the performance of more than one safety function. See society of automotive engineers ARP 4761 for guidance on conducting a FHA. Other useful tools for identifying hazards include PRA, CMA, ZHA, etc. (see Appendix A). Classify the severity of the hazard The severity of the worst-case consequence (i.e. the hazardous situation) is typically classified using the safety criteria as defined in Table 5.1. It is advisable to consult individuals with operational experience (e.g. operating and maintenance crews) when analyzing the effects of a potential hazard so that the severity can be properly determined and any assumptions validated. Allocate the safety targets/objectives This provides an indication of the acceptable probability of occurrence for the hazard/ failure condition and is typically done by using the criteria in Table 5.2. Prove safety objective accomplishment For each hazardous effect, it should be determined how the aircraft/system will satisfy the safety objectives. This could mean that an analysis might need to be performed to identify all failure conditions (e.g. sub-system or line replaceable units failures) which could lead to the hazardous effect. Each of these failure conditions is either allocated a derived safety objective (i.e. safety objectives have been set for the systems/functions, then apportioned to sub-functions/sub-systems, then apportioned to components) or, in the case of commercial off the shelf equipment, existing data is used, and the system architecture is manipulated to obtain the safety objectives discussed above. Example: using a goal-based approach Consider what happens if the loss of aircraft engine power occurs at low altitude, low air speed, or at high gross weight. The effects would include loss of attitude control, stalls, high rate of decent and terrain collision. These obviously have the potential for catastrophic losses. Using the goal-based approach in Chapter 5, this hazard (loss of power) would be rated ‘catastrophic’ at low altitude, air speed, or at high gross weight and should be ‘extremely improbable’ in occurrence for this system state. We next need to determine what the likelihood is of the hazard occurring (i.e. the probability of occurrence) and how often the ‘effects or harm’ will occur, considering the worst-case system state. Here’s how it works. First, determine how often the hazard is expected to occur. This can be a quantified or a qualified estimate. Usually, it is a function of the likelihood of the combinations of the cause(s), but sometimes this can be determined by evaluating incident or accident databases to see how often the hazard has been recorded in the field. Let us assume that the likelihood of ‘loss of engine power’ turns out to be 0.0001 per operational hour. We then need to make an estimate of the likelihood of the worst-case system state. This estimate also can be quantified or qualified. In many systems the operational or system description will provide many clues that will allow the development of this answer. For this example, assume that the likelihood of being in the worst-case system state (low altitude, low air speed, high gross weight) is 0.002 per operational hour. For the effects to be manifested in the worst case, both the hazard (loss of power) and the worst-case system state (low altitude, etc.) must occur at the same time. The likelihood of the worst-case effect is thus 0.0001 x 0.002 = 0.0000002 or 2 x 10^–7 per operational hour. Using the definitions in Chapter 5, would lead to a characterization of the likelihood as ‘remote,’ which does not meet the safety objective. The options open to the designer are to either apply for a deviation from the approval authority, increase the reliability of the system, or add instruction in the flight manual and flight cards so as to limit operational exposure to this system state. 8.7.2 Risk-based safety assessment Identify hazards The hazard identification process should be planned and managed to provide the necessary assurance that all relevant hazards have been identified (see Chapter 4 for the definition of hazards). There are a variety of tools and techniques available to identify hazards. The application of these tools and techniques depends on the specific product/process being considered, its complexity, the product lifecycle phase, etc. The most common tools and techniques used are FHAs, HAZOPs, occupation health hazard analysis, historical records, etc. (for more details on these tools, see Appendix A). Classify accident severity Each identified hazard is allocated severity classification according to the defined safety criteria. Accident severity categories are defined to provide a qualitative measure of the consequences resulting from personnel error, environmental conditions, design inadequacies, procedural deficiencies or system, sub-system or component failures. Severity is the worst credible consequence of a hazard (i.e. the worst accident) and is independent of random or systemic failure modes. Determine the probability of the accident The purpose is to identify circumstances which could lead to the accident under credible conditions. In order to do this, we need to identify all the contributing failures and events which need to combine to cause the accident (see Fig. 4.1). This may require a number of hazard assessment techniques in order to identify appropriate contributing failures and events. Assess the risk The risk is the combination of the severity and the probability of the accident, see Chapter 4. Reduce or manage the risk This is accomplished by influencing any of the failures or events in the accident sequence in order to reduce the probability of the undesired outcome. Example: using risk-based approach Consider again what happens if the loss of aircraft engine power occurs at low altitude, low airspeed, or at high gross weight. Using the risk-based approach in Chapter 4, the accident would be classified as either ‘critical’ or ‘catastrophic’ depending on the amount of people on the aircraft. To be able to calculate the risk we then need to determine the probability of the accident occurring. Here’s how it might work. Let us again assume that the likelihood of being in the worst-case system state is 0.002 per operational hour and that the likelihood for ‘loss of engine power’ is 0.0001 per operational hour. For the accident to occur, it means that the pilots were unable to recover the situation in time. Now, determining this likelihood is very subjective. Let us assume that there is a 50% chance that the pilot can recover from the situation. The likelihood of this accident is thus 0.0001 x 0.002 x 0.5 = 1 x 10–7 per operational hour, which is classified as ‘occasional’. The risk of the ‘catastrophic’ event is thus Risk Class A, which is deemed as being ‘intolerable’ and shall be removed by the use of safety features. These may include warning devices of impending degraded performance, which could allow the crew to cope better once the hazard occurs. The risk of a ‘critical’ event is thus Risk Class B, which is considered as being ‘undesirable’, and shall only be accepted when risk reduction is impracticable. A cost-benefit analysis may just result in no further action being taken to mitigate this hazard. 8.8 Generating the system safety assessment report The system safety assessment (SSA) is an evolving document that provides the evidence (justification) to support the major procurement milestones and modification activities leading up to certification, acceptance into service and subsequent changes of design and operational use. It is the means for demonstrating that all the safety issues relating to the design and development task have been addressed. Ideally, from the point of view of a smooth development process, requirements should be validated before design implementation commences. However, in practice (particularly for complex and integrated systems), the necessary visibility of the whole set of consequences that flow from the requirements may not be obtainable until the implemented system is available and can be tested in its operational context. In consequence, validation is normally a staged process contributing through the development cycle. At each stage the validation activity provides increasing confidence in the correctness and completeness of the requirements. The safety assessment process thus remains live throughout the development life cycle. From the earliest stages it should be known what type of evidence will be required to demonstrate that safety will be achieved. The process must take into account any additional complexities and interdependencies which arise during integration. Incremental development of the safety assessment is thus very important. The first iteration is all about intentions rather than claims. It provides an opportunity to provide safety objectives, design constraints, assumptions, justifications, etc., to the system engineers who are developing the architecture, layout and draft specifications. Subsequent versions should develop the argument further and populate the structure with references out of the supportive evidence, with the plans of how to achieve the outstanding results. Although the safety assessment process is iterative in nature, it does progress along with the development process. Σ It begins with the concept design and derives the safety requirements for it. Σ As the design evolves, changes are made that require re-assessment, which might influence the design again. Hazard and safety analysis help evaluate design tradeoffs. Σ The safety process produces the safety assessment, which substantiates the developed products as safe enough to be taken into use and/or verifies that the design meets the safety requirements. The flowchart in Fig. 8.7 shows a typical aircraft product life cycle from concept through to disposal at the end of the product’s useful life. Applying the most commonly used (refer, inter alia, society of automotive engineers ARP4761) safety assessment tools and techniques, Fig. 8.8 takes this product life cycle and shows the safety assessment activities which may typically follow for each phase. For complex or large programs, it may be useful to divide the design phase up into discrete elements and milestones as shown in Fig. 8.9. Σ System requirements review (SRR): the SRR is the first top-level, multi-disciplinary, internal review of the perceived system requirements (including regulatory requirements). It is effectively a sanity check upon what the system is required to achieve, a top-level overview of requirements and review against the original objectives. This review may be held during the feasibility phase. Successful attainment of this milestone leads to a preliminary system design, in turn to the parallel development of the hardware and software requirements analysis, albeit with significant co-ordination between the two. Σ System design review (SDR): the hardware system design review immediately follows the preliminary design phase and will encompass a top-level review of the system hardware characteristics such that preliminary design may proceed with confidence. Key hardware characteristics will be reviewed at this stage to ensure that there are no major mismatches between the system requirements and what the hardware is capable of supporting. Σ Software specification review (SSR): the system safety requirements is essentially a similar process to the hardware system design review but applying to the software when a better appreciation of the software requirements has become apparent and possibly embracing any limitations such as throughput, timing or memory which the adopted hardware solution may impose. Both the system design review and system safety requirements allow the preliminary design to be developed up to the PDR. Σ Preliminary design review (PDR): the preliminary design review process is the first detailed review of the initial design (H/W & S/W) versus the derived requirements. This is usually the last review before commencing major design resources to the detailed design process and often involves the customer. The status of specification and certification compliance is reviewed, as this stage in the design process is the last before major commitments to providing the necessary program resources and investment. Σ Critical design review (CDR): by the time of the critical design review major effort will have been committed to the program design effort. The critical design review offers the possibility of identifying final design flaws or, more likely, trading the risks of one implementation path versus another. The critical design review represents the last opportunity to review and alter the direction of the design before very large commitments and final design decisions are taken. As such the customer participates in the CDR. Major changes in system design (H/W & S/W) after the critical design review will result in severe impact on program costs and schedule. Figure 8.8 then considers the milestones after the design phase: Σ Flight trials readiness review (FTRR): this is conducted to challenge the preparedness of the whole test team. Senior representatives of the flight test organization, including some from outside the specific program undertake to examine all the safety and support aspects of the trial. Only when the entire review team are satisfied can the trial progress to the flying phase. The safety of flight test is paramount and risks need to be understood and bounded from the outset. Familiarity with the fundamentals of design, the anticipated aircraft behavior and safety procedures applicable are reviewed, and if necessary, rehearsed and practiced to retain a high degree of proficiency. Σ Final design review (FDR): the goal of the review is to convince the executives in charge that the design is mature enough to be released into service. All safety objectives should be accomplished and continued airworthiness activities published. In general the safety assessment can be a very large document, so we need strategies to break it into manageable chunks. Not all evidence has to be fully contained in it (often the evidence is found in reports that were generated for other purposes), but the data should be sufficient to convince a third party, who may have had no prior insight into the program. This implies strict configuration control requirements. If a reference document (such as a test report, or a flammability assessment) is updated, then its implication on the safety assessment needs to be considered. The challenge of assembling a safety assessment brings all of the elements of the safety management system into sharp focus. A system safety analysis is not a repository for a forest of FTAs and FMEAs bound in a dust gathering tome. Instead, a system safety analysis is Σ a written argument, supported by evidence, that it is safe to operate a particular service, system or process Σ a document which – describes the service, system or process – lists the principal hazards associated with operation – links to each hazard the design safety principles and operating safety principles which govern safe operation, use, maintenance, etc. – references (or includes) compliance to regulatory requirements (e.g. compliance check-list) and assesses any deviations from these requirements. The safety assessment report should be Σ accurate and concise Σ easy to understand by those persons who need to make use of it Σ suit the purpose for which it was intended Σ be accessible and maintainable. 8.9 Discussion There are19 two primary causes of aircraft accidents: Σ operational, such as pilot error, weather and operating procedures Σ technical, such as design errors (including those that can cause pilot error), manufacturing errors, maintenance errors and part failures. The Level 4 (refer Fig. 8.1) system safety assessment is interesting in the latter (i.e. technical failure probability). The safety assessment is an evolving document that provides the evidence (i.e. justification) to support the modification activities leading up to certification – and any subsequent changes of design and operational use thereafter. It is the means for demonstrating that all the safety issues relating to the design and development task have been addressed. The system designer’s task of producing a safety assessment is not only to satisfy the airworthiness authorities. Although the assessment is aimed primarily at obtaining a balance between the probability of system failure and the associated effect, this is not the only benefit. In practice, it is the critical and logical scrutiny of the systems that is of most value, and not the precision of the numerical conclusions. The design itself is likely to benefit from lessons learned from a systematic assessment of safety (Rhys, 2002). In all cases involving integrated systems, the safety assessment process is of fundamental importance in establishing appropriate safety objectives for the system and determining that the implementation satisfies these objectives. The safety assessment process should be planned and managed to provide the necessary assurance that all relevant failure conditions have been identified and that all significant combinations of failures which could cause those failure conditions to have been considered. The safety assessment process is thus an inherent part of the design process and should be initiated at the earliest possible stage so that hazards are identified and dealt with while the opportunities for their exclusion exist. There is no doubt that a well-executed (i.e. complete, consistent and correct) safety assessment can provide a reasonable basis upon which system certification can be based. It must be remembered, however, that the analysis can only be as good as the failure cases it identifies, and the rates of failure predicted/assumed. The system should therefore be amenable to the safety assessment tools and techniques employed, so apply them appropriately. Chapter 9 The safety case 9.1 History The development of the Safety Case as a European approach to safety management can be traced though a series of major accidents. Aberfan, UK (21 Oct. 1966) Wet weather conditions in this Welsh mining village allowed a colliery waste tip to become unstable and, without warning, the whole tip (half a million tons of coal waste) slid down a hill and enveloped some buildings – including the village school. There were a total of 144 casualties, including 116 children (mostly aged between 7–10). This accident had a defining influence on UK legislation1 and led ultimately to the Health and Safety at Work Etc2 Act of 1974 (after the Flixborough disaster (Health and Safety Executive, 1975)). The fundamental effect of this legislation was to require companies to demonstrate that their works were safe and that workers were adequately trained. This was the start of the Safety Case concept (McLean (1997) and http://www.nuff.ox.ac.uk/politics/aberfan/eoafinal.htm). Flixborough, UK (1 June 1974) The Caprolactam plant was a modern well-designed facility. However, following modifications to the plant, a large-diameter pipe failed, leading to the release of 40 tons of pressurized cyclohexane, which drifted across the site until an ignition source was encountered. The explosion immediately killed 28 plant operators and damaged hundreds of homes. The accident could have been much worse had it not happened over the weekend when most employees were not at work. The investigation showed that: Σ The plant modification occurred without a full assessment of the potential consequences. Only limited calculations were undertaken on the integrity of the bypass line. No calculations were undertaken for the dog-legged shaped line or for the bellows. No drawing of the proposed modification was produced. Σ No pressure testing was carried out on the installed pipework modification. Σ Those concerned with the design, construction and layout of the plant did not consider the potential for a major disaster happening instantaneously. Σ The incident happened during start up when critical decisions were made under operational stress. In particular the shortage of nitrogen for inerting would tend to inhibit the venting of off-gas as a method of pressure control/reduction. This event led to the creation of the tri-partite (government, industry and union) Advisory Committee on Major Hazards, which developed a system for the regulation of major hazards in industry. Sevesco, Italy (10 July 1976) On 10 July 1976, a safety plate on a reactor vessel burst in an industrial plant during the manufacture of trichlorophenol (TCP) near Sevesco, Italy, releasing a mixture of chemicals including 2-,3-,7-,8-tetrachlorodibenzo-p-dioxin (TCDD). As a result, several thousand people, many animals and much of the surrounding vegetation in the Sevesco area were exposed to an aerosol of TCDD. Fear by the authorities for the health of local residents was justified by the known high toxicity of TCDD in animals and its ability to cause cancer under experimental conditions. As soon as results allowed for the definition of the contaminated area, an immediate evacuation of the people living within this region was ordered. Medical examinations of this potentially exposed population began immediately with long-term studies continuing up to the present day. The response to Sevesco (as well as other incidents such as Flixborough) was the ‘Sevesco Directive’ issued by the European Commission via Council Directive 96/82/EC of 9 December 1996 on the control of major-accident hazards involving dangerous substances. This directive aims at the prevention of major accidents which involve dangerous substances and the limitation of their consequences for man and the environment, with a view to ensuring high levels of protection throughout the Community in a consistent and effective manner. It requires the demonstration of safe design and operation of hazardous facilities. Herald of Free Enterprise, Belgium (6 Mar. 1987) A vehicle and passenger ferry capsized at the entrance to Zeebrugge harbor. The bow doors were not closed at departure from Zeebrugge because the responsible crew member had overslept, the supervising officer failed to check, and the master did not require a positive report – even though he could not see the doors from the bridge. Water flowed into the vehicle deck as the ship accelerated and the ship rolled from 0∞ to 90∞ within 90 seconds. There was no time to launch lifeboats or life rafts. The ship continuing airworthines management exposition to rest on a sandbank with the starboard side out of the water, from where survivors were rescued by helicopter. Of the estimated 539 people on board almost 200 died (the ship was certified to carry 1400 people). The owner and operator of the vessel were charged with corporate manslaughter, although legal technicalities prevented an eventual conviction. Mr. Justice Sheen’s inquiry expanded the horizon of disaster investigation by not only looking for the direct responsibility, but also looking for the systemic causes. Mr. Sheen’s summation established the principle that every employee – however far removed from the front line – bears some responsibility for their company’s safety record. This accident was instrumental in leading to the establishment of the International Safety Management (ISM) Code by the International Maritime Organization (Kuo, 1990). Kings Cross, UK (19 Nov. 1987) A passenger noticed a fire on one of the Piccadilly line escalators, which he reported to station staff. Seventeen minutes later the fire had developed so rapidly that a flashover occurred, spreading the fire and thick smoke into the ticket hall and surrounding subways. Thirty-one people died. The Fennell Investigation found several deficiencies in the management of safety by London Underground – made worse by the fact that there had been a history of similar fires with no remedial action initiated. In addition, there was a lack of emergency planning and command/control, and staff were inadequately trained to deal with fires and emergencies. The investigation recommended a more formal safety management system in London Underground Ltd. This initiated moves to transfer the Railway Inspectorate to the HSE from the Dept of Transport, and informal discussions commenced about implementing safety cases for the railways. Clapham Junction, UK (12 Dec. 1988) The driver of a Basingstoke to Waterloo train (carrying 700 passengers) stopped to telephone the signalman as he had noticed that the signals were malfunctioning. The signals behind his train should have automatically turned to danger, protecting his train while stationary. This did not occur, and a Poole to Waterloo train (carrying more than 500 passengers) ran into the back of the Basingstoke train. At the same time a third train (thankfully empty) was passing in the opposite direction on the adjacent track. The first two coaches of the Poole train were crushed between the oncoming and stationary trains. Thirty-five people were killed and 500 were injured. The fault was traced to a short circuit in a signal box. This short circuit originated during maintenance a fortnight before when a wire was disconnected but not insulated or tied down. The subsequent investigation found a succession of individual and system failures at all levels of British Rail. These ranged from poor working practices; to lack of supervision and effective management; to lack of necessary skills, competence and training. This accident accelerated the development and implementation by British Rail of its Total Quality Management Initiative with associated internal and external auditing. The Railway Inspectorate’s remit in monitoring safety of the railways was extended and transferred from the Department of Transport to the Health and Safety Executive in order to improve its degree of independence. This accident was instrumental in the establishment of a requirement for railway Safety Cases. The official inquiry report on the capsize of the Herald of Free Enterprise ferry included the following statements: A full investigation into the circumstances of the disaster leads inexorably to the conclusion that the underlying or cardinal faults lay higher up in the organisation. The Board of Directors did not appreciate their responsibility for the safety management of their ships. All concerned in management, from the members of the Board of Directors down to the junior superintendents, were guilty of fault in that all must be regarded as sharing responsibility for the failure of management. From the top to the bottom the body corporate was infected with the disease of sloppiness. It is apparent that the new top management has taken to heart the gravity of this catastrophe and the company has shown a determination to put its house in order. 9.1.1 Summary Until quite recently, only the people directly involved would have been held to blame for an accident. Now it is recognized that safety is everybody’s concern. All stakeholders have an obligation to assume responsibility. Key lessons learned from these disasters included (refer, inter alia, Kuo (1997a, Ch. 1): Σ Engineering: visibility is needed for decisions/assumptions that affect safety. However, it is also recognized that engineering alone cannot guarantee safety. Σ Operations: systems evolve as they make their operational application. Procedures and maintenance do affect safety. Frequent training can improve effectiveness. Σ Management: responsible for the development of a safety culture in their organizations by defining safety policies and allocating resources in the development thereof. Where an industry or activity is recognized as dangerous, it is common to have regulations instructing designers and operators what to do (and what not to do) to make it ‘safe.’ These rules and standards come from experts analyzing accidents that have occurred (or might occur) and how they could be prevented in the future. Often, the industry has a regulator or inspectorate that provides a license for operation only when the safety standards are followed. As seen from the examples above, this approach has not stopped major accidents from happening in regulated industries. Some of the regulatory failings include (David, 2001): Σ encouraging compliance only with minimal-level standards – there is no incentive to go beyond the safety standard defined in a regulation Σ making operators/designers jump through the regulation hoop without encouraging them to think about safety. This does not encourage a ‘safety culture’ within a company Σ making it difficult to apply to systems of novel technology – rules are struggling to keep up with engineering advances. A new approach was therefore called for, and this was the safety case. The major push in the development of the safety case concept was the tri-partite (i.e. government, industry and unions) Advisory Committee on Major Hazards (ACMH), which was formed after the Flixborough disaster. The most important and far-reaching of their recommendations was that owners of major hazardous sites/facilities should develop a living safety case. This safety case concept became widely adopted in other industries (e.g. UK MoD) as good HSWA practice. 9.2 Developing the requirement 9.2.1 Defining the safety case The Health and Safety Commission defines (refer, inter alia, JSP553, 1st edition (para 2.43))6 a safety case as: a suite of documents providing a written demonstration that risks have been reduced as low as is reasonably practicable. It is intended to be a living dossier which underpins every safety-related decision made by the licensee. In essence, the safety case is a documented description of the hazards that the operator of a system/facility faces and the means employed to control those hazards. It is the systematic and structured demonstration by a company to provide assurance, through comprehensive evidence and argument, that the company has an adequately safe operation. The company will have identified and assessed the hazards and safety risks and be able to demonstrate that they can manage them to levels which are as low as reasonably practicable. Note that it is a living dossier (i.e. the ‘through-life’ concept is implied, even though not always explicitly), which requires that it needs continuous management to ensure its currency and validity. A safety case must be reviewed continuously while the system is in operation. It is necessary to consider not only changes to the system itself (e.g. due to wear in tear) but also the possible effects of current/intended maintenance and operational practices. The safety case approach makes the system owner/manager responsible for proving that their activities are ‘safe’ – and continue to remain so. Every system owner/manager is thus required to undertake a formal assessment of the safety of their facility and develop a report which documented the hazards, safeguards, safety and management systems, and emergency response plans for the system/facility throughout its life cycle. The purpose of this safety case is to assess the hazards of the specific system/facility, review preventive measures, and define adequate risk management and emergency response procedures.7 This approach was a fundamental change from the prescriptive controls at the time, which had allowed owners to believe that they were safe based on compliance with these controls. Continuing accidents proved that this was an illusion. The application of modern technology is often complex, with many interactions and codependences between systems and people. It was unrealistic, therefore, to assume that government departments and industry associations would be able to continue to generate prescriptive regulations and codes of practice in adequate detail to meet the continuously changing technology. 9.2.2 Why have a safety case? The arguments against having a safety case range along the lines of: Σ The industry is already heavily regulated. Σ We already do everything safely. Σ We have enough procedures and systems already. Σ Most of our problems lie in human error and not in our systems. If these arguments are considered individually, research by Edwards8 has shown that: Σ The regulations establish the minimum requirements for safe operations. However, not everybody will always meet those minima in the execution of their job, as performance cannot always be 100%. Indeed, minimum standards are exactly that. To ensure an adequate buffer above the regulated minima to cater for the less effective employees or the mistakes they make, a company should establish its own standards, at least meeting the regulated minima; however, where additional risk exists, the standards should be above those set by the regulators. Σ Procedures do not, in themselves, solve the problem unless they are used systematically and reviewed frequently. Σ The working systems of companies are frequently found to be not sufficiently robust, and reported accidents show this to be a fact. Understanding the underlying causes of accidents or occurrences and the review of the actions subsequently taken supports this view. Many companies do not carry out systematic working reviews as part of the process of continuous improvement. Σ Human error is undoubtedly the most significant problem faced but it is often not the will of the individual to do wrong that is at fault, but a combination of company systems and cognitive shortfalls, coupled with real or perceived pressures, that underlie the majority of occurrences. 9.2.3 The relationship between the system safety assessment and the safety case Safety is not self-sustaining (SAE ARP5150). When a system (e.g. an aircraft) is delivered and in its pristine condition, it has an initial level of safety often justified by the prime contractor via some form of system safety assessment (i.e. managed at System Level 4 as illustrated in Fig. 8.1). However, the ongoing safety of a system depends on numerous factors, including the original design, manufacturing, operating crew and maintenance actions, operational and environmental effects, quality of parts, modifications and system managing (such as configuration control), etc. Once released into service, the system is continually evolving and changing. As it is operated, the level of safety is maintained through a continuing process of monitoring service experience, identifying safety-related issues and opportunities, and then addressing these issues or opportunities through appropriate product changes (e.g., repairs, modifications) or procedural changes (e.g., maintenance techniques and scheduling) or additional training. For these reasons, safety (i.e., maintaining and enhancing safety wherever possible) should also be continually reassessed during the ‘in-service’ phase of the product life cycle via the safety case (i.e., managed at System Level 5 or 6 as illustrated in Fig. 8.1). In contrast to the safety case (Level 5 or 6), the safety assessment (Levels 1 to 4) is usually applicable to one specific point in time only and is deliverable to the system manager/owner or a certifying/accepting authority upon initial system delivery. Concerning Section 5.2 and Appendix B (para 3.3), only a minority of accidents are due to technical failures, so the safety case (Level 5 or 6) should not duplicate the lower-level safety assessments. Rather, it should extract from these lower-level assessments the issues (e.g., hazards) and evidence required to manage the operator’s risk of an accident. A Safety Case receives its input from these lower-level Safety Assessments, which are updated only if specifically contracted. For instance, when in-service monitoring has identified an uncorrected assumption or some other deficiency (such as lower than expected MTBFs), or when the author of the Level 4 safety assessment is contracted to incorporate a new lower level assessment (e.g. such as for a sub-system upgrade). This distinction between a safety case and safety assessment may seem only like semantics, but the contractual expectations justify a clear distinction: Σ a safety case is much wider in scope and term of duration Σ the hazard identification process remains live throughout the product life cycle Σ the process must take into account any additional complexities and interdependencies which arise during integration, operational application, and disposal. To summarize, the safety case is a living dossier that needs continuous management to maintain its validity. Its relevance and accuracy must continue to be reviewed. It should be updated if: Σ the equipment/system is modified Σ There are changes in how or where it is used Σ there are changes in legislation or the safety requirements Σ there is a deviation between actual performance and design intention. Upon system delivery, the safety case should, therefore, fall within the remit of the system owner/manager/operator to ensure the application of the requisite ongoing management functions. 9.3 Core components The core components of a typical Level 5/6 (see Fig. 8.1) safety case are: 1. A safety argument, which summarises and justifies the claim that the system is adequately safe. This is discussed in Section 9.3.1 below. 2. A hazard log, which shows how identified hazards are managed, see Section 9.3.2 below. 3. A safety management system (SMS), which facilitates the above two points, see Section 9.3.3. The safety case is a live, ‘virtual’, document containing all of the above. It is never complete, because the hazard log should never be closed (unless the product ceases to exist) and the safety management system is a living process, tailored to effect the current roleplayers. When printed, the safety case report is a reflection of the safety case at a specific point in time (i.e. a snapshot). 9.3.1 Safety argument Within the safety case, the safety argument needs to be able to stand scrutiny by a court of law, and should show that the owner of the system (i.e. platform or facility) did what is reasonably practicable (for a person in their position) to ensure the prevention of an accident. A convincing argument safety case requires three elements (see Fig. 9.1): 1. Safety objective(s) or goals 2. Supporting evidence (e.g. FMECA, HAZOPs, etc.) 3. A clearly discernible ‘thread’ or argument that flows through the document (including any assumptions and justifications needed to support the argument). An argument can be provided in textual format but is likely to be cumbersome and, for complex arguments, the ‘devil may get lost in the detail’. It is easier with pictures – especially if the picture ‘carries’ the reader through the argument with sufficient, judiciously placed ‘stepping stones’ (i.e. sub-goals and sub-arguments down to an inevitable solution). Goal-structured notation (GSN), see Appendix A, could be usefully applied here as it illustrates a ‘discernible thread’ to sustain a logical argument. 9.3.2 Hazard log Clearly, without a robust list of hazards that require management (relevant to the activities covered in the safety case), the operator cannot be assured that effective controls have been established. A central part of any safety case is the capture, assessment, and management of hazards. This is often done in some form of hazard log. The hazard log is considered by the UK MoD as one of the most important tools for managing safety. The MoD defines the hazard log as a record of all significant hazards identified, and which acts as a directory for the safety justification by providing a summary of all safety activities11 throughout the product life cycle. This hazard log provides traceability of how safety issues have been dealt with. Outstanding issues should be regularly reviewed by the project safety panel to make sure that safety-related actions are completed and unacceptable risks are resolved/mitigated. The hazard log should contain all possible hazards and accidents for the system. This includes those that are considered tolerable and those regarded as not credible (such as an accident caused by volcanic security and hazardous materials safety or an earthquake). The hazard log will show that they have been considered and provide the audit trail of reasons why they are closed. Should the circumstances change, for example, if the system is to be used in an earthquake zone, then the safety argument can be re-examined. In order to compile a sensible list of hazards (i.e. sources of harm), a clear understanding will be required of what is meant by the term ‘hazard’. As discussed in Chapter 5, a hazard is something that can lead to an undesirable outcome in the process of meeting an objective. It is any situation with the potential for human injury, damage to property/assets or the environment. It is a set of conditions in the operation of a product with the potential for initiating or contributing to events that could result in personal injury, damage to property or harm to the environment. Hazards are properties (states) of an entire system and may be defined at any level (see Fig. 8.1). However, it is essential to select the right level. A common fault is to select it too low, which results in too many hazards, no system properties, being expensive (or impossible) to track, and over-engineering. If you select it too high, then it is hard to ensure complete management. Example: hazard identification in aircraft Is ‘flight management system (FMS) failure’ or ‘navigation display error’ a hazard? No, the real hazard is ‘loss of situational awareness.’ Distinguishing between hazards and their causes will assist in this regard, with the above two failures contributing causes. Note that there are other causes of this hazard, e.g., Coriolis illusions (caused by large head/eye movement during instrument scanning due to badly placed instruments), primary flight display (e.g., artificial horizon) failure, etc. From a technical airworthiness perspective, a number of factors and inherent dangers exist, particularly for military aviation, that may influence the achievement of an acceptable level of aviation safety, including the following: Σ Aircraft are very complex and highly integrated with a multitude of critical systems involving interfaces between hardware, software, and operators Σ As aviation technology advances, this complexity will increase, introducing new hazards and failure conditions Σ aircraft are required to operate in a very demanding environment, especially in the military aircraft types Σ weight restrictions require aircraft designs to be optimized with minimum margins of safety Σ Redundancy is often considered an unaffordable luxury, especially for military aircraft types Σ Design restrictions (e.g., space, weight, etc.) often place limitations on safety measures actual testing under realistic environmental conditions is not possible in all cases, especially for systems which involve software Σ despite testing, unexpected hazardous conditions (e.g. CFIT, flutter, stores separation, birdstrike, etc.) may occur Σ other imperatives, such as mission accomplishment, available financial resources and schedule constraints may at times conflict with the technical airworthiness rules and standards. It is therefore the objective of the hazard log, as a management13 tool, to track the identification, mitigation and acceptance of risk and also the control of residual risks associated with the operation. The hazard log is a live document which, throughout the life of the product provides an auditable record of the management of hazards for the specific system/facility/operation/activity. It should be a database that contains information to show how safety issues are being dealt with and resolved.14 The big advantage of the hazard log is that all risks can be compared and prioritized, and (via the ‘Pareto Principle’) the operator can prioritize the hazard management effort on the most likely causes of an accident. 9.3.3 Safety Management System An essential part of any safety case is a safety management system (SMS). A safety case may cover all or part of an operation, and therefore, there may be several safety cases, but each will be managed by a single corporate safety management system (see Chapter 12). The choice of how the safety cases are delineated is made by the safety management system in such a manner that the resulting package (see Fig. 9.2) of safety cases covers all safety-critical activities. Each safety case is subordinate to its corporate safety management system (SMS), but used nevertheless to interact with the SMS. This results in each safety case, based on a specific part of the company’s operation, using the safety management system to assure and control hazards and receiving much of its input from narrowly scoped16 safety assessments. Furthermore, the safety case should provide safety requirements/input/criteria for any future modification to the platform via the safety management system. Example An operator may require a contractor to install and certify the installation of a single standby instrument (for attitude, airspeed, and altitude display). The operator’s safety case would have identified the hazard ‘loss of situational awareness,’ and one of the contributing causes will be ‘loss of primary flight data.’ Loss of primary flight data will require loss of the primary display and loss of the standby. Assuming the safety case allocated ‘loss of primary flight data’ an acceptable probability of 1 x 10–9 per flight hr, and the probability of loss of primary display has proven to be 1 x 10–7 per flight hour, then the safety case would accept a single standby instrument with a failure probability of 1 x 10–2 per flight hour (i.e. mean time between failures = 100 h). Personnel associated with the design, manufacture, maintenance and material support of aeronautical products may be required to make a decision or recommendation involving a balance between aviation safety requirements and other imperatives such as cost, operational requirements, etc.). This is the role of the safety management system. As with a case in law, the safety case is a body of evidence presented as a reasoned argument. Unlike most areas of the law, the designers and operators are not presumed innocent until proven guilty; the safety case must proactively prove that a system is safe. The operator’s safety management system is an important part of this evidence, as it demonstrates an ongoing commitment to continuous safety monitoring and improvement. Safety management is intended to bring together all the facets of safety including engineering design, risk assessment (includes hazard identification, control and risk reduction), training, operation, maintenance, upkeep and disposal. Many modern systems are very complex and the consequences of possible accidents for them are enormous in scale. It is seldom possible to rely simply on designs and practices which have been ‘safe’ in the past. It is recognised that there is no such thing as absolute safety. Design and maintenance standards are established to ensure a minimum ‘acceptable’ level of safety is achieved, determined by such factors as community expectations; public, industry and government preparedness to pay; relativity of safety levels in other fields affecting overall safety, etc. These factors are not necessarily quantifiable. Whilst efforts are made to do so, inevitably judgements based on experience must be made and continually reassessed. In other words, there is a risk; the judgement as to the acceptable level of risk is one of the prime functions of safety management. Investigations of accidents show that there are often common themes as to why they happen: Σ problems which have shown up as minor incidents but were never addressed Σ no-one ever imagined that the circumstances of the accident could happen, so there were no systems or emergency procedures to deal with them Σ people thinking that it is someone else’s job to deal with safety Σ sloppy work practices building up over time (e.g. because they are easier and cheaper to do) Σ equipment being modified or used outside of their design intent Σ people not reporting safety concerns because a blame culture exists in the organisation. Safety management attempts to deal with these common root causes by emphasizing a proactive approach; prevention, rather than reacting to harm once it has occurred. It is essential that a management system has procedures in place to identify and manage major hazards. Hazard management should consider methods of prevention, detection, control, and mitigation to reduce the risks to ‘as low as reasonably practicable’ (ALARP). A simple way of understanding the safety management system is to consider five basic questions (refer inter alia, Kuo (1997a) Ch. 6). 1. What could go wrong? (i.e., hazard identification and analysis) 2. What are the chances of it going wrong? (i.e., probability assessment) 3. How bad could it be? (i.e., risk assessment) 4. What has been done about it? (i.e., hazard reduction/control plus supporting evidence) 5. What if it happens? (i.e., emergency and contingency arrangements) The safety case should provide the answer to these questions. The safety management system should be the enabler. 9.4 The safety case report In Section 9.2.1 we defined a safety case as a structured body of evidence that provides a convincing and valid argument that a system is adequately safe for a given application in a given environment. This ‘evidence’ is collated and presented in a safety case report, which is a snapshot of a defined point in time. It usually provides a traceable reference to evidence in test results, detailed safety analysis reports, etc. The size and scope of a safety case can vary enormously and will be appropriate to the system complexity and the level of risk involved. Throughout the life of a system/facility/operation it will be necessary to abstract evidence from the safety case and present it in the form of a safety case report to support life-cycle milestones. Each safety case report will present a safety argument (a reasoned justification) that a safety claim or target has been met. The safety case report should be readable by non-safety experts but have sufficient detail to assist senior management to review performance and provide authority to either proceed from one stage of the product life cycle to the next, or to fund/prioritise appropriate changes where required. Angove (1999) advises that a safety case report does not have to be a large set of documentation compiled at huge cost. Rather, a safety case report should be viewed as the result of a through-life practical and iterative development of evidence relating to safety risks and their tolerability. The following sub-sections provide information that will typically be contained in a safety case report. 9.4.1 Defining the system In order to validate the safety of a system, we first need to provide an accurate definition of the system and description of its operation. Depending on the scope (see Section 9.4.2) of the safety case, this description will need to include issues such as: Σ the specific equipment/system: – hierarchy and interface with other systems – functionality – configuration – build standard – performance Σ the operating environment: – operating limits, flight conditions and envelope – the operational scenario – sortie profiles – any plausible environmental conditions – role changes (if applicable) Σ the maintenance environment Σ the design and certification authorities involved to date. 9.4.2 Aim, scope and objectives of the safety case The next step for a successful safety case it to define its scope, aim, and objectives: (a) Scope defines the extent of the safety case. In effect, we are establishing boundaries for our responsibilities. If the boundaries are not clear to everyone involved in the assessment, some vital parts may be overlooked. Boundaries help with responsibility allocation, especially when products from sub-contractors are integrated into a more complex system. Furthermore, the scope may include activities during the development phase or may be limited to continuing the effort started during the aircraft design phase by beginning with the new type’s introduction into service and continuing until retirement. It may also address a system in use for which there has been no official safety strategy or safety management system to date. Example scope A complete aircraft safety case would typically be scoped to address the safety of the platform (inclusive of its on-board systems and ground/test equipment); safety of operations; and safety during all maintenance activities. (b) The aim of the safety case provides us with the general intent. Example aim ‘to ensure the aircraft is modified, maintained and operated with its intended level of safety throughout its operating life’. (c) Objectives are measurable results against which the success of the safety case will be evaluated. These objectives are obviously dependent on the scope and aim. Example objectives for an MoD aircraft Σ Hazards have been, and are being, systematically identified. Σ All identified hazards have been incorporated into the hazard log. Σ All risks have been prioritized and reduced to ALARP. Σ The cumulative probability of loss of an aircraft due to technical fault and the cumulative probability of the aircraft (inclusive of its systems, structures, and stores) which could result in the death of any aircrew or passengers, has been assessed to be of the order of one in a million per flying hour (probability of occurrence 1 x 10–6 per flying hour) when operated within the conditions used for the airworthiness demonstration. 9.4.3 Safety requirements and safety criteria Define all applicable safety requirements, targets, and objectives that guide the hazard assessment process and are used to judge the acceptability of the hazard (see Appendix B for example, safety criteria). The necessity for this step is explained in the following discussion. Discussion on safety criteria in safety cases Consider a system that has undergone numerous upgrades and modifications during its operational life. The owner/operator is supposed to be responsible for the safety case and demands safety assessments from contractors/suppliers/system integrators. However, there are few examples in industry of how the various safety assessments conducted by the different contractors are integrated into the operator’s safety case, or how in-service monitoring of all these safety assessments is efficiently accomplished. I suspect that the main contributory factor is that the operator’s safety case (and especially its safety criteria) seldom drives the approach taken by externally supplied safety assessments. If a safety assessment has different safety criteria and, even more so, if a different approach (see Chapter 1 Section 1.4) is taken to justify an adequate level of safety, then managing the safety case is bound to be complicated. JSP430 advises that: The Safety Case is to be prepared in outline at presentation of the Staff Requirement and is to be updated at each major procurement milestone up to and including hand-over from the procurement to the maintenance authority…ideally there should be a seamless development of the Safety Case from one phase to the next. This should perhaps be extended also to say: ‘Furthermore, the safety case should provide safety requirements/input/criteria for any future modification to the platform. This should not be limited to the usual criteria for risk assessment, but should also provide safety targets for specific failures/events based upon known accident sequences.’ Bear in mind that the goal-based safety criteria used to satisfy JAR25.1309 and JSP553 para 1.38 is not directly compatible with the risk-based approach required by the safety case. In this case, the former should rather be used to identify contributing factors within a sequence of events that may lead to an accident. The goal-based approach is thus a useful source of failure/event inputs, along with their probability of occurrence. 9.4.4 Safety case strategy/approach/argument The purpose of a safety case can be defined in the following terms (Kelly and Weaver, 1994): ‘A safety case should communicate a clear, comprehensive and defensible argument that a system is acceptably safe to operate in a particular context.’ The following are important aspects of this definition: Σ ‘argument’ – above all, the safety case exists to communicate an argument. It is used to demonstrate how someone can reasonably conclude that a system is acceptably safe from the evidence available. Σ ‘clear’ – a safety case is a device for communicating ideas and information, usually to a third party (e.g. a regulator). In order to do this convincingly, it must be as clear as possible. Σ ‘system’ – the system can be anything, see Fig. 8.1 for more information. Σ ‘acceptably’ – absolute safety is an unobtainable goal (see Chapter 1). Safety cases are there to convince someone that the system is safe enough (when compared against some definition or notion of tolerable risk). Σ ‘context’ – context-free safety is impossible to argue. Almost any system can be unsafe if used in an inappropriate or unexpected manner. A robust safety case needs to define/identify the context within which safety is to be argued. The Safety Case should, therefore, clearly describe the approach and arguments and reference the evidence used to justify the safety of the system so that agreement can be reached on the validity of the conclusions. The safety argument should be structured hierarchically so that this safety justification can be summarized in a safety case report. The safety argument should be developed from the safety objectives defined in Section 9.4.2. It should present the case supporting the use of the system in the defined roles and environments, giving the outstanding risks. For more on the safety argument, see Appendix C. 9.4.5 Risk analysis The hazard log (see Section 9.3.2) is to include a description of all identified hazards and potential accidents, the relevant mitigation, their safety risk and acceptability, any actions to be taken to reduce the risk, and reference the supporting safety analyses. Ideally all hazards would have been designed out of the system and there would be no risk to consider. In practice this is rarely the case. The hazard log needs to be coupled to logical decision process and the following steps are essential in the development of a hazard log (see Fig. 9.3). Σ identify conditions and situations that may result in an accident Σ provide a measure of the risk by determining the severity and the probability of the accident occurring (see Chapter 6) Σ control the implementation of risk reduction measures so as to ensure that the risk of an accident remains as low as reasonably possible Σ accept the level of risk Σ track the risk to make sure it does not change. The hazard log is therefore used to determine the risk of each hazard turning into an accident (see Fig. 6.2). There is an important decision that senior management must make as to the level of risk the company will accept in order to manage the hazards identified. The as low as reasonably possible principle demands that if a control is technically possible, is reasonable to do and can be achieved without causing financial distress to the company, then those controls should be set in place. 9.4.6 Recommendations and Limitations The safety case report should provide a list of recommendations and limitations needed to ensure that the required level of safety is retained. Particular issues that should be provided include: Σ emergency/contingency arrangements to cater for certain foreseeable accidents. Σ any operational or other limitations that may be necessary for risk mitigation Σ minimum check and maintenance intervals for all system elements (including personnel training) considered in the safety analysis, particularly where exposure time to latent failure conditions is critical Σ action, if any, relevant to outstanding risks Σ procedures for safety case maintenance in the light of changes to the system, its operational role or environment. 9.5 Discussion The safety case is thus a documented description of the hazards that the operator of a system/facility faces and the means employed to control those hazards. It is the systematic and structured demonstration by a company to provide assurance, through comprehensive evidence and argument, that the company has an adequately safe operation. The company will have identified and assessed the hazards and safety risks and will be able to demonstrate that they can manage them to levels that are as low as reasonably practicable. Safety cases can be considered the tangible products of an effective safety management system. The intangible product is a safer system. Note that the safety case was never intended to replace the OEM’s safety assessment, which is intended to support the certification of the airworthiness of the platform. However, the safety case does need a safety assessment to mitigate the probability of operational hazards turning into accidents. In an ideal world, there should be: Σ a live safety assessment (Level 4) maintained by the original equipment manufacturer reflecting the current build standard of the aircraft. This will contain much proprietary data and would require a specific contract with the original equipment manufacturer and all third-party modification agencies to ensure it reflects the current build standard. Σ a live safety case (Level 5/6) generated by the operator, which proves that the intended level of safety (i.e., as designed) is actually accomplished in service and that all operator-related hazards (e.g., CFIT, maintenance error, etc.) are being identified and managed. Safety cases tend to be very large documents, containing complex internal interdependencies, which include the results of a wide range of related analyses. They often rest upon a number of implicit assumptions and have a long lifespan, going through many revisions in the course of their production. Both product and process issues need to be addressed in the safety case. It must be shown both that: Σ the system meets its safety requirements and Σ that the processes for deriving the requirements, constructing the system, and assessing the system are of appropriate integrity. The safety analyses that appear in the safety case depend crucially upon the formulation of suitable models of the system at various levels of abstraction produced during the development process. Given these characteristics, it is not surprising that safety cases are difficult and expensive to produce and maintain. Not all safety cases are acceptable. The HSE has reviewed many safety cases in its role as regulator, and some of the problems it has found with poor examples include (David, 2002): Σ They contain assertions rather than reasoned arguments. Σ There are unjustified and implicit assumptions. Σ Some major hazards have not been identified and are therefore never studied. Σ There is a poor treatment of uncertainty of data and sensitivity of the assessments to this. Σ They do not deal well with human factors. Σ They do not deal well with software. Σ There is inadequate involvement of senior management. Σ Ownership of the safety case is not always clear. Furthermore, many safety cases contain a great deal of evidence from the various safety analysis techniques employed, but they do not always draw this evidence together in a clear and understandable manner. Dr. Tim Kelly (University of York) is known to have said, ‘An assertion (or argument) without evidence is unsubstantiated, and evidence without an argument is unexplained.’ There should be a clearly discernible thread of argument that flows through the whole safety case. A properly structured safety case argument (see Section 9.3.1) will go a long way to improving the quality and completeness of the safety case. The user must be involved in safety throughout the life cycle, from setting appropriate safety requirements through to managing residual risk and feeding back information on shortfalls in service use. It is the operator who will be exposed to most safety risks in service, so it seems logical that they must have a major role in accepting the level of risk they will be prepared to tolerate for the benefits the new equipment will bring (Rhys, 2002). Any safety margins should also be made explicit. The end users need not be given the full safety case since they do not need to know all the information contained in it. However, the parts dealing with emergency arrangements and with limitations for safe use must be available to them (e.g., through updated operational/maintenance manuals).