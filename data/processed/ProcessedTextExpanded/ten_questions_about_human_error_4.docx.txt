Title: Ten Questions About Human Error: A New View Of Human Factors And System Safety Chapter 4 Author(s): Sidney W. A. Dekker Category: Analysis, Human, Factors, Errors Tags: human, error, factors, accident Human factors as a discipline take a very realistic view. It lives in a world of real things, of facts and concrete observations. It presumes the existence of an external world in which phenomena occur that can be captured and described objectively. In this world there are errors and violations, and these errors and violations are quite real. The flight-deck observer from chapter 3, for example, would see that pilots do not arm the spoilers before landing and marks this up as an error or a procedural violation. The observer considers his observation quite true, and the error quite real. Upon discovering that the spoilers had not been armed, the pilots themselves too may see their omission as an error, as something that they missed but should not have missed. But just as it did for the flight-deck observer, the error becomes real only because it is visible from outside the stream of experience. From the inside of this stream, while things are going on and work is being accomplished, there is no error. In this case there are only procedures that get inadvertently mangled through the timing and sequence of various tasks. And not even this gets noticed by those applying the procedures. Recall how Feyerabend (1993) pointed out that all observations are ideational, that facts do not exist without an observer wielding a particular theory that tells him or her what to look for. Observers are not passive recip­ ients, but active creators of the empirical reality they encounter. There is no clear separation between observer and observed. As said in chapter 3, none of this makes the error any less real to those who observe it. But it does not mean that the error exists out there, in some independent empirical universe. This was the whole point of ontological relativism: What it means to be in a particular situation and make certain observations is quite flexible and connected systematically to the observer. None of the possible worldviews can be judged superior or privileged uniquely by empirical data about the world, because objective, impartial access to that world is impossible. Yet in the pragmatic and optimistically realist spirit of human factors, error counting methods have gained popularity by selling the belief that such impartial access is possible. The claim to privileged access lies (as modernism and Newtonian science would dictate) in method. The method is strong enough to discover errors that the pilots themselves had not seen. Errors appear so real when we step or set ourselves outside the stream of experience in which they occur. They appear so real to an observer sitting behind the pilots. They appear so real to even the pilot himself after the fact. But why? It cannot be because the errors are real, since the autonomy principle has been proven false. As an observed fact, the error only exists by virtue of the observer and his or her position on the outside of the stream of experience. The error does not exist because of some objective empirical reality in which it putatively takes place, since there is no such thing and if there was, we could not know it. Recall the air-traffic control test of chapter 3: Actions, omissions, and postponements related to air-traffic clearances carry entirely different meanings for those on the inside and on the outside of the work experience. Even different observers on the outside cannot agree on a common denominator because they have diverging backgrounds and conceptual looking glasses. The autonomy principle is false: facts do not exist without an observer. So why do errors appear so real? ERRORS ARE ACTIVE, CORRECTIVE INTERVENTIONS IN HISTORY Errors are an active, corrective intervention in (immediate) history. It is im­ possible for us to give a mere chronicle of our experiences: Our assumptions, past experiences and future aspirations cause us to impress a certain organization on that which we just went through or saw. Errors are a powerful way to impose structure onto past events. Errors are a particular way in which we as observers (or even participants) reconstruct the reality we just experienced. Such reconstruction, however, inserts a severe discontinuity between past and present. The present was once an uncertain, perhaps vanishingly improbable, future. Now we see it as the only plausible outcome of a pretty deterministic past. Being able to stand outside an unfolding sequence of events (either as participants from hindsight or as observers from outside the setting) makes it exceedingly difficult to see how unsure we once were (or could have been if we had been in that situation) of what was going to happen. History as seen through the eyes of a retrospective outsider (even if the same observer was a participant in that history not long ago) is substantially different from the world as it appeared to the decision makers of the day. This endows history, even immediate history, with a determinism it lacked when it was still unfolding. Errors, then, are expost facto constructs. The research base on the hindsight bias contains some of the strongest evidence on this. Errors are not empirical facts. They are the result of outside observers squeezing nowknown events into the most plausible or convenient deterministic scheme. In the research base on hindsight, it is not difficult to see how such retrospective restructuring embraces a liberal take on the history it aims to re­ count. The distance between reality as portrayed by a retrospective observer and as experienced by those who were there (even if these were once the same people) grows substantially with the rhetoric and discourse employed and the investigative practices used. We see a lot of this later in the discussion. We also look at developments in psychology that have (since not so long ago) tried to get away from the normativist bias in our understanding of human performance and decision making. This intermezzo is necessary because errors and violations do not exist without some norm, even if implied. Hindsight of course has a powerful way of importing criteria or norms from outside people's situated contexts, and highlighting where actual performance at the time fell short. To see errors as expost constructs rather than as objective, observed facts, we have to understand the influence of implicit norms on our judgments of past performance. Doing without errors means doing with normativism. It means that we cannot question the accuracy of insider accounts (something human factors consistently does, e.g., when it asserts a "loss of situation awareness"), as there is no objective, normative reality to hold such accounts up to, and relative to which we can deem them accurate or inaccurate. Reality as experienced by people at the time was reality as it was experienced by them at the time, full stop. It was that experi­ enced world that drove their assessments and decisions, not our (or even their) retrospective, outsider rendering of that experience. We have to use local norms of competent performance to understand why what people did made sense to them at the time. Finally, an important question we must look ahead to: Why is it that errors fulfill such an important function in our reconstructions of history, of even our own histories? Seeing errors in history may actually have little to do with historical explanation. Rather, it may be about controlling the future. What we see toward the end of this chapter is that the hindsight bias may not at all be about history, and may not even be a bias. Retrospective reconstruction, and the hindsight bias, should not be seen as the primary phenomenon. Rather, it represents and serves a larger purpose, answering a highly pragmatic concern. The almost inevitable urge to highlight past choice moments (where people went the wrong way), the drive to identify errors, is forward looking, not backward looking. The hindsight bias may not be a bias because it is an adaptive response, an oversimplification of history that primes us for complex futures and allows us to project simple models of past lessons onto those futures, lest history repeat itself. This means that retrospective recounting tells us much more about the observer than it does about reality—if there is such an objective thing. Making Tangled Histories Linear The hindsight bias (Fischoff, 1975) is one of the most consistent biases in psychology. One effect is that "people who know the outcome of a complex prior history of tangled, indeterminate events, remember that history as being much more determinant, leading 'inevitably' to the outcome they al­ ready knew" (Weick, 1995, p. 28). Hindsight allows us to change past indeterminacy and complexity into order, structure, and oversimplified causal­ ity (Reason, 1990). As an example, take the turn towards the mountains that a Boeing 757 made just before an accident near Cali, Colombia in 1995. According to the investigation, the crew did not notice the turn, at least not in time (Aeronautica Civil, 1996). What should the crew have seen in order to know about the turn? They had plenty of indications, according to the manufacturer of their aircraft: Indications that the airplane was in a left turn would have included the follow­ ing: the EHSI (Electronic Horizontal Situation Indicator) Map Display (if se­ lected) with a curved path leading away from the intended direction of flight; the EHSI vhf ominidirectional range display, with the CDI (Course Deviation Indicator) displaced to the right, indicating the airplane was left of the direct Cali vhf ominidirectional range course, the EaDI indicating approximately 16 degrees of bank, and all heading indicators moving to the right. Additionally the crew may have tuned Rozo in the automatic direction finder and may have had bearing pointer information to Rozo nondirectional beacon on the RMDI. (Boeing Commercial Airplane Group, 1996, p. 13) This is a standard response after mishaps: Point to the data that would have revealed the true nature of the situation. In hindsight, there is an over­ whelming array of evidence that did point to the real nature of the situation, and if only people had paid attention to even some of it, the outcome would have been different. Confronted with a litany of indications that could have prevented the accident, we wonder how people at the time could not have known all of this. We wonder how this "epiphany" was missed, why this bloated shopping bag full of revelations was never opened by the people who most needed it. But knowledge of the critical data comes only with the omniscience of hindsight. We can only know what really was critical or highly relevant once we know the outcome. Yet if data can be shown to have been physically available, we often assume that it should have been picked up by the practitioners in the situation. The problem is that pointing out that something should have been noticed does not explain why it was not noticed, or why it was interpreted differently back then. This confusion has to do with us, not with the people we are investigating. What we, in our reaction to failure, fail to appreciate is that there is a dissociation between data availability and data observability—between what can be shown to have been physically available and what would have been observable given people's multiple in­ terleaving tasks, goals, attentional focus, expectations, and interests. Data, such as the litany of indications in the previous example, do not reveal themselves to practitioners in one big monolithic moment of truth. In situations where people do real work, data can get drip-fed into the operation: a little bit here, a little bit there. Data emerges over time. Data may be uncertain. Data may be ambiguous. People have other things to do too. Sometimes the successive or multiple data bits are contradictory, often they are unremarkable. It is one thing to say how we find some of these data important in hindsight. It is quite another to understand what the data meant, if anything, to the people in question at the time. The same kind of confusion occurs when we, in hindsight, get an impression that certain assessments and actions point to a common condition. This may be true at first sight. In trying to make sense of past performance, it is always tempting to group individual fragments of human performance that seem to share something, that seem to be connected in some way, and connected to the eventual outcome. For example, "hurry" to land was such a leitmotif extracted from the evidence in the Cali investigation. Haste in turn is enlisted to explain the errors that were made: Investigators were able to identify a series of errors that initiated with the flightcrew's acceptance of the controller's offer to land on runway 19 ... The cockpit voice recorder (Cockpit Voice Recorder) indicates that the decision to accept the offer to land on runway 19 was made jointly by the captain and the first officer in a 4-second exchange that began at 2136:38. The captain asked: "would you like to shoot the one nine straight in?" The first officer responded, 'Yeah, we'll have to scramble to get down. We can do it." This interchange followed an earlier discussion in which the captain indicated to the first officer his desire to hurry the arrival into Cali, following the delay on departure from Miami, in an apparent to minimize the effect of the delay on the flight attendants' rest requirements. For example, at 2126:01, he asked the first officer to "keep the speed up in the descent" . . . (This is) evidence of the hurried nature of the tasks performed. (Aeronautica Civil, 1996, p. 29) In this case the fragments used to build the argument of haste come from over half an hour of extended performance. Outside observers have treated the record as if it were a public quarry to pick stones from, and the accident explanation the building he needs to erect. The problem is that each fragment is meaningless outside the context that produced it: Each fragment has its own story, background, and reasons for being, and when it was produced it may have had nothing to do with the other fragments it is now grouped with. Moreover, behavior takes place in between the fragments. These intermediary episodes contain changes and evolutions in perceptions and assessments that separate the excised fragments not only in time, but also in meaning. Thus, the condition, and the constructed linearity in the story that binds these performance fragments, does not arise from the circumstances that brought each of the fragments forth; it is not a feature of those circumstances. It is an artifact of the outside observer. In the case just described, hurry is a condition identified in hindsight, one that plausibly couples the start of the flight (almost 2 hours behind schedule) with its fatal ending (on a mountainside rather than an airport). Hurry is a retro­ spectively invoked leitmotif that guides the search for evidence about itself. It leaves the investigator with a story that is admittedly more linear and plausible and less messy and complex than the actual events. Yet it is not a set of findings, but of tautologies. Counterfactual Reasoning Tracing the sequence of events back from the outcome—that we as outside observers already know about—we invariably come across joints where peo­ ple had opportunities to revise their assessment of the situation but failed to do so, where people were given the option to recover from their route to trouble, but did not take it. These are counterfactuals—quite common in accident analysis. For example, "The airplane could have overcome the windshear encounter if the pitch attitude of 15 degrees nose-up had been maintained, the thrust had been set to 1.93 EPR (Engine Pressure Ratio) and the landing gear had been retracted on schedule" (NTSB, 1995, p. 119). Counterfactuals prove what could have happened if certain minute and often Utopian conditions had been met. Counterfactual reasoning may be a fruitful exercise when trying to uncover potential countermeasures against such failures in the future. But say­ ing what people could have done in order to prevent a particular outcome does not explain why they did what they did. This is the problem with counterfactuals. When they are enlisted as explanatory proxy, they help circumvent the hard problem of investigations: finding out why people did what they did. Stressing what was not done (but if it had been done, the accident would not have happened) explains nothing about what actually happened, or why. In addition, counterfactuals are a powerful tributary to the hindsight bias. They help us impose structure and linearity on tangled prior histories. Counterfactuals can convert a mass of indeterminate actions and events, themselves overlapping and interacting, into a linear series of straightforward bifurcations. For example, people could have perfectly executed the go-around maneuver but did not; they could have denied the runway change but did not. As the sequence of events rolls back into time, away from its outcome, the story builds. We notice that people chose the wrong prong at each fork, time and again—ferrying them along inevitably to the outcome that formed the starting point of our investigation (for without it, there would have been no investigation). But human work in complex, dynamic worlds is seldom about simple di­ chotomous choices (as in: to err or not to err). Bifurcations are extremely rare—especially those that yield clear previews of the respective outcomes at each end. In reality, choice moments (such as there are) typically reveal multiple possible pathways that stretch out, like cracks in a window, into the ever denser fog of futures not yet known. Their outcomes are indeterminate, hidden in what is still to come. In reality, actions need to be taken under uncertainty and under the pressure of limited time and other re­ sources. What from the retrospective outside may look like a discrete, leisurely two-choice opportunity to not fail, is from the inside really just one fragment caught up in a stream of surrounding actions and assessments. In fact, from the inside it may not look like a choice at all. These are often choices only in hindsight. To the people caught up in the sequence of events, there was perhaps not any compelling reason to reassess their situation or decide against anything (or else they probably would have) at the point the investigator has now found significant or controversial. They were likely doing what they were doing because they thought they were right, given their understanding of the situation, their pressures. The challenge for an investigator becomes to understand how this may not have been a discrete event to the people whose actions are under investigation. The investigator needs to see how other people's decisions to continue were likely nothing more than continuous behavior—reinforced by their current understanding of the situation, confirmed by the cues they were focusing on, and reaffirmed by their expectations of how things would develop. Judging Instead of Explaining When outside observers use counterfactuals, even as explanatory proxy, they themselves often require explanations as well. After all, if an exit from the route to trouble stands out so clearly to outside observers, how was it possible for other people to miss it? If there was an opportunity to recover, to not crash, then failing to grab it demands an explanation. The place where observers often look for clarification is the set of rules, professional standards, and available data that surrounded people's operation at the time, and how people did not see or meet that which they should have seen or met. Recognizing that there is a mismatch between what was done or seen and what should have been done or seen—as per those standards—we easily judge people for not doing what they should have done. Where fragments of behavior are contrasted with written guidance that can be found to have been applicable in hindsight, actual performance is of­ ten found wanting; it does not live up to procedures or regulations. For example, "One of the pilots . .. executed [a computer entry] without having verified that it was the correct selection and without having first obtained approval of the other pilot, contrary to procedures" (Aeronautica Civil, 1996, p. 31). Investigations invest considerably in organizational archeology so that they can construct the regulatory or procedural framework within which the operations took place, or should have taken place. Inconsistencies between existing procedures or regulations and actual behavior are easy to expose when organizational records are excavated after the fact and rules uncovered that would have fit this or that particular situation. This is not, however, very informative. There is virtually always a mismatch between actual behavior and written guidance that can be located in hindsight. Pointing out a mismatch sheds little light on the why of the behavior in question, and, for that matter, mismatches between procedures and practice are not unique to mishaps. There are also less obvious or undocumented standards. These are often invoked when a controversial fragment (e.g., a decision to accept a runway change, Aeronautica Civil, 1996, or the decision to go around or not, NTSB, 1995) knows no clear preordained guidance but relies on local, situated judgment. For these cases there are always supposed standards of good practice, based on convention and putatively practiced across an entire industry. One such standard in aviation is "good airmanship," which, if nothing else can, will explain the variance in behavior that had not yet been accounted for. When micromatching, observers frame people's past assessments and actions inside a world that they have invoked retrospectively. Looking at the frame as overlay on the sequence of events, they see that pieces of behavior stick out in various places and at various angles: a rule not followed here, available data not observed there, professional standards not met over there. But rather than explaining controversial fragments in relation to the circumstances that brought them forth, and in relation to the stream of preceding as well as succeeding behaviors that surrounded them, the frame merely boxes performance fragments inside a world observers now know to be true. The problem is that this after-the-fact-world may have very little relevance to the actual world that produced the behavior under study. The behavior is contrasted against the observer's reality, not the reality surrounding the behavior at the time. Judging people for what they did not do relative to some rule or standard does not explain why they did what they did. Saying that people failed to take this or that pathway—only in hindsight the right one—-judges other people from a position of broader insight and outcome knowledge that they themselves did not have. It does not explain a thing yet; it does not shed any light on why people did what they did given their surrounding circumstances. Out­ side observers have become caught in what William James called the "psychologist's fallacy" a century ago: They have substituted their own reality for the one of their object of study. The More We Know, the Less We Understand We actually have interesting expectations of new technology in this regard. Technology has made it increasingly easy to capture and record the reality that surrounded other people carrying out work. In commercial aviation, the electronic footprint that any flight produces is potentially huge. We can use these data to reconstruct the world as it must have been experienced by other people back then, potentially avoiding the psychologist's fallacy. But capturing such data addresses only one side of the problem. Our ability to make sense of these data, to employ them in a reconstruction of the sensemaking processes of other people at another time and place, has not kept pace with our growing technical ability to register traces of their behav­ ior. In other words, the presumed dominance of human factors in incidents and accidents is not matched by our ability to analyze or understand the human contribution for what it is worth. Data used in accident analysis often come from a recording of human voices and perhaps other sounds (ruffling charts, turning knobs), which can be coupled to a greater or lesser extent with contemporaneous system or process behavior. A voice trace, however, represents only a partial data record. Human behavior in rich, unfolding settings is much more than the voice trace it leaves behind. The voice trace always points beyond itself, to a world that was unfolding around the practitioners at the time, to tasks, goals, perceptions, intentions, thoughts, and actions that have since evaporated. But most investigations are formally restricted in how they can couple the cockpit voice recording to the world that was unfolding around the practitioners (e.g., instrument indications, automation-mode settings). In aviation, for example, International Civil Aviation Organization (ICAO An­ nex 13) prescribes how only those data that can be factually established may be analyzed in the search for cause. This provision often leaves the cockpit voice recording as only a factual, decontextualized, and impoverished foot­ print of human performance. Making connections between the voice trace and the circumstances and people in which it was grounded quickly falls outside the pale of official analysis and into the realm of what many would call inference or speculation. This inability to make clear connections between behavior and world straightjackets any study of the human contribution to a cognitively noisy, evolving sequence of events. international civil aviation organization Annex 13 thus regulates the disembodiment of data: Data must be studied away from their context, for the context and the connections to it are judged as too tentative, too abstract, too unreliable. Such a provision, contradicted by virtually all cognitive psychological research, is devastating to our ability to make sense of puzzling performance. Apart from the provisions of international civil aviation organization Annex 13, this problem is complicated by the fact that current flight-data recorders (FDRs) often do not capture many automation-related traces: precisely those data that are of immediate importance to understanding the problem-solving environment in which most pilots today carry out their work. For example, FDRs in many highly automated aircraft do not record which ground-based navigation beacons were selected by the pilots, which automation-mode control-panel selections on airspeed, heading, altitude, and vertical speed were made, or what was shown on both pilots' moving map displays. As operator work has shifted to the management and supervision of a suite of automated resources, and problems leading to accidents increasingly start in human-machine interactions, this represents a large gap in our ability to access the reasons for human as­ sessments and actions in modern operational workplaces. INVERTING PERSPECTIVES Knowing about and guarding against the psychologist's fallacy, against the mixing of realities, is critical to understanding error. When looked at from the position of retrospective outsider, the error can look so very real, so compelling. They failed to notice, they did not know, they should have done this or that. But from the point of view of people inside the situation, as well as potential other observers, this same error is often nothing more than normal work. If we want to begin to understand why it made sense for people to do what they did, we have to reconstruct their local rationality. What did they know? What was their understanding of the situation? What were their multiple goals, resource constraints, pressures? Behavior is rational within situational contexts: People do not come to work to do a bad job. As historian Barbara Tuchman put it: "Every scripture is entitled to be read in the light of the circumstances that brought it forth. To understand the choices open to people of another time, one must limit oneself to what they knew; see the past in its own clothes, as itwere, notin ours" (1981, p. 75). This position turns the exigent social and operational context into the only legitimate interpretive device. This context becomes the constraint on what meaning we, who were not there when it happened, can now give to past controversial assessments and actions. Historians are not the only ones to encourage this switch, this inversion of perspectives, this persuasion to put ourselves in the shoes of other people. In hermeneutics it is known as the difference between exegesis (reading out of the text) and eisegesis (reading into the text). The point is to read out of the text what is has to offer about its time and place, not to read into the text what we want it to say or reveal now. Jens Rasmussen points out that if we cannot find a satisfactory answer to questions such as "how could they not have known?", then this is not because these people were behaving bizarrely. It is because we have chosen the wrong frame of reference for understanding their behavior (Vicente, 1999). The frame of reference for understanding people's behavior is their own normal, individual work context, the context they are embedded in and from whose point of view the decisions and assessments made are mostly normal, daily, unremarkable, perhaps even unnoticeable. A challenge is to understand how assessments and actions that from the outside look like errors become neutralized or normalized so that from the inside they appear unremarkable, routine, normal. If we want to understand why people did what they did, then the adequacy of the insider's representation of the situation cannot be called into question. The reason is that there are no objective features in the domain on which we can base such a judgment. In fact, as soon as we make such a judgment, we have imported criteria from the outside—from another time and place, from another rationality. Ethnographers have always championed the point of view of the person on the inside. Like Rasmussen, Emerson advised that, instead of using criteria from outside the setting to examine mistake and error, we should investigate and apply local notions of competent performance that are honored and used in particular social settings (Vaughan, 1999). This excludes generic rules and motherhoods (e.g., "pilots should be immune to commercial pressures"). Such putative standards ignore the subtle dynamics of localized skills and priority setting, and run roughshod over what would be considered "good" or "competent" or "normal" from inside actual situations. Indeed, such criteria impose a rationality from the outside, impressing a frame of context-insensitive, ideal­ ized concepts of practice upon a setting where locally tailored and subtly adjusted criteria rule instead. The ethnographic distinction between etic and emic perspectives was coined in the 1950s to capture the difference between how insiders view a setting and how outsiders view it. Emic originally referred to the language and categories used by people in the culture studied, whereas etic language and categories were those of outsiders (e.g., the ethnographer) based on their analysis of important distinctions. Today, emic is often understood to be the view of the world from the inside out, that is, how the world looks from the eyes of the person studied. The point of ethnography is to develop an insider's view of what is happening, an inside-out view. Etic is contrasted as the perspective from the outside in, where researchers or observers attempt to gain access to some portions of an insider's knowledge through psychological methods such as surveys or laboratory studies. Emic research considers the meaning-making activities of individual minds. It studies the multiple realities that people construct from their experiences with their empirical reality. It assumes that there is no direct access to a single, stable, and fully knowable external reality. Nobody has this access. Instead, all understanding of reality is contextually embedded and limited by the local rationality of the observer. Emic research points at the unique experience of each human, suggesting that any observer's way of making sense of the world is as valid as any other, and that there are no objective criteria by which this sensemaking can be judged correct or incorrect. Emic researchers resist distinguishing between objective features of a situation, and subjective ones. Such a distinction distracts the observer from the situation as it looked to the person on the inside, and in fact distorts this insider perspective. A fundamental concern is to capture and describe the point of view of people inside a system or situation, to make explicit that which insiders take for granted, see as common sense, find unremarkable or normal. When we want to understand error, we have to embrace ontological relativity not out of philosophical intransigence or philanthropy, but for trying to get the inside-out view. We have to do this for the sake of learning what makes a system safe or brittle. As we saw in chapter 2, for example, the notion of what constitutes an incident (i.e., what is worthy of reporting as a safety threat) is socially constructed, shaped by history, institutional constraints, cultural and linguistic notions. It is negotiated among insiders in the system. None of the structural measures an organization takes to put an incident report­ ing system in place will have any effect if insiders do not see safety threats as incidents that are worth sending into the reporting system. Nor will the organization ever really improve reporting rates if it does not understand the notion of incident (and, conversely, the notion of normal practice) from the point of view of the people who do it everyday. To succeed at this, outsiders need to take the inside-out look, they need to embrace ontological relativity, as only this can crack the code to system safety and brittleness. All the processes that set complex systems onto their drifting paths toward failure—the conversion of signals of danger into normal, expected problems, the incrementalist borrowing from safety, the assumption that past operational success is a guarantee of future safety—are sustained through implicit social-organizational consensus, driven by insider language and rationalizations. The internal workings of these processes are simply impervious to outside inspection, and thereby numb to external pressure for change. Outside observers cannot attain an emic perspective, nor can they study the multiple rationalities created by people on the inside if they keep seeing errors and violations. Outsiders can perhaps get some short-term leverage by (re) imposing context-insensitive rules, regulations, or exhortations and making moral appeals for people to follow them, but the effects are generally short-lived. Such measures cannot be supported by operational ecologies. There, actual practice is always under pressure to adapt in an open system, exposed to pressures of scarcity and competition. It will once again inevitably drift into niches that generate greater operational returns at no apparent cost to safety. ERROR AND (IR)RATIONALITY Understanding error against the background of local rationality, or rationality for that matter, has not been an automatic by-product of studying the psychology of error. In fact, research into human error had a very rationalist bias up to the 1970s (Reason, 1990), and in some quarters in psychology and human factors such rationalist partiality has never quite disappeared. Rationalist means that mental processes can be understood with reference to normative theories that describe optimal strategies. Strategies may be op­ timal when the decision maker has perfect, exhaustive access to all relevant information, takes time enough to consider it all, and applies clearly defined goals and preferences to making the final choice. In such cases, errors are explained by reference to deviations from this rational norm, this ideal. If the decision turns out wrong it may be because the decision maker did not take enough time to consider all information, or that he or she did not generate an exhaustive set of choice alternatives to pick from. Errors, in other words, are deviant. They are departures from a standard. Errors are irrational in the sense that they require a motivational (as opposed to cognitive) component in their explanation. If people did not take enough time to consider all information, it is because they could not be bothered to. They did not try hard enough, and they should try harder next time, perhaps with the help of some training or procedural guidance. Investigative practice in human factors is still rife with such rationalist reflexes. It did not take long for cognitive psychologists to find out how humans could not or should not even behave like perfectly rational decision makers. Whereas economists clung to the normative assumptions of decision making (decision makers have perfect and exhaustive access to information for their decisions, as well as clearly defined preferences and goals about what they want to achieve), psychology, with the help of artificial intelligence, posited that there is no such thing as perfect rationality (i.e., full knowledge of all relevant information, possible outcomes, relevant goals), because there is not a single cognitive system in the world (neither human nor machine) that has sufficient computational capacity to deal with it all. Rationality is bounded. Psychology subsequently started to chart people's imperfect, bounded, or local rationality. Reasoning, it discovered, is governed by people's local understanding, by their focus of attention, goals, and knowledge, rather than some global ideal. Human performance is embedded in, and systematically connected to, the situation in which it takes place: It can be understood (i.e., makes sense) with reference to that situational context, not by reference to some universal standard. Human actions and assessments can be described meaningfully only in reference to the localized setting in which they are made; they can be understood by inti­ mately linking them to details of the context that produced and accompanied them. Such research has given rationality an interpretive flexibility: What is locally rational does not need to be globally rational. If a decision is locally rational, it makes sense from the point of view of the decision maker, which is what matters if we want to learn about the underlying reasons for what from the outside looks like error. The notion of local rationality removes the need to rely on irrational explanations of error. Errors make sense: They are rational, if only locally so, when seen from the inside of the situation in which they were made. But psychologists themselves often have trouble with this. They keep on discovering biases and aberrations in decision making (e.g., groupthink, confirmation bias, routine violations) that seem hardly rational, even from within a situational context. These deviant phenomena require motivational explanations. They call for motivational solutions. People should be motivated to do the right thing, to pay attention, to double check. If they do not, then they should be reminded that it is their duty, their job. Notice how easily we slip back into prehistoric behaviorism: Through a modernist system of rewards and punishments (job incentives, bonuses, threats of retribution) we hope to mold human performance after supposedly fixed fea­ tures of the world. That psychologists continue to insist on branding such action as irrational, referring it back to some motivational component, may be due to the limits of the conceptual language and power of the discipline. Putatively motivational issues (such as deliberately breaking rules) must themselves be put back into context, to see how human goals (getting the job done fast by not following all the rules to the letter) are made congruent with system goals through a collective of subtle pressures, subliminal messages about organizational preferences, and empirical success of operating outside existing rules. The system wants fast turnaround times, maximization of capacity utilization, efficiency. Given those system goals (which are often kept implicit) , rulebreaking is not a motivational shortcoming, but rather an indication of a well-motivated human operator: Personal goals and system goals are harmonized, which in turn can lead to total system goal displacement: Efficiency is traded off against safety. But psychology often keeps seeing motivational shortcomings. And human factors keeps suggesting countermeasures such as injunctions to follow the rules, better training, or more top-down task analysis. Human factors has trouble incorporating the subtle but powerful influences of organizational environments, structures, processes, and tasks into accounts of individual cognitive practices. In this regard the discipline is conceptually underdeveloped. Indeed, how unstated cultural norms and values travel from the institutional, organizational level to express themselves in individual assessments and actions (and vice versa) is a concern central to sociology, not human factors. Bridging this macromicro connection in the systematic production of rule violations means understanding the dynamic interrelationships between issues as wide ranging as organizational characteristics and preferences, its environment and history; incrementalism in trading safety off against production, unintentional structural secrecy that fragments problem-solving activities across different groups and departments; patterns and representations of safety-related information that are used as imperfect input to organizational decision making, the influence of hierarchies and bureaucratic accountability on people's choice, and others (e.g., Vaughan, 1996, 1999). The structuralist lexicon of human factors and system safety today has no words for many of these concepts, let alone models for how they go together. From Decision Making to Sensemaking In another move away from rationalism and toward the inversion of perspectives (i.e., trying to understand the world the way it looked to the decision maker at the time), large swathes of human factors have embraced the ideas of naturalistic decision making (NDM) over the last decade. By importing cyclical ideas about cognition (situation assessment informs action, which changes the situation, which in turn updates assessment, Neisser, 1976) into a structuralist, normativist psychological lexicon, NDM virtually reinvented decision making (Orasanu & Connolly, 1993). The focus shifted from the actual decision moment, back into the preceding realm of situation assessment. This shift was accompanied by a methodological reorientation, where decision making and decision makers were increasingly studied in their complex, natural environments. Real decision problems, it quickly turned out, resist the rationalistic format dictated for so long by economics: Options are not enumerated exhaustively, access to information is incomplete at best, and people spend more time assessing and measuring up situations than making decisions—if that is indeed what they do at all (Klein, 1998). In contrast to the prescriptions of the normative model, decision makers tend not to generate and evaluate several courses of action concurrently, in order to then determine the best choice. People do not typically have clear or stable sets of preferences along which they can even rank the enumer­ ated courses of action, picking the best one, nor do most complex decision problems actually have a single correct answer. Rather, decision makers in action tend to generate single options at the time, mentally simulate wheth­ er this option would work in practice, and then either act on it, or move on to a new line of thought. NDM also takes the role of expertise more seriously than previous decision-making paradigms: What distinguishes good decision makers from bad decision makers most is their ability to make sense of situations by using a highly organized experience base of relevant knowledge. Once again neatly folding into ideas developed by Neisser, such reasoning about situations is more schema-driven, heuristic, and recognitional than it is computational. The typical naturalistic decision setting does not allow the decision maker enough time or information to generate perfect solutions with perfectly rational calculations. Decision making in action calls for judgments under uncertainty, ambiguity and time pressure. In those settings, options that appear to work are better than perfect options that never get computed. The same reconstructive, corrective intervention into history that produces our clear perceptions of errors, also generates discrete decisions. What we see as decision making from the outside is action embedded in larger streams of practice, something that flows forth naturally and continually from situation assessment and reassessment. Contextual dynamics are a joint product of how problems in the world are developing and the actions taken to do something about it. Time becomes nonlinear: Decision and action are interleaved rather than temporally segregated. The decision maker is thus seen as in step with the continuously unfolding environment, simultaneously influenced by it and influencing it through his or her next steps. Understanding decision making, then, requires an understanding of the dynamics that lead up to those supposed decision moments, because by the time we get there, the interesting phenomena have evaporated, gotten lost in the noise of action. NDM research is front-loaded: it studies the front end of decision making, rather than the back end. It is interested, indeed, in sensemaking more than in decision making. Removing decision making from the vocabulary of human factors investigations is the logical next step, suggested by Snook (2000). It would be an additional way to avoid counterfactual reasoning and judgmentalism, as decisions that eventually led up to a bad outcome all too quickly become bad decisions: Framing such tragedies as decisions immediately focuses our attention on an individual making choices . . . such a framing puts us squarely on a path that leads straight back to the individual decision maker, away from the potentially powerful contextual features and right back into the jaws of the fundamental attribution error. "Why did they decide . . . ?" quickly becomes "Why did they make the wrong decision?" Hence, the attribution falls squarely onto the shoulders of the decision maker and away from potent situational factors that influence action. Framing the . . . puzzle as a question of meaning rather than deciding shifts the emphasis away from individual decision makers toward a point somewhere "out there" where context and individual action overlap. (Snook, p. 206) Yet sensemaking is not immune to counterfactual pressure either. If what made sense to the person inside the situation still makes no sense given the outcome, then human factors hastens to point that out (see chap. 5). Even in sensemaking, the normativist bias is an everpresent risk. THE HINDSIGHT BIAS IS NOT A BIAS AND IS NOT ABOUT THE PAST Perhaps the pull in the direction of the position of retrospective outsider is irresistible, inescapable, whether we make lexical adjustments in our investigative repertoire or not. Even with the potentially judgmental notion of decision making removed from the forensic psychological toolbox, it remains incredibly difficult to see the past in its own clothes, not in ours. The fundamental attribution error is alive and well, as Scott Snook puts it (2000, p. 205). We blame the human in the loop and underestimate the influence of context on performance, despite repeated warnings of this frailty in our reasoning. Perhaps we are forever unable to shed our own projection of reality onto the circumstances of people at another time and place. Perhaps we are doomed to digitizing past performance, chunking it up into discrete decision moments that inevitably lure us into counterfactual thinking and judgments of performance instead of explanations. Just as any act of obser vation changes the observed, our very observations of the past inherently intervene in reality, converting complex histories into more linear, more cer­ tain, and disambiguated chronicles. The mechanisms described earlier in this chapter may explain how hindsight influences our treatment of human performance data, but they hardly explain why. They hardly shed light on the energy behind the continual pull toward the position of retrospective outsider; they merely sketch out some of the routes that lead to it. In order to explain failure, we seek failure. In order to explain missed opportunities and bad choices, we seek flawed analyses, inaccurate perceptions, violated rules—even if these were not thought to be influential or obvious or even flawed at the time (Starbuck & Milliken, 1988). This search for failures is something we cannot seem to escape. It is enshrined in the accident models popular in transportation human factors of our age (see chaps. 1 and 2) and proliferated in the fashionable labels for "human error" that human factors keeps inventing (see chap. 6). Even where we turn away from the etic pitfalls of looking into people's decision making, and focus on a more benign, emic, situated sensemaking, the rationalist, norma­ tivist perspective is right around the corner. If we know the outcome was bad, we can no longer objectively look at the behavior leading up to it—it must also have been bad (Fischoff, 1975). To get an idea, think of the Greek mythological figure Oedipus, who shared Jocasta's bed. How large is the difference between Oedipus' memory of that experience before and after he found out that Jocasta was his mother? Once he knew, it was simply impossible for him to look at the experience the same way. What had he missed? Where did he not do his homework? How could he have become so distracted? Outcome knowledge afflicts all retrospective observers, no matter how hard we try not to let it influence us. It seems that bad decisions always have something in common, and that is that they all seemed like a good idea at the time. But try telling that to Oedipus. The Hindsight Bias Is an Error That Makes Sense, Too When a phenomenon is so impervious to external pressure to change, one would begin to suspect that it has some adaptive value, that it helps us preserve something, helps us survive. Perhaps the hindsight bias is not a bias, and perhaps it is not about history. Instead, it may be a highly adaptive, forward-looking, rational response to failure. This putative bias may be more about predicting the future than about explaining the past. The linearization and simplification that we see happen in the hindsight bias may be a form of abstraction that allows us to export and project our and others' experiences onto future situations. Future situations can never be predicted at the same level of contextual detail as the new view encourages us to explain past situations. Predictions are possible only because we have created some kind of model for the situation we wish to gain control over, not because we can exhaustively foresee every contextual factor, influence, or data point. This model—any model—is an abstraction away from context, an inherent simplification. The model we create—naturally, effortlessly, automatically—after past events with a bad outcome inevitably becomes a model of binary choices, bifurcations, and unambiguous decision moments. That is the only useful kind of model we can take with us into the future if we want to guard against the same type of pitfalls and forks in the road. The hindsight bias, then, is about learning, not about explaining. It is forward looking, not backward looking. This applies to ourselves and our own failures as much as it applies to our observations of other people's failures. When confronted by failures that occurred to other people, we may imperatively be tripped into vicarious learning, spurned by our own urge for survival: What do I do to avoid that from happening to me? When con­ fronted by our own performance, we have no privileged insight into our own failures, even if we would like to think we do. The past is the past, whether it is our own or somebody else's. Our observations of the past inevitably intervene and change the observed, no matter whose past it is. This is something that the fundamental attribution error cannot account for. It explains how we overestimate the influence of stable, personal characteristics when we look at other people's failings. We underplay the influence of context or situational factors when others do bad things. But what about our own failings? Even here we are susceptible to reframing past complexity as simple binary decisions, wrong decisions due to personal shortcomings: things we missed, things we should have done or should not have done. Snook (2000) investigated how, in the fog of postGulf War Iraq, two helicopters carrying U.N. peacekeepers were shot down by American fighter jets. The situation in which the shoot-down occurred was full of risk, role ambiguity, operational complexity, resource pressure, slippage between plans and practice. Yet immediately after the incident, all of this gets converted into binary simplicity (a choice to err or not to err) by DUKE—the very command onboard the airborne control center whose job it was not to have such things happen. Allowing the fighters to shoot down the helicopters was their error, yet they do not blame context at all, as the fundamental attribution error predicts they should. It was said of the DUKE that immediately after the incident: "he hoped we had not shot down our own helicopters and that he couldn't believe anybody could make that dumb a mistake" (Snook, p. 205). It is DUKE himself who blames his own dumb mistake. As with the errors in chapter 3, the dumb mistake is something that jumps into view only with knowledge of outcome, its mistakeness a function of the outcome, its dumbness a function of the severity of the consequences. While doing the work, helping guide the fighters, identifying the targets, all DUKE was ding was his job. It was normal work. He was not sitting there making dumb mistakes. They are a product of hindsight, his own hindsight, directed at his own "mistakes." The fundamental attribution error does not apply. It is overridden. The fighter pilots, too, engage in self-blame, literally converting the ambiguity, risk, uncertainty, and pressure of their encounter with potentially hostile helicopters into a linear series of decision errors, where they repeatedly and consistently took wrong turns on their road to perdition (we mis­ identified, we engaged, and we destroyed): "Human error did occur. We misidentified the helicopters; we engaged them; and we destroyed them. It was a tragic and fatal mistake" (Tiger 02 quoted in Snook, 2000, p. 205). Again, the fundamental attribution error makes the wrong prediction. If it were true, then these fighter pilots would tend to blame context for their own errors. Indeed, it was a rich enough context—fuzzy, foggy, dangerous, multi-player, pressurized, risky—with plenty of blameworthy factors to go around, if that is where you would look. Yet these fighter pilots do not. "We" misidentified, "we" engaged, "we" destroyed. The pilots had the choice not to; in fact, they had a series of three choices not to instigate a tragedy. But they did. Human error did occur. Of course, elements of self-identity and control are wrapped up in such an attribution, a self-identity for which fighter pilots may well be poster children. It is interesting to note that the tendency to convert past complexity into binary simplicity—into twofold choices to identify correctly or incorrectly, to engage or not, to destroy or not—overrides the fundamental attribution error. This confirms the role of the hindsight bias as a catalyst for learning. Learning (or having learned) expresses itself most clearly by doing some­ thing differently in the future, by deciding or acting differently, by remov­ ing one's link in the accident chain, as fighter pilot Tiger 02 put it: "Remove any one link in the chain and the outcome would be entirely different. I wish to God I could go back and correct my link in this chain— my actions which contributed to this disaster" (Tiger 02, quoted in Snook, 2000, p. 205). We cannot undo the past. We can only undo the future. But undoing the future becomes possible only when we have abstracted away past failures, when we have decontextualized them, stripped them, cleaned them from the fog and confusion of past contexts, highlighted them, blown them up into obvious choice moments that we, and others, had better get right next time around. Prima facie, the hindsight bias is about misassessing the con­ tributions of past failings to bad outcomes. But if the phenomenon is really as robust as it is documented to be and if it actually manages to override the fundamental attribution error, it is probably the expression of more primary mechanisms running right beneath its surface. The hindsight bias is a meaningful adaptation. It is not about explaining past failures. It is about preventing future ones. In preparing for future confrontations with situations where we or others might err again, and do not want to, we are in some sense taking refuge from the banality of accidents thesis. The thought that accidents emerge from murky, ambiguous, everyday decision making renders us powerless to do anything meaningful about it. This is where the hindsight bias is so fundamentally adaptive. It highlights for us where we could fix things (or where we think we could fix things), so that the bad thing does not happen again. The hindsight bias is not a bias at all, in the sense of a departure from some rational norm. The hindsight bias is rational. It in itself represents and sustains rationality. We have to see the past as a binary choice, or a linear series of binary choices, because that is the only way we can have any hope of controlling the future. There is no other basis for learning, for adapting. Even if those adaptations may consist of rather coarse adjustments, of undamped and overcontrolling regulations, even if these adaptations occur at the cost of making oversimplified predictions. But making oversimplified predictions of how to control the future is apparently better than having no predictions at all. Quite in the spirit of Saint Augustine, we accept the reality of errors, and the guilt that comes with it, in the quest for control over our futures. Indeed, the human desire to attain control over the future surely predates the Scientific Revolution. The more refined and empirically testable tools for gaining such control, however, were profoundly influenced and extended by it. Control could best be attained through mechanization and technology—away from nature, spirit, away from primitive incantations to divine powers to spare us the next disaster. These Cartesian-Newtonian reflexes have tumbled down the centuries to proffer human factors legitimate routes for gaining control over complex, dynamic futures today and tomorrow. For example, when we look at the remnants of a crashed automated airliner, we, in hindsight, exclaim, "they should have known they were in open descent mode!" The legitimate solution for meeting such technology surprises is to throw more technology at the problem (additional warning systems, paperless cockpits, automatic cocoons). But more technology often creates more problems of a kind we have a hard time anticipating, rather than just solving existing ones. As another example, take the error-counting methods discussed in chapter 3. A more formalized way of turning the hindsight bias into an oversimplified forward looking future controller is hardly imaginable. Errors, which are uniquely the product of retrospective observations conducted from the outside, are measured, categorized, and tabulated. This produces bar charts that putatively point toward a future, jutting their dire predictions of rule violations or proficiency errors out into a dark and fearful time to come, away from a presumed "safe" baseline. It is normativism in pretty forms and colors. These forecasting techniques, which are merely an assignment of categories and numbers to the future, are appearing everywhere. However, their categorical and numerical output can at best be as adequate or as inadequate as the input. Using such forecasts as a strategic tool is only a belief that numbers are meaningful in relation to the fearful future. Strategy becomes a matter of controlling the future by labelling it, rather than continually re-evaluating the uncertain situation. This approach, searching for the right and numerical label to represent the future, is more akin to numerology or astrology. It is the modern-day ritual equivalent of "reading the runes" or "divining the entrails." (Angell & Straub, 1999, p. 184) Human factors holds on to the belief that numbers are meaningful in relation to a fearful future. And why not? Measuring the present and mathematically modeling it (with barcharts, if you must), and thereby predicting and controlling the future has been a legitimate pursuit since at least the 16th century. But as chapters 1, 2, and 3 show, such pursuits are getting to be deeply problematic. In an increasingly complex, dynamic sociotechnical world, their predictive power is steadily eroding. It is not only a problem of garbage in, garbage out (the categorical and numerical output is as adequate or inadequate as the input). Rather, it is the problem of not seeing that we face an uncertain situation in the first place, where mistake, failure, and disaster are incubated in systems much larger, much less transparent, and much less deterministic than the counters of individual errors believe. This, of course, is where the hindsight bias remains a bias. But it is a bias about the future, not about the past. We are biased to believe that thinking about action in terms of binary choices will help us undo bad futures, that it prepares us sufficiently for coming complexity. It does not. Recall how David Woods (2003) put it: Although the past is incredible (DUKE couldn't believe anybody could make that dumb a mistake), the future is implausible. Mapping digitized historic lessons of failure (which span the airworthiness review certificate from error bar charts to Tiger O2's wish to undo his link in the chain) into the future will only be partly effective. Stochastic variation and complexity easily outrun our computational capacity to predict with any accuracy. PRESERVATION OF SELF- AND SYSTEM IDENTITY There is an additional sense in which our dealings with past failures go beyond merely understanding what went wrong and preventing recurrence. Mishaps are surprising relative to prevailing beliefs and assumptions about the system in which they happen, and investigations are inevitably affected by the concern to reconcile a disruptive event with existing views and beliefs about the system. Such reconciliation is adaptive too. Our reactions to failure, and our investigations into failure, must be understood against the backdrop of the "fundamental surprise error" (Lanir, 1986) and examined for the role they play in it. Accidents tend to create a profound asymmetry between our beliefs (or hopes) of a basically safe system, and new evidence that may suggest that it is not. This is the fundamental surprise: the aston­ ishment that we feel when the most basic assumptions we held true about the world may turn out to be untrue. The asymmetry creates a tension, and this tension creates pressure for change: Something will have to give. Either the belief needs changing (i.e., we have to acknowledge that the system is not basically safe—that mistake, mishap, and disaster are systematically organized by that system; Vaughan, 1996), or we change the people involved in the mishap—even if this means us. We turn them into unrepresentative, uniquely bad individuals: The pilots of a large military helicopter that crashed on a hillside in Scotland in 1994 were found guilty of gross negligence. The pilots did not survive—29 people died in total—so their side of the story could never be heard. The official inquiry had no problems with "destroying the reputation of two good men," as a fellow pilot put it. Potentially fundamental vulnerabilities (such as 160 reported cases of Uncommanded Flying Control Movement or UFCM in computerized helicopters alone since 1994) were not looked into seriously. (Dekker, 2002, p. 25) When we elect to "destroy the reputation of two good men," we have committed the fundamental surprise error. We have replaced a fundamental challenge to our assumptions, our beliefs (the fundamental surprise) with a mere local one: The pilots were not as good as we thought they were, or as good as they should have been. From astonishment (and its concomitant: fear about the basic safety of the system, as would be raised by 160 cases of UFCM) we move to mere, local wonder: How could they not have seen the hill? They must not have been very good pilots after all. Thus we strive to preserve our self- and system identity. We pursue an adaptive strategy of safeguarding the essence of our world as we understand it. By letting the reputation of individual decision makers slip, we have relieved the tension between broken beliefs (the system is not safe after all) and fervent hopes that it still is. That phenomena such as the hind­ sight bias and the fundamental attribution error may not be primary, but rather ancillary expressions of more adaptive, locally rational, and useful identity-preserving strategies for the ones committing them, is consonant with observations of a range of reasoning errors. People keep committing them not because they are logical (i.e., globally rational) or because they only produce desired effects, but because they serve an even weightier purpose: "This dynamic, this 'striving to preserve identity,' however strange the means or effects of such striving, was recognised in psychiatry long ago. [This phenomenon] is seen not as primary, but as attempts (however misguided) at restitution, at reconstructing a world reduced by complete chaos" (Sacks, 1998, p. 7). However "strange the means or effects of such striving," the fundamental surprise error allows us to rebuild a world reduced by chaos. And the hindsight bias allows us to predict and avoid future roads to perdition. Through the fundamental surprise error, we rehabilitate our faith in something larger than ourselves, something in which we too are vulnerable to breakdown, something that we too are at the mercy of in varying degrees. Breaking out of such locally rational reasoning, where the means and consequences of our striving for preservation and rehabilitation create strange and undesirable side effects (blaming individuals for system failures, not learning from accidents, etc.) requires extraordinary courage. It is not very common. Yet people and institutions may not always commit the fundamental surprise error, and may certainly not do so intentionally. In fact, in the immediate aftermath of failure, people may be willing to question their underlying assumptions about the system they use or operate. Perhaps things are not as safe as previously thought; perhaps the system contains vulnerabilities and residual weaknesses that could have spawned this kind of failure earlier, or worse, could do it again. Yet such openness does not typically last long. As the shock of an accident subsides, parts of the system mobilize to contain systemic self-doubt and change the fundamental surprise into a merely local hiccup that temporarily ruffled an otherwise smooth operation. The reassurance is that the system is basically safe. It is only some people or other parts in it that are unreliable. In the end, it is not often that an existing view of a system gives in to the reality of failure. Instead, to redress the asymmetry, the event or the players in it are changed to fit existing assumptions and beliefs about the system, rather than the other way around. Expensive lessons about the system as a whole, and the subtle vulnerabilities it contains, can go completely unlearned. Our inability to deal with the fundamental surprise of failure shines through the investigations we commission. The inability to really learn is sometimes legitimized and institutionalized through resourcintensive official investigations. The cause we end up attributing to an acci­ dent may sometimes be no more than the "cause" we can still afford, given not just our financial resources, but also our complex of hopes and beliefs in a safe and fair world. As Perrow (1984) has noted: Formal accident investigations usually start with an assumption that the oper­ ator must have failed, and if this attribution can be made, that is the end of serious inquiry. Finding that faulty designs were responsible would entail enormous shutdown and retrofitting costs; finding that management was responsible would threaten those in charge, but finding that operators were responsible preserves the system, with some soporific injunctions about better training, (p. 146) Real change in the wake of failure is often slow to come. Few investigations have the courage to really challenge our beliefs. Many keep feeding the hope that the system is still safe—except for this or that little broken component, or this or that Bad Apple. The lack of courage shines through how we deal with human error, through how we react to failure. It affects the words we choose, the rhetoric we rely on, the pathways for "progress" we put our bets on. Which cause can we afford? Which cause renders us too uncomfortable? Accuracy is not the dominant criterion, but plausibility is—plausibility from the perspective of those who have to accommodate the surprise that the failure represents for them and their organization, their worldview. "Is it plausible?," is the same as asking, "Can we live (on) with this explanation? Does this explanation help us come to terms with the puzzle of bad performance?" Answering this question, and generating such comfort and selfassurance, is one purpose that our analysis of past failures has to fulfill, even if it becomes a selective oversimplification because of it. Even if, in the words of Karl Weick (1995), they make lousy history.