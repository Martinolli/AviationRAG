Title: international civil aviation organization Doc 9859 – Safety Management Manual – Fourth Edition – 2018 Author(s): International Civil Aviation Organization Category: Safety, Management Tags: Safety, Management, System, Manual FOREWORD This fourth edition of the Safety Management Manual (SMM) supersedes the third edition, published in May 2013, in its entirety. The development of this edition was initiated after the adoption of Amendment 1 to Annex 19 to address the changes introduced by the amendment and to reflect the knowledge and experience gained since the last revision. To address the needs of the diverse aviation community implementing safety management and a recommendation stemming from the second High-level Safety Conference held in 2015, the Safety Management Implementation (SMI) website (www.icao.int/SMI) has been developed to complement the SMM and serves as a repository for the sharing of best practices. Practical examples, tools, and supporting educational material will be collected, reviewed and posted on the website on an ongoing basis. This edition is intended to support States in implementing effective State safety programmes (SSP). This includes ensuring that service providers implement safety management systems (SAFETY MANAGEMENT SYSTEM) in accordance with the provisions of Annex 19. In order to be consistent with Safety management principles, a concerted effort has been made to focus on the intended outcome of each Standard and Recommended Practice (SARP), purposely avoiding being overly prescriptive. Emphasis has been placed on the importance of each organization tailoring the implementation of safety management to fit their specific environment. Note 1.— In this manual, the term “organization” is used to refer to both States and service providers. Note 2.— In this manual, the term “service provider” is used to refer to an aviation industry organization implementing SAFETY MANAGEMENT SYSTEM whether on a mandatory or voluntary basis, unlike Annex 19 which uses the term to refer to a very specific list of organizations found in Chapter 3, which excludes international general aviation operators. The fourth edition is divided into nine chapters which progressively build the reader’s understanding of safety management. These chapters can be grouped under the following three themes: 1) Safety management fundamentals – Chapters 1 to 3 build the reader’s understanding of the fundamental principles underpinning safety management. 2) Developing safety intelligence – Chapters 4 to 7 build on the fundamentals. These chapters comprise four interrelated topics concerned with leveraging safety data and safety information to develop actionable insights which can be used by an organization’s leadership to make data-driven decisions, including those related to the most effective and efficient use of resources. 3) Safety management implementation – Chapters 8 and 9 explain how to apply the concepts from the preceding chapters to institutionalize safety management at the State and service provider level. Guidance to support sector-specific safety management Standards and Recommended Practices found outside of Annex 19 (e.g. flight data analysis programmes) is not addressed in this manual. The Manual of Aircraft Accident and Incident Investigation (Doc 9756) contains guidance on the conduct of independent State accident and incident investigations in accordance with Annex 13 — Aircraft Accident and Incident Investigation. INTERNATIONAL CIVIL AVIATION ORGANIZATION gratefully acknowledges the contributions of the Safety Management Panel (SMP) and the Safety Information Protection Implementation Group (SIP IG) as well as other expert groups and individual experts who provided support, advice and input for this manual. The content was developed over a period of two years and was thereafter submitted for an extensive peer review to collect and take into account comments from the expert community, taking into consideration that the manual is expected to be the overarching guidance for safety management for a wide community. Comments on this manual, particularly with regard to its application and usefulness, would be appreciated from all States, safety oversight audit missions and INTERNATIONAL CIVIL AVIATION ORGANIZATION technical cooperation field missions. These will be taken into consideration in the preparation of subsequent editions. Comments should be addressed to: The Secretary General International Civil Aviation Organization 999 Robert-Bourassa Boulevard Montréal, Quebec Canada H3C 5H7 GLOSSARY DEFINITIONS When the following terms are used in the manual, they have the meanings indicated below. Note.— Where an asterisk appears beside a term, the term has already been defined as such in Annexes and Procedures for Air Navigation Services (PANS). Acceptable level of safety performance (Acceptable level of safety performance). The level of safety performance agreed by State authorities to be achieved for the civil aviation system in a State, as defined in its State safety programme, expressed in terms of safety performance targets and safety performance indicators. Accountable executive. A single, identifiable person having responsibility for the effective and efficient performance of the service provider’s SAFETY MANAGEMENT SYSTEM. Change management. A formal process to manage changes within an organization in a systematic manner, so that changes which may impact identified hazards and risk mitigation strategies are accounted for, before the implementation of such changes. Defences. Specific mitigating actions, preventive controls or recovery measures put in place to prevent the realization of a hazard or its escalation into an undesirable consequence. Errors. An action or inaction by an operational person that leads to deviations from organizational, or the operational person’s, intentions or expectations. *Hazard. A condition or an object with the potential to cause or contribute to an aircraft incident or accident. Risk mitigation. The process of incorporating defences, preventive controls or recovery measures to lower the severity and/or likelihood of a hazard’s projected consequence. Safety. The state in which risks associated with aviation activities, related to, or in direct support of the operation of aircraft, are reduced and controlled to an acceptable level. *Safety data. A defined set of facts or set of safety values collected from various aviation-related sources, which is used to maintain or improve safety. Note.— Such safety data is collected from proactive or reactive safety-related activities, including but not limited to: a) accident or incident investigations; b) safety reporting; c) continuing airworthiness reporting; d) operational performance monitoring; e) inspections, audits, surveys; or f) safety studies and reviews. *Safety information. Safety data processed, organized or analyzed in a given context so as to make it useful for safety management purposes. *Safety management system (SAFETY MANAGEMENT SYSTEM). A systematic approach to managing safety, including the necessary organizational structures, accountability, responsibilities, policies and procedures. Safety objective. A brief, high-level statement of safety achievement or desired outcome to be accomplished by the State safety programme or service provider’s safety management system. Note. — Safety objectives are developed from the organization’s top safety risks and should be taken into consideration during subsequent development of safety performance indicators and targets. *Safety oversight. A function performed by a State to ensure that individuals and organizations performing an aviation activity comply with safety-related national laws and regulations. *Safety performance. A State’s or service provider´s safety achievement as defined by its safety performance targets and safety performance indicators. *Safety performance indicator. A data-based parameter used for monitoring and assessing safety performance. *Safety performance target. The State or service provider’s planned or intended target for a safety performance indicator over a given period that aligns with the safety objectives. *Safety risk. The predicted probability and severity of the consequences or outcomes of a hazard. *State safety programme (SSP). An integrated set of regulations and activities aimed at improving safety. *Surveillance. The State activities through which the State proactively verifies through inspections and audits that aviation licence, certificate, authorization or approval holders continue to meet the established requirements and function at the level of competency and safety required by the State. System. An organized, purposeful structure that consists of interrelated and interdependent elements and components, and related policies, procedures and practices created to carry out a specific activity or solve a problem. Trigger. An established level or criteria value for a particular safety performance indicator that serves to initiate an action required, (e.g., an evaluation, adjustment or remedial action). ABBREVIATIONS AND ACRONYMS ADREP Accident/incident data reporting AIA Accident investigation authority Acceptable level of safety performance Acceptable level of safety performance air operator certificate Air operator certificate ATS Air traffic service(s) civil aviation authority Civil aviation authority cockpit voice recorder Cockpit voice recorder D3M Data-driven decision-making Doc Document ERP Emergency response plan FDA Flight data analysis flight data recorder Flight data recorder flight management system Financial management system FRMS Fatigue risk management systems Global Aviation Safety Plan Global Aviation Safety Plan INTERNATIONAL CIVIL AVIATION ORGANIZATION International Civil Aviation Organization iSTARS Integrated Safety Trend Analysis and Reporting System LOSA Line operations safety audit OHSAFETY MANAGEMENT SYSTEM Occupational health and safety management system OSHE Occupational safety, health and environment PIRG Planning and implementation regional group quality management system Quality management system RASG Regional aviation safety group RSOO Regional safety oversight organization SAG Safety action group Standards and Recommended Practices Standards and Recommended Practices SD Standard deviation SDCPS Safety data collection and processing system SeMS Security management system SMM Safety management manual SMP Safety Management Panel SAFETY MANAGEMENT SYSTEM Safety management system(s) SPI Safety performance indicator SPT Safety performance target SRB Safety review board SRBS Safety risk-based surveillance safety risk management Safety risk management SSO State safety oversight state safety program State safety programme STDEVP Population standard deviation TNA Training needs analysis universal safety oversight program Universal Safety Oversight Audit Programme ______________________ PUBLICATIONS (referred to in this manual) The following documents are referred to in this manual or may provide additional guidance material. INTERNATIONAL CIVIL AVIATION ORGANIZATIONDOCUMENTS Annexes to the Convention on International Civil Aviation Annex 1 — Personnel Licensing Annex 6 — Operation of Aircraft Part I — International Commercial Air Transport — Aeroplanes Part II — International General Aviation — Aeroplanes Annex 8 — Airworthiness of Aircraft Annex 13 — Aircraft Accident and Incident Investigation Annex 14 — Aerodromes Volume I — Aerodrome Design and Operations Annex 18 — The Safe Transport of Dangerous Goods by Air Annex 19 — Safety Management PANS Procedures for Air Navigation Services (PANS) — Aerodromes (Doc 9981) Procedures for Air Navigation Services — Air Traffic Management (PANS-ATM, Doc 4444) Manuals Airport Services Manual (Doc 9137), Part 3 — Wildlife Control and Reduction Air Traffic Services Planning Manual (Doc 9426) Airworthiness Manual (Doc 9760) Aviation Security Manual (Doc 8973 — Restricted) Global Aviation Safety Plan (Global Aviation Safety Plan) (Doc 10004) Manual of Aircraft Accident and Incident Investigation (Doc 9756) Part I — Organization and Planning Part II — Procedures and Checklists Part III — Investigation Part IV — Reporting Manual for the Oversight of Fatigue Management Approaches (Doc 9966) Manual on Laser Emitters and Flight Safety (Doc 9815) Manual on Remotely Piloted Aircraft Systems (RPAS) (Doc 10019) Manual on the Competencies of Civil Aviation Safety Inspectors (Doc 10070) Manual on the INTERNATIONAL CIVIL AVIATION Organization Bird Strike Information System (IBIS) (Doc 9332) Manual on Protection of Safety Information (Doc 10053) Part I — Protection of Accident and Incident Investigation Records Safety Oversight Manual (Doc 9734) Part A — The Establishment and Management of a State Safety Oversight System Part B — The Establishment and Management of a Regional Safety Oversight Organization Technical Instructions for the Safe Transport of Dangerous Goods by Air (Doc 9284) Chapter 1 INTRODUCTION 1.1 WHAT IS SAFETY MANAGEMENT? 1.1.1 Safety management seeks to proactively mitigate safety risks before they result in aviation accidents and incidents. Through the implementation of safety management, States can manage their safety activities in a more disciplined, integrative and focused manner. Possessing a clear understanding of its role and contribution to safe operations enables a State, and its aviation industry, to prioritize actions to address safety risks and more effectively manage its resources for the optimal benefit of aviation safety. 1.1.2 The effectiveness of a State’s safety management activities is strengthened when implemented in a formal and institutionalized way through a State safety programme (SSP) and through safety management systems (SAFETY MANAGEMENT SYSTEMs) for its service providers. A State’s safety programme, combined with the SAFETY MANAGEMENT SYSTEMs of its service providers, systematically addresses safety risks, improves the safety performance of each service provider, and collectively, improves the State’s safety performance. 1.1.3 The State Safety Programme is developed and maintained by each State as a structured approach to assist in managing its aviation safety performance. The existing aviation safety record is achieved through a traditional compliance-based approach and should continue to be treated as the foundation of the State Safety Programme. As such, States should ensure they have effective safety oversight systems in place. More information on the State Safety Programme may be found in Chapter 8. 1.1.4 A State shall require that an SAFETY MANAGEMENT SYSTEM is developed and maintained by those service providers under its authority, as identified in Annex 19 — Safety Management, to continuously improve safety performance by identifying hazards, collecting and analysing data, and continuously assessing and managing safety risks (see paragraph 1.2 for details on SAFETY MANAGEMENT SYSTEM applicability). More information on the implementation of SAFETY MANAGEMENT SYSTEM may be found in Chapter 9. 1.1.5 The INTERNATIONAL CIVIL AVIATION ORGANIZATIONGlobal Aviation Safety Plan (Global Aviation Safety Plan, Doc 10004) objectives call for States to put in place robust and sustainable safety oversight systems and to progressively evolve these into a more sophisticated means of managing safety performance. These objectives align with ICAO’s requirements for the implementation of SSPs by States and SAFETY MANAGEMENT SYSTEMs by service providers. 1.1.6 This performance-based approach to safety offers improvements as it focuses on achieving the desired outcome rather than concentrating solely on whether a State is compliant or not. It is important to note, however, that the implementation of a safety performance approach is collaborative as it requires effort on the part of the aviation industry to develop appropriate means to achieve the specified outcomes and, with respect to States, to evaluate each service provider’s approach. 1.1.7 BENEFITS OF SAFETY MANAGEMENT There are many benefits to implementing safety management, some of which include: a) Strengthened safety culture – An organization’s safety culture can be strengthened by making visible the commitment of management and actively involving personnel in the management of safety risk. When management actively endorses safety as a priority, it is typically well-received by personnel and becomes part of normal operations. b) Documented, process-based approach to assure safety – Establishes a clear and documented approach to achieving safe operations that is understandable by personnel and can be readily explained to others. In addition, clearly defining baseline performance allows controlled changes when continuously improving the safety programme/system, thereby helping the organization optimize resources required to implement change. c) Better understanding of safety-related interfaces and relationships – The process of documenting and defining safety management interfaces can benefit the organization’s understanding of the inter- process relationships, leading to an enhanced understanding of the end-to-end process and exposing opportunities for increased efficiencies. d) Enhanced early detection of safety hazards – Improves the State/service provider's ability to detect emerging safety issues, which can prevent accidents and incidents through the proactive identification of hazards and management of safety risks. e) Safety data-driven decision-making – Improves the State/service provider's ability to gather safety data for the purpose of safety analysis. With some strategic thinking to determine what questions need to be answered, the resulting safety information can airport information desk decision makers, in near real-time, to make better-informed, valid decisions. An important aspect of this decision-making is the allocation of resources to areas of greater concern or need. f) Enhanced communication of safety – Provides a common safety language throughout an organization and industry. A common safety language is a key enabler to the development of a common understanding of the organization’s safety goals and accomplishments. In particular, it provides an appreciation for the organization's safety objectives and its safety performance indicators (SPIs) and safety performance targets (SPTs), which provide the direction and motivation for safety. Personnel will be more aware of the organization’s performance and the progress being made toward achieving the defined safety objectives, as well as how they contribute to the organization’s success. The common safety language enables service providers with multiple aviation businesses to aggregate safety information across organizational entities. It is necessary to support the management of interfaces across the aviation system. g) Evidence that safety is a priority – Demonstrates how management supports and enables safety, how safety risks are identified and managed, and how safety performance is continually improved, resulting in increased confidence by the aviation community, internal and external to the organization. This also results in personnel who are confident about the organization’s safety performance, which can lead to the increased attraction and retention of high calibre staff. It also allows for States and regional safety oversight organizations (RSOOs) to develop confidence in the safety performance of service providers. h) Possible financial savings – May allow for some service providers to qualify for a discount on their insurance premiums and/or a reduction to their workers’ compensation premiums based on their SAFETY MANAGEMENT SYSTEM results. i) Improved efficiencies – Possible reduction in the cost of operations by exposing inefficiencies in existing processes and systems. Integration with other internal or external management systems may also save on additional costs. j) Cost avoidance – Through the proactive identification of hazards and safety risk management (SRM), the cost incurred due to accidents and incidents can be avoided. In such cases, direct costs may include: injuries; property damage; equipment repairs; and schedule delays. Indirect costs may include: legal action; loss of business and damaged reputation; surplus spares; tools and training; increased insurance premiums; loss of staff productivity; equipment recovery and clean-up; loss of use of equipment leading to short-term replacement equipment; and internal investigations. 1.2 SAFETY MANAGEMENT APPLICABILITY State safety management responsibilities are outlined in Annex 19, Chapter 3, and include requiring service providers identified in the Standards and Recommended Practices to implement SAFETY MANAGEMENT SYSTEM. Provisions related to the implementation of SAFETY MANAGEMENT SYSTEMs by service providers may be found in Chapter 4 and Appendix 2 of Annex 19. 1.2.1 SAFETY MANAGEMENT SYSTEM applicability 1.2.1.1 The assessment to determine the applicability of SAFETY MANAGEMENT SYSTEM for Amendment 1 to Annex 19 was based on a set of criteria. These same criteria are expected to be used periodically by INTERNATIONAL CIVIL AVIATION ORGANIZATION and the Safety Management Panel (SMP) to reassess the need to extend the applicability to other aviation organizations. Total system safety approach 1.2.1.2 A total system safety approach considers the entire aviation industry as a system. All service providers, and their systems for the management of safety, are considered as sub-systems. This allows a State to consider the interactions, and cause and effect, throughout the whole system. It is often impossible or impractical to build all safety systems in the same way. Therefore, a primary concern for States and service providers is how to best manage the interfaces between dissimilar interacting systems. 1.2.1.3 When reviewing SAFETY MANAGEMENT SYSTEM applicability, the link between service providers who already have an SAFETY MANAGEMENT SYSTEM requirement under Annex 19 and other organizations conducting an aviation activity was considered. Application of SAFETY MANAGEMENT SYSTEM should reduce the risk of safety gaps or overlaps, not increase safety risk through decreased interoperability. Subcontracting implications 1.2.1.4 For Safety Risk Management to be effective across service providers it is important to clearly define the responsibilities for the identification of hazards and management of associated safety risks for the entire chain of services within the system, without gaps or overlaps. Where a service provider with an SAFETY MANAGEMENT SYSTEM requirement contracts to an organization not subject to SAFETY MANAGEMENT SYSTEM, the hazards and safety risks potentially introduced by the contractor are addressed by the SAFETY MANAGEMENT SYSTEM of the service provider. This places additional Safety Risk Management responsibilities on the service provider to ensure they are knowledgeable about the safety risks induced by the activities of their contractor(s). For more information on Safety Risk Management, see Chapter 2. Safety risk control through regulations 1.2.1.5 States should assess whether the existing legislation and regulations effectively address the hazards entailed by the activity. It could be that existing requirements provide sufficient safety risk mitigation and imposing a requirement for SAFETY MANAGEMENT SYSTEM for those organizations not applicable under Annex 19 may not yield substantial safety benefit. 1.2.2 Extension of discretionary SAFETY MANAGEMENT SYSTEM applicability 1.2.2.1 The applicability criteria outlined above may also serve as guidance for States when considering an extension of SAFETY MANAGEMENT SYSTEM applicability beyond that defined in Annex 19 or the promotion of voluntary implementation. Application of discretionary SAFETY MANAGEMENT SYSTEM applicability should be thoughtfully considered. The decision to extend the SAFETY MANAGEMENT SYSTEM applicability to sectors or service providers should take into account the safety risks identified in the State and if the decision is taken, the SAFETY MANAGEMENT SYSTEM implementation should be monitored as part of the State Safety Programme. Before requiring SAFETY MANAGEMENT SYSTEM, States are asked to consider whether: a) there are any other viable options for achieving the desired improvement in safety performance; and b) sufficient resources are available for the State and industry sector to implement and monitor the SAFETY MANAGEMENT SYSTEM. In particular, consideration needs to be given to the possible impact on staffing and the potential challenge of acquiring and integrating the necessary skills and knowledge. 1.2.2.2 Each State should consider the acceptable level of safety performance (Acceptable level of safety performance) across their industry and institute an SAFETY MANAGEMENT SYSTEM applicability scheme that is most likely to achieve their State’s safety objectives. The SAFETY MANAGEMENT SYSTEM applicability scheme applied will likely evolve in continual alignment with the State’s Acceptable level of safety performance. 1.2.3 Safety management responsibility No provision of Annex 19 is intended to transfer to the State the responsibilities of the aviation service provider or operator. States possess many tools to manage safety within their system. As part of its State Safety Programme, each State should consider the best options for the oversight of aviation activities that may not fall within current INTERNATIONAL CIVIL AVIATION Organization Annexes or that of new or emerging activities. 1.2.4 Applicability for State-owned or military service providers 1.2.4.1 In some States, the service provider function is provided by the State civil service or military. Some civilian service providers provide contracted services to the military, and some military organizations provide civilian service. Regardless of the arrangement, the service provider for the civilian service in the State should be required to address all the applicable INTERNATIONAL CIVIL AVIATION ORGANIZATION Standards and Recommended Practices, including the Annex 19 SAFETY MANAGEMENT SYSTEM requirements without regard to the specific nature of such organization. The State’s or service provider’s system description should have regard for the functions of these organizations and their relationship to each other. The accountable executive of the service provider, whether civil or military, should be capable of explaining the arrangements and how safety risks are managed. Put simply, service providers should manage safety regardless of the organizational arrangements. 1.2.4.2 Where the State operates as a service provider there should be a clear separation between its functions as the service provider and that of the State regulatory authority. This is accomplished by having clearly defined roles and responsibilities for State authority and service provider personnel to avoid any conflicts of interest. 1.2.5 Occupational safety, health and environment versus aviation safety Occupational safety, health and environment (OSHE) (also referred as occupational health and safety (OHS) or workplace health and safety (WHS)) is a field concerned with the safety, health, and welfare of people at work. The primary difference between aviation safety management and Occupational safety, health and environment systems is the intent. In many States employers have a legal duty to take reasonable care of the health and safety of their employees. The intention of Occupational safety, health and environment programmes is to meet the legal and ethical obligations of employers by fostering a safe and healthy work environment. These issues are normally addressed under a different government body from the one that handles aviation matters. As such, Annex 19, Chapter 2, Applicability, intentionally focuses on “safety management functions related to, or in direct support of, the safe operation of aircraft”. 1.3 IMPLEMENTING SAFETY MANAGEMENT 1.3.1 Establishing a solid foundation is essential to achieving effective safety management implementation. The following aspects should be addressed as the first steps in implementing State Safety Programme or SAFETY MANAGEMENT SYSTEM requirements: a) Senior management commitment: It is essential that senior management of all State aviation agencies is committed to effective safety management implementation. b) Compliance with prescriptive requirements: The State should ensure that a mature safety oversight system is in place for the licensing, certification, authorization and approval of individuals and organizations performing aviation activities in their State, including qualified technical personnel. Service providers should ensure that they have processes in place to ensure continued compliance with the established prescriptive requirements. c) Enforcement regime: The State should establish an enforcement policy and frameworks to enable parties to manage and resolve deviations and minor violations. d) Safety information protection: It is essential that States put in place a protective legal framework to ensure the continued availability of safety data and safety information. 1.3.2 System description The system description is a summary of the organization’s (State or service provider) processes, activities and interfaces that need to be assessed for hazard identification and safety risk assessment that is covered by their safety system. It describes the aviation system, within which the organization functions, and the various entities and authorities involved. It includes interfaces within the organization, as well as interfaces with external organizations that contribute to the safe delivery of services. The system description provides a starting point to implement the State Safety Programme/SAFETY MANAGEMENT SYSTEM. More information on the system description for States and service providers may be found in Chapters 8 and 9, respectively. 1.3.3 Interfaces 1.3.3.1 When States and service providers are considering implementing safety management it is important to consider the safety risks induced by interfacing entities. Interfaces can be internal (e.g. between operations and maintenance, or finance, human resources or legal departments), or they can be external (e.g. other State, service providers or contracted services). States and service providers have greater control over any related safety risks when interfaces are identified and managed. Interfaces are defined as part of the system description. Interface safety impact assessment 1.3.3.2 Once a State or service provider has identified its interfaces, the safety risk posed by each interface is assessed using the organization’s existing safety risk assessment processes (see Chapter 2 for details). Based on the safety risks identified, the State or service provider may consider working with other organizations to determine an appropriate safety risk control strategy. Organizations working collaboratively may be able to identify more interface hazards; assessing any related safety risks and determining mutually appropriate controls. Collaboration is highly desirable because the safety risk perception may vary between organizations. 1.3.3.3 It is also important to recognize that each organization involved is responsible for identifying and managing any identified hazards that affect its organization. The criticality of the interface may differ for each organization. Each organization might reasonably apply different safety risk classifications and have different safety risk priorities (in terms of safety performance, resources, time). Monitoring and management of interfaces 1.3.3.4 States and service providers are responsible for ongoing monitoring and management of their interfaces to ensure the safe provision of services. An effective approach to interface Safety Risk Management is to establish formal agreements between interfacing organizations with clearly defined monitoring and management responsibilities. Documenting and sharing all interface safety issues, safety reports and lessons learned, as well as safety risks between interfacing organizations will ensure clear understanding. Sharing enables transfer of knowledge and working practices that could improve the safety effectiveness of each organization. 1.3.4 Implementation planning 1.3.4.1 Performing a gap analysis before embarking on the implementation of State Safety Programme/SAFETY MANAGEMENT SYSTEM will allow an organization to identify the gap between the current organizational structures and processes, and those required for effective State Safety Programme or SAFETY MANAGEMENT SYSTEM operation. For State Safety Programme, it is important to include a review of the Universal Safety Oversight Audit Programme (USOAP) protocol questions which are considered as the foundation of the State Safety Programme. 1.3.4.2 The State Safety Programme or SAFETY MANAGEMENT SYSTEM implementation plan is, as the name implies, a plan for State Safety Programme/SAFETY MANAGEMENT SYSTEM implementation. It provides a clear description of the resources, tasks and processes required, and an indicative timing and sequencing of key tasks and responsibilities. More information on the implementation of safety management for States and service providers may be found in Chapters 8 and 9, respectively. Maturity assessment 1.3.4.3 Soon after the key components and elements of the State Safety Programme or SAFETY MANAGEMENT SYSTEM are implemented, periodic assessments should be conducted to monitor how effectively it is working. As the system matures, the organization should seek assurance that it is operating as intended and is effective at achieving its stated safety objectives and targets. Safety management takes time to mature and the aim should be to maintain or continuously improve the safety performance of the organization. 1.3.5 Size and complexity considerations 1.3.5.1 Each State and each service provider is different. SSPs and SAFETY MANAGEMENT SYSTEMs are designed to be tailored to meet the specific needs of each State or service provider. All components and all elements of State Safety Programme/SAFETY MANAGEMENT SYSTEM are interconnected and interdependent, and necessary to function effectively. It is important that State Safety Programme and SAFETY MANAGEMENT SYSTEM requirements are not implemented only in a prescriptive manner. The traditional prescriptive requirements are to be complemented with a performance-based approach. 1.3.5.2 The programme/system is designed to deliver the desired outcomes for each organization without undue burden. State Safety Programme and SAFETY MANAGEMENT SYSTEM, well implemented, are intended to complement and enhance the organization’s existing systems and processes. Effective safety management will be achieved through thoughtful planning and implementation, ensuring each requirement is addressed in ways that fit the organization’s culture and operating environment. More information on what to consider when implementing State Safety Programme/SAFETY MANAGEMENT SYSTEM for States and service providers may be found in Chapters 8 and 9, respectively. 1.3.6 Integrating the basic elements It is important to note that all systems are composed of three basic elements: people; processes; and technology. Safety management is no exception. When establishing or maintaining the different processes, activities and functions, all States and service providers should ensure they have considered the intention of each requirement and, most importantly, how they will work together to enable the organization to meet its safety objectives. Each of these elements of safety management, and the interrelationships, will be covered throughout this manual. 1.4 INTEGRATED RISK MANAGEMENT 1.4.1 The aviation system as a whole comprises many and different functional systems such as finance, environment, safety and security. The latter two are the primary operational domains of the greater aviation system. As concepts they share important features as they are all concerned with the risk of events with consequence of various magnitudes. Nevertheless, they differ in the important element of intent. Security is concerned with malicious, intentional acts to disrupt the performance of a system. Safety focuses on the negative impact to the concerned systems’ performance caused by unintended consequences of a combination of factors. 1.4.2 In the operational context, all of the functional systems produce some sort of risk that needs to be appropriately managed to lessen any adverse consequence. Traditionally, each system has developed sector specific risk management frameworks and practices designed to address the distinct characteristics of each system. Most of those risk management practices include comprehensive analysis on intra-system consequences, often referred to as the management of unintended consequences. Another aspect is inter-system consequences resulting from system specific risk management processes. This relates to the fact that an effective risk management strategy of one specific sector can have an adverse impact on another operational sector of aviation. In aviation, the most often emphasized inter-system dependence is the safety/security dilemma. Effective security measures may have negative impacts on safety, and vice versa. Safety and security domains may differ in the element of underlying intent, but they converge in their common goal to protect people and assets (e.g. addressing cyber threats and risks requires coordination across the aviation safety and security domains). In some cases the management of the inherent risk of one may affect the other domain in unforeseen ways, such as in the following examples: a) reinforced cockpit doors necessitated due to security risks may have safety implications on the operation of an aircraft; b) restrictions on the carriage of personal electronic devices in the cabin may displace the security risk from the cabin to the cargo hold, leading to heightened safety risk; and c) change of routes to avoid flying over conflict zones may result in congested air corridors that pose a safety issue. 1.4.3 Successful risk management in aviation should aim for overall risk reduction in the system, including all of the involved functional systems. This process requires the analytical assessment of the whole system at the highest level of the appropriate entity (State, regional organizations, service providers). The assessment and integration of functional system needs and interdependence are referred to as integrated risk management (IRM). IRM focuses on the overall risk reduction of the organization. This is achieved through the quantitative and qualitative analysis of both the inherent risks, and the effectiveness and impact of sector-specific risk management processes. IRM has a system-wide responsibility to coordinate, harmonize and optimize risk management processes with the single goal of risk reduction. IRM cannot replace the operating specific risk managements of the functional systems, and does not intend to delegate additional duties and responsibilities to them. IRM is a distinct high-level concept to leverage the expert advice of sector- specific risk management and provide holistic feedback to achieve the highest level of system performance at a socially acceptable level. More information related to safety risk management, which is within the scope of this manual, may be found in Chapters 2, 8 (for States) and 9 (for service providers). Note.— The structure and areas of responsibility of the government within the State may affect oversight of each area. For example, the civil aviation authority (CAA) having responsibility for aviation safety, while the environmental protection agency has responsibility for environmental oversight. Each oversight entity may have different requirements and methodologies. Chapter 2 SAFETY MANAGEMENT FUNDAMENTALS 2.1 THE CONCEPT OF SAFETY AND ITS EVOLUTION 2.1.1 This chapter provides an overview of fundamental safety management concepts and practices. It is important to understand these fundamentals before focusing on the specifics of safety management found in the subsequent chapters. 2.1.2 Within the context of aviation, safety is “the state in which risks associated with aviation activities, related to, or in direct support of the operation of aircraft, are reduced and controlled to an acceptable level”. 2.1.3 Aviation safety is dynamic. New safety hazards and risks continuously emerge and must be mitigated. As long as safety risks are kept under an appropriate level of control, a system as open and dynamic as aviation can still be kept safe. It is important to note that acceptable safety performance is often defined and influenced by domestic and international norms and culture. 2.1.4 Progress in aviation safety can be described by four approaches, which roughly align with eras of activity. The approaches are listed below and are illustrated in Figure 2-1. a) Technical — From the early 1900s until the late 1960s, aviation emerged as a form of mass transportation in which identified safety deficiencies were initially related to technical factors and technological failures. The focus of safety endeavours was therefore placed on the investigation and improvement of technical factors (the aircraft, for example). By the 1950s, technological improvements led to a gradual decline in the frequency of accidents, and safety processes were broadened to encompass regulatory compliance and oversight. b) Human factors — By the early 1970s, the frequency of aviation accidents had significantly declined due to major technological advances and enhancements to safety regulations. Aviation became a safer mode of transportation, and the focus of safety endeavours was extended to include human factors, including such things as the “man/machine interface”. Despite the investment of resources in error mitigation, human factors continue to be cited as a recurring factor in accidents. Human factors tended to focus on the individual, without fully considering the operational and organizational context. It was not until the early 1990s that it was acknowledged that individuals operate in a complex environment that included multiple factors which could affect behaviour. c) Organizational — During the mid-1990s, safety began to be viewed from a systemic perspective and began encompassing organizational factors as well as human and technical factors. The notion of an “organizational accident” was introduced. This perspective considered the impact of such things as organizational culture and policies on the effectiveness of safety risk controls. Additionally, routine safety data collection and analysis using reactive and proactive methodologies enabled organizations to monitor known safety risks and detect emerging safety trends. These enhancements provided the learning and foundation which lead to the current safety management approach. d) Total system — From the beginning of the 21st century, many States and service providers had embraced the safety approaches of the past and evolved to a higher level of safety maturity. They have begun implementing State Safety Programme or SAFETY MANAGEMENT SYSTEMs and are reaping the safety benefits. However, safety systems to date have focused largely on individual safety performance and local control, with minimal regard for the wider context of the total aviation system. This has led to growing recognition of the complexity of the aviation system and the different organizations that all play a part in aviation safety. There are many examples of accidents and incidents showing that the interfaces between organizations have contributed to negative outcomes. 2.1.5 The steady, compounding evolution of safety has led States and service providers to a point where they are giving serious consideration to the interactions and interfaces between the components of the system: people, processes, and technologies. This has led to a greater appreciation for the positive role people play in the system. Safety benefits from collaboration between service providers, and between service providers and States. This perspective has nurtured multiple collaborative initiatives between service providers and an appreciation of the benefits of collaboration when addressing safety issues. The INTERNATIONAL CIVIL AVIATION ORGANIZATIONRunway Safety Programme is a good example. 2.1.6 For the collaborative total system approach to flourish, the interfaces and interactions between the organizations (including States) need to be well understood and managed. States are also beginning to recognize the role the total aviation system approach can play in their State Safety Programme development. For example, it helps to manage safety risks which cut across multiple aviation activities. 2.2 HUMANS IN THE SYSTEM 2.2.1 How people think about their responsibilities towards safety and how they interact with others to perform their tasks at work significantly affects their organization’s safety performance. Managing safety needs to address how people contribute, both positively and negatively, to organizational safety. Human factors is about: understanding the ways in which people interact with the world, their capabilities and limitations, and influencing human activity to improve the way people do their work. As a result, the consideration of human factors is an integral part of safety management, necessary to understand, identify and mitigate risks as well as to optimize the human contributions to organizational safety. 2.2.2 The following are key ways in which safety management processes consider human factors: a) senior management commitment to creating a working environment that optimizes human performance and encourages personnel to actively engage in and contribute to the organization’s safety management processes; b) responsibilities of personnel with respect to safety management are clarified to ensure common understanding and expectations; c) personnel are provided with information by the organization that: 1) describes the expected behaviours in respect to the organizational processes and procedures; 2) describes what actions will be taken by the organization in response to individual behaviours; d) human resourcing levels are monitored and adjusted to ensure there are enough individuals to meet operational demands; e) policies, processes and procedures are established to encourage safety reporting; f) safety data and safety information are analysed to allow consideration of those risks related to variable human performance and human limitations, with particular attention to any associated organizational and operational factors; g) policies, processes and procedures are developed that are clear, concise and workable, with the aim of: 1) optimizing human performance; 2) preventing inadvertent errors; 3) reducing the unwanted consequences of variable human performance; the effectiveness of these are continually monitored during normal operations; h) ongoing monitoring of normal operations includes assessment of whether processes and procedures are followed and, when they are not followed, investigations are carried out to determine the cause; i) safety investigations include the assessment of contributing human factors, examining not only behaviours but reasons for such behaviours (context), with the understanding that in most cases people are doing their best to get the job done; j) management of change process includes consideration of the evolving tasks and roles of the human in the system; k) personnel are trained to ensure they are competent to perform their duties, the effectiveness of training is reviewed and training programmes are adapted to meet changing needs. 2.2.3 The effectiveness of safety management depends largely on the degree of senior support and management commitment to create a working environment that optimizes human performance and encourages personnel to actively engage in and contribute to the organization’s safety management processes. 2.2.4 To address the way that the organization influences human performance there must be senior level support to implement effective safety management. This includes management commitment to create the right working environment and the right safety culture to address human factors. This will also influence the attitudes and behaviours of everyone in the organization. More information on safety culture can be found in Chapter 3. 2.2.5 A number of models have been created to support the assessment of human factors on safety performance. The software hardware environment liveware Model is well known and useful to illustrate the impact and interaction of the different system components on the human, and emphasizes the need to consider human factors as an integrated part of Safety Risk Management. 2.2.6 Figure 2-2 illustrates the relationship between the human (at the centre of the model) and workplace components. The software hardware environment liveware Model contains four satellite components: a) Software (S): procedures, training, support, etc.; b) Hardware (H): machines and equipment; c) Environment (E): the working environment in which the rest of the L-H-S system must function; and d) Liveware (L): other humans in the workplace. 2.2.7 Liveware. The critical focus of the model is the humans at the front line of operations, and depicted in the centre of the model. However, of all the dimensions in the model, this is the one which is least predictable and most susceptible to the effects of internal (hunger, fatigue, motivation, etc.) and external (temperature, light, noise, etc.) influences. Although humans are remarkably adaptable, they are subject to considerable variations in performance. Humans are not standardized to the same degree as hardware, so the edges of this block are not simple and straight. The effects of irregularities at the interfaces between the various software hardware environment liveware blocks and the central Liveware block should be understood to avoid tensions that may compromise human performance. The jagged edges of the modules represent the imperfect coupling of each module. This is useful in visualizing the following interfaces between the various components of the aviation system: a) Liveware-Hardware (L-H). The L-H interface refers to the relationship between the human and the physical attributes of equipment, machines and facilities. This considers the ergonomics of operating the equipment by personnel, how safety information is displayed and how switches and operating levers are labelled and operated so they are logical and intuitive to operate. b) Liveware-Software (L-S). The L-S interface is the relationship between the human and the supporting systems found in the workplace, e.g. regulations, manuals, checklists, publications, processes and procedures, and computer software. It includes such issues as the recency of experience, accuracy, format and presentation, vocabulary, clarity and the use of symbols. L-S considers the processes and procedures - how easy they are to follow and understand. c) Liveware-Liveware (L-L). The L-L interface is the relationship and interaction between people in their work environment. Some of these interactions are within the organization (colleagues, supervisors, managers), many are between individuals from different organizations with different roles (air traffic controllers with pilots, pilots with engineers etc.). It considers the importance of communication and interpersonal skills, as well as group dynamics, in determining human performance. The advent of crew resource management and its extension to air traffic services (ATS) and maintenance operations has enabled organizations to consider team performance in the management of errors. Also within the scope of this interface are staff/management relationships and organizational culture. d) Liveware-Environment (L-E). This interface involves the relationship between the human and the physical environment. This includes things such as temperature, ambient light, noise, vibration and air quality. It also considers the externally environmental factors, such as weather, infrastructure and terrain. 2.3 ACCIDENT CAUSATION 2.3.1 The “Swiss-Cheese” (or Reason) Model, developed by Professor James Reason and well known to the aviation industry, illustrates that accidents involve successive breaches of multiple defences. These breaches can be triggered by a number of enabling factors such as equipment failures or operational errors. The Swiss-Cheese Model contends that complex systems such as aviation are extremely well defended by layers of defences (otherwise known as “barriers”). A single-point failure is rarely consequential. Breaches in safety defences can be a delayed consequence of decisions made at the higher levels of the organization, which may remain dormant until their effects or damaging potential are activated by certain operating conditions (known as latent conditions). Under such specific circumstances, human failures (or “active failures”) at the operational level act to breach the final layers of safety defence. The Reason Model proposes that all accidents include a combination of both active failures and latent conditions. 2.3.2 Active failures are actions or inactions, including errors and rule-breaking, that have an immediate adverse effect. They are viewed, with the benefit of hindsight, as unsafe acts. Active failures are associated with front-line personnel (pilots, air traffic controllers, aircraft maintenance engineers, etc.) and may result in a harmful outcome. 2.3.3 Latent conditions can exist in the system well before a damaging outcome. The consequences of latent conditions may remain dormant for a long time. Initially, these latent conditions are not perceived as harmful, but under certain conditions may become clear when the operational level defences are breached. People far removed in time and space from the event can create these conditions. Latent conditions in the system may include those created by the safety culture; equipment choices or procedural design; conflicting organizational goals; defective organizational systems; or management decisions. 2.3.4 The “organizational accident” paradigm assists by identifying these latent conditions on a system-wide basis, rather than through localized efforts, to minimize active failures by individuals. Importantly, latent conditions, when created, had good intentions. Organizational decision makers are often balancing finite resources, and potentially conflicting priorities and costs. The decisions taken by decision makers, made on a daily basis in large organizations, might, in particular circumstances, unintentionally lead to a damaging outcome. 2.3.5 Figure 2-3 illustrates how the Swiss-Cheese Model assists in understanding the interplay of organizational and managerial factors in accident causation. Multiple defensive layers are built into the aviation system to protect against variations in human performance or decisions at all levels of the organization. But each layer typically has weaknesses, depicted by the holes in the slices of “Swiss cheese”. Sometimes all of the weaknesses align (represented by the aligned holes) leading to a breach that penetrates all defensive barriers and may result in a catastrophic outcome. The Swiss-Cheese Model represents how latent conditions are ever present within the system and can manifest through local trigger factors. 2.3.6 It is important to recognize that some of the defences, or breaches, can be influenced by an interfacing organization. It is therefore vitally important that service providers assess and manage these interfaces. 2.3.7.1 The “Swiss-Cheese” Model can be used as an analysis guide by both States and service providers by looking past the individuals involved in an incident or identified hazard, into the organizational circumstances which may have allowed the situation to manifest. It can be applied during Safety Risk Management, safety surveillance, internal auditing, change management and safety investigation. In each case, the model can be used to consider which of the organization’s defences are effective, which can or have been breached, and where the system could benefit from additional defences. Once identified, any weaknesses in the defences can be reinforced against future accidents and incidents. 2.3.7.2 In practice, the event will breach the defences in the direction of the arrow (hazards to losses) as displayed in the rendering of Figure 2-3. The assessments of the situation will be conducted in the opposite direction, in this case losses to hazard. Actual aviation accidents will usually include a degree of additional complexity. There are more sophisticated models which can help States and service providers to understand how and why accidents happen. 2.3.8 Practical drift 2.3.8.1 Scott A. Snook's theory of practical drift is used to understand how performance of any system “drifts away” from its original design. Tasks, procedures, and equipment are often initially designed and planned in a theoretical environment, under ideal conditions, with an implicit assumption that nearly everything can be predicted and controlled, and where everything functions as expected. This is usually based on three fundamental assumptions that the: a) technology needed to achieve the system production goals is available; b) personnel are trained, competent and motivated to properly operate the technology as intended; and c) policy and procedures will dictate system and human behaviour. These assumptions underlie the baseline (or ideal) system performance, which can be graphically presented as a straight line from the start of operational deployment as shown in Figure 2-4. 2.3.8.2 Once operationally deployed, the system should ideally perform as designed, following baseline performance (orange line) most of the time. In reality, the operational performance often differs from the assumed baseline performance as a consequence of real-life operations in a complex, ever-changing and usually demanding environment (red line). Since the drift is a consequence of daily practice, it is referred to as a “practical drift”. The term “drift” is used in this context as the gradual departure from an intended course due to external influences. 2.3.8.3 Snook contests that practical drift is inevitable in any system, no matter how careful and well thought out its design. Some of the reasons for the practical drift include: a) technology that does not operate as predicted; b) procedures that cannot be executed as planned under certain operational conditions; c) changes to the system, including the additional components; d) interactions with other systems; e) safety culture; f) adequacy (or inadequacy) of resources (e.g. support equipment); g) learning from successes and failures to improve operations, and so forth. 2.3.8.4 In reality people will generally make the system work on a daily basis despite the system’s shortcomings, applying local adaptations (or workarounds) and personal strategies. These workarounds may bypass the protection of existing safety risk controls and defences. 2.3.8.5 Safety assurance activities such as audits, observations and monitoring of SPIs can help to expose activities that are “practically drifting”. Analysing the safety information to find out why the drift is happening helps to mitigate the safety risks. The closer to the beginning of the operational deployment that practical drift is identified, the easier it is for the organization to intervene. More information on safety assurance for States and service providers may be found in Chapters 8 and 9, respectively. 2.4 MANAGEMENT DILEMMA 2.4.1 In any organization engaged in the delivery of services, production/profitability and safety risks are linked. An organization must maintain profitability to stay in business by balancing output with acceptable safety risks (and the costs involved in implementing safety risk controls). Typical safety risk controls include technology, training, processes and procedures. For the State, the safety risk controls are similar, i.e. training of personnel, the appropriate use of technology, effective oversight and the internal processes and procedures supporting oversight. Implementing safety risk controls comes at a price – money, time, resources – and the aim of safety risk controls is usually to improving safety performance, not production performance. However, some investments in “protection” can also improve “production” by reducing accidents and incidents and thereby their associated costs. 2.4.2 The safety space is a metaphor for the zone where an organization balances desired production/profitability while maintaining required safety protection through safety risk controls. For example, a service provider may wish to invest in new equipment. The new equipment may simultaneously provide the necessary efficiency improvements as well as improved reliability and safety performance. Such decision-making involves an assessment of both the benefits to the organization as well as the safety risks involved. The allocation of excessive resources to safety risk controls may result in the activity becoming unprofitable, thus jeopardizing the viability of the organization. 2.4.3 On the other hand, excess allocation of resources for production at the expense of protection can have an impact on the product or service and can ultimately lead to an accident. It is therefore essential that a safety boundary be defined that provides early warning that an unbalanced allocation of resources exists, or is developing. Organizations use financial management systems to recognize when they are getting too close to bankruptcy and apply the same logic and tools used by safety management to monitor their safety performance. This enables the organization to operate profitably and safely within the safety space. Figure 2-5 illustrates the boundaries of an organization’s safety space. Organizations need to continuously monitor and manage their safety space as safety risks and external influences change over time. 2.4.4 The need to balance profitability and safety (or production and protection) has become a readily understood and accepted requirement from a service provider perspective. This balance is equally applicable to the State’s management of safety, given the requirement to balance resources required for State protective functions that include certification and surveillance. 2.5 SAFETY RISK MANAGEMENT Safety Risk Management (Safety Risk Management) is a key component of safety management and includes hazard identification, safety risk assessment, safety risk mitigation and risk acceptance. Safety Risk Management is a continuous activity because the aviation system is constantly changing, new hazards can be introduced and some hazards and associated safety risks may change over time. In addition, the effectiveness of implemented safety risk mitigation strategies must be monitored to determine if further action is required. 2.5.1 Introduction to hazards 2.5.1.1 In aviation, a hazard can be considered as a dormant potential for harm which is present in one form or another within the system or its environment. This potential for harm may appear in different forms, for example: as a natural condition (e.g. terrain) or technical status (e.g. runway markings). 2.5.1.2 Hazards are an inevitable part of aviation activities, however, their manifestation and possible adverse consequences can be addressed through mitigation strategies which aim to contain the potential for the hazard to result in an unsafe condition. Aviation can coexist with hazards so long as they are controlled. Hazard identification is the first step in the Safety Risk Management process. It precedes a safety risk assessment and requires a clear understanding of hazards and their related consequences. 2.5.2 Understanding hazards and their consequences 2.5.2.1 Hazard identification focuses on conditions or objects that could cause or contribute to the unsafe operation of aircraft or aviation safety-related equipment, products and services (guidance on distinguishing hazards that are directly pertinent to aviation safety from other general/industrial hazards is addressed in subsequent paragraphs). 2.5.2.2 Consider, for example, a fifteen-knot wind. Fifteen-knots of wind is not necessarily a hazardous condition. In fact, a fifteen-knot wind blowing directly down the runway improves aircraft take-off and landing performance. But if the fifteen-knot wind is blowing across the runway, a crosswind condition is created which may be hazardous to operations. This is due to its potential to contribute to aircraft instability. The reduction in control could lead to an occurrence, such as a lateral runway excursion. 2.5.2.3 It is not uncommon for people to confuse hazards with their consequences. A consequence is an outcome that can be triggered by a hazard. For example, a runway excursion (overrun) is a potential consequence related to the hazard of a contaminated runway. By clearly defining the hazard first, one can more readily identify possible consequences. 2.5.2.4 In the crosswind example above, an immediate outcome of the hazard could be loss of lateral control followed by a consequent runway excursion. The ultimate consequence could be an accident. The damaging potential of a hazard can materialize through one or many consequences. It is important that safety risk assessments identify all of the possible consequences. The most extreme consequence - loss of human life - should be differentiated from those that involve lesser consequences, such as: aircraft incidents; increased flight crew workload; or passenger discomfort. The description of the consequences will inform the risk assessment and subsequent development and implementation of mitigations through prioritization and allocation of resources. Detailed and thorough hazard identification will lead to more accurate assessment of safety risks. Hazard identification and prioritization 2.5.2.5 Hazards exist at all levels in the organization and are detectable through many sources including reporting systems, inspections, audits, brainstorming sessions and expert judgement. The goal is to proactively identify hazards before they lead to accidents, incidents or other safety-related occurrences. An important mechanism for proactive hazard identification is a voluntary safety reporting system. Additional guidance on voluntary safety reporting systems can be found in Chapter 5. Information collected through such reporting systems may be supplemented by observations or findings recorded during routine site inspections or organizational audits. 2.5.2.6 Hazards can also be identified in the review or study of internal and external investigation reports. A consideration of hazards when reviewing accident or incident investigation reports is a good way to enhance the organization’s hazard identification system. This is particularly important when the organization’s safety culture is not yet mature enough to support effective voluntary safety reporting, or in small organizations with limited events or reports. An important source of specific hazards linked to operations and activities is from external sources such as ICAO, trade associations or other international bodies. 2.5.2.7 Hazard identification may also consider hazards that are generated outside of the organization and hazards that are outside the direct control of the organization, such as extreme weather or volcanic ash. Hazards related to emerging safety risks are also an important way for organizations to prepare for situations that may eventually occur. 2.5.2.8 The following should be considered when identifying hazards: a) system description; b) design factors, including equipment and task design; c) human performance limitations (e.g. physiological, psychological, physical and cognitive); d) procedures and operating practices, including documentation and checklists, and their validation under actual operating conditions; e) communication factors, including media, terminology and language; f) organizational factors, such as those related to the recruitment, training and retention of personnel, compatibility of production and safety goals, allocation of resources, operating pressures and corporate safety culture; g) factors related to the operational environment (e.g. weather, ambient noise and vibration, temperature and lighting); h) regulatory oversight factors, including the applicability and enforceability of regulations, and the certification of equipment, personnel and procedures; i) performance monitoring systems that can detect practical drift, operational deviations or a deterioration of product reliability; j) human-machine interface factors; and k) factors related to the State Safety Programme/SAFETY MANAGEMENT SYSTEM interfaces with other organizations. Occupational safety health and environment hazards 2.5.2.9 Safety risks associated with compound hazards that simultaneously impact aviation safety as well as Occupational safety, health and environment may be managed through separate (parallel) risk mitigation processes to address the separate aviation and Occupational safety, health and environment consequences, respectively. Alternatively, an integrated aviation and Occupational safety, health and environment risk mitigation system may be used to address compound hazards. An example of a compound hazard is a lightning strike on an aircraft at an airport transit gate. This hazard may be deemed by an Occupational safety, health and environment inspector to be a “workplace hazard” (ground personnel/workplace safety). To an aviation safety inspector, it is also an aviation hazard with risk of damage to the aircraft and a risk to passenger safety. It is important to consider both the Occupational safety, health and environment and aviation safety consequences of such compound hazards, since they are not always the same. The purpose and focus of preventive controls for Occupational safety, health and environment and aviation safety consequences may differ. Hazard identification methodologies 2.5.2.10 The two main methodologies for identifying hazards are: a) Reactive. This methodology involves analysis of past outcomes or events. Hazards are identified through investigation of safety occurrences. Incidents and accidents are an indication of system deficiencies and therefore can be used to determine which hazard(s) contributed to the event. b) Proactive. This methodology involves collecting safety data of lower consequence events or process performance and analysing the safety information or frequency of occurrence to determine if a hazard could lead to an accident or incident. The safety information for proactive hazard identification primarily comes from flight data analysis (FDA) programmes, safety reporting systems and the safety assurance function. 2.5.2.11 Hazards can also be identified through safety data analysis which identifies adverse trends and makes predictions about emerging hazards, etc. Hazards related to SAFETY MANAGEMENT SYSTEM interfaces with external organizations 2.5.2.12 Organizations should also identify hazards related to their safety management interfaces. This should, where possible, be carried out as a joint exercise with the interfacing organizations. The hazard identification should consider the operational environment and the various organizational capabilities (people, processes, technologies) which could contribute to the safe delivery of the service or product’s availability, functionality or performance. 2.5.2.13 As an example, an aircraft turnaround involves many organizations and operational personnel all working in and around the aircraft. There are likely to be hazards related to the interfaces between operational personnel, their equipment and the coordination of the turnaround activity. 2.5.3 Safety risk probability 2.5.3.1 Safety risk probability is the likelihood that a safety consequence or outcome will occur. It is important to envisage a variety of scenarios so that all potential consequences can be considered. The following questions can assist in the determination of probability: a) Is there a history of occurrences similar to the one under consideration, or is this an isolated occurrence? b) What other equipment or components of the same type might have similar issues? c) What is the number of personnel following, or subject to, the procedures in question? d) What is the exposure of the hazard under consideration? For example, during what percentage of the operation is the equipment or activity in use? 2.5.3.2 Taking into consideration any factors that might underlie these questions will help when assessing the probability of the hazard consequences in any foreseeable scenario. 2.5.3.3 An occurrence is considered foreseeable if any reasonable person could have expected the kind of occurrence to have happened under the same circumstances. Identification of every conceivable or theoretically possible hazard is not possible. Therefore, good judgment is required to determine an appropriate level of detail in hazard identification. Service providers should exercise due diligence when identifying significant and reasonably foreseeable hazards related to their product or service. Note.— Regarding product design, the term “foreseeable” is intended to be consistent with its use in airworthiness regulations, policy, and guidance. 2.5.3.4 Table 1 presents a typical safety risk probability classification table. It includes five categories to denote the probability related to an unsafe event or condition, the description of each category, and an assignment of a value to each category. This example uses qualitative terms; quantitative terms could be defined to provide a more accurate assessment. This will depend on the availability of appropriate safety data and the sophistication of the organization and operation. Table 1. Safety risk probability table Likelihood Meaning Value Frequent Likely to occur many times (has occurred frequently) 5 Occasional Likely to occur sometimes (has occurred infrequently) 4 Remote Unlikely to occur, but possible (has occurred rarely) 3 Improbable Very unlikely to occur (not known to have occurred) 2 Extremely improbable Almost inconceivable that the event will occur 1 Note.— This is an example only. The level of detail and complexity of tables and matrices should be adapted to the particular needs and complexities of each organization. It should also be noted that organizations might include both qualitative and quantitative criteria. 2.5.4 Safety risk severity 2.5.4.1 Once the probability assessment has been completed, the next step is to assess the severity, taking into account the potential consequences related to the hazard. Safety risk severity is defined as the extent of harm that might reasonably be expected to occur as a consequence or outcome of the identified hazard. The severity classification should consider: a) fatalities or serious injury which would occur as a result of: 1) being in the aircraft; 2) having direct contact with any part of the aircraft, including parts which have become detached from the aircraft; or 3) having direct exposure to jet blast; and b) damage: 1) damage or structural failure sustained by the aircraft which: i) adversely affects the structural strength, performance or flight characteristics of the aircraft; ii) would normally require major repair or replacement of the affected component; 2) damage sustained by Air Traffic Services or aerodrome equipment which: i) adversely affects the management of aircraft separation; or ii) adversely affects landing capability. 2.5.4.2 The severity assessment should consider all possible consequences related to a hazard, taking into account the worst foreseeable situation. Table 2 presents a typical safety risk severity table. It includes five categories to denote the level of severity, the description of each category, and the assignment of a value to each category. As with the safety risk probability table, this table is an example only. Table 2. Example safety risk severity table Catastrophic • Aircraft / equipment destroyed • Multiple deaths A Hazardous • A large reduction in safety margins, physical distress or a workload such that operational personnel cannot be relied upon to perform their tasks accurately or completely • Serious injury • Major equipment damage B Major • A significant reduction in safety margins, a reduction in the ability of operational personnel to cope with adverse operating conditions as a result of an increase in workload or as a result of conditions impairing their efficiency • Serious incident • Injury to persons C Minor • Nuisance • Operating limitations • Use of emergency procedures • Minor incident D Negligible • Few consequences E 2.5.5 Safety risk tolerability 2.5.5.1 The safety risk index rating is created by combining the results of the probability and severity scores. In the example above, it is an alphanumeric designator. The respective severity/probability combinations are presented in the safety risk assessment matrix in Table 3. The safety risk assessment matrix is used to determine safety risk tolerability. Consider, for example, a situation where the safety risk probability has been assessed as Occasional (4), and the safety risk severity has been assessed as Hazardous (B), resulting in a safety risk index of (4B). Table 3. Example safety risk matrix Safety Risk Severity Probability Catastrophic A Hazardous B Major C Minor D Negligible E Frequent 5 5A 5B 5C 5D 5E Occasional 4 4A 4B 4C 4D 4E Remote 3 3A 3B 3C 3D 3E Improbable 2 2A 2B 2C 2D 2E Extremely improbable 1 1A 1B 1C 1D 1E Note.— In determining the safety risk tolerability, the quality and reliability of the data used for the hazard identification and safety risk probability should be taken into consideration. 2.5.5.2 The index obtained from the safety risk assessment matrix should then be exported to a safety risk tolerability table that describes — in a narrative form — the tolerability criteria for the particular organization. Table 4 presents an example of a safety risk tolerability table. Using the example above, the criterion for safety risk assessed as 4B falls in the “intolerable” category. In this case, the safety risk index of the consequence is unacceptable. The organization should therefore take risk control action to reduce: a) the organization’s exposure to the particular risk, i.e., reduce the probability component of the risk to an acceptable level; b) the severity of consequences related to the hazard, i.e., reduce the severity component of the risk to an acceptable level; or c) both the severity and probability so that the risk is managed to an acceptable level. 2.5.5.3 Safety risks are conceptually assessed as acceptable, tolerable or intolerable. Safety risks assessed as initially falling in the intolerable region are unacceptable under any circumstances. The probability and/or severity of the consequences of the hazards are of such a magnitude, and the damaging potential of the hazard poses such a threat to safety, that mitigation action is required or activities are stopped. Table 4. Example of safety risk tolerability 5A, 5B, 5C, 4A, 4B, 3A INTOLERABLE Take immediate action to mitigate the risk or stop the activity. Perform priority safety risk mitigation to ensure additional or enhanced preventative controls are in place to bring down the safety risk index to tolerable. 5D, 5E, 4C, 4D, 4E, 3B, 3C, 3D, 2A, 2B, 2C, 1A TOLERABLE Can be tolerated based on the safety risk mitigation. It may require management decision to accept the risk. 3E, 2D, 2E, 1B, 1C, 1D, 1E ACCEPTABLE Acceptable as is. No further safety risk mitigation required. 2.5.6 Assessing human factors related risks 2.5.6.1 The consideration of human factors has particular importance in Safety Risk Management as people can be both a source and a solution of safety risks by: a) contributing to an accident or incident through variable performance due to human limitations; b) anticipating and taking appropriate actions to avoid a hazardous situation: and c) solving problems, making decisions and taking actions to mitigate risks. 2.5.6.2 It is therefore important to involve people with appropriate human factors expertise in the identification, assessment and mitigation of risks. 2.5.6.3 Safety Risk Management requires all aspects of safety risk to be addressed, including those related to humans. Assessing the risks associated with human performance is more complex than risk factors associated with technology and environment since: a) human performance is highly variable, with a wide range of interacting influences internal and external to the individual. Many of the effects of the interaction between these influences are difficult, or impossible to predict; and b) the consequences of variable human performance will differ according to the task being performed and the context. 2.5.6.4 This complicates how the probability and the severity of the risk is determined. Therefore, human factors expertise is valuable in the identification and assessment of safety risks. (The management of fatigue using SAFETY MANAGEMENT SYSTEM processes is addressed in the Manual for the Oversight of Fatigue Management Approaches (Doc 9966)). 2.5.7 Safety risk mitigation strategies 2.5.7.1 Safety risk mitigation is often referred to as a safety risk control. Safety risks should be managed to an acceptable level by mitigating the safety risk through the application of appropriate safety risk controls. This should be balanced against the time, cost and difficulty of taking action to reduce or eliminate the safety risk. The level of safety risk can be lowered by reducing the severity of the potential consequences, reducing the likelihood of occurrence or by reducing exposure to that safety risk. It is easier and more common to reduce the likelihood than it is to reduce the severity. 2.5.7.2 Safety risk mitigations are actions that often result in changes to operating procedures, equipment or infrastructure. Safety risk mitigation strategies fall into three categories: a) Avoidance: The operation or activity is cancelled or avoided because the safety risk exceeds the benefits of continuing the activity, thereby eliminating the safety risk entirely. b) Reduction: The frequency of the operation or activity is reduced, or action is taken to reduce the magnitude of the consequences of the safety risk. c) Segregation: Action is taken to isolate the effects of the consequences of the safety risk or build in redundancy to protect against them. 2.5.7.3 The consideration of human factors is an integral part of identifying effective mitigations because humans are required to apply, or contribute to, the mitigation or corrective actions. For example, mitigations may include the use of processes or procedures. Without input from those who will be using these in “real world” situations and/or individuals with human factors expertise, the processes or procedures developed may not be fit for their purpose and result in unintended consequences. Further, human performance limitations should be considered as part of any safety risk mitigation, building in error capturing strategies to address human performance variability. Ultimately, this important human factors perspective results in more comprehensive and effective mitigations. 2.5.7.4 A safety risk mitigation strategy may involve one of the approaches described above or may include multiple approaches. It is important to consider the full range of possible control measures to find an optimal solution. The effectiveness of each alternative strategy must be evaluated before a decision is made. Each proposed safety risk mitigation alternative should be examined from the following perspectives: a) Effectiveness. The extent to which the alternatives reduce or eliminate the safety risks. Effectiveness can be determined in terms of the technical, training and regulatory defences that can reduce or eliminate safety risks. b) Cost/benefit. The extent to which the perceived benefits of the mitigation outweighs the costs. c) Practicality. The extent to which mitigation can be implemented and how appropriate it is in terms of available technology, financial and administrative resources, legislation, political will, operational realities, etc. d) Acceptability. The extent to which the alternative is acceptable to those people that will be expected to apply it. e) Enforceability. The extent to which compliance with new rules, regulations or operating procedures can be monitored. f) Durability. The extent to which the mitigation will be sustainable and effective. g) Residual safety risks. The degree of safety risk that remains subsequent to the implementation of the initial mitigation and which may necessitate additional safety risk control measures. h) Unintended consequences. The introduction of new hazards and related safety risks associated with the implementation of any mitigation alternative. i) Time. Time required for the implementation of the safety risk mitigation alternative. 2.5.7.5 Corrective action should take into account any existing defences and their (in)ability to achieve an acceptable level of safety risk. This may result in a review of previous safety risk assessments that may have been impacted by the corrective action. Safety risk mitigations and controls will need to be verified/audited to ensure that they are effective. Another way to monitor the effectiveness of mitigations is through the use of SPIs. See Chapter 4 for more information on safety performance management and SPIs. 2.5.8 Safety risk management documentation 2.5.8.1 Safety risk management activities should be documented, including any assumptions underlying the probability and severity assessment, decisions made, and any safety risk mitigation actions taken. This may be done using a spread sheet or table. Some organizations may use a database or other software where large amounts of safety data and safety information can be stored and analysed. 2.5.8.2 Maintaining a register of identified hazards minimizes the likelihood that the organization will lose sight of its known hazards. When hazards are identified, they can be compared with the known hazards in the register to see if the hazard has already been registered, and what action(s) were taken to mitigate it. Hazard registers are usually in a table format and typically include: the hazard, potential consequences, assessment of associated risks, identification date, hazard category, short description, when or where it applies, who identified it and what measure have been put in place to mitigate the risks. 2.5.8.3 Safety risk decision-making tools and processes can be used to improve the repeatability and justification of decisions taken by organizational safety decision makers. An example of a safety risk decision airport information desk is provided below in Figure 2-6. 2.5.9 Cost-benefit analysis Cost-benefit or cost-effectiveness analysis is normally carried out during the safety risk mitigation activities. It is commonly associated with business management, such as a regulatory impact assessment or project management processes. However, there may be situations where a safety risk assessment may have a significant financial impact. In such situations, a supplementary cost-benefit analysis or cost-effectiveness process to support the safety risk assessment may be warranted. This will ensure cost-effectiveness analysis or justification of recommended safety risk control actions has been taken into consideration, with the associated financial implications. SAFETY CULTURE 3.1 INTRODUCTION 3.1.1 A safety culture is the natural consequence of having humans in the aviation system. Safety culture has been described as “how people behave in relation to safety and risk when no one is watching”. It is an expression of how safety is perceived, valued and prioritized by management and employees in an organization, and is reflected in the extent to which individuals and groups are: a) aware of the risks and known hazards faced by the organization and its activities; b) continuously behaving to preserve and enhance safety; c) able to access the resources required for safe operations; d) willing and able to adapt when facing safety issues; e) willing to communicate safety issues; and f) consistently assessing the safety related behaviours throughout the organization. 3.1.2 Annex 19 requires that both States and service providers promote a positive safety culture with the aim of fostering effective safety management implementation through the State Safety Programme/SAFETY MANAGEMENT SYSTEM. This chapter provides guidance on the promotion of a positive safety culture. 3.2 SAFETY CULTURE AND SAFETY MANAGEMENT 3.2.1 Whether an organization realizes it or not, it will have a number of different “safety cultures” that reflect group-level attitudes and behaviours. No two organizations are identical, and even within the same organization, different groups may have various ways of thinking about safety, talking about safety and acting on safety issues. This variation may be appropriate for different activities. 3.2.2 How safety values are incorporated into practices by management and personnel directly affects how key elements of the State Safety Programme and SAFETY MANAGEMENT SYSTEM are established and maintained. As a consequence, safety culture has a direct impact on safety performance. If someone believes that safety is not that important then workarounds, cutting corners, or making unsafe decisions or judgements may be the result, especially when the risk is perceived as low and there is no apparent consequence or danger. The safety culture of an organization therefore significantly influences how their State Safety Programme or SAFETY MANAGEMENT SYSTEM develops and how effective it becomes. Safety culture is arguably the single most important influence on the management of safety. If an organization has instituted all the safety management requirements but does not have a positive safety culture, it is likely to underperform. 3.2.3 When the organization has a positive safety culture, and this is visibly supported by upper- and middle- management, front-line personnel tend to feel a sense of shared responsibilities towards achieving the organization’s safety objectives. Effective safety management also supports efforts to drive towards an increasingly positive safety culture by increasing the visibility of management’s support and improving active involvement of personnel in managing safety risk. 3.2.4 A positive safety culture relies on a high degree of trust and respect between personnel and management. Time and effort are needed to build a positive safety culture, which can be easily damaged by management decisions and actions, or inactions. Continuous effort and reinforcement are needed. When leadership actively endorses safe practices, it becomes the normal way of doing things. The ideal situation is a fully implemented and effective State Safety Programme/SAFETY MANAGEMENT SYSTEM and a positive safety culture. Hence, an organization’s safety culture is often seen as a reflection of the maturity of its State Safety Programme/SAFETY MANAGEMENT SYSTEM. Effective safety management empowers a positive safety culture and a positive safety culture empowers effective safety management. 3.2.5 Safety culture and its influence on safety reporting 3.2.5.1 SSPs and SAFETY MANAGEMENT SYSTEMs are sustained by safety data and safety information that is necessary to address existing and potential safety deficiencies and hazards, including safety issues identified by personnel. The success of a reporting system depends entirely on the continuous flow of information from, and feedback to, organizations and individuals. The protection of safety data, safety information and related sources is essential to ensure continued availability of information. For example, in voluntary safety reporting systems, this may be realized through a system that is confidential, and not used for purposes other than maintaining or improving safety. The benefits are twofold. Often personnel are the closest to safety hazards, so a voluntary reporting system enables them to actively identify these hazards and suggest workable solutions. At the same time, the regulator or management is able to gather important safety information and build trust with the organizations or operational personnel who are reporting the information. For more information about the protection of safety data and safety information refer to Chapter 7. 3.2.5.2 Whether organizations or individuals are willing to report their experiences and errors is largely dependent on the perceived benefits and disadvantages associated with reporting. Safety reporting systems may be anonymous or confidential. In general, in an anonymous reporting system a reporter does not provide their identity. In this case there is no opportunity for further clarification of the report’s contents, or the ability to provide feedback. In a confidential reporting system, any identifying information about the reporter is known only to a designated custodian. If organizations and individuals who report safety issues are protected and treated in a fair and consistent manner, they are more likely to divulge such information and work with the regulator or management to effectively manage the associated safety risk(s). 3.2.5.3 States are expected to adopt laws to adhere to the provisions outlined in Annex 19 for the protection of safety data, safety information and related sources. In the case of a voluntary reporting system, confidentiality should be ensured and the reporting system operated in accordance with safety protection laws. Further, organizations need to have an appropriate disciplinary policy, which is accessible to all and widely understood. A disciplinary policy should clearly indicate what behaviours are considered unacceptable and how the organization will respond in such cases. The disciplinary policy needs to be applied fairly, reasonably and consistently. Finally, organizations and individuals are more likely to report their experiences and errors in an environment where they will not be judged or treated unfairly by their peers or their employer. 3.2.5.4 Overall, organizations and individuals must believe they will be supported when reporting in the interest of safety. This includes organizational and personal errors and mistakes. An increase in confidential reports and a decrease in anonymous reports is usually indicative of the organization’s progress towards a positive safety culture. 3.2.6 Safety culture and cultural diversity 3.2.6.1 National culture differentiates the characteristics of particular nations, including the role of the individual within society, the manner in which authority is distributed, and national priorities with respect to resources, accountabilities, morality, objectives and legal systems. 3.2.6.2 From a safety management perspective, national culture influences organizational culture and plays a large part in determining the nature and scope of regulatory enforcement policies, including the relationship between regulatory authority personnel and industry personnel, and the extent to which safety information is protected. These, in turn, impact on peoples’ willingness to report safety issues. 3.2.6.3 The majority of organizations today employ people from multiple cultural backgrounds, which may be defined by their nationality, ethnicity, religion, and/or gender. Aviation operations and safety rely on effective interaction between different professional groups, each with its own professional culture. Hence, the organization’s safety culture may also be significantly affected by the variety of cultural backgrounds of the members of its workforce. 3.2.6.4 Managing safety within the aviation system therefore requires interaction with, and management of, culturally diverse personnel. However, when implementing safety management, managers should be capable of moulding their culturally-diverse workforce into effective teams. Eliminating differences in safety risk perceptions that may derive from different cultural interpretations and enhancing other safety-related aspects, such as communication, leadership styles and interaction between supervisors and subordinates is key. The degree of success will depend on management’s ability to promote a common understanding of safety and each individual’s role in its effectiveness. Regardless of an individual’s cultural background, effective safety management relies on a shared safety culture, with everyone in the organization understanding how they are expected to behave in relation to safety and risk “even when no one is watching”. 3.2.7 Safety culture and organizational change Safety management requires that organizations manage the safety risks associated with organizational and operational changes. Staff concerns about workload, job security and access to training are associated with significant change in organizations and can have a negative impact on safety culture. The degree to which staff feel involved in the development of change and understand their role in the process will also influence the safety culture. 3.3 DEVELOPING A POSITIVE SAFETY CULTURE 3.3.1 A positive safety culture has the following features: a) managers and employees, individually and collectively, want to make decisions and take actions that promote safety; b) individuals and groups continually critique their behaviours and processes and welcome the critique of others searching for opportunities to change and improve as their environment changes; c) management and staff share a common awareness of the hazards and risks faced by the organization and its activities, and the need to manage risks; d) individuals act and make decisions according to a common belief that safety is part of the way they do business; e) individuals value being informed, and informing others, about safety; f) individuals trust their colleagues and managers with information about their experiences, and the reporting of errors and mistakes is encouraged to improve how things are done in the future. 3.3.2 Actions by management and employees can help drive their safety culture to be more positive. Table 5 provides examples of the types of management and employee actions that will enable or disable a positive safety culture in an organization. Organizations should focus on providing enablers and removing any disablers to promote and achieve a positive safety culture. Table 5. Examples of actions that will enable or disable a positive safety culture Commitment to safety: Commitment to safety reflects the extent to which senior management within the organization have a positive attitude towards safety and recognizes its importance. Senior management should be genuinely committed to achieving and maintaining a high level of safety and give employees motivation and the means to do so also. Enablers: • Management leads safety culture and is actively motivating its employees to care for safety, not only by talking but by acting as role models • Management provides resources for a range of safety related tasks (e.g. training) • Continuous safety management oversight and governance is established Disablers: • Management is actively demonstrating that profit, cost reduction and efficiency come first • Investments to improve safety are often made when required by regulations or after accidents • Neither oversight nor governance with regard to safety management is established Adaptability Adaptability reflects the extent to which employees and management are willing to learn from past experiences and are able to take action necessary in order to enhance the level of safety within the organization. Enablers: • Employee input is actively encouraged when addressing safety issues • All incidents and audit findings are investigated and acted upon • Organizational processes and procedures are questioned for their safety impact (high extent of self- criticism) • A clear proactive approach to safety is demonstrated and followed Disablers: • Employee input on safety issues is not sought from all levels of the employees • Actions are often taken only after accidents or when required by regulations • Organizational processes and procedures are considered adequate as long as no accident occurs (complacency or lack of self-criticism) • Even when an accident occurs the organization is unwilling to question itself. • A reactive approach to safety is demonstrated and followed. Awareness Awareness reflects the extent to which employees and management are aware of the aviation risks faced by the organization and its activities. From a State perspective personnel are aware of both the safety risks induced by their own activities and the organizations they oversee. Employees and management should be constantly maintaining a high degree of vigilance with respect to safety issues. Enablers: • An effective way of hazard identification has been established • Investigations seek to establish the root cause • The organization stays abreast of important safety improvements, and adapts itself accordingly as necessary • The organization systematically evaluates if safety improvements are implemented and working as intended • Where appropriate members of the organization are well aware of the safety risks induced by their individual actions and company operations / activities Disablers: • No effort is spent on hazard identification • Investigations stop at the first viable cause rather than seek the root cause • The organization does not stay abreast of important safety improvements • The organization does not evaluate if safety improvements are implemented properly • Where appropriate members of the organization are not aware of the safety risks induced by their individual actions and company operations • Safety data is gathered but not analysed and acted upon Behaviour with respect to safety Behaviour with respect to safety reflects the extent to which every level of the organization behaves such as to maintain and improve the level of safety. The importance of safety should be recognized and processes and procedures needed to maintain it should be put in place. Enablers: • The employees motivate themselves to act safely and by acting as role models • Continuous monitoring of safe behaviour is practised • Intentional unsafe behaviour is not tolerated by management and colleagues • The working conditions support aviation safety at all times Disablers: • Employees are not punished for intentional unsafe behaviour to the benefits of their own or other interests • The working conditions provoke behaviour and work arounds that are detrimental to aviation safety • No monitoring of aviation safety within the organization’s products or services is practised • Constructive criticism to the benefit of aviation safety is not welcomed Information Information reflects the extent to which information is distributed to all necessary people within the organization. Employees should be enabled and encouraged to report aviation safety concerns and receive feedback on their reports. Work information related to aviation safety has to be communicated meaningfully to the right people in order to avoid miscommunication that could lead to hazardous aviation system situations and consequences. The State is open to share aviation safety related information to all service providers. Enablers: • An open and just safety- reporting environment exists. • Employees are provided with safety-relevant information in a timely manner in order to allow for safe operations or decisions to be made. • Management and supervisors regularly check whether safety- relevant information is understood and acted upon • Knowledge transfer and training with regard to aviation safety is actively practiced (e.g. sharing of lessons learned) Disablers: • A blaming safety reporting environment is evident • Safety-relevant information is withheld • Safety communication is not monitored for its effectiveness • No knowledge transfer or training is provided Trust Employee contribution to safety thrives in a reporting environment that fosters trust - trust that their actions or omissions, commensurate with their training and experience, will not be punished. A workable approach is to apply a reasonableness test – i.e. is it reasonable that a person with the same level of experience and training might do the same thing. Such an environment is fundamental to effective and efficient safety reporting. Effective safety reporting systems help to ensure that people are willing to report their errors and experiences, so that States and service providers have access to relevant data and information that is necessary to address existing and potential safety deficiencies and hazards. These systems create an environment in which people can be confident that safety data and safety information will be used exclusively for improving safety. Enablers: • There is a distinction between acceptable and unacceptable behaviour, which is known to all employees. • Occurrences (including accidents and incidents) investigations consider individual as well as organizational factors. • Good aviation safety performance is recognized and rewarded on a regular basis. • There is willingness among employees and operational personnel to report events in which they have been involved. Disablers: • There is no identifiable distinction between acceptable and unacceptable behaviour. • Employees are systematically and rigorously punished for human errors. • Accident and occurrence investigations focus on individual factors only. • Good safety performance and safe behaviour is taken for granted. 3.3.3 Monitoring safety culture 3.3.3.1 Safety culture is subject to many influences and organizations may choose to assess their safety culture to: a) understand how people feel about the organization and how importantly safety is perceived; b) identify strengths and weaknesses; c) identify differences between various groups (subcultures) within an organization; and d) examine changes over time (e.g. in response to significant organizational changes such as following an accident, a change in senior management or altered industrial relations arrangement). 3.3.3.2 There are a number of tools which are used to assess safety culture maturity, usually in combination: a) questionnaires; b) interviews and focus groups; c) observations; and d) document reviews. 3.3.3.3 Assessing safety culture maturity can provide valuable insight, leading to actions by management that will encourage the desired safety behaviours. It should be noted that there is a degree of subjectivity with such assessments and they may reflect the views and perceptions of the people involved at a particular moment only. Also, scoring safety culture maturity can have unintended consequences by inadvertently encouraging the organization to strive to achieve the “right” score, rather than working together to understand and improve the safety culture. 4.1 INTRODUCTION 4.1.1 Safety performance management is central to the functioning of SSPs and SAFETY MANAGEMENT SYSTEMs. Properly implemented, it will provide an organization with the means to determine whether its activities and processes are working effectively to achieve its safety objectives. This is accomplished through the identification of safety performance indicators (SPIs), which are used to monitor and measure safety performance. Through the identification of SPIs, information obtained will allow senior management to be aware of the current situation and support decision-making, including determining whether actions are required to further mitigate safety risks to ensure the organization achieves its safety goals. 4.1.2 The generic safety performance management process and how it is linked with safety data collection and processing systems (SDCPS) and safety analysis, discussed in Chapters 5 and 6, respectively, is shown in Figure 4-1 below. The link to safety promotion is shown to highlight the importance of communicating this information throughout the organization. More information on safety promotion, an important component of State Safety Programme and SAFETY MANAGEMENT SYSTEM which is often underappreciated, can be found in Chapters 8 and 9, respectively. 4.1.3 Safety performance management helps the organization to ask and to answer the four most important questions regarding safety management: a) What are the organization’s top safety risks? Derived from a review of aviation accident and incident data as well as predictive analysis to identify and define emerging risks. b) What does the organization want to achieve in terms of safety and what are the top safety risks that need to be addressed? The organization’s safety objectives. c) How will the organization know if it is making progress toward its safety objectives? Through SPIs, SPTs and, if practicable, safety triggers. d) What safety data and safety information are needed to make informed safety decisions? Including the allocation of the organization’s resources. Through an evolving Safety Data Collection and Processing System and safety data analysis. 4.1.4 The safety performance management process can also be used to establish an acceptable level of safety performance (Acceptable level of safety performance). More details on the establishment of an Acceptable level of safety performance can be found in Chapter 8. 4.1.5 Relationship between States and service providers 4.1.5.1 There are similarities between the State and service providers in the use and application of safety performance techniques. While the guidance in this chapter has been developed for both States and service providers, some differences are identified in this section. 4.1.5.2 The development of State safety performance should focus on what the State considers to be its most important aspects to managing safety. For the State, an effectively implemented State Safety Programme is used as a decision-making tool for the management of safety performance, which should include: the safety performance of its service providers; the State’s oversight capability; and the support provided to service providers through the establishment of guidelines. States should consider measuring their ability to: a) maintain their safety oversight system; b) apply specific safety actions and introduce safety initiatives; and c) adapt existing safety risk controls to ensure they remain effective. 4.1.5.3 For service providers, the primary function of safety performance management is to monitor and measure how well it is managing its safety risks. This is achieved through the effective implementation of an SAFETY MANAGEMENT SYSTEM that generates information that will be used to make decisions regarding the management of safety, including the implementation of safety risk controls and the allocation of resources. 4.1.5.4 The success of safety management depends on the commitment between the State and its service providers. There may be benefits in the State identifying suitable SPIs that could be monitored by service providers and then shared with the State, in particular for the establishment of the Acceptable level of safety performance (see Chapter 8 for more information). The information received from service providers will assist the State with its assessment of the safety performance of its aviation industry and its own ability to provide effective oversight and support to service providers. However, service providers should ensure their SPIs are appropriate to their operational context, performance history and expectations. 4.1.6 Safety performance management and interfaces 4.1.6.1 When States and service providers are considering implementing safety management, it is important to consider the safety risks induced by interfacing entities. Interfaces can be internal (e.g. between operations and maintenance or finance, human resources or legal departments), or they can be external (e.g. other State, service providers or contracted services). Hazards and related risks at the interface points are among the most common contributors to safety occurrences. States and service providers have greater control over interface-related risks when their interfaces are identified and managed. Interfaces should be defined in the organization’s system description. 4.1.6.2 States and service providers are responsible for ongoing monitoring and management of their interfaces to ensure safe outcomes. The safety risk posed by each interface should, ideally, be collaboratively assessed by the interfacing entities. Collaboration is highly desirable because the perception of safety risks and their tolerability may vary between the interfacing organizations. Sharing of interface risk management, through the establishment and monitoring of SPIs, encourages the mutual awareness of safety risks rather than ignorance or potentially one-sided risk management. It also creates an opportunity for transfer of knowledge and working practices that could improve the safety effectiveness of both organizations. 4.1.6.3 For this reason, SPIs should be agreed and established to monitor and measure the risks and the effectiveness of mitigating actions. A formal interface management agreement between interfacing organizations, with clearly defined monitoring and management responsibilities, is an example of an effective approach. 4.2 SAFETY OBJECTIVES 4.2.1 Safety objectives are brief, high-level statements of safety achievements or desired outcomes to be accomplished. Safety objectives provide direction to the organization’s activities and should therefore be consistent with the safety policy that sets out the organization’s high-level safety commitment. They are also useful to communicate safety priorities to personnel and the aviation community as a whole. Establishing safety objectives provides strategic direction for the safety performance management process and provides a sound basis for safety related decision-making. The management of safety performance should be a primary consideration when amending policies or processes, or allocating the organization’s resources in pursuit of improving safety performance. 4.2.2 Safety objectives may be: a) process-oriented: stated in terms of safe behaviours expected from operational personnel or the performance of actions implemented by the organization to manage safety risk; or b) outcome-oriented: encompass actions and trends regarding containment of accidents or operational losses. 4.2.3 The suite of safety objectives should include a mix of both process-oriented and outcome-oriented objectives to provide enough coverage and direction for the SPIs and SPTs. Safety objectives on their own do not have to be specific, measurable, achievable, relevant and timely (SMART) (George T. Doran, 1981), provided the safety objectives and accompanying SPIs and SPTs form a package that allows an organization to demonstrate whether it is maintaining or improving its safety performance. Table 6. Examples of safety objectives Examples of safety objectives process-oriented State or service provider Increase safety reporting levels. outcome-oriented service provider Reduce rate of adverse apron safety events. (high-level) or Reduce the annual number of adverse apron safety events from the previous year. outcome-oriented State Reduce the annual number of safety events in sector X. 4.2.4 An organization may also choose to identify safety objectives at the tactical or operational level or apply them to specific projects, products and processes. A safety objective may also be expressed by the use of other terms with a similar meaning (e.g. goal or target). 4.3 SAFETY PERFORMANCE INDICATORS AND SAFETY PERFORMANCE TARGETS 4.3.1 Types of safety performance indicators Qualitative and quantitative indicators 4.3.1.1 SPIs are used to help senior management know whether or not the organization is likely to achieve its safety objective; they can be qualitative or quantitative. Quantitative indicators relate to measuring by the quantity, rather than its quality, whereas qualitative indicators are descriptive and measure by quality. Quantitative indicators are preferred over qualitative indicators because they are more easily counted and compared. The choice of indicator depends on the availability of reliable data that can be measured quantitatively. Does the necessary evidence have to be in the form of comparable, generalizable data (quantitative), or a descriptive image of the safety situation (qualitative)? Each option, qualitative or quantitative, involves different kinds of SPIs, and requires a thoughtful Safety Performance Indicator selection process. A combination of approaches is useful in many situations, and can solve many of the problems which may arise from adopting a single approach. An example of a qualitative indicator for a State could be the maturity of their service providers’ SAFETY MANAGEMENT SYSTEM in a particular sector, or for a service provider the assessment of the safety culture. 4.3.1.2 Quantitative indicators can be expressed as a number (x incursions) or as a rate (x incursions per n movements). In some cases, a numerical expression will be sufficient. However, just using numbers may create a distorted impression of the actual safety situation if the level of activity fluctuates. For example, if air traffic control records three altitude busts in July and six in August, there may be great concern about the significant deterioration in safety performance. But August may have seen double the movements of July meaning the altitude busts per movement, or the rate, has decreased, not increased. This may or may not change the level of scrutiny, but it does provide another valuable piece of information that may be vital to data-driven safety decision-making. 4.3.1.3 For this reason, where appropriate, SPIs should be reflected in terms of a relative rate to measure the performance level regardless of the level of activity. This provides a normalized measure of performance; whether the activity increases or decreases. As another example, an Safety Performance Indicator could measure the number of runway incursions. But if there were fewer departures in the monitored period, the result could be misleading. A more accurate and valuable performance measure would be the number of runway incursions relative to the number of movements, e.g. x incursions per 1 000 movements. Lagging and leading indicators 4.3.1.4 The two most common categories used by States and service providers to classify their SPIs are lagging and leading. Lagging SPIs measure events that have already occurred. They are also referred to as “outcome-based SPIs” and are normally (but not always) the negative outcomes the organization is aiming to avoid. Leading SPIs measure processes and inputs being implemented to improve or maintain safety. These are also known as “activity or process SPIs” as they monitor and measure conditions that have the potential to lead to or contribute to a specific outcome. 4.3.1.5 Lagging SPIs help the organization understand what has happened in the past and are useful for long-term trending. They can be used as a high-level indicator or as an indication of specific occurrence types or locations, such as “types of accidents per aircraft type” or “specific incident types by region”. Because lagging SPIs measure safety outcomes, they can measure the effectiveness of safety mitigations. They are effective at validating the overall safety performance of the system. For example, monitoring the “number of ramp collisions per number of movements between vehicles following a redesign of ramp markings” provides a measure of the effectiveness of the new markings (assuming nothing else has changed). The reduction in collisions validates an improvement in the overall safety performance of the ramp system; which may be attributable to the change in question. 4.3.1.6 Trends in lagging SPIs can be analyzed to determine conditions existing in the system that should be addressed. Using the previous example, an increasing trend in ramp collisions per number of movements may have been what led to the identification of sub-standard ramp markings as a mitigation. 4.3.1.7 Lagging SPIs are divided into two types: a) low probability/high severity: outcomes such as accidents or serious incidents. The low frequency of high severity outcomes means that aggregation of data (at industry segment level or regional level) may result in more meaningful analyses. An example of this type of lagging Safety Performance Indicator would be “aircraft and/or engine damage due to bird strike”. b) high probability/low severity: outcomes that did not necessarily manifest themselves in a serious accident or incident, these are sometimes also referred to as precursor indicators. SPIs for high probability/low severity outcomes are primarily used to monitor specific safety issues and measure the effectiveness of existing safety risk mitigations. An example of this type of precursor Safety Performance Indicator would be “bird radar detections”, which indicates the level of bird activity rather than the amount of actual bird strikes. 4.3.1.8 Aviation safety measures have historically been biased towards SPIs that reflect “low probability/high severity” outcomes. This is understandable in that accidents and serious incidents are high profile events and are easy to count. However, from a safety performance management perspective, there are drawbacks in an overreliance on accidents and serious incidents as a reliable indicator of safety performance. For instance, accidents and serious incidents are infrequent (there may be only one accident in a year, or none) making it difficult to perform statistical analysis to identify trends. This does not necessarily indicate that the system is safe. A consequence of a reliance on this sort of data is a potential false sense of confidence that an organization’s or system’s safety performance is effective, when it may in fact be perilously close to an accident. 4.3.1.9 Leading indicators are measures that focus on processes and inputs that are being implemented to improve or maintain safety. These are also known as “activity or process SPIs” as they monitor and measure conditions that have the potential to become or to contribute to a specific outcome. 4.3.1.10 Examples of leading SPIs driving the development of organizational capabilities for proactive safety performance management include such things as “percentage of staff who have successfully completed safety training on time” or “frequency of bird scaring activities”. 4.3.1.11 Leading SPIs may also inform the organization about how their operation copes with change, including changes in its operating environment. The focus will be either on anticipating weaknesses and vulnerabilities as a result of the change, or monitoring the performance after a change. An example of an Safety Performance Indicator to monitor a change in operations would be “percentage of sites that have implemented procedure X”. 4.3.1.12 For a more accurate and useful indication of safety performance, lagging SPIs, measuring both “low probability/high severity” events and “high probability/low severity” events should be combined with leading SPIs. Figure 4-2 illustrates the concept of leading and lagging indicators that provide a more comprehensive and realistic picture of the organization’s safety performance. 4.3.2 Selecting and defining SPIs 4.3.2.1 SPIs are the parameters that provide the organization with a view of its safety performance: where it has been; where it is now; and where it is headed, in relation to safety. This picture acts as a solid and defensible foundation upon which the organization’s data-driven safety decisions are made. These decisions, in turn, positively affect the organization’s safety performance. The identification of SPIs should therefore be realistic, relevant, and linked to safety objectives, regardless of their simplicity or complexity. 4.3.2.2 It is likely the initial selection of SPIs will be limited to the monitoring and measurement of parameters representing events or processes that are easy and/or convenient to capture (safety data that may be readily available). Ideally, SPIs should focus on parameters that are important indicators of safety performance, rather than on those that are easy to attain. 4.3.2.3 SPIs should be: a) related to the safety objective they aim to indicate; b) selected or developed based on available data and reliable measurement; c) appropriately specific and quantifiable; and d) realistic, by taking into account the possibilities and constraints of the organization. 4.3.2.4 A combination of SPIs is usually required to provide a clear indication of safety performance. There should be a clear link between lagging and leading SPIs. Ideally lagging SPIs should be defined before determining leading SPIs. Defining a precursor Safety Performance Indicator linked to a more serious event or condition (the lagging Safety Performance Indicator) ensures there is a clear correlation between the two. All of the SPIs, lagging and leading, are equally valid and valuable. An example of these linkages is illustrated in Figure 4-3. 4.3.2.5 It is important to select SPIs that relate to the organization’s safety objectives. Having SPIs that are well defined and aligned will make it easier to identify SPTs, which will show the progress being made towards the attainment of safety objectives. This allows the organization to assign resources for greatest safety effect by knowing precisely what is required, and when and how to act to achieve the planned safety performance. For example, a State has a safety objective of “reduce the number of runway excursions by 50 per cent in three years” and an associated, well-aligned Safety Performance Indicator of “number of runway excursions per million departures across all aerodromes”. If the number of excursions drops initially when monitoring commences, but starts to climb again after twelve months, the State could choose to reallocate resources away from an area where, according to the SPIs, the safety objective is being easily achieved and towards the reduction of runway excursions to alleviate the undesirable trend. Defining SPIs 4.3.2.6 The contents of each Safety Performance Indicator should include: a) a description of what the Safety Performance Indicator measures; b) the purpose of the Safety Performance Indicator (what it is intended to manage and who it is intended to inform); c) the units of measurement and any requirements for its calculation; d) who is responsible for collecting, validating, monitoring, reporting and acting on the Safety Performance Indicator (these may be staff from different parts of the organization); e) where or how the data should be collected; and f) the frequency of reporting, collecting, monitoring and analysis of the Safety Performance Indicator data. SPIs and safety reporting 4.3.2.7 Changes in operational practices may lead to underreporting until their impact is fully accepted by potential reporters. This is known as “reporting bias”. Changes in the provisions related to the protection of safety information and related sources could also lead to over-reporting. In both cases, reporting bias may distort the intent and accuracy of the data used for the Safety Performance Indicator. Employed judiciously, safety reporting may still provide valuable data for the management of safety performance. 4.3.3 Setting safety performance targets 4.3.3.1 Safety performance targets (SPTs) define short-term and medium-term safety performance management desired achievements. They act as “milestones” that provide confidence that the organization is on track to achieving its safety objectives and provide a measurable way of verifying the effectiveness of safety performance management activities. Safety Performance Target setting should take into consideration factors such as the prevailing level of safety risk, safety risk tolerability, as well as expectations regarding the safety of the particular aviation sector. The setting of SPTs should be determined after considering what is realistically achievable for the associated aviation sector and recent performance of the particular Safety Performance Indicator, where historical trend data is available. 4.3.3.2 If the combination of safety objectives, SPIs and SPTs working together are SMART, it allows the organization to more effectively demonstrate its safety performance. There are multiple approaches to achieving the goals of safety performance management, especially, setting SPTs. One approach involves establishing general high- level safety objectives with aligned SPIs and then identifying reasonable levels of improvements after a baseline safety performance has been established. These levels of improvements may be based on specific targets (e.g. percentage decrease) or the achievement of a positive trend. Another approach which can be used when the safety objectives are SMART is to have the safety targets act as milestones to achieving the safety objectives. Either of these approaches are valid and there may be others that an organization finds effective at demonstrating their safety performance. Different approaches can be used in combination as appropriate to the specific circumstances. Setting targets with high-level safety objectives 4.3.3.3 Targets are established with senior management agreeing on high-level safety objectives. The organization then identifies appropriate SPIs that will show improvement of safety performance towards the agreed safety objective(s). The SPIs will be measured using existing data sources, but may also require the collection of additional data. The organization then starts gathering, analysing and presenting the SPIs. Trends will start to emerge, which will provide an overview of the organization’s safety performance and whether it is steering towards or away from its safety objectives. At this point the organization can identify reasonable and achievable SPTs for each Safety Performance Indicator. Setting targets with SMART safety objectives 4.3.3.4 Safety objectives can be difficult to communicate and may seem challenging to achieve; by breaking them down into smaller concrete safety targets, the process of delivering them is easier to manage. In this way, targets form a crucial link between strategy and day-to-day operations. Organizations should identify the key areas that drive the safety performance and establish a way to measure them. Once an organization has an idea what their current level of performance is by establishing the baseline safety performance, they can start setting SPTs to give everyone in the State a clear sense of what they should be aiming to achieve. The organization may also use benchmarking to support setting performance targets. This involves using performance information from similar organizations that have already been measuring their performance to get a sense of how others in the community are doing. 4.3.3.5 An example of the relationship between safety objectives, SPIs and SPTs is illustrated in Figure 4-4. In this example, the organization recorded 100 runway excursions per million movements in 2018. It has been determined this is too many, and an objective to reduce the number of runway excursions by fifty per cent by 2022 has been set. Specific targeted actions and associated timelines have been defined to meet these targets. To monitor, measure and report their progress, the organization has chosen “RWY excursions per million movements per year” as the Safety Performance Indicator. The organization is aware that progress will be more immediate and effective if specific targets are set which align with the safety objective. They have therefore set a safety target which equates to an average reduction of 12.5 per year over the reporting period (four years). As shown in the graphical representation, the progress is expected to be greater in the first years and less so in the later years. This is represented by the curved projection towards their objective. In the Figure 4-4: a) the SMART safety objective is “50 per cent reduction in runway excursions rate by 2022”; b) the Safety Performance Indicator selected is the “number runway excursions per million movements per year”; and c) the safety targets related to this objective represent milestones for reaching the SMART safety objective and equate to a ~12 per cent reduction each year until 2022; 1) Safety Performance Target 1a is “less than 78 runway excursions per million movement in 2019”; 2) Safety Performance Target 1b is “less than 64 runway excursions per million movement in 2020”; 3) Safety Performance Target 1c is “less than 55 runway excursions per million movement in 2021”. Additional considerations for Safety Performance Indicator and Safety Performance Target selection 4.3.3.6 When selecting SPIs and SPTs, the following should also be considered: a) Workload management. Creating a workable amount of SPIs can help personnel manage their monitoring and reporting workload. The same is true of the SPIs complexity, or the availability of the necessary data. It is better to agree on what is feasible, and then prioritize the selection of SPIs on this basis. If an Safety Performance Indicator is no longer informing safety performance, or been given a lower priority, consider discontinuing in favour of a more useful or higher priority indicator. b) Optimal spread of SPIs. A combination of SPIs that encompass the focus areas will help gain an insight to the organization’s overall safety performance and enable data-driven decision-making. c) Clarity of SPIs. When selecting an Safety Performance Indicator, it should be clear what is being measured and how often. SPIs with clear definitions airport information desk understanding of results, avoid misinterpretation, and allow meaningful comparisons over time. d) Encouraging desired behaviour. SPTs can change behaviours and contribute to desired outcomes. This is especially relevant if achievement of the target is linked to organizational rewards, such as management remuneration. SPTs should foster positive organizational and individual behaviours that deliberately result in defensible decisions and safety performance improvement. It is equally important to consider the potential unintended behaviours when selecting SPIs and SPTs. e) Choosing valuable measures. It is imperative that useful SPIs are selected, not only ones which are easy to measure. It should be up to the organization to decide what the most useful safety parameters are; those that guide the organization to improve decision-making, safety performance management, and achievement of its safety objectives. f) Achieving SPTs. This is a particularly important consideration, and linked to the desired safety behaviours. Achieving the agreed SPTs is not always indicative of safety performance improvement. The organization should distinguish between just meeting SPTs and actual, demonstrable organizational safety performance improvement. It is imperative that the organization consider the context within which the target was achieved, rather than looking at an Safety Performance Target in isolation. Recognition for overall improvement in safety performance, rather than an individual Safety Performance Target achievement, will foster desirable organizational behaviours and encourage exchange of safety information that lies at the heart of both Safety Risk Management and safety assurance. This could also enhance the relationship between the State and the service provider and their willingness to share safety data and ideas. Caveats on setting SPTs 4.3.3.7 It is not always necessary or appropriate to define SPTs as there may be some SPIs that are better to monitor for trends rather than use to determine a target. Safety reporting is an example of when having a target could either discourage people not to report (if the target is not to exceed a number) or to report trivial matters to meet a target (if the target is to reach a certain number). There may also be SPIs better used to define a direction of travel to target continuous safety performance improvement (i.e. to reduce the number of events) rather than used to define an absolute target, as these may be difficult to determine. The following should also be considered in deciding appropriate SPTs: a) Drive undesirable behaviours; if managers or organizations are too focused on achievement of the numbers as an indicator of success they may not achieve the intended improvement in safety performance. b) Operational targets; too much focus on achieving operational targets (such as: on time departures, reduction in overhead costs, etc.) without a balance of SPTs can lead to “achieving the operational targets” while not necessarily improving safety performance. c) Focus on quantity rather than quality; this can encourage personnel or departments to meet the target but in doing so deliver a poor product or service. d) Cap innovation; although not intended, once a target is met this can lead to a relaxation and that no further improvements are needed and complacency can set in. e) Organizational conflict; targets can create conflict between departments and organizations as they argue over who is responsible rather than focusing on trying to work together. 4.3.4 Safety Performance Measurement Getting safety performance measurement right involves deciding how best to measure the achievement of the safety objectives. This will vary from State to State and from service provider to service provider. Organizations should take the time to develop their strategic awareness of what it is that drives safety improvement for their safety objectives. 4.3.5 Use of SPIs and SPTs SPIs and SPTs can be used in different ways to demonstrate safety performance. It is crucial that organizations tailor, select and apply various measurement tools and approaches depending on their specific circumstances and the nature of what is being measured. For instance, in some cases, organizations could adopt SPIs that all have specific associated SPTs. In another situation, it may be preferable to focus on achieving a positive trend in the SPIs, without specific target values. The package of selected performance metrics will usually employ a combination of these approaches. 4.4 MONITORING SAFETY PERFORMANCE 4.4.1 Once an organization has identified the targets based on the SPIs they believe will deliver the planned outcome, they must ensure the stakeholders follow through by assigning clear responsibility for delivery. Defining SPTs for each aviation authority, sector and service provider supports the achievement of the Acceptable level of safety performance for the State by assigning clear accountability. 4.4.2 MechaniSafety Management System for monitoring and measuring the organization’s safety performance should be established to identify what changes may be needed if the progress made isn't as expected and reinforce the commitment of the organization to meet its safety objectives. 4.4.3 Baseline safety performance Understanding how the organization plans to progress towards its safety objectives requires that they know where they are, in relation to safety. Once the organization’s safety performance structure (safety objectives, indicators, targets, triggers) has been established and is functioning, it is possible to learn their baseline safety performance through a period of monitoring. Baseline safety performance is the safety performance at the commencement of the safety performance measurement process, the datum point from which progress can be measured. In the example used in figures 4-3 and 4-4, the baseline safety performance for that particular safety objective was “100 runway excursions per million movements during the year (2018)”. From this solid basis, accurate and meaningful indications and targets can be recorded. 4.4.4 Refinement of SPIs and SPTs 4.4.4.1 SPIs and associated SPTs will have to be reviewed to determine if they are providing the information needed to track the progress being made toward the safety objectives and to ensure that the targets are realistic and achievable. 4.4.4.2 Safety performance management is an ongoing activity. Safety risks and/or availability of data change over time. Initial SPIs may be developed using limited resources of safety information. Later, more reporting channels may be established, more safety data may be available and the organization’s safety analysis capabilities will likely mature. It may be appropriate for organizations to develop simple (broader) SPIs initially. As they gather more data and safety management capability, they can consider refining the scope of SPIs and SPTs to better align with the desired safety objectives. Small non-complex organizations may elect to refine their SPIs and SPTs and/or select generic (but specific) indicators which apply to most aviation systems. Some examples of generic indicators would be: a) events including structural damage to equipment; b) events indicating circumstances in which an accident nearly occurred; c) events in which operational personnel or members of the aviation community were fatally or seriously injured; d) events in which operational personnel became incapacitated or unable to perform their duties safely; e) rate of voluntary occurrence reports; and f) rate of mandatory occurrence reports. 4.4.4.3 Larger more complex organizations may elect to institute a broader and/or deeper range of SPIs and SPTs and to integrate generic indicators such as those listed above with activity-specific ones. A large airport, for example, providing services to major airlines and situated under complex airspace, might consider combining some of the generic SPIs with deeper-scope SPIs representing specific aspects of their operation. The monitoring of these may require greater effort but will likely produce superior safety results. There is a clear correlation between the relative complexity of SPIs and SPTs and the scale and complexity of the State’s or service providers’ operations. This relative complexity should be reflected in the indicator and target set. Those responsible for establishing safety performance management should be conscious of this. 4.4.4.4 The set of SPIs and SPTs selected by an organization should be periodically reviewed to ensure their continued meaningfulness as indications of organizational safety performance. Some reasons to continue, discontinue or change SPIs and SPTs include: a) SPIs continually report the same value (such as zero per cent or 100 per cent); these SPIs are unlikely to provide meaningful input to senior management decision-making; b) SPIs that have similar behaviour and as such are considered a duplication; c) the Safety Performance Target for an Safety Performance Indicator implemented to measure the introduction of a programme or targeted improvement has been met; d) another safety concern becomes a higher priority to monitor and measure; e) to gain a better understanding of a particular safety concern by narrowing the specifics of an Safety Performance Indicator (i.e. reduce the “noise” to clarify the “signal”); and f) safety objectives have changed and as a consequence the SPIs require updating to remain relevant. 4.4.5 Safety triggers 4.4.5.1 A brief perspective on the notions of triggers is relevant to assist in their eventual role within the context of the management of safety performance by an organization. 4.4.5.2 A trigger is an established level or criteria value that serves to trigger (start) an evaluation, decision, adjustment or remedial action related to the particular indicator. One method for setting out-of-limits trigger criteria for SPTs is the use of the population standard deviation (STDEVP) principle. This method derives the standard deviation (SD) value based on the preceding historical data points of a given safety indicator. The Standard Deviation value plus the average (mean) value of the historical data set forms the basic trigger value for the next monitoring period. The Standard Deviation principle (a basic statistical function) sets the trigger level criteria based on actual historical performance of the given indicator (data set), including its volatility (data point fluctuations). A more volatile historical data set will usually result in a higher (more generous) trigger level value for the next monitoring period. Triggers provide early warnings which enable decision makers to make informed safety decisions, and thus improve safety performance. An example of trigger levels based on standard deviations (SDs) is provided at Figure 4-5 below. In this example, data-driven decisions and safety mitigation actions may need to be taken when the trend goes beyond +1SD or +2SD from the mean of the preceding period. Often the trigger levels (in this case +1SD, +2SD or beyond +2SD) will align with decision management levels and urgency of action. 4.4.5.3 Once SPTs and trigger settings (if used) have been defined, their associated Safety Performance Indicator may be tracked for their respective performance status. A consolidated summary of the overall Safety Performance Target and trigger performance outcome of the complete SPIs package may also be compiled and/or aggregated for a given monitoring period. Qualitative values (satisfactory/unsatisfactory) may be assigned for each Safety Performance Target achievement and each trigger level not breached. Alternatively, numeric values (points) may be used to provide a quantitative measurement of the overall performance of the SPIs package. 4.4.5.4 It should be noted that trigger values serve to trigger (start) an evaluation, decision, adjustment or remedial action related to the particular indicator. An Safety Performance Indicator being triggered is not necessarily catastrophic or an indication of failure. It is merely a sign that the activity has moved beyond the predetermined limit. The trigger aims to attract the attention of decision makers who are now in a position to take remedial action, or not, depending on the circumstances. 4.4.6 Caveat on triggers 4.4.6.1 There are challenges in identifying reliable trigger levels. Triggers and their associated levels work best when there are ample safety data and safety data management capabilities. This can impose an additional workload on the organization. The notion of trigger was designed and is best suited to Safety Risk Management of purely technical systems (e.g. aircraft engine monitoring). In this case, large amounts of quantitative data support the identification of accurate triggers and trigger levels. The notion of triggers is arguably less relevant to Safety Risk Management of socio-technical systems. Socio-technical systems are systems where people actively interact with the processes and technologies to achieve the system’s service delivery or production objectives. Both State Safety Programme and SAFETY MANAGEMENT SYSTEM are socio-technical systems. The less reliable and meaningful triggers used in socio-technical systems are due to the limitations of reliable measures when humans are involved. 4.4.6.2 A more flexible approach is therefore needed for the triggers to be meaningful. Annex 19 does not require that States or service providers define trigger levels for each Safety Performance Indicator. However, there are benefits for organizations where their data for an Safety Performance Indicator is very specific, there are enough data points and the data is sufficiently trustworthy. 4.4.6.3 Figure 4-6 below is an extension of the previous example, “50 per cent reduction in runway excursions by 2022”. In this scenario, it is now the year 2020. The organization has been collecting safety data (Safety Performance Indicator – “No runway excursions/million movement/yr”) and working with stakeholders to reduce the instances. The Safety Performance Target for 2019 (<78 runway excursions/million movement in year) was achieved. However, the Safety Performance Indicator shows that, not only was the Safety Performance Target for 2020 (<64 runway excursions/million movement in year) not achieved, the number of excursions has exceeded the trigger in two consecutive reporting periods. The decision makers have been alerted to the deterioration in safety performance and are in a position to make decisions based on the data to take further action(s). Their data-driven decisions will aim to drive the safety performance back to within the acceptable zone, and on track to achieve their safety objective. 4.4.7 Identifying actions required 4.4.7.1 Arguably the most important outcome of establishing a safety performance management structure is the presentation of information to the organization’s decision makers so they can make decisions based on current, reliable safety data and safety information. The aim should always be to make decisions in accordance with the safety policy and towards the safety objectives. 4.4.7.2 In relation to safety performance management, data-driven decision-making is about making effective, well-informed decisions based on the results of monitored and measured SPIs, or other reports and analysis of safety data and safety information. Using valid and relevant safety data combined with information that provides context supports the organization in making decisions that align with its safety objectives and targets. Contextual information may also include other stakeholder priorities, known deficiencies in the data, and other complementary data to evaluate the pros, cons, opportunities, limitations and risks associated with the decision. Having the information readily available and easy to interpret helps to mitigate bias, influence and human error in the decision-making process. 4.4.7.3 Data-driven decision-making also supports the evaluation of decisions made in the past to support any realignment with the safety objectives. More guidance about data-driven decision-making is provided in Chapter 6. 4.5 UPDATE OF SAFETY OBJECTIVES Safety performance management is not intended to be “set and forget”. Safety performance management is dynamic and central to the functioning of every State and every service provider, and should be reviewed and updated: a) routinely, in accordance with the periodic cycle established and agreed upon by the high-level safety committee; b) based on inputs from safety analyses (see Chapter 6 for details); and c) in response to major changes in the operation, top risks or environment. Chapter 5 SAFETY DATA COLLECTION AND PROCESSING SYSTEMS 5.1 INTRODUCTION 5.1.1 The distinction between safety data and safety information is made in the definitions found in Annex 19. Safety data is what is initially reported or recorded as the result of an observation or measurement. It is transformed to safety information when it is processed, organized, integrated or analysed in a given context to make it useful for management of safety. Safety information may continue to be processed in different ways to extract different meanings. 5.1.2 The effective management of safety is highly dependent on the effectiveness of safety data collection, analysis and overall management capabilities. Having a solid foundation of safety data and safety information is fundamental for safety management, since it is the basis for data-driven decision-making. Reliable safety data and safety information is needed to identify trends, make decisions and evaluate safety performance in relation to safety targets and safety objectives, and to assess risk. 5.1.3 Annex 19 requires that service providers develop and maintain a formal process to collect, record, act on and generate feedback on hazards in their activities, based on a combination of reactive and proactive methods of safety data collection. 5.1.4 Similarly, Chapter 8 of Annex 13 — Aircraft Accident and Incident Investigation requires States to establish and maintain an accident and incident database to facilitate the effective analysis of information on actual or potential safety deficiencies, and to determine any preventive actions required. 5.1.5 Annex 19 requires States to establish safety data collection and processing systems (SDCPS) to capture, store, aggregate, and enable the analysis of safety data and safety information to support their safety performance management activities. Safety Data Collection and Processing System is a generic term used to refer to processing and reporting systems, databases and schemes for exchange of safety information and recorded information. The term “safety database” may refer to a single or multiple database(s). State authorities with responsibilities for the implementation of the State Safety Programme should have access to the Safety Data Collection and Processing System to support their safety responsibilities. 5.1.6 Service providers are also required to develop and maintain the means to verify their safety performance with reference to their SPIs and SPTs, in support of their safety objectives by means of Safety Data Collection and Processing System. They may be based on reactive and proactive methods of safety data and safety information collection. 5.1.7 The guidance in this chapter is equally valid for States and service providers to assure that the safety data and safety information collected will enable effective and valid decision-making. 5.1.8 Organizations should ensure they have personnel qualified to collect and store safety data, and the competencies needed to process safety data. This usually requires individuals with strong information technology skills as well as knowledge of data requirements, data standardization, data collection and storage, data governance and the ability to understand potential queries that may be needed for analysis. Additionally, the organization should ensure that each Safety Data Collection and Processing System has a designated custodian to apply the protection to safety data, safety information and related sources in accordance with Appendix 3 to Annex 19. Chapter 7 contains further details. 5.2 SAFETY DATA AND SAFETY INFORMATION COLLECTION 5.2.1 Objectives at different levels of the aviation system 5.2.1.1 INTERNATIONAL CIVIL AVIATION ORGANIZATIONhas been introducing provisions across Annexes, Procedures for Air Navigation Services (PANS) and documents since the 1970s requiring States to establish reporting systems to collect safety data and safety information. Most of these provisions relate to sector-specific safety reporting systems, with the exception of Annex 13, which focuses specifically on the reporting of accidents and serious incidents. The provisions for mandatory and voluntary safety reporting systems found in Annex 19 originated in Annex 13. 5.2.1.2 Many service providers have collected a wealth of safety data and safety information, including mandatory and voluntary safety reporting systems as well as automated data capture systems. This safety data and safety information allows service providers to identify hazards and supports safety performance management activities at the service provider level. There are many benefits to sharing safety information, not least of which is the identification of hazards that are beyond the view of a single service provider. Information on the sharing and exchange of safety information can be found in Chapter 6. 5.2.1.3 Annex 19 requires States to establish Safety Data Collection and Processing System to capture, store, aggregate and enable the analysis of safety data and safety information to support the identification of hazards which cut across the aviation system. This implies more than just having access to view the data for the purposes of monitoring the safety performance of service providers. Furthermore, putting in place reporting systems and databases for the collection of safety data and safety information is not sufficient to ensure the availability of the safety data to enable the analysis. States must also put in place laws, regulations, processes and procedures to make sure that safety data and safety information identified in Annex 19 are reported and collected from service providers and others to feed the Safety Data Collection and Processing System. This requires having the protections in place, as per Annex 19, Appendix 3, to ensure the use of safety data and safety information for purposes of maintaining or improving safety. Arrangements may also be put in place for a third party to collect, store and analyse the safety data and safety information on behalf of the State. Information on the protection of safety data and safety information can be found in Chapter 7. 5.2.1.4 Furthermore, safety data and safety information need to be collected, stored and analysed at the regional level through the regional aviation safety groups (RASGs) to facilitate the identification of hazards that transcend State borders and to promote collaborative efforts to mitigate safety risks. 5.2.2 Determining what to collect 5.2.2.1 Each organization needs to determine what safety data and safety information it must collect to support the safety performance management process and make safety decisions. Safety data and safety information requirements can be determined using a top-down and/or a bottom-up approach. The chosen approach can be influenced by different considerations, such as national and local conditions and priorities, or the need to provide the data to support the monitoring of the SPIs. 5.2.2.2 Identifying and collecting the safety data should be aligned with the organization’s need to effectively manage safety. In some cases, the Safety Risk Management process will highlight the need for additional safety data to better assess the impact (the level of probability and severity) and determine the associated risks. Equally, the safety performance management process may highlight a need for additional information for a more comprehensive understanding of a particular safety issue or to facilitate the establishment or refinement of SPIs. 5.2.2.3 Possible bias needs to be taken into account when collecting and using safety data and safety information. For example, the language used in voluntary reports can sometimes be emotive or aimed at achieving the objectives of an individual, which may not necessarily be in the best interests of the whole organization. In these cases, the information should be used judiciously. 5.2.2.4 States and service providers should consider taking an integrated approach to the collection of safety data that come from different sources, both internal and external. Integration allows organizations to get a more accurate view of their safety risks and the organization’s achievement of its safety objectives. It is worth noting that safety data and safety information that initially seems to be unrelated, may later turn out to be critical for identifying safety issues and supporting data-driven decision-making. 5.2.2.5 It is advisable to streamline the amount of safety data and safety information by identifying what specifically supports the effective management of safety within their organization. The safety data and safety information collected should support the reliable measure of the system’s performance and the assessment of known risks, as well as the identification of emerging risks, within the scope of the organization’s activities. The safety data and safety information required will be influenced by the size and complexity of the organization’s activities. 5.2.2.6 Figure 5-1 provides examples of typical safety data and safety information, which in many cases are already available. Coordination among departments or divisions is necessary to streamline efforts for reporting and collecting safety data to avoid duplication. 5.2.3 Accident and incident investigations Annex 13 requires States to establish and maintain an accident and incident database to facilitate the effective analysis of information on actual or potential safety deficiencies and to determine any preventive actions required. State authorities responsible for the implementation of the State Safety Programme should have access to the State accident and incident database to support their safety responsibilities. Additional information on which to base preventive actions may be contained in the Final Reports on accidents and incidents that have been investigated. 5.2.4 Safety investigations by State authorities or aviation service providers 5.2.4.1 According to the provisions in Annex 13, States are required to investigate accidents, as well as serious incidents of aircraft of a maximum mass of over 2 250 kg which have occurred in their territory. These investigations are conducted by the State’s accident investigation authority (AIA) in compliance with Annex 13. The conducting of such investigations may be delegated to another State or a regional accident and incident investigation organization (RAIO) by mutual arrangement and consent. 5.2.4.2 Safety investigations outside of those mandated by Annex 13 are encouraged as they provide useful safety information to support safety performance improvement. Additional information on service provider safety investigations can be found in Chapter 9. Typical safety data and safety information sources Data systems • Flight data analysis (FDA) • Flight recorders • air traffic control radar Persons • Occurrence reports • Voluntary reports States • Accident/incident database • State audits • National aviation reviews • State safety programme • SPIs and SPTs • INTERNATIONAL CIVIL AVIATION ORGANIZATION universal safety oversight program OLF • In-flight medical incapacity database • Other state partner Civil aviation authority • Mandatory occurrence reports • Voluntary reports • Risk assessments • Risk profiles • Industry SPIs/trend analysis • Service provider surveillance • External and internal audits • Enforcement records • Incident/accident reports • Certification records • Aircrew in-flight medical incapacity reports • Trends in medical assessment findings Regional safety oversight organization/RAIOs • Regional safety programmes • Regional accident investigations INTERNATIONAL CIVIL AVIATION ORGANIZATION• Universal Safety Oversight Audit Programme activities • State safety briefings • Regional safety briefings Other States • Significant safety concerns Accident investigation authority • Accident/incident Notifications/reports • Safety investigation and analysis Approved training organizations • Mandatory occurrence reports • Voluntary reports • Risk assessment register • SPIs trend analysis • Training data • Quality assurance reports Organizations responsible for type design or manufacture of aircraft, engines or propellers • Mandatory occurrence reports, voluntary reports, risk assessment register • SPIs/trend analysis • Internal audits • Service difficulty reports (SDR) • Maintenance and operational experience reports Air traffic services (Air Traffic Services) providers • Mandatory occurrence reports • Voluntary reports • Risk assessment register • SPIs/trend analysis • Internal audits • Special air-report (AIREPs) • Training records • Communication records Operators of certified aerodromes • Mandatory occurrence reports • Voluntary reports • Risk assessment register • SPIs/trend analysis • Aerodromes safety report • Internal audits • Inspections of the movement area Air operators • Mandatory occurrence reports • Voluntary reports • Flight data analysis (FDA) • Fatigue risk management system • Recorded data (flight data recorder, cockpit voice recorder, video, ambient, streamed data) • Risk assessment register • SPIs/trend analysis • Maintenance records • Internal audits • Reliability programme reports • Training records Approved Maintenance Organizations • Mandatory occurrence reports • Voluntary reports • Risk assessment register • SPIs/trend analysis • Internal audits • Quality programme reports • Training records • Service difficulty reports (SDR) • In-service occurrence reports • Maintenance and operational experience reports • Service information reports (faults, malfunctions, defects) • Unapproved parts reports Service providers 5.2.5 Mandatory safety reporting systems 5.2.5.1 Annex 19 requires States to establish a mandatory safety reporting system that includes, but is not limited to, the reporting of incidents. The reporting systems developed by States and service providers should be made as simple as possible to access, generate and submit mandatory reports. Mandatory safety reporting systems should aim to capture all of the valuable information about an occurrence, including: what happened, where, when and to whom the report is addressed. In addition, mandatory safety reporting systems should provide for the capture of some specific hazards which are known to contribute to accidents, the timely identification and communication of which is considered valuable (e.g. routine meteorological conditions, volcanic activity, etc.). 5.2.5.2 Regardless of the scope of the mandatory reporting system(s), it is recommended that all mandatorily collected reports be protected as per the principles detailed in Chapter 7. 5.2.5.3 Mandatory occurrence reporting systems tend to collect more technical information (e.g. hardware failures) than human performance aspects. To address the need for a greater range of safety reporting, States should also implement a voluntary safety reporting system. This aims to acquire more information, such as human factors related aspects, and enhance aviation safety. Reporting of accidents and incidents 5.2.5.4 Accident and incident reporting is relevant to every stakeholder in aviation. Operational personnel are required to report accidents and certain types of incidents as soon as possible and by the quickest means available to the State’s Accident Investigation Authority. Serious incidents must be reported, a list of examples of incidents that are likely to be serious incidents may be found in Attachment C of Annex 13. 5.2.5.5 The following are two main aspects to consider when deciding whether an incident should be classified as a serious incident: a) Were there circumstances indicating that there was a high probability of an accident? b) Was the accident avoided only due to providence? 5.2.6 Voluntary safety reporting systems 5.2.6.1 Voluntary safety reporting systems should be established to collect safety data and safety information not captured by the mandatory safety reporting system. These reports go beyond typical incident reporting. Voluntary reports tend to illuminate latent conditions, such as inappropriate safety procedures or regulations, human error, etc. One way to identify hazards is through voluntary reporting. 5.2.6.2 States should accord protection to safety data captured by, and safety information derived from, voluntary safety reporting systems and related sources. States and service providers are advised to refer to Chapter 7 for guidance on how to apply the protection to safety data, safety information and related sources. Appropriate application of the protection will ensure the continued availability of safety data and safety information. States should also consider means to promote voluntary reporting. 5.2.7 Sector-specific safety reporting provisions Provisions for safety reporting systems continue to evolve. New sector-specific reporting requirements, such as fatigue and remotely piloted aircraft systems (RPAS), have been introduced more recently to address specific safety concerns and emerging aviation activities. Table 7 provides some examples of sector-specific reporting systems included in various Annexes, PANS and documents. 5.2.8 Self-disclosure reporting systems Service providers’ systems for the collection of safety data through self-disclosure reporting systems, including automatic data capture such as aviation safety action programme (ASAP) and Flight Data Analysis programmes (flight operations quality assurance (FOQA) programme, line operations safety audit (LOSA) and the normal operations safety survey (NOSS)), are examples of systems that capture safety data through direct observations of flight crews or air traffic controllers, respectively. All these systems permit recording successful system and human performance. Please see Chapter 7 for information regarding the protection of safety data and safety information captured by self-disclosure reporting systems and their sources. 5.2.9 Results of inspections, audits or surveys Results of interactions between State representatives and service providers, such as inspections, audits or surveys, can also be a useful input to the pool of safety data and safety information. The safety data and safety information from these interactions can be used as evidence of the efficacy of the surveillance programme itself. 5.2.10 Optimal safety data and safety information collection Much of the safety data and safety information used as the basis for data-driven decision-making comes from routine, everyday operations which are available from within the organization. The organization should first identify what specific question the safety data and safety information aims to answer or what problem needs to be addressed. This will help determine the appropriate source and clarify the amount of data or information needed. 5.3 TAXONOMIES 5.3.1 Safety data should ideally be categorized using taxonomies and supporting definitions so that the data can be captured and stored using meaningful terms. Common taxonomies and definitions establish a standard language, improving the quality of information and communication. The aviation community's capacity to focus on safety issues is greatly enhanced by sharing a common language. Taxonomies enable analysis and facilitate information sharing and exchange. Some examples of taxonomies include: a) Aircraft model: The organization can build a database with all models certified to operate. b) Airport: The organization may use INTERNATIONAL CIVIL AVIATION ORGANIZATION or International Air Transport Association (IATA) codes to identify airports. c) Type of occurrence: An organization may use taxonomies developed by INTERNATIONAL CIVIL AVIATION Organization and other international organizations to classify occurrences. 5.3.2 There are a number of industry common aviation taxonomies. Some examples include: a) Accident Incident Data Reporting: an occurrence category taxonomy that is part of ICAO’s accident and incident reporting system. It is a compilation of attributes and the related values that allow safety trend analysis on these categories. b) Commercial Aviation Safety Team (CAST)/International Civil Aviation Organization (ICAO) Common Taxonomy Team (CICTT): tasked with developing common taxonomies and definitions for aircraft accident and incident reporting systems. c) Safety Performance Indicators Task Force (Safety Performance Indicator-TF): tasked with developing globally harmonized metrics for service providers’ SPIs as part of their SAFETY MANAGEMENT SYSTEM, to ensure uniformity in the collection of information and comparison of analysis results. 5.3.3 An excerpt of taxonomy from the CICTT is provided in Table 8 as an example only. 5.3.4 Hazard taxonomies are especially important. Identification of a hazard is often the first step in the risk management process. Commencing with a commonly recognized language makes the safety data more meaningful, easier to classify and simpler to process. The structure of a hazard taxonomy may include a generic and specific component. 5.3.5 The generic component allows users to capture the nature of a hazard with a view to airport information desk in identification, analysis, and coding. A high-level taxonomy of hazards has been developed by the CICTT which classifies hazards in families of hazard types (Environmental, Technical, Organizational, and Human). 5.3.6 The specific component adds precision to the hazard definition and context. This enables more detailed risk management processing. The following criteria may be helpful when formulating hazard definitions. When naming a hazard, it should be: a) clearly identifiable; b) described in the desired (controlled) state; and c) identified using accepted names. 5.3.7 Common taxonomies may not always be available between databases. In such a case, data mapping should be used to allow the standardization of safety data and safety information based on equivalency. Using an aircraft type example, a mapping of the data could show that a “Boeing 787-8” in one database is equivalent with a “788” in another. This may not be a straightforward process as the level of detail during safety data and safety information capture may differ. Most Safety Data Collection and Processing System will be configured to assist with the standardization of data capture, easing the burden of data mapping. 5.4 SAFETY DATA PROCESSING Safety data processing refers to the manipulation of safety data to produce meaningful safety information in useful forms such as diagrams, reports, or tables. There are a number of important considerations related to safety data processing, including: data quality, aggregation, fusion, and filtering. 5.4.1 Data quality 5.4.1.1 Data quality relates to data that is clean and fit for purpose. Data quality involves the following aspects: a) cleanliness; b) relevance; c) timeliness; and d) accuracy and correctness. 5.4.1.2 Data cleansing is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. 5.4.1.3 Relevant data is data which meets the organization’s needs and represents their most important issues. An organization should assess the relevance of data based on its needs and activities. 5.4.1.4 Safety data and safety information timeliness is a function of its currency. Data used for decisions should reflect what is happening as close to real time as possible. Judgement is often required based on the volatility of the situation. For example, data collected two years ago on an aircraft type still operating the same route, with no significant changes, may provide a timely reflection of the situation. Whereas data collected one week ago on an aircraft type no longer in service may not provide a meaningful, timely reflection of the current reality. 5.4.1.5 Data accuracy refers to values that are correct and reflect the given scenario as described. Data inaccuracy commonly occurs when users enter the wrong value or make a typographical error. This problem can be overcome by having skilled and trained data entry personnel or by having components in the application such as spell check. Data values can become inaccurate over time, also known as “data decay”. Movement is another cause of inaccurate data. As data is extracted, transformed and moved from one database to another, it may be altered to some extent, especially if the software is not robust. 5.4.2 Aggregation of safety data and safety information Data aggregation is when safety data and safety information is gathered and stored in the organization’s Safety Data Collection and Processing System and expressed in a summary form for analysis. To aggregate safety data and safety information is to collect them together, resulting in a larger data set. In the case of Safety Data Collection and Processing System, individual items of safety data are aggregated into a database without giving one piece of safety data precedence over another. A common aggregation purpose is to get information about a particular group or type of activity based on specific variables such as: location; fleet type; or professional group. Data aggregation can sometimes be helpful across multiple organizations or regions that do not have enough data to ensure proper de-identification to protect the sources of the safety data and safety information, and to support analysis. 5.4.3 Data fusion Data fusion is the process of merging multiple safety data sets to produce more coherent, linked and useful safety data than that provided by any individual set of safety data. The integration of safety data sets followed by its reduction or replacement improves the reliability and usability of said data. Thus, for example, data from Flight Data Analysis systems of air operators could be merged with meteorological data and radar data to obtain a more useful data set for further processing. 5.4.4 Filtering of safety data and safety information Safety data filtering refers to a wide range of strategies or solutions for refining safety data sets. This means the data sets are refined into simply what the decision-maker needs, without including other data that can be repetitive, irrelevant or even sensitive. Different types of data filters can be used to generate reports or present the data in ways that facilitate communication. 5.5 SAFETY DATA AND SAFETY INFORMATION MANAGEMENT 5.5.1 Safety data and safety information management can be defined as the development, execution and supervision of plans, policies, programmes and practices that ensure the overall integrity, availability, usability, and protection of the safety data and safety information used by the organization. 5.5.2 Safety data and safety information management which addresses the necessary functions will ensure that the organization’s safety data and safety information is collected, stored, analysed, retained and archived, as well as governed, protected and shared, as intended. Specifically, it should identify: a) what data will be collected; b) data definitions, taxonomy and formats; c) how the data will be collected, collated and integrated with other safety data and safety information sources; d) how the safety data and safety information will be stored, archived and backed up; for example, database structure, and, if an IT system, supporting architecture; e) how the safety data and safety information will be used; f) how the information is to be shared and exchanged with other parties; g) how the safety data and safety information will be protected, specific to the safety data and safety information type and source; and h) how quality will be measured and maintained. 5.5.3 Without clearly defined processes to produce safety information, an organization cannot achieve defensible, reliable, and consistent information upon which data-driven decisions are confidently made. 5.5.4 Data governance Data governance is the authority, control and decision-making over the processes and procedures that support an organization’s data management activities. It dictates how safety data and safety information are collected, analysed, used, shared and protected. Data governance ensures that the data management system(s) has the desired effect through the key characteristics of integrity, availability, usability and protection as described below. Integrity — Data integrity refers to the reliability of the sources, information, and events it contains. However, data integrity includes the maintenance and the assurance of the accuracy and consistency of data over its entire life-cycle. This is a critical aspect to the design, implementation and usage of the Safety Data Collection and Processing System when storing, processing, or retrieving the data. Availability — It should be clear who has permission to use or share the stored safety data and safety information. This has to take into account the agreement between the data/information owner and custodian. For the entities that are allowed to use the data, it should be clear how to gain access and how to process it. A variety of techniques exist to maximize data availability, including redundancy of storage locations and data access methods and tools. Usability — In order to maximize returns on safety data and safety information, it is important to also consider usability standards. Humans are continuously interacting and engaging with safety data and safety information as they are acquired. Organizations should minimize human error as automation applications are applied. Tools which can increase usability include data dictionaries and metadata repositories. As human interaction evolves towards big data applications and machine learning processes, it will become increasingly important to better understand human usability as it is applied to machines to minimize safety data and safety information miscalculations in the future. Protection — States should ensure that safety data, safety information and related sources are afforded appropriate protection. For more information refer to Chapter 7. 5.5.5 Metadata management 5.5.5.1 Metadata is defined as a set of data that describes and gives information about other data, in other words, data about data. Using metadata standards provides a common meaning or definition of the data. It ensures proper use and interpretation by owners and users, and that data is easily retrieved for analysis. 5.5.5.2 It is important that organizations catalogue their data based on its properties, including but not limited to: a) what the data is; b) where it comes from (the original source); c) who created it; d) when it was created; e) who used it; f) what it was used for; g) frequency of collection; and h) any processing or transformation. 5.5.5.3 Metadata provides a common understanding of what the data is and ensures correct use and interpretation by its owners and users. This can also identify errors in the data collection which leads to continuous improvements of the program. Chapter 6 SAFETY ANALYSIS 6.1 INTRODUCTION 6.1.1 Safety analysis is the process of applying statistical or other analytical techniques to check, examine, describe, transform, condense, evaluate and visualize safety data and safety information in order to discover useful information, suggest conclusions and support data-driven decision-making. Analysis helps organizations to generate actionable safety information in the form of statistics, graphs, maps, dashboards and presentations. Safety analysis is especially valuable for large and/or mature organization with rich safety data. Safety analysis relies on the simultaneous application of statistics, computing and operations research. The result of a safety analysis should present the safety situation in ways that enable decision makers to make data-driven safety decisions. 6.1.2 States are required to establish and maintain a process to analyse the safety data and safety information from the Safety Data Collection and Processing System and associated safety databases. One of the objectives of safety data and safety information analysis at the State level is the identification of systemic and cross-cutting hazards that might not otherwise be identified by the safety data analysis processes of individual service providers. 6.1.3 Safety analysis may be a new function the State or service provider may need to establish. It should be noted that the required competencies to conduct effective safety analysis might be outside of the purview of a traditional safety inspector. States and service providers should consider the skills necessary to analyse safety information and decide whether this role, with appropriate training, should be an extension of an existing position or whether it would be more efficient to establish a new position, outsource the role, or use a hybrid of these approaches. The decision will be driven by the plans and circumstances of each State or service provider. 6.1.4 In parallel with the human resourcing considerations should be an analysis of the existing software, and business and decision-making policies and processes. To be effective, the safety analysis should be integrated with the organization’s existing core tools, policies and processes. Once amalgamated, the ongoing development of safety intelligence should be seamless and part of the organization’s usual business practice. 6.1.5 Safety data and safety information analysis can be conducted in many ways, some requiring more robust data and analytic capabilities than others. The use of suitable tools for analysis of safety data and safety information provides a more accurate understanding of the overall situation by examining the data in ways that reveal the existing relationships, connections, patterns and trends that exist within. 6.1.6 An organization with a mature analysis capability is better able to: a) establish effective safety metrics; b) establish safety presentation capabilities (e.g. safety dashboard) for ready interpretation of safety information by decision makers; c) monitor safety performance of a given sector, organization, system or process; d) highlight safety trends, safety targets; e) alert safety decision makers, based on safety triggers; f) identify factors that cause change; g) identify connections or “correlations” between or among various factors; h) test assumptions; and i) develop predictive modelling capabilities. 6.1.7 Organizations should include a range of appropriate information sources in their safety analysis, not just “safety data”. Examples of useful additions to the data set include: weather, terrain, traffic, demographics, geography, etc. Having access to and exploiting a broader range of data sources will ensure analysts and safety decision makers are aware of the bigger picture, within which the safety decisions are made. 6.1.8 States, in particular, should be especially interested in information which identifies safety trends and hazards that cut across the aviation system. 6.2 TYPES OF ANALYSIS Analysis of safety data and safety information also allows decision makers to compare information to other groups (i.e. a control or comparison group) to help draw conclusions from the safety data. Common approaches include descriptive analysis (describing), inferential analysis (inferring) and predictive analysis (predicting), as illustrated in Figure 6-1. 6.2.1 Descriptive analysis 6.2.1.1 Descriptive statistics are used to describe or summarize data in ways that are meaningful and useful. They help describe, show or summarize data in ways so patterns can emerge from the data and help to clearly define case studies, opportunities and challenges. Descriptive techniques provide information about the data; however, they do not allow users to make conclusions beyond the analysed data or to reach conclusions regarding any hypotheses about the data. They are a way to describe the data. 6.2.1.2 Descriptive statistics are helpful because if we simply presented the risk achievement worth data, particularly in large quantities, it would be hard to visualize what the data is showing us. Descriptive statistics enable users to present and see the data in a more meaningful way, allowing simpler interpretation of the data. Tools such as tables and matrices, graphs and charts and even maps are examples of tools used for summarizing data. Descriptive statistics include measures of central tendency such as mean (average), median and mode, as well as measures of variability such as range, quartiles, minimum and maximum, frequency distributions, variance and standard deviation (SD). These summaries may either be the initial basis for describing the data as part of a more extensive statistical analysis or they may be sufficient in and of themselves for a particular investigation. 6.2.2 Inferential analysis Inferential (or inductive) statistics aim to use the data to learn about the larger population the sample of data represents. It is not always convenient or possible to examine each item of an entire population and to have access to a whole population. Inferential statistics are techniques that allow users of available data to make generalizations, inferences and conclusions about the population from which the samples were taken to describe trends. These include methods for estimating parameters, testing of statistical hypotheses, comparing the average performance of two groups on the same measure to identify differences or similarities, and identifying possible correlations and relationships among variables. 6.2.3 Predictive analysis Other types of analyses include probability or predictive analyses that extract information from historical and current data and use it to predict trends and behaviour patterns. The patterns found in the data help identify emerging risks and opportunities. Often the unknown event of interest is in the future, but predictive analysis can be applied to any type of unknown in the past, present or future. The core of predictive analysis relies on capturing relationships between variables from past occurrences and exploiting them to predict the unknown outcome. Some systems allow users to model different scenarios of risks or opportunities with different outcomes. This enables decision makers to assess the decisions they can make in the face of different unknown circumstances and to evaluate how they can effectively allocate limited resources to areas where the highest risks or best opportunities exist. 6.2.4 Combined analysis 6.2.4.1 Various types of statistical analyses are interconnected and often conducted together. For example, an inferential technique may be the main tool used to draw conclusions regarding a set of data, but descriptive statistics are also usually used and presented. Also, outputs of inferential statistics are often used as the basis for predictive analysis. 6.2.4.2 Analytical techniques can be applied to safety analysis in order to: a) identify the causes and contributing factors related to hazards and elements which are detrimental to the continuous improvement of aviation safety; b) examine areas for improvement and increase in the effectiveness of safety controls; and c) support ongoing monitoring of safety performance and trends. 6.3 REPORTING OF ANALYSIS RESULTS 6.3.1 Results of safety data analysis can highlight areas of high safety risk and assist decision makers and managers to: a) take immediate corrective actions; b) implement safety risk-based surveillance; c) define or refine safety policy or safety objectives; d) define or refine SPIs; e) define or refine SPTs; f) set Safety Performance Indicator triggers; g) promote safety; and h) conduct further safety risk assessment. 6.3.2 The results of a safety analysis should be made available to aviation safety stakeholders in a way that can be easily understood. The results should be presented with the audience, such as organizational decision makers, external service providers, Civil Aviation Authoritys and other States, in mind. Safety analysis results may be presented several ways; the following are some examples: a) Imminent safety alerts: for the transmittal to other States or service providers of safety hazards with potential outcomes that could be catastrophic, and which require immediate actions. b) Safety analysis reports: usually present quantitative and qualitative information with a clear description of the degree and source of the uncertainty involved in the analysis findings. These reports may also include relevant safety recommendations. c) Safety conferences: for States and service providers to share safety information and safety analysis results that can promote collaborative initiatives. 6.3.3 It is helpful to translate recommendations into action plans, decisions and priorities that decision makers in the organization must consider and, if possible, to outline who needs to do what about the analysis results and by when. 6.3.4 Visualization tools such as charts, graphs, images and dashboards are simple yet effective means of presenting results of data analysis. Several examples of visual data analysis reports can be found on ICAO’s integrated Safety Trend Analysis and Reporting System (iSTARS) at https://icao.int/safety/iSTARS. 6.3.5 Safety dashboards 6.3.5.1 The safety performance of the organization should be demonstrable and should clearly indicate to all interested parties that safety is being managed effectively. One approach to demonstrating this is through a “safety dashboard”, which is a visual representation that enables senior executives, managers, and safety professionals a quick and easy way to view the organization’s safety performance. 6.3.5.2 In addition to a real time display of the organization’s SPIs and SPTs, dashboards may also include information relating to category, cause and severity of specific hazards. Ideally, the information presented on the dashboard can be customized to display the information required to support the decision-making at varying levels of the organization. The use of triggers is useful for providing basic visuals to highlight if there are any issues to be addressed for a specific indicator. Analysts and decision makers will want the ability to configure the dashboard to display their top indicators as well as a feature which allows them to delve deeper into the metrics. 6.3.5.3 Collecting and analysing the data required for effective management and decision-making is an ongoing process. The results of data analysis may reveal that more and better data must be collected and analysed in support of the actions and decisions that the organization needs to take. Figure 6-2 shows how reporting of analysis results may determine further requirements for data to be collected. 6.4 SAFETY INFORMATION SHARING AND EXCHANGE Safety can be further improved when safety information is shared or exchanged. It ensures a consistent, data-driven and transparent response to safety concerns at the global, State and organizational levels. Sharing of safety information refers to giving, while exchange refers to giving and receiving in return. 6.4.1 Sharing within the State 6.4.1.1 States should promote the establishment of safety information sharing or exchange networks among users of the aviation system, and facilitate the sharing and exchange of safety information, unless their national law provides otherwise. Safety promotion guidance for States and service providers is provided in Chapters 8 and 9, respectively. 6.4.1.2 The level of protection and conditions under which safety information will be shared or exchanged between State authorities and service providers must be consistent with national laws. Further information on the protection of safety data and safety information can be found in Chapter 7. 6.4.2 Sharing between States States should share safety information with other States as soon as possible if, in the analysis of the information contained in its Safety Data Collection and Processing System, safety matters that may be of interest to another State are identified. States are also encouraged to share safety information within their Regional Aviation Safety Group. Prior to the sharing of safety information, States should ensure that the level of protection and conditions under which safety information will be shared are in line with Annex 19, Appendix 3. Detailed guidance is available in Chapter 7. 6.5 DATA-DRIVEN DECISION-MAKING 6.5.1 The primary purpose of safety analysis and safety reporting is to present a picture of the safety situation to decision makers which will empower them to make decisions based on the data presented. This is known as data-driven decision-making (also referred to as DDDM or Data-driven decision-making), a process-driven approach to decision-making. 6.5.2 Many aviation occurrences have resulted, at least in part, from poor management decisions, which can result in wasted money, labour and resources. The goal of safety decision makers is, in the short term, to minimize poor outcomes and achieve effective results, and in the long term, to contribute to the achievement of the organization’s safety objectives. 6.5.3 Good decision-making is not easy. Decisions are often made without being able to consider all the relevant factors. Decision makers are also subject to bias that, whether consciously or not, affects decisions made. 6.5.4 The intent of Data-driven decision-making is not necessarily to make the “perfect” or ideal decision, but rather to make a good decision that achieves the short-term objective (about which the actual decision is being made) and works towards satisfying the longer-term objective (improved organizational safety performance). Good decisions meet the following criteria: a) Transparent: the aviation community should know all the factors that influence a decision, including the process used to arrive at the decision. b) Accountable: the decision maker “owns” the decision and the associated outcomes. Clarity and transparency also bring about accountability – it’s not easy to hide behind a decision where roles and responsibilities are defined in detail and where expectations associated with the new decision are clearly outlined. c) Fair and objective: the decision maker is not influenced by considerations that are not relevant (e.g. monetary gain or personal relationships). d) Justifiable and defendable: the decision can be shown to be reasonable given the inputs to the decision and the process followed. e) Reproducible: given the same information that was available to the decision maker, and using the same process, another person would arrive at the same decision. f) Executable: the decision is clear enough and that clarity minimizes uncertainty. g) Pragmatic: humans are creatures of emotion, which means eliminating emotion from a decision isn't feasible. However, what can be eliminated are self-serving emotional biases. A healthy question to ask in the face of difficult decisions is: whom does the decision serve? 6.5.5 Advantages of data-driven decision-making 6.5.5.1 Data-driven decision-making enables decision makers to focus on desired safety outcomes which align with the safety policy and objectives, and address various aspects related to change management, safety risk assessments, etc. Data-driven decision-making can assist with decisions related to: a) changes that can be expected in statutory and regulatory requirements, emerging technologies or resources which may affect the organization; b) potential changes in the needs and expectations of the aviation community and interested parties; c) various priorities that need to be established and managed (e.g. strategic, operational, resources); d) new skills, competencies, tools and even change management processes that may be needed to implement new decision(s); e) risks that must be assessed, managed or minimized; f) existing services, products and processes that currently provide the most value for interested parties; and g) evolving demands for new services, products and processes. 6.5.5.2 A structured approach such as Data-driven decision-making drives decision makers to decisions that are aligned with what the safety data is indicating. This requires trust in the safety performance management framework; if there is confidence in the Safety Data Collection and Processing System, there will be trust in any decisions derived from them. 6.5.6 Common challenges with data-driven decision-making 6.5.6.1 Implementing processes for data collection and analysis takes time and money, as well as expertise and skills that may not be readily available to the organization. The appropriate amount of time and resources vested into the decision-making process needs to be carefully considered. Factors to consider include the amount of money involved in the decision, the extent of the influence of the decision and the decision’s safety permanence. If the organization does not understand what is involved, then the Data-driven decision-making process may become a source of frustration for safety decision makers, causing them to undermine or abandon the process. Like State Safety Programme, SAFETY MANAGEMENT SYSTEM, Data-driven decision-making and safety performance management require a commitment to build and sustain the structures and skills necessary to maximize the opportunities presented by Data-driven decision-making. 6.5.6.2 It is harder to build trust in data than it is to trust an expert’s input and opinion. Adopting the Data-driven decision-making approach requires a shift in the culture and mindset of the organization where decisions are based upon reliable SPIs and the results of other safety data analysis. 6.5.6.3 In some cases the decision-making process may become bogged down in an attempt to find the “best possible” solution, also known as “analysis paralysis”. Strategies that can be used to avoid this include: a) setting a deadline; b) having a well-defined scope and objective; and c) not aiming for a “perfect” decision or solution the first time, but rather coming up with a “suitable” and “practical” decision and improving further decisions. 6.5.7 Data-driven decision-making process 6.5.7.1 The Data-driven decision-making process can be a critical tool that increases the value and effectiveness of the State Safety Programme and SAFETY MANAGEMENT SYSTEM. Effective safety management depends on making defendable and informed decisions. In turn, effective Data-driven decision-making relies on clearly defined safety data and information requirements, standards, collection methods, data management, analysis and sharing, all of which are components of a Data-driven decision-making process. Figure 6-3 illustrates the Data-driven decision-making process. Step 1 — Defining the problem or objective 6.5.7.2 The first step in planning and establishing the Data-driven decision-making process is to define the problem that needs to be solved or the safety objective that must be achieved. What is the question that needs to be answered? What decision must the safety decision makers make? How will it align with the more strategic organizational objectives? In the process of defining the problem statement, decision makers should ask themselves the following questions: a) Does the collection and analysis of data support and relate to the organization’s safety objectives or safety targets? b) Is the required data available? Or can it be obtained in a reasonable manner? c) Is it practical and feasible to collect and analyse the data? d) Are the required resources (people, equipment, software, funds) available? 6.5.7.3 In the safety management context, the main problem statements within the organization are related to evaluating and selecting safety priorities – in alignment with the safety objectives – and establishing measures for safety risk mitigation. Step 2 — Access to data to support the decision-making 6.5.7.4 The next step is to identify what data is needed to answer the problem (taking into account the provisions on information protection). No data is any more valuable than other data. Focus should be on whether the available data is appropriate to help answer and resolve the problem. If the data required is available, proceed to step 4. If the right data is not available, the organization will need to collect, store, analyse and present new safety data and safety information in meaningful ways. Step 3 — Request data to support the decision-making 6.5.7.5 If the data isn’t already available, the organization needs to find ways of collecting it. This may mean establishing another Safety Performance Indicator and perhaps aligned SPTs. Establishing additional indicators can come at a cost. Once the cost is known, the organization should estimate if the benefits outweigh those costs. The focus should primarily be on identifying, monitoring and measuring safety data that is needed to make effective data-driven safety decisions. If the costs outweigh the benefits, consider alternative data sources and/or indicators. 6.5.7.6 In the planning phase of the Data-driven decision-making process, the organization must define what it wants to achieve by establishing the SPTs and SPIs, and analysing the data. Why does the organization need to address the identified problem? What is a reasonable target? And how and where will safety decision makers use the results of data collection and analysis? Having a clear understanding of why the organization needs to collect, analyse, share and exchange safety data and information is fundamental for any Safety Data Collection and Processing System. 6.5.7.7 The following elements combine to enable an organization to identify trends, make informed decisions, evaluate the safety performance in relation to defined objectives, assess risks or fulfil its requirements: a) safety performance management - as the safety data and safety information governance framework; b) Safety Data Collection and Processing System - as the safety data collection and processing functionality; and c) Data-driven decision-making as a dependable decision-making process. Step 4 — Interpret results of data analysis and make data-driven decision 6.5.7.8 The data gathered must be presented to the decision makers at the right time and in meaningful ways. The appropriateness and size of the data sets, the sophistication of the analytics and the skills of the data analysts will only be effective if the data is presented when needed and in formats that make it easy for decision makers to comprehend. The insights gained from the data should inform decision-making, and ultimately, improve safety performance. 6.5.7.9 There are many decision-making models available. Using an agreed and standardized approach will maximize consistency and effectiveness of the organization’s data-driven decisions, most include the following steps: a) assemble a team/group with the necessary skills and experience (e.g. safety action group (SAG)); b) clearly define the safety problem or objective and the context; c) review the organization’s SPTs and safety objectives to ensure continued alignment; d) review and interpret the safety data to understand what it is indicating; e) consider and analyse the viable alternatives; f) consider the risk of feasible actions (or inactions); g) gain consensus among the decision-making group; h) commit to the data-driven decision and act on the decision (turning data into action); and i) monitor and evaluate the outcomes. Step 5 — Communicate the decision 6.5.7.10 For the safety decision to be effective, it needs to be communicated to stakeholders, these include: a) staff required to enact the necessary actions; b) person who reported the situation (if required); c) all personnel, to ensure they are kept informed of safety improvements (safety promotion; States refer to chapter 8; service providers refer to chapter 9); and d) organizational knowledge managers to ensure the safety decision is incorporated into the learning of the organization. 6.5.7.11 For more information on safety communications, refer to 8.6 for States and 9.6 for service providers.