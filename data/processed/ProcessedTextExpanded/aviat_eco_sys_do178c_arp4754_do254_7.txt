Title: The Aviation Development Ecosystem – Applying DO-178C, ARP4754A, DO-254, & Related Guidelines – Chapter(s) 7 Author(s): Vance Hilderman, with 25+ Industry Experts Category: Aircraft, Safety, Regulations Tags: Regulations, Safety, Airworthiness, Certification DO-178C Avionics Software DO-178 has an innocuous title: “Software Considerations in Airborne Systems and Equipment Certification.” But it’s best not to judge that small 144-page book by its cover. In reality, “178,” as industry slang calls it, is largely considered the bible of avionics software development. Interestingly, since it was first developed in the 80’s (a time in which there was relatively little software in safety-critical systems), it has become the de facto embodiment for many of those systems. In fact, a careful examination of standards within military aviation, medical devices, railroads, automotive, and nuclear power will reveal striking similarities with 178. An accidental coincidence? Hardly. Was 178 the first such standard? That answer hardly matters, except that history is important. And why is history important in the case of DO-178? Because 178 has evolved via three subsequent iterations, it’s important to understand the reasons for those changes. Remember the parable about the seven blind men and the elephant? Each touched a different part of the elephant and tried to describe what they were touching without knowing anything about the other parts of the elephant. An impossible task indeed, yet it’s an easy trap to fall into within aviation: aircraft and their systems are so complex that no single person can fully understand all the intricacies. In other words, without enlightenment, we’re all a little better than the blind men. DO-178 attempts to lay a framework so that development personnel and certification authorities can work with full vision and leave residual blindness behind. 178 does succeed in providing such a framework. However, it doesn’t guarantee clarity of vision and certainly not perfection. What, avionics software is not perfect? Of course not. DO-178 isn’t perfect? Hardly. In fact, paraphrasing Winston Churchill’s famous quotation concerning democracy, “DO-178 is the worst standard in the world … except for all the others!” Avionics systems are generally comprised of a Line Replaceable Unit (LRU) with the allocation of logic to software (DO-178C) or hardware (DO-254), as reflected in the following figure: In the book “Avionics Certification – A Complete Guide To DO-178 & DO-254” the principal author (the same as the author of this book you’re now reading) took 200+ pages to describe what DO-178 is. Describing what DO-178 is NOT can be much briefer: e.g., DO-178 is NOT: A prescriptive and rigid standard for software. An inflexible government standard with little bearing on market realities; A technical standard is outdated as soon as it’s published A treatise on software development. A government standard was written by people with little hands-on experience. To be certain, DO-178 is the opposite of the list above. Truly, DO-178’s very success is generally recognized to be based on the facts (or at least perceptions) that it is: A flexible framework for the development of airborne software; Able to accommodate almost all types of systems regardless of application or complexity and which can evolve with technological advancements; A standard largely written by smart and successful technical personnel who, with their peers, will be bound to adhere to it and thus have a vested interest in its usefulness; A standard that avoids telling the industry a rigid method of “how” to develop and verify software and focuses primarily on the “what” is required; a standard that modulates how many checks and balances (“what”) are minimally required depending on the criticality of the airborne software in that it reduces the cost commensurate with safety. To begin understanding DO-178, it is necessary to understand “eco-systems”. For example, one cannot understand Calculus without first understanding addition, subtraction, equations, and a few more mathematical concepts; Calculus is merely a part of the mathematics eco-system. Similarly, DO-178 is a part of the avionics development and certification eco-system that includes a safety assessment process (ARP-4761), avionics system development (ARP-4754A), hardware design assurance (DO-254), environmental and electromagnetic interference testing (DO-160), and many other relevant aspects. In many ways, DO-178 is like that leg of the elephant, a necessary aspect of the elephant’s ability to stably exist: the leg doesn’t guarantee health but certainly lessens the probability of an early demise … DO-178 then starts with the premise (and for commercial avionics, a certification authority mandate) that the user applies DO-178 as merely one link within the avionics certification chain. Avionics will be neither safe nor compliant without a safe foundation to precede DO-178. What then comes before the application of DO-178? Typically, a Project Specific Certification Plan (PSCP) will be developed, which defines the avionics eco-system for the avionics system, including the applicability of DO-178. That PROJECT SPECIFIC CERTIFICATION PLAN-cited eco-system normally includes the performance of a formal Functional Hazard Assessment (FHA) per ARP-4761 followed by a definition of system-level avionics requirements per ARP-4754A. What happens if you initiate DO-178 activities without considering the preceding? Generally, you will have enjoyed some nice practice activities, so you’ll “get” to go back and do it all again … Truly, DO-178 requires a safe foundation with formally defined system requirements, and any pretext to the contrary is foolhardy. In short, DO- 178 does not verify that the system requirements are good or bad; DO-178 assumes the system requirements are good and ensures they get transposed and verified to software adequately commensurate with the design level. However, the quality of avionics software can be no better than that of the apriori system requirements. DO-178 is a relatively small document, vastly smaller than the documents you’ll produce to prove you followed it. It’s been said that the documents used to develop an aircraft would never fit inside that aircraft. Likewise, it’s guaranteed that the documents developed to comply with DO-178 exceed the page count of the printed source code. Why? Because, like any regulatory framework for safety-critical software, there are plans, standards, specifications, designs, reviews, analyses, tests, and audits necessary to define, evaluate, and prove conformance. The same is true for military aviation, medical devices, nuclear power, railroads, and safety-critical automotive software. After a formal framework is in place to yield reviewed and controlled functional hazard assessments and system-level requirements, it’s time to consider the application of DO-178. Where to start? You can borrow a copy of DO-178 and the ancillary documents and try to understand them. After you’ve read those documents several times, you’ve at least convinced yourself that they are interesting and important. But how do you really begin to apply it, and where do you start when it seems like everything is strict, but nothing is really required? Now, you need to really understand DO-178 Development Assurance (Criticality) Levels. First, you need to understand the Development Assurance Level (DAL) of the software you are developing for DO-178. The rigor applied to planning, development, and correctness of your software is directly associated with its DEVELOPMENT ASSURANCE LEVEL—often referred to as “criticality level.” There are five levels, with increasing rigor from Level E to the most stringent Level A, as depicted below: Remember, prior to developing the software DEVELOPMENT ASSURANCE LEVEL must be determined via the formal safety assessment process. The initial DEVELOPMENT ASSURANCE LEVEL is an output of the Aircraft Functional Hazard Assessment (FHA) and System FUNCTIONAL HAZARD ASSESSMENT; the DEVELOPMENT ASSURANCE LEVEL could possibly be changed later as part of the continuous safety assessment process which includes feedback and reconsideration by Safety Engineering. As previously noted, the level of development and verification engineering rigor is based upon the DEVELOPMENT ASSURANCE LEVEL as depicted below: Note that the assigned level and corresponding reliability are dependent upon the type of aircraft; the following is for Part 25 aircraft, e.g., larger aircraft: Engineering “Independence” per Development Assurance Level. (Note: Quality Assurance must always be independent) In the above figure, “Independence” is reflected as being required for DEVELOPMENT ASSURANCE LEVEL B, and increasingly more so for DEVELOPMENT ASSURANCE LEVEL A; this is verification independence, meaning a different person following a different process performs the verification. Such independence does not need to be a different company or different location; in fact, it’s helpful if the independent verifier is co-located to maximize applied expertise and knowledge retention. Surely you can simply choose your own assurance level for your software to make your work easier, correct? Of course not. The criticality level is essentially determined via the Safety Assessment process preceding the application of DO-178 plus the application of applicable TSOs with consideration of actual operations. Why? Because the assurance level is related to the intended usage and design of the larger “system,” of which software forms only a part. For example, secondary Nav is normally Level C, but if that same VOR/ILS receiver is used in a clear air turbulence III zero feet decision height, it may exceed the technical standard order minimum requirement. So, the level is not always cut and dry. The federal aviation administration took a stab at defining levels in Part 23 AC 23.1309-1D, and Appendix A goes where the other parts do not go. However, each unique system has a DEVELOPMENT ASSURANCE LEVEL and the software within that system has criticality levels equal to or less than the DEVELOPMENT ASSURANCE LEVEL of its system. Less than? Yes, it’s possible for software, or a portion of the software, to have a criticality level less than its system. In fact, it is increasingly common for systems to have software with multiple criticality levels. Why not make all the software Level A? Well, Level A software is likely of higher quality than lower levels, just as Level B generally has higher quality than Level C. So why not make everything Level A? The plane would be safer, right? Perhaps, but the cost would be higher. There are no free lunches in safety-critical software. More safety costs more money. The avionics safety assessment process determines how much safety is required for each system in order to provide acceptable levels of safety; providing additional safety is simply not required. It’s hard enough to comply with the minimum requirement…raising the bar above the minimum requirement, although noble, can put companies at financial risk. Nobility, in this case, is defined as staying within the minimum requirement. DO-178 has specific objectives based on the criticality level of the software. Higher DEVELOPMENT ASSURANCE LEVELs must satisfy more DO-178 objectives than lower levels. After the software criticality level has been determined, you examine DO-178 to determine exactly which objectives must be satisfied for the software. Now you are ready for planning. This is where DO-178 is similar to building a house: you’ve performed geographic analysis to determine what type of foundation is required—that is your “safety assessment.” Then, you need a Planning Process, followed by a Development Process. A concurrent Correctness Process is ongoing throughout both Planning and Development. Avionics software engineering under DO-178 is thus the same as building a house and follows the same three-phased process approach DO-178, DO-254, and DO-278 have three integral processes: Planning, Development, and Integral Correctness: As can be seen in the above figure: the Planning process comes first, and when complete is followed by a larger Development process. In the background the largest process, Correctness (or Integral Process of QUALITY ASSURANCE, Configuration Management, Verification, and Certification Liaison) is performed continuously. What is meant by Planning, Development, and Correctness? Here is a brief summary. Planning Process. Before development or before re-using pre-existing or legacy software, you need to plan your activities. Just like building a house, the building inspectors first need to inspect a set of plans, followed by regular inspections of the house as it’s being built: foundation, walls, electrical, plumbing, roof, and the finished building. DO-178 is similar. There are five plans and three standards associated with the Planning Process. The following figure summarizes the key aspects of these five plans: Quality Assurance (QA) is the principal signatory of the five plans. Why? Because QUALITY ASSURANCE is subsequently responsible for auditing the engineer’s compliance with those Plans, if QUALITY ASSURANCE has not reviewed and approved the plans, they could not reasonably be expected to perform thorough audits against those plans. In North America, civil aviation under the Federal Aviation Administration (FAA) historically augments its certification activities by deploying Designated Engineering Representatives (Designated Engineering Representatives) who assist with certification and sometimes can even be formally given “approval” authority to essentially act as an federal aviation administration auditor/approver. In Europe, a less powerful role is sometimes used in place of Designated Engineering Representatives, that of the Compliance Verification Engineer (CVE), with less authority than a Designated Engineering Representative but still supporting QUALITY ASSURANCE. The five plans, often referred to as the “Keys of DO-178,” are summarized below: The Five Plans Summary: Plan for Software Aspects of Certification (PSAC): an overall synopsis of how your software engineering will comply with DO-178, including references to Safety and the System (as described in the preceding ARP4761A and ARP4754A chapters): Software Quality Assurance Plan (SQAP): details how DO-178’s quality assurance objectives, including approvals, audits, and corresponding record-keeping, will be met for this project: Software Configuration Management Plan (SCMP): details how DO-178’s change management and baseline/storage objectives will be performed on this project: Software Development Plan (SDP): summarizes how software requirements development, design, code, and integration will be performed in conjunction with the usage of associated processes, standards, and tools to satisfy DO-178’s development objectives: Software Verification Plan (SVP): summarizes the review, test, and analysis activities, along with associated verification tools, to satisfy DO-178’s verification objectives: The Three Standards. DO-178 (and DO-254 / DO-278) recognizes that requirements, design, and implementation (code) are fundamentally important and must be assessed. Assessment requires a comparison of those requirements, design, and code to some pre-defined criteria. However, such criteria are inherently subjective. DO-178C has 71 “Objectives,” which are objective because each can be unambiguously and completely assessed. However, requirements, design, and code for a particular system can have vastly differing assessment criteria. For this reason, DO-178 does not provide explicit objectives for requirements, design, or code. Instead, each project defines its own explicit criteria for such via three detailed Standards as follows: 1. Software Requirements Standard: provides criteria for the decomposition and assessment of System Requirements into software high-level requirements and high-level to low-level requirements; including derived and safety related requirements. Note the low-level requirements (LLR’s) can optionally be included in the Software Design Standard since Design includes completion of LLR’s, and the software architecture. 2. Software Design Standard: provides criteria for completing the low-level requirements, defining and assessing the software architecture and design including both internal and external interfaces, data flow and control flow with coupling analysis. 3. Software Coding Standard: provides criteria for implementing and assessing the software source-code; typically, these are expressed as a subset of MISRA-C. The above five plans and three standards comprise the planning documents required for DO-178. Since Level D is the least rigorous of the five DO-178 levels, the three standards are not required; they are required for Levels A through C. (Level E is practically not a DO-178 level since there are no required DO-178 objectives for that lowest level; you merely need a Functional Hazard Assessment (FHA) proving it’s Level E hence not safety-related.) The three standards (Requirements, Design, and Code) therefore codify project-specific criteria. Requirements are dealt with more thoroughly in a subsequent chapter of this book. Coding standards are generally just re-purposed from other industry coding standards such as Institute of Electrical and Electronics Engineers (IEEE) or Motor Industry Software Reliability Association (MISRA) C-code standards. However, the DO-178 software design is unique and covers project-specific software design techniques, processes, and documentation criteria to enable subsequent design verification. However, at what point do subtle software requirement details overlap with software architecture, and is it possible to know all requirement details fully before initiating that software architecture? The answers lie in DO-178C’s treating of the Requirement/Design/Code relationships as depicted below: The key aspects of a DO-178C software design standard are summarized in the following figure: Since software design is the bridge between software requirements and code, safety-critical software design specification and verification comprise key traits of the DO-178C software development. A key facet of understanding DO-178C is the knowledge that the boundary between High-Level Requirements, Low-Level Requirements, and Design is “flexible”; each project simply defines its boundaries within the Software Requirements Standard and the Software Design Standard. Essentially, the Low-Level Requirements comprise the overlap of High-Level Requirements and Design, as depicted below: A Word On Data Flow & Control Flow. DO-178C’s design for DEVELOPMENT ASSURANCE LEVEL C through A must include documentation and then analysis of the software data flow and control flow. This is performed to ensure determinism and assessment of data and control coupling. Data coupling is summarized in the following graphic: Control coupling is summarized in the next graphic: Since both data coupling and control coupling are assessable via DO-178C’s software design documentation, coupling analysis is performed for the four primary reasons noted below: After your project-specific DO-178 Plans and Standards are reviewed and approved by your Quality Assurance department and the relevant certification authority, formal software development activities can proceed. Of course, if you have previously performed DO-178 or safety-critical software projects, you should already have basic software engineering plans and standards that you re-used to build the project-specific DO-178 plans/standards. In practice, it is common for preliminary software requirements and design to be initiated while the plans and standards are being developed to save time and clarify the details within the plans and standards. However, in theory, DO-178 postulates a classic waterfall model utilizing sequential planning, development, and verification processes. Why does DO-178 generally follow such a waterfall approach? Because of History … As noted in the preceding chapter, DO-178 was first composed in the 1980s, during the emergence of software “engineering” to accompany increasingly complex systems. Revised twice through the early 1990s, DO-178 generally reflected a number of software development best practices and philosophies prevalent at the time, including CMM, military standards, and the now-infamous Waterfall methodology. Today, almost no one uses a pure waterfall approach, and in fact, adherents today often use model-based development (MBD), particularly in conjunction with DO-178C and DO-331. Typical V-Model and even a safe subset of Agile are often applied, with the latter more prevalent among lower DEVELOPMENT ASSURANCE LEVELs. While DO-178 allows virtually any deterministic, verifiable software development methodology, vestiges of the Waterfall are strongly imbued within the minds of many avionics’ development practitioners and auditors, particularly those with graying hair such as this author. What Waterfall attributes are commonly used? V-model accompaniment, with an assessment of each phase’s outputs Abundant documentation Sequencing and Transitioning of Requirements, Design, Code, Integration, Test Now, back to avionics software development planning. You have now completed a strong Functional Hazard Assessment based on ARP-4761. You have solid system-level requirements in place based on ARP-4754A, which considers safety requirements. You have great plans and standards in place that are fully compliant with DO-178. Wow—you are ready to start developing software and writing code; quickly build a prototype so you can show management and the customer how proficient you are! Just one problem: you are not even close to being able to write real avionics software. Why? Let us count the reasons. Reasons You Are Not Yet Ready to Write Software: What, you can’t just dream up some requirements for a nice house, buy some building supplies, and then invite all your friends over to start building your dream house while partaking in some pleasant beverages? Well, you could, but not if you want to legally occupy or resell your dream home someday. Avionics is the same way: while philosophical debates rage on whether or not DO-178 is formally “required,” there is no debate that if you want to legitimately sell your avionics systems worldwide, the software needs to comply with DO-178. Where does that leave you? Simply put, each of the reasons above should be addressed BEFORE you start writing software. Why? Well, there are regulatory and business reasons for such. From a regulatory standpoint, it’s important to be able to assess each artifact within the development process. Each development step has entry criteria and exit criteria, and those criteria are associated with discrete acceptance criteria. Where are such acceptance criteria defined? Within your development standards (Remember the Requirements Standard, Design Standard, and Coding Standard?) and within associated checklists for each artifact. Checklists? Yes, aviation is based on checklists. Every time an airplane prepares for takeoff, the pilot and crew perform checks based on checklists. When a commercial airplane (almost always with a pilot and a copilot) prepares for landing, the same thing occurs: the pilot may say “Prepare for landing; flaps set to 10 degrees,” whereupon the copilot affirms by reviewing the flap setting and announcing, “Confirmed. Flaps set 10 degrees.” What just happened? The pilot followed a written landing checklist, and the copilot reviewed the pilot’s interpretation and actions based on that checklist. Thus, the copilot performed an “independent review” of the pilot’s required activities. Surely, the pilot and copilot have both memorized the landing checklists and do not need to actually follow a written checklist in the cockpit? Truly experienced pilots should have the checklist memorized. However, as we all know, memory can fail, particularly under stress, and checklists are written in advance and formally used during actual performance. Paper, digital, it doesn’t matter. What matters is that required activities have defined criteria that are specified in a checklist. Each checklist is reviewed, configured, and approved. That means there is one, and only one, valid version of each checklist for each operation. DO-178 is the same … A quick note on Objectives, Transition Criteria, and Checklists. DO-178 Objectives are identified in the annexes at the end of the official DO-178 document. Transition criteria act as acceptance criteria that correlate to those Objectives. Transitions are associated with the Life Cycle model as opposed to the standards. Checklists are good, but they need to be correlated with the Life Cycle Transition Criteria. Checklists are one level lower. DO-178 uses checklists that you tailor to project specifics and standards; checklists also codify the success criteria applicable to DO-178 compliance. Checklists then satisfy the following attributes for each of your principal artifacts: Prove you have advanced, formal details of acceptance criteria for each process and artifact Prove the review considered at least the minimum criteria, as contained within the checklist Show you can prove review and audit occurrence, with independence when required What gets reviewed via checklists? Virtually everything, particularly items associated with required DO-178 objectives: Plans, Standards, Safety, Requirements, Design, Code, Tests & Results, Tool Qualification(s), Suppliers, etc. When your DO-178 Plans, Standards, and Checklists are complete, you are ready to begin software development. You should have passed the first of four mandatory certification meetings called Stage Of Involvement (SOI) since Stage Of Involvement #1 assesses those very Plans, Standards, and Checklists. Following the classic Waterfall method, the following activities are performed, generally in order to show that you met the entry/exit criteria termed “transitions” within DO-178 parlance. Classic DO-178 Software Development Activities Of course, each of the above software development activities will be performed according to the following previously reviewed and accepted documents: 1. Software Development Plan 2. Software Requirements, Design, and Code Standards 3. Corresponding Checklist for each activity, independently reviewed for applicable Level A and B software development activities. In practice, today’s complex safety-critical systems, such as avionics, rarely follow such a strict Waterfall sequence. Instead, model-based development, lean methods, prototyping, and evolving customer requirements are likely to be found. And the beauty of DO-178 is its non-prescriptive nature meaning flexibility. However, whatever method of software development you choose, including reusing legacy software, that method has to be detailed in advance via reviewed plans, standards, and checklists. Then you have to prove you actually followed such. When you’re done with software implementation, you have requirements that trace to code, and that code can be traced back to requirements. The requirements, design, and code are all documented and reviewed. You’re done, right? Hardly. You are merely ready for the next Stage Of Involvement review, e.g., #2. What does Stage Of Involvement #2 do? It affirms your design complies with your plans/standards, and such can be proven via review checklists. Stage Of Involvement #2 should also affirm that your own project configuration management and Software Quality Assurance (SQA) audits were conducted throughout your implementation. What is an Software Quality Assurance audit? DO-178 is about advanced planning and then proof (checklists) that the work was accomplished according to plans and standards. However, the higher the criticality level, the less trust is allowed for the author because lives are at stake. The federal aviation administration has a written Order (FAA Order 8110-49, Chapter 3) that dictates the Level of federal aviation administration Involvement (LOFI) based on Criticality and takes into consideration many other factors, such as the developer’s software certification experience. The higher the criticality, the higher the risk the certification authorities perceive, and since they have limited resources, they tend to focus on those higher-risk projects, particularly when being performed by newcomers. In fact, DO-178 has multiple levels of individuals who are involved with reviews or audits: 1. A technical review is performed upon a completed artifact (“ready for review”) to assess its compliance to plans, standards, and checklist. Higher criticality levels require review independence, e.g. by a technical individual uninvolved with authorship. 2. Software Quality Assurance audits are performed to assess adherence to process, not necessarily technical compliance. Were the processes used during authorship compliant with plans and standards, and were the transition criteria (appropriate use of entry and exit criteria for each process) adhered to? 3. Certification liaisons (such as designated engineering representatives and Designated Engineering Representatives in the United States) audits assess prior technical reviews and Software Quality Assurance audits. 4. Certification Authority (such as european union aviation safety agency or FAA) audits the above. As can be seen, Software Quality Assurance within avionics is different from that of other industries. Where those industries often have Software Quality Assurance performing technical reviews and tests, DO-178 focuses Software Quality Assurance upon two primary activities: first, define or approve the project plans and standards; then assess via audits whether or not those plans and standards were properly followed. DO-178 does not dictate how Software Quality Assurance does their work; it simply wants Software Quality Assurance to hit the minimum set of objectives and maintain that position throughout the process. Simple. After Stage Of Involvement #2 has been approved, the software implementation is largely done, though there are always minor requirements changes and bug fixes. However, full software testing needs to be performed. While software testing in other domains is often considered “art,” the problem with “art” is that it is subjective: we cannot agree on a definition for art, but we all “know it when we see it.” The problem is that humans never fully agree on what is good art versus bad art. Same as music, food, weather, etc.: those likewise are subjective. So DO-178 transforms the art of software testing into science by requiring detailed tests of both the software requirements and the code. DO-178 software testers must be proficient at writing tests to assess whether the logic meets the full extent of the written requirements. DO-178 also introduces the concept of “Are we done yet?”. If left to perfection, software testing could take on virtually infinite combinations and permutations. DO-178 limits that even for the highest criticality. Also, the higher the criticality level of the software, the more that software code must be shown to be covered by tests, e.g., structural coverage assessment. Classic DO-178 Software Verification Activities Have you ever heard the statement “I am a V&V Engineer; some consider me an expert.”? Software testing is always part art and part science. In DO-178, software testers truly need to be experts in software design and code. In non-avionics domains, it’s common for “software” testers to know very little about the intricacies of software design and code. Not so in DO-178. First, a little about “V & V.” As everyone knows, V & V is a common moniker for Verification and Validation. Unfortunately, the real meaning of V & V, particularly within avionics software, is often misunderstood. Hundreds of books have been written on software V&V. So the following paragraph does not give justice to this very involved topic. Verification is the assessment of a process result to determine if the output meets specifications. For avionics software, what then is “verification”? Technical people work best with quantitative equations (if you’re not“technical,” then why are you reading this?!?). So, avionics verification has the following equation: V = R + T + A Where: V is “Verification”; R is “Review”; T is “Test”; A is “Analysis” (Note that the “A” for “Analysis” is intentionally shown as small as analysis is only used when the combination of Review and Test does not fully verify the requirement.) What does this equation mean for avionics software? Here’s the answer: Human-created artifacts should be Reviewed (via Checklists) Software Requirements and Code should be Tested If the Test is not conclusive by itself, additional Analysis is required. Of course, the degree of reviews, tests, and analysis is wholly dependent upon the criticality level of the software, as shown below. However, that summarizes Verification, but what about Validation? Remember: verification assesses an artifact to determine if it meets specifications. For software verification, the specification is the “requirements.” However, what if the requirements aren’t great? That’s where Validation comes in: Validation assesses the Requirements to determine if they are great, where great means clear, concise, correct, complete, and verifiable. Garbage in, garbage out. The most important input to avionics software quality is the Requirements. So, must avionics software requirements be validated? Actually, DO-178 does not mandate such a process because validating software requirements is considered subjective without a completed system, and DO-178 focuses on objective criteria. Officially, requirements must be validated at the Hardware and System levels, per DO-254 and ARP-4754A, respectively. Unofficially, best practices should include validating software requirements during the requirements review process, e.g., they are correct and they are complete. As mentioned above, software testing is part art and part science. And like software development, perfection is impossible. However, DO-178 was written by industry professionals with only minor government oversight, so the focus is on reasonable cost-effectiveness. It’s easy to spend more resources testing the software than were used to develop it. Over the avionics product's lifetime, it’s certain more time will be spent testing than developing. What then comprises reasonable, cost-effective software testing? First, ensure you have good software requirements and that those requirements are fully verified. Next, additional robustness testing tries to find errors in the logic via stress/performance testing, exercising boundary and invalid values, and executing all transitions. Finally, structural coverage assessment ensures that all logic that could potentially be executed on the plane is verified and behaves as intended. Top-to-bottom traceability is formalized to prove all documented requirements are verified, and bottom-to-top traceability is included to prove there is no undocumented logic. Be careful, readers: bottom-up traceability is not simply inverting the matrices. This means that each segment of the code needs to be traced to a requirement (high, low, or derived). The segment of the code correlates to the depth required by the criticality. What is the scope and relative effort of DO-178 Testing? A picture may help: First Focus: Tests Based Upon Requirements (E.g. Functional Testing) First, testing is based on requirements for DO-178 (the latest version, DO-178C, goes further by stating all major code segments should trace to at least one requirement). Since DO-178 does not provide subjective thresholds for requirement granularity, testing of requirements is dependent upon the requirements themselves. However, DO-178 compensates for potentially weak requirements by requiring Level A through C software to undergo additional robustness testing and structural coverage assessment. If you have good requirements, testing those requirements should typically yield 90% coverage of the requisite robustness cases and 80% of the code for Level B. Why? Because good requirements provide good detail for low-level functionality and potential robustness conditions, such can be gleaned from the requirements; thus, test cases can cover 80-90% of the necessary conditions just by assessing the requirements and writing test cases for them. However, if you have weak requirements, then writing test cases from those weak requirements may only yield 50-60% of the requisite coverage; in that case, you will discover the missing requirement detail during testing structural coverage activity (though not required Level D and E) and be required to go back and add requirements. Clearly, like most things in real life, it’s much more cost-effective to do it well the first time instead of going back to improve it and do it again. So, if there is a lesson learned to be shared here, invest in good requirements up front and support them with structured reviews and checklists. It’s imperative to note that DO-178’s five criticality levels call for significantly more software testing as the criticality level increases. For software development, criticality level has less impact but not so for testing as the following figure summarizes the testing required per DEVELOPMENT ASSURANCE LEVEL. It’s imperative to note that DO-178’s five criticality levels call for significantly more software testing as the criticality level increases. For software development, criticality level has less impact but not so for testing as the following figure summarizes the testing required per DEVELOPMENT ASSURANCE LEVEL. Major Testing Differences Between DO-178 Criticality Levels: After testing, Stage Of Involvement #3 assesses the completeness of the verification activities. Then Stage Of Involvement #4 accompanies the Conformity Review, which addresses compliance to all applicable DO-178 objectives, including interim “changes.” Putting it All Together. The following figure comprising an optimal DO-178 implementation, which this author uses in most of his teaching classes, is called the Green Snake: As depicted above, the Green Snake begins in the upper left with the Safety process, then proceeds left-to-right as the system and software planning processes are performed, culminating in a review of the plans and standards (Stage Of Involvement-1) prior to the official start of development. Then, on the bottom half, right-to-left, development, testing, and conformity are performed. Of course, “Certification” occurs only in the context of the system, and aircraft with Stage of Involvement-4 are generally required to be completed prior to flight testing so that all safety-related aspects can be affirmed prior to such flight testing. Why is this snake displayed in green? Simple: it’s this author’s opinion that while DO-178 is flexible and non-prescriptive, the above Green Snake path optimally reduces overall cost. This author is American, and American money is green: to save money, follow the Green Snake.