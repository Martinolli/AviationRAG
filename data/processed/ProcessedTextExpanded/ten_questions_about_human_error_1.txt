Title: Ten Questions About Human Error: A New View of Human Factors And System Safety Chapter 1 Author(s): Sidney W. A. Dekker Category: Analysis, Human, Factors, Errors Tags: human, error, factors, accident Acknowledgments Just like errors, ideas come from somewhere. The ideas in this book were developed over a period of years in which discussions with the following people were particularly constructive: David Woods, Erik Hollnagel, Nancy Leveson, James Nyce, John Flach, Gary Klein, Diane Vaughan, and Charles Billings. Jens Rasmussen has always been ahead of the game in certain ways: Some of the questions about human error were already taken up by him in decades past. Erik Hollnagel was instrumental in helping shape the ideas in chapter 6, and Jim Nyce has had a significant influence on chapter 9. I also want to thank my students, particularly Arthur Dijkstra and Margareta Lutzhoft, for their comments on earlier drafts and their useful suggestions. Margareta deserves special gratitude for her help in decoding the case study in chapter 5, and Arthur for his ability to signal "Cartesian anxiety" where I did not recognize it. A special thanks to series editor Barry Kantowitz and editor Bill Webber for their confidence in the project. The work for this book was supported by a grant from the Swedish Flight Safety Directorate. Preface Transportation human factors have always been concerned with human er­ror. In fact, as a field of scientific inquiry, it owes its inception to investiga­tions of pilot error and researchers' subsequent dissatisfaction with the la­bel. In 1947, Paul Fitts and Richard Jones, building on pioneering work by people like Alphonse Chapanis, demonstrated how features of World War II airplane cockpits systematically influenced the way in which pilots made errors. For example, pilots confused the flap and landing-gear handles be­cause these often looked and felt the same and were located next to one an­other (identical toggle switches or nearly identical levers). In the typical incident, a pilot would raise the landing gear instead of the flaps after landing—with predictable consequences for propellers, engines, and airframe. As an immediate wartime fix, a rubber wheel was affixed to the landing-gear control and a small wedge-shaped end to the flap control. This ba­sically solved the problem, and the design fix eventually became a certification requirement. Pilots would also mix up throttle, mixture, and propeller controls because their locations kept changing across different cockpits. Such errors were not surprising; they were random degradations of human performance. Rather, they were actions and assessments that made sense once researchers understood fea­tures of the world in which people worked once they had analyzed the situation surrounding the operator. Human errors are systematically connected to features of people's tools and tasks. It may be difficult to predict when or how often errors will occur (though human reliability techniques have cer­tainly been tried). With a critical examination of the system in which people work, however, it is not that difficult to anticipate where errors will occur. Human factors have worked off this premise ever since: The notion of designing error-tolerant and error-resistant systems is founded on it. Human factors were preceded by a mental Ice Age of behaviorism, in which any study of the mind was seen as illegitimate and unscientific. Behavior­ism itself had been a psychology of protest, coined in sharp contrast against Wundtian experimental introspection that, in turn, preceded it. If behaviorism was a psychology of protest, then human factors was a psychology of pragmatics. The Second World War brought such a furious pace of techno­logical development that behaviorism was caught short-handed. Practical problems in operator vigilance and decision-making emerged that were alto­gether immune against Watson's behaviorist repertoire of motivational exhortations. Up to that point, psychology had largely assumed that the world was fixed and that humans had to adapt to its demands through selection and training. Human factors showed that the world was not fixed: Changes in the environment could easily lead to performance increments not achievable through behaviorist interventions. In behaviorism, performance had to be shaped by features of the world. In human factors, features of the world were shaped by the limits and capabilities of performance. As a psychology of pragmatics, human factors adopted the Cartesian-Newtonian view of science and scientific method (just as both Wundt and Watson had done). Descartes and Newton were both dominant players in the 17th-century scientific revolution. This wholesale transformation in thinking installed a belief in the absolute certainty of scientific knowledge, especially in Western culture. The aim of science was to achieve control by deriving general, and ideally mathematical, laws of nature (as we try to do for human and system performance). A heritage of this can still be seen in human factors, particularly in the predominance of experiments, the nomothetic rather than ideographic inclination of its research, and a strong faith in the realism of observed facts. It can also be recognized in the reductive strategies human factors and system safety rely on to deal with complexity. Cartesian-Newtonian problem-solving is analytic. It consists of breaking up thoughts and problems into pieces and arranging these in some logical order. Phenomena need to be decomposed into more basic parts, and the whole can be explained exhaustively by reference to its con­stituent components and their interactions. In human factors and system safety, the mind is understood as a box-like construction with a mechanistic trade in internal representations; work is broken into procedural steps through hierarchical task analyses; organizations are not organic or dynamic but consist of static layers and compartments and linkages; and safety is a structural property that can be understood in terms of its lower order mechanisms (reporting systems, error rates and audits, safety management function in the organizational chart, and quality systems). These views are with us today. They dominate thinking in human fac­tors and system safety. The problem is that linear extensions of these same notions cannot carry us into the future. The once pragmatic ideas of hu­man factors and system safety are falling behind the practical problems that have started to emerge from today's world. We may be in for a repeti­tion of the shifts that continuing airworthines management exposition with the technological developments of World War II, where behaviorism was shown to fall short. This time, it may be the turn of human factors and system safety. Contemporary developments, however, are not just technical. They are sociotechnical: Understanding what makes systems safe or brittle requires more than knowledge of the human-machine interface. As David Meister recently pointed out (and he has been around for a while), human factors have not made much progress since 1950. "We have had 50 years of research," he wonders rhetorically, "but how much more do we know than we did at the beginning?" (Meister, 2003, p. 5). It is not that approaches taken by human factors and system safety are no longer useful, but their usefulness can only really be appreci­ated when we see their limits. This book is but one installment in a larger transformation that has begun to identify both deep-rooted constraints and new leverage points in our views of human factors and system safety. The 10 questions about human error are not just questions about the human error as a phenomenon if they are that at all (and if human error is some­thing in and of itself in the first place). They are actually questions about human factors and system safety as disciplines and where they stand to­day. In asking these questions about the error and in sketching the answers to them, this book attempts to show where our current thinking is limited, where our vocabulary, our models, and our ideas are constraining progress. In every chapter, the book tries to provide directions for new ideas and models that could perhaps better cope with the complexity of prob­lems facing us now. One of those problems is that, apparently, safe systems can drift into failure. Drift toward safety boundaries occurs under pressures of scarcity and competition. It is linked to the opacity of large, complex sociotechnical systems and the patterns of information on which insiders base their decisions and trade-offs. Drift into failure is associated with normal adaptive organiza­tional processes. Organizational failures in safe systems are not preceded by failures by the breaking or lack of quality of single components. Instead, organizational failure in safe systems is preceded by normal work, by nor­mal people doing normal work in seemingly normal organizations. This ap­pears to severely challenge the definition of an incident and may under­mine the value of incident reporting as a tool for learning beyond a certain safety level. The border between normal work and incident is clearly elastic and subject to incremental revision. With every little step away from previ­ous norms, past success can be taken as a guarantee of future safety. Incrementalism notches the entire system closer to the edge of breakdown but without compelling empirical indications that it is headed that way. Current human factors and system safety models cannot deal with drift into failure. They require failures as a prerequisite for failures. They are still oriented toward finding failures (e.g., human errors, holes in layers of de­fense, latent problems, organizational deficiencies, and resident patho­gens) and rely on externally dictated standards of work and structure rather than taking insider accounts (of what is failure vs. normal work) as canonical. Processes of sense-making, of the creation of local rationality by those who actually make the thousands of little and larger trade-offs that ferry a system along its drifting course, lie outside today's human factors lexicon. Current models typically view organizations as Newtonian-Cartesian machines with components and linkages between them. Mishaps are modeled as a sequence of events (actions and reactions) between a trigger and an outcome. Such models can say nothing about the build-up of latent failures, about the gradual, incremental loosening or loss of control. The processes of erosion of constraints, attrition of safety, and drift toward margins cannot be captured because structuralist approaches are static metaphors for resulting forms, not dynamic models oriented toward processes of formation. Newton and Descartes, with their particular take on natural science, have a firm grip on human factors and systems safety in other areas, too. The information-processing paradigm, for example, is so useful in explaining early information-transfer problems in World War II radar and radio operators, all but colonized human factors research. It is still a dominant force, buttressed by the Spartan laboratory experiments that seem to confirm its utility and validity. The paradigm has a mechanized mind, chunked up into separate components (e.g., iconic memory, short-term memory, long-term memory) with linkages in between. Newton would have loved the mechan­ics of it. Descartes would have liked it too: A clear separation between mind and world solved (or circumvented, rather) a lot of problems associated with the transactions between the two. A mechanistic model such as information processing, of course, holds special appeal for engineering and other consumers of human factors research results. Pragmatics dictate bridging the gap between practitioner and science, and having a cognitive model that is a simile of a technical device familiar to applied people is one power­ful way to do just that. But there is no empirical reason to restrict our under­standing of attitudes, memories, or heuristics as mentally encoded disposi­tions, as some contents of consciousness with certain expiry dates. In fact, such a model severely restricts our ability to understand how people use talk and act to construct perceptual and social order; how, through dis­course and action, people create environments that, in turn, determine further action and possible assessments, and that constrain what will subsequently be seen as acceptable discourse or rational decisions. We cannot understand how people drift into failure without understanding how groups of people, through assessment and action, assemble versions of the world in which they assess and act. Information processing fits within a larger, dominant metatheoretical perspective that takes the individual as its central focus (Heft, 2001). This view, too, is a heritage of the Scientific Revolution, which increasingly popularized the humanistic idea of a "self-contained individual." For most of psychology, this means that all processes worth studying take place within the boundaries of the body (or mind), something epitomized by the mentalist focus of information processing. In their inability to meaningfully address drift into failure, which intertwines technical, social, institutional, and individual factors, human factors and system safety are currently paying for their theoretical exclusion of transactional and social processes between individuals and the world. The componential and fragmentation of human factors research are still obstacles to progress in this respect. An en­largement of the unit of analysis (as done in the ideas of cognitive systems engineering and distributed cognition) and a call to make action central in understanding assessments and thought have been ways to catch up with new practical developments for which human factors and system safety were not prepared. The individualist emphasis of Protestantism and Enlightenment also reverberates in ideas about control and culpability. Should we hold people ac­countable for their mistakes? Sociotechnical systems have grown in complexity and size, moving some to say that there is no point in expecting or demanding individual insiders (engineers, managers, operators) to live up to some reflective moral ideal. Pressures of scarcity and competition insidi­ously get converted into organizational and individual mandates, which in turn severely constrain the decision options and rationality (and thus au­tonomy) of every actor on the inside. Yet lone antiheroes continue to have lead roles in our stories of failure. Individualism is still crucial to self-identity in modernity. The idea that it takes teamwork, an entire organi­zation, or an entire industry to break a system (as illustrated by cases of drift into failure) is too unconventional relative to our inherited cultural preconceptions. even before we get to complex issues of action and responsibility, we can recognize the prominence of Newtonian-Cartesian deconstruction and componential in much human factors research. For example, empiri­cist notions of perception of elements that gradually get converted into meaning through stages of mental processing are legitimate theoretical no­tions today. Empiricism was once a force in the history of psychology. Yet buoyed by the information-processing paradigm, its central tenets have made a comeback in, for example, theories of situation awareness. In adopting such a folk model from an applied community and subjecting it to putative scientific scrutiny, human factors, of course, meet its pragmatist ideal. Folk models fold neatly into the concerns of human factors as an ap­plied discipline. Few theories can close the gap between researcher and practitioner better than those that apply and dissect practitioner vernacular for scientific study. But folk models come with an epistemological price tag. Research that claims to investigate a phenomenon (say, shared situation awareness or complacency) but that does not define that phenomenon (because, as a folk model, everybody is assumed to know what it means) cannot make falsifiable contact with empirical reality. This leaves such human factors research without the major mechanism for scientific quality control since Karl Popper. Connected to information processing and the experimentalist approach to many human factors problems is a quantitative bias, first championed in psychology by Wilhelm Wundt in his Leipzig laboratory. Although Wundt quickly had to admit that chronometry of mind was too bold a research goal, experimental human factors research projects can still reflect pale ver­sions of his ambition. Counting, measuring, categorizing, and statistically analyzing are chief tools of the trade, whereas qualitative inquiries are often dismissed as subjectivist and unscientific. Human factors have a realist orien­tation, thinking that empirical facts are stable, objective aspects of reality that exist independent of the observer or his or her theory. Human errors are among those facts that researchers think they can see out there in some objective reality. But the facts researchers see would not exist without them or their method or their theory. None of this makes the facts generated through experiments less real to those who observe them, publish them, or read about them. Heeding Thomas Kuhn (1962), however, states that this reality should be seen for what it is: an implicitly negotiated settlement among like-minded researchers rather than a common denominator accessible to all. There is no final arbiter here. It is possible that a componential, experi­mentalist approach could enjoy an epistemological privilege. But that also means there is no automatic imperative for the experimental approach to uniquely stand for legitimate research, as it sometimes seems to do in main­stream human factors. Ways of getting access to empirical reality are infi­nitely negotiable, and their acceptability is a function of how well they conform to the worldview of those to whom the researcher makes his appeal. The persistent quantitative supremacy (particularly in North American human factors) seems saddled with this type of consensus authority (it must be good because everybody is doing it). Such methodological hysteresis could have more to do with primeval fears of being branded "unscientific" (the fears shared by Wundt and Watson) than with a steady return of signifi­cant knowledge increments generated by the research. Technological change gave rise to human factors and system safety thinking. The practical demands posed by technological changes endowed human factors and system safety with the pragmatic spirit they have to this day. But pragmatic is no longer pragmatic if it does not match the demands created by what is happening around us now. The pace of sociotechno­logical change is not likely to slow down any time soon. If we think that World War II generated a lot of interesting changes, giving birth to human factors as a discipline, then we may be living in even more exciting times to­day. If we, in human factors and system safety, keep doing what we have been doing simply because it worked for us in the past, we may become one of those systems that drift into failure. Pragmatics requires that we, too, adapt to better cope with the complexity of the world facing us now. Our past suc­cesses are no guarantee of continued future achievement. Chapter One Was It Mechanical Failure or Human Error? ‘These are exciting and challenging times for human factors and system safety. And there are indications that we may not be entirely well equipped for them. There is an increasing recognition that mishaps (a commercial aircraft accident or a Space Shuttle disaster) are inextricably linked to the functioning of surrounding organizations and institutions. The operation of commercial airliners, Space Shuttles, or passenger ferries spawns vast networks of organizations to support it, to advance and improve it, and to control and regulate it. Complex technologies cannot exist without these organizations and institutions—carriers, regulators, government agencies, manufacturers, subcontractors, maintenance facilities, and training outfits— that, in principle, are designed to protect and secure their operations. Their very mandate boils down to not having accidents happen. Since the 1978 nuclear accident at Three Mile Island, however, people have increasingly realized that the very organizations meant to keep technology safe and stable (hu­man operators, regulators, management, maintenance) are among the major contributors to the breakdown. Sociotechnical failures are impossi­ble without such contributions. Despite this growing recognition, human factors and system safety rely on a vocabulary based on a particular conception of the natural sciences derived from its roots in engineering and experimental psychology. This vocabulary, the subtle use of metaphors, images, and ideas, is more and more at odds with the interpretative demands posed by modern organiza­tional accidents. The vocabulary expresses a worldview (perhaps) appropri­ate for technical failures but incapable of embracing and penetrating the relevant areas of sociotechnical failures — those failures that involve the in­tertwined effects of technology and the organized social complexity sur­rounding its use. Which is to say, most failures today. Any language and the worldview it mediates imposes limitations on our understanding of failure. Yet these limitations are now becoming increas­ingly evident and pressing. With the growth in system size and complexity, the nature of accidents is changing (system accidents, sociotechnical failures). Resource scarcity and competition mean that systems incrementally push their operations toward the edges of their safety envelopes. They have to do this in order to remain successful in their dynamic environments. Commer­cial returns at the boundaries are greater, but the difference between hav­ing and not having an accident is up to stochastics more than available margins. Open systems are continually adrift within their safety envelopes, and the processes that drive such migration are not easy to recognize or control, nor is the exact location of the boundaries. Large, complex systems seem capable of acquiring hysteresis, an obscure will of their own, whether they are drifting towards greater resilience or towards the edges of failure. At the same time, the fast pace of technological change creates new types of hazards, especially those that come with increased reliance on com­puter technology. Both engineered and social systems (and their interplay) rely to an ever greater extent on information technology. Although compu­tational speed and access to information would seem a safety advantage in principle, our ability to make sense of data is not at all keeping pace with our ability to collect and generate it. By knowing more, we may actually know a lot less. Managing safety by numbers (incidents, error counts, safety threats), as if safety is just another index of a Harvard business model, can create a false impression of rationality and managerial control. It may ig­nore higher-order variables that could unveil the true nature and direction of system drift. It may also come at the cost of a deeper understanding of real sociotechnical functioning. DECONSTRUCTION, DUALISM, AND STRUCTURALISM What is that language, then, and what is the increasingly obsolete technical worldview it represents? Its defining characteristics are deconstruction, dualism, and structuralism. Deconstruction means that a system's functioning can be understood exhaustively by studying the arrangement and interaction of its constituent parts. Scientists and engineers typically look at the world this way. Accident investigations deconstruct, too. In order to rule out mechani­cal failure or to locate the offending parts, accident investigators speak of "reverse engineering." They recover parts from the rubble and reconstruct them into a whole again, often quite literally. Think of the TWA800 Boeing 747 that exploded in midair after takeoff from New York's Kennedy Airport in 1998. It was recovered from the Atlantic Ocean floor and painstakingly pieced back together—if heavily scaffolded—in a hangar. With the puzzle as complete as possible, the broken part(s) should eventually get exposed, allowing investigators to pinpoint the source of the explosion. Accidents are puzzling wholes. But it continues to defy sense; it continues to be puz­zling only when the functioning (or nonfunctioning) of its parts fail to ex­plain the whole. The part that caused the explosion that ignited it was never actually pinpointed. This is what makes the TWA800 investigation scary. Despite one of the most expensive reconstructions in history, the re­constructed parts refused to account for the behavior of the whole. In such a case, a frightening, uncertain realization creeps into the investigator corps and into the industry. A whole failed without a failed part. An accident happened without a cause; no cause—nothing to fix, nothing to fix—it could happen again tomorrow or today. The second defining characteristic is dualism. Dualism means that there is a distinct separation between material and human cause—between human error and mechanical failure. In order to be a good dualist, you, of course, have to deconstruct: You have to disconnect human contributions from mechanical contributions. The rules of the International Civil Avia­tion Organization that govern aircraft accident investigators prescribe ex­actly that. They force accident investigators to separate human contribu­tions from mechanical ones. Specific paragraphs in accident reports are reserved for tracing the potentially broken human components. Investiga­tors explore the anteceding 24- and 72-hour histories of the humans who would later be involved in a mishap. Was there alcohol? Was there stress? Was there fatigue? Was there a lack of proficiency or experience? Were there previous problems in the training or operational record of these peo­ple? How many flight hours did the pilot really have? Were there other distractions or problems? This investigative requirement reflects a primeval interpretation of human factors, an aeromedical tradition where human error is reduced to the notion of "fitness for duty." This notion has long been overtaken by developments in human factors towards the study of normal people doing normal work in normal workplaces (rather than physiologically or mentally deficient miscreants), but the overextended aero­medical model is retained as a kind of comforting positivist, dualist, decon­structive practice. In the fitness-for-duty paradigm, sources of human error must be sought in the hours, days, or years before the accident, when the human component was already bent and weakened and ready to break. Find the part of the human that was missing or deficient, the "unfit part," and the human part will carry the interpretative load of the accident. Dig into recent history, find the deficient pieces, and put the puzzle together: deconstruction, reconstruction, and dualism. The third defining characteristic of the technical worldview that still gov­erns our understanding of success and failure in complex systems is structuralism. The language we use to describe the inner workings of successful, and failed systems is a language of structures. We speak of layers of defense, of holes in those layers. We identify the "blunt ends" and "sharp ends" of organizations and try to capture how one has effects on the other. Even safety culture gets treated as a structure consisting of other building blocks. How much of a safety culture an organization has depends on the routines and components it has in place for incident reporting (this is meas­urable), to what extent it is just in treating erring operators (this is more dif­ficult to measure, but still possible), and what linkages it has between its safety functions and other institutional structures. A deeply complex social reality is thus reduced to a limited number of measurable components. For example, does the safety department have a direct route to the highest management? What is the reporting rate compared to other companies? Our language of failures is also a language of mechanics. We describe ac­cident trajectories, we seek causes and effects, and interactions. We look for initiating failures, or triggering events, and trace the successive domino-like collapse of the system that follows it. This worldview sees sociotechnical systems as machines with parts in a particular arrangement (blunt vs. sharp ends, defenses layered throughout), with particular interactions (trajecto­ries, domino effects, triggers, initiators), and a mix of independent or inter­vening variables (blame culture vs. safety culture). This is the worldview in­herited from Descartes and Newton, the worldview that has successfully driven technological development since the scientific revolution half a mil­lennium ago. The worldview, and the language it produces, is based on par­ticular notions of natural science, and exercises a subtle but very powerful influence on our understanding of sociotechnical success and failure today. As it does with most of Western science and thinking, it pervades and di­rects the orientation of human factors and system safety. Yet language, if used unreflectively, easily becomes imprisoning. Lan­guage expresses but also determines what we can see and how we see it. Language constrains how we construct reality. If our metaphors encourage us to model accident chains, then we will start our investigation by looking for events that fit in that chain. But which events should go in? Where should we start? As Nancy Leveson (2002) pointed out, the choice of which events to put in is arbitrary, as are the length, the starting point and level of detail of the chain of events. What, she asked, justifies assuming that initiat­ing events are mutually exclusive, except that it simplifies the mathematics of the failure model? These aspects of technology, and of operating it, raise questions about the appropriateness of the dualist, deconstructed, structuralist model that dominates human factors and system safety. In its place we may seek a true systems view, which not only maps the structural deficiencies behind individual human errors (if indeed it does that at all), but that appreciates the organic, ecological adaptability of complex sociotech­nical systems. Looking for Failures to Explain Failures Our most entrenched beliefs and assumptions often lie locked up in the simplest of questions. The question about mechanical failure or human er­ror is one of them. Was the accident caused by mechanical failure or by hu­man error? It is a stock question in the immediate aftermath of a mishap. Indeed, it seems such a simple, innocent question. To many it is a normal question to ask: If you have had an accident, it makes sense to find out what broke. The question, however, embodies a particular understanding of how accidents occur, and it risks confining our causal analysis to that understanding. It lodges us into a fixed interpretative repertoire. Escaping from this repertoire may be difficult. It sets out the questions we ask, provides the leads we pursue and the clues we examine, and determines the conclusions we will eventually draw. Which components were broken? Was it something engineered, or some human? How long had the component been bent or otherwise deficient? Why did it eventually break? What were the latent fac­tors that conspired against it? Which defenses had eroded? These are the types of questions that dominate inquiries in human fac­tors and system safety today. We organize accident reports and our dis­course about mishaps around the struggle for answers to them. Investiga­tions turn up broken mechanical components (a failed jackscrew in the vertical trim of an Alaska Airlines MD-80, perforated heat tiles in the Columbia Space Shuttle), underperforming human components (e.g., break­downs in crew resource management, a pilot who has a checkered training record), and cracks in the organizations responsible for running the system (e.g., weak organizational decision chains, deficient maintenance, failures in regulatory oversight). Looking for failures—human, mechanical, or organizational—in order to explain failures is so common sensical that most investigations never stop to think whether these are indeed the right clues to pursue. That failure is caused by failure is prerational — we do not consciously consider it any longer as a question in the decisions we make about where to look and what to conclude. Here is an example. A twin-engined Douglas DC-9-82 landed at a re­gional airport in the Southern Highlands of Sweden in the summer of 1999. Rain showers had passed through the area earlier, and the runway was still wet. While on approach to the runway, the aircraft got a slight tailwind, and after touchdown the crew had trouble slowing down. Despite increas­ing crew efforts to brake, the jet overran the runway and ended up in a field a few hundred feet from the threshold. The 119 passengers and crew on board were unhurt. After coming to a standstill, one of the pilots made his way out of the aircraft to check the brakes. They were stone cold. No wheel braking had occurred at all. How could this have happened? Investi­gators found no mechanical failures on the aircraft. The braking systems were fine. Instead, as the sequence of events was rolled back in time, investigators realized that the crew had not armed the aircraft's ground spoilers before landing. Ground spoilers help a jet aircraft brake during rollout, but they need to be armed before they can do their work. Arming them is the job of the pilots, and it is a before-landing checklist item and part of the proce­dures that both crew members are involved in. In this case, the pilots forgot to arm the spoilers. "Pilot error," the investigation concluded. Or actually, they called it "Breakdowns in cockpit resource management (Crew Resource Manage­ment)" (Statens Haverik omission, 2000, p. 12), a more modern, more eu­phemistic way of saying "pilot error." The pilots did not coordinate what they should have; for some reason they failed to communicate the required configuration of their aircraft. Also, after landing, one of the crew members had not called "Spoilers!" as the procedures dictated. This could, or should, have alerted the crew to the situation, but it did not happen. Human errors had been found. The investigation was concluded. "Human error" is our default when we find no mechanical failures. It is a forced, inevitable choice that fits nicely into an equation, where human er­ror is the inverse of the amount of mechanical failure. Equation 1 shows how we determine the ratio of causal responsibility: human error =f(1 - mechanical failure) (1) If there is no mechanical failure, then we know what to start looking for instead. In this case, there was no mechanical failure. Equation 1 continuing airworthines management exposition out as a function of 1 minus 0. The human contribution was 1. It was human er­ror, a breakdown of CRM. Investigators found that the two pilots onboard the MD-80 were actually both captains, and not a captain and a copilot as is usual. It was a simple and not altogether uncommon scheduling fluke, a stochastic fit flying onboard the aircraft since that morning. With two captains on a ship, responsibilities risk getting divided unstably and incoher­ently. Division of responsibility easily leads to its abdication. If it is the role of the copilot to check that the spoilers are armed, and there is no copilot, the risk is obvious. The crew was in some sense "unfit," or at least prone to breaking down. It did (there was a "breakdown of CRM"). But what does this explain? These are processes that themselves require an explanation, and they may be leads that go cold anyway. Perhaps there is a much deeper reality lurking beneath the prima facie particulars of such an incident, a reality where mechanistic and human cause are much more deeply inter­twined than our formulaic approaches to investigations allow us to grasp. In order to get better glimpses of this reality, we first have to turn to dualism. It is dualism that lies at the heart of the choice between human error and me­chanical failure. We take a brief peek at its past and confront it with the un­stable, uncertain empirical encounter of an unarmed spoilers case. Dualism Destitute The urge to separate human cause from machinic cause is something that must have puzzled even the early nascent human factors tinkerers. Think of the fiddling with World War II cockpits that had identical control switches for a variety of functions. Would a flap-like wedge on the flap handle and a wheel-shaped cog on the gear lever avoid the typical confusion between the two? Both common sense and experience said "yes." By changing some­thing in the world, human factors engineers (to the extent that they existed already) changed something in the human. By toying with the hardware that people worked with, they shifted the potential for correct versus incorrect action, but only the potential. For even with functionally shaped con­trol levers, some pilots, in some cases, still got them mixed up. At the same time, pilots did not always get identical switches mixed up. Similarly, not all crews consisting of two captains fail to arm the spoilers before landing. Hu­man error, in other words, is suspended, unstably, somewhere between the human and the engineered interfaces. The error is neither fully human, nor fully engineered. At the same time, mechanical "failures" (providing identical switches located next to one another) get to express themselves in human action. So if a confusion between flaps and gear occurs, then what is the cause? Human error or mechanical failure? You need both to succeed; you need both to fail. Where one ends and the other begins is no longer so clear. One insight of early human factors work was that machinistic feature and human action are intertwined in ways that resist the neat, dualist, de­constructed disentanglement still favored by investigations (and their con­sumers) today. DUALISM AND THE SCIENTIFIC REVOLUTION The choice between human cause and material cause is not just a product of recent human factors engineering or accident investigations. The choice is firmly rooted in the Cartesian-Newtonian worldview that governs much of our thinking to this day, particularly in technologically dominated pro­fessions such as human factors engineering and accident investigation. Isaac Newton and Rene Descartes were two of the towering figures in the Scientific Revolution between 1500 and 1700 A.D. which produced a dramatic shift in worldview, as well as profound changes in knowledge and in ideas on how to acquire and test knowledge. Descartes proposed a sharp distinction between what he called res cogitans, the realm of mind and thought, and res extensa, the realm of matter. Although Descartes admitted to some interaction between the two, he insisted that mental and physical phenomena cannot be understood by reference to each other. Problems that occur in either realm require entirely separate approaches and differ­ent concepts to solve them. The notion of separate mental and material worlds became known as dualism and its implications can be recognized in much of what we think and do today. According to Descartes, the mind is outside of the physical order of matter and is in no way derived from it. The choice between human error and mechanical failure is such a dualist choice: According to Cartesian logic, human error cannot be derived from material things. As we will see, this logic does not hold up well—in fact, on closer in­spection, the entire field of human factors is based on its abrogation. Separating the body from the soul, and subordinating the body to the soul, not only kept Descartes out of trouble with the Church. His dualism, his division between mind and matter, addressed an important philosophi­cal problem that had the potential of holding up scientific, technological, and societal progress: What is the link between mind and matter, between the soul and the material world? How could we, as humans, take control of and remake our physical world as long as it was indivisibly allied to or even synonymous with an irreducible, eternal soul? A major aim during the 16th­and 17th-century Scientific Revolution was to see and understand (and be­come able to manipulate) the material world as a controllable, predictable, programmable machine. This required it to be seen as nothing but a ma­chine: No life, no spirit, no soul, no eternity, no immaterialism, no unpredictability. Descartes' res extensa, or material world, answered to just that concern. The res extensa was described as working like a machine, following mechanical rules and allowing explanations in terms of the arrangement and movement of its constituent parts. Scientific progress became easier be­cause of what it excluded. What the Scientific Revolution required, Des­cartes' disjunction provided. Nature became a perfect machine, governed by mathematical laws that were increasingly within the grasp of human un­derstanding and control, and away from things humans cannot control. Newton, of course, is the father of many of the laws that still govern our un­derstanding of the universe today. His third law of motion, for example, lies at the basis of our presumptions about cause and effect, and causes of acci­dents: For every action there is an equal and opposite reaction. In other words, for each cause there is an equal effect, or rather, for each effect there must be an equal cause. Such a law, though applicable to the release and transfer of energy in mechanical systems, is misguiding and disorienting when applied to sociotechnical failures, where the small banalities and subtleties of normal work done by normal people in normal organizations can slowly degenerate into enormous disasters, into disproportionately huge releases of energy. The cause-consequence equivalence dictated by Newton's third law of motion is quite inappropriate as a model for organizational accidents. Attaining control over a material world was critically important for peo­ple five hundred years ago. The inspiration and fertile ground for the ideas of Descartes and Newton can be understood against the background of their time. Europe was emerging from the Middle Ages—fearful and fateful times, where life spans were cut short by wars, disease, and epidemics. We should not underestimate anxiety and apprehension about humanity's abil­ity to make it at all against these apocryphal odds. After the Plague, the pop­ulation of Newton's native England, for example, took until 1650 to recover to the level of 1300. People were at the mercy of ill-understood and barely controllable forces. In the preceding millennium, piety, prayer, and peni­tence were among the chief mechanisms through which people could hope to attain some kind of sway over ailment and disaster. The growth of insight produced by the Scientific Revolution slowly be­gan to provide an alternative, with measurable empirical success. The Scientific Revolution provided new means for controlling the natural world. Telescopes and microscopes gave people new ways of studying components that had thus far been too small or too far away for the naked eye to see, cracking open a whole new view of the universe and, for the first time, re­vealing causes of phenomena hitherto ill-understood. Nature was not a monolithic, inescapable bully, and people were no longer just on the receiving, victimized end of its vagaries. By studying it in new ways, with new instruments, nature could be decomposed, broken into smaller bits, meas­ured, and, through all of that, better understood and eventually controlled. Advances in mathematics (geometry, algebra, calculus) generated models that could account for and begin to predict newly discovered phenomena in, for example, medicine and astronomy. By discovering some of the build­ing blocks of life and the universe, and by developing mathematical imi­tates of their functioning, the Scientific Revolution reintroduced a sense of predictability and control that had long been dormant during the Middle Ages. Humans could achieve dominance and preeminence over the vicissi­tudes and unpredict abilities of nature. The route to such progress would come from measuring, breaking down (known variously today as reducing, decomposing, or deconstructing) and mathematically modeling the world around us—to subsequently rebuild it on our terms. Measurability and control are themes that animated the Scientific Revo­lution, and they resonate strongly today. Even the notions of dualism (ma­terial and mental worlds are separate) and deconstruction (larger wholes can be explained by the arrangement and interaction of their constituent lower level parts) have long outlived their initiators. The influence of Des­cartes is judged so great in part because he wrote in his native tongue, rather than in Latin, thereby presumably widening access and popular ex­posure to his thoughts. The mechanization of nature spurred by his dualism, and Newton's and others' enormous mathematical advances, heralded centuries of unprecedented scientific progress, economic growth, and en­gineering success. As Fritjof Capra (1982) put it, national aeronautics and space administration would not have been able to put a man on the moon without Rene Descartes. The heritage, however, is definitely a mixed blessing. Human factors and systems safety is stuck with a language, with metaphors and images that em­phasize structure, components, mechanics, parts and interactions, cause and effect. While giving us initial direction for building safe systems and for find­ing out what went wrong when it turns out we have not, there are limits to the usefulness of this inherited vocabulary. Let us go back to that summer day of 1999 and the MD-80 runway overrun. In good Cartesian-Newtonian tradi­tion, we can begin by opening up the aircraft a bit more, picking apart the various components and procedures to see how they interact, second by sec­ond. Initially we will be met with resounding empirical success—as indeed Descartes and Newton frequently were. But when we want to recreate the whole on the basis of the parts we find, a more troubling reality swims into view: It does not go well together anymore. The neat, mathematically pleas­ing separation between human and mechanical cause, between social and structural issues, has blurred. The whole no longer seems a linear function of the sum of the parts. As Scott Snook (2000) explained it, the two classical Western scientific steps of analytic reduction (the whole into parts) and in­ductive synthesis (the parts back into a whole again) may seem to work, but simply putting the parts we found back together does not capture the rich complexity hiding inside and around the incident. What is needed is a holis­tic, organic integration. What is perhaps needed is a new form of analysis and synthesis, sensitive to the total situation of organized sociotechnical activity. But first let us examine the analytical, componential story. SPOILERS, PROCEDURES, AND HYDRAULIC SYSTEMS Spoilers are those flaps that come up into the airstream on the topside of the wings after an aircraft has touched down. Not only do they help brake the aircraft by obstructing the airflow, they also cause the wing to lose the ability to create lift, forcing the aircraft's weight onto the wheels. Extension of the ground spoilers also triggers the automatic braking system on the wheels: The more weight the wheels carry, the more effective their braking becomes. Before landing, pilots select the setting they wish on the auto­matic wheel-braking system (minimum, medium or maximum), depending on runway length and conditions. After landing, the automatic wheel-braking system will slow down the aircraft without the pilot having to do anything, and without letting the wheels skid or lose traction. As a third mechanism for slowing down, most jet aircraft have thrust reversers, which redirect the outflow of the jet engines into the oncoming air, instead of out toward the back. In this case, no spoilers continuing airworthines management exposition out, and no automatic wheel braking was triggered as a result. While rolling down the runway, the pilots checked the setting of the automatic braking system multiple times to ensure it was armed and even changed its setting to maximum as they saw the end of the runway coming up. But it would never engage. The only remaining mecha­nism for slowing down the aircraft were the thrust reversers. Thrust revers­ers, however, are most effective at high speeds. By the time the pilots no­ticed that they were not going to make it before the end of the runway, the speed was already quite low (they ended up going into the field at 10-20 knots) and thrust reversers no longer had much immediate effect. As the jet was going over the edge of the runway, the captain closed the reversers and steered somewhat to the right in order to avoid obstacles. How are spoilers armed? On the center pedestal, between the two pilots, are a number of levers. Some are for the engines and thrust reversers, one is for the flaps, and one is for the spoilers. In order to arm the ground spoilers, one of the pilots needs to pull the lever upward. The lever goes up by about one inch and sits there, armed until touchdown. When the system senses that the aircraft is on the ground (which it does in part through switches in the landing gear), the lever will come back automatically and the spoilers come out. Asaf Degani, who studied such procedural problems extensively, has called the spoiler issue not one of human error, but one of timing (e.g., Degani, Heymann, & Shafto, 1999). On this aircraft, as with many others, the spoilers should not be armed before the landing gear has been selected and is entirely in place. This has to do with the switches that can tell when the aircraft is on the ground. These are switches that compress as the aircraft's weight settles onto the wheels, but not only then. There is a risk in this type of aircraft that the switch in the nose gear will even compress as the landing gear is coming out of its bays. This can happen because the nose gear folds out into the oncoming airstream. As the nose gear is coming out and the aircraft is slicing through the air at 180 knots, the sheer wind force can compress the nose gear, activate the switch, and subsequently risk extending the ground spoilers (if they had been armed). This is not a good idea: The aircraft would have trouble flying with ground spoilers out. Hence the requirement: The landing gear needs to be all the way out, pointing down. Only when there is no more risk of aerody­namic switch compression can the spoilers be armed. This is the order of the before-landing procedures: Gear down and locked. Spoilers armed. Flaps FULL. On a typical approach, pilots select the landing gear handle down when the so-called glide slope comes alive: when the aircraft has come within range of the electronic signal that will guide it down to the runway. Once the landing gear is out, spoilers must be armed. Then, once the aircraft captures that glide slope (i.e., it is exactly on the electronic beam) and starts to descend down the approach to the runway, flaps need to be set to FULL (typically 40°). Flaps are other devices that extend from the wing, changing the wing size and shape. They allow aircraft to fly more slowly for a landing. This makes the procedures conditional on context. It now looks like this: Gear down and locked (when glide slope live). Spoilers armed (when gear down and locked). Flaps FULL (when glide slope captured). But how long does it take to go from "glide slope live" to "glide slope capture"? On a typical approach (given the airspeed) this takes about 15 sec­onds. On a simulator, where training takes place, this does not create a problem. The whole gear cycle (from gear lever down to the "gear down and locked" indication in the cockpit) takes about 10 seconds. That leaves 5 seconds for arming the spoilers, before the crew needs to select flaps FULL (the next item in the procedures). In the simulator, then, things look like this: At t = 0 Gear down and locked (when glide slope live). At t + 10 Spoilers armed (when gear down and locked). At t + 15 Flaps FULL (when glide slope captured). But in real aircraft, the hydraulic system (which, among other things, extends the landing gear) is not as effective as it is on a simulator. The simulator, of course, only has simulated hydraulic aircraft systems, modeled on how the aircraft is when it has flown zero hours, when it is sparkling new, straight out of the factory. On older aircraft, it can take up to half a minute for the gear to cycle out and lock into place. This makes the procedures look like this: At t = 0 Gear down and locked (when glide slope live). At t + 30 Spoilers armed (when gear down and locked). BUT! at t + 15 Flaps FULL (when glide slope captured). In effect, then, the "flaps" item in the procedures intrudes before the "spoilers" item. Once the "Flaps" item is completed and the aircraft is de­scending towards the runway, it is easy to go down the procedures from there, taking the following items. Spoilers never get armed. Their arming has tumbled through the cracks of a time warp. An exclusive claim to hu­man error (or cockpit resource management breakdown) becomes more difficult to sustain against this background. How much human error was there, actually? Let us remain dualist for now and revisit Equation 1. Now apply a more liberal defi­nition of mechanical failure. The nose gear of the actual aircraft, fitted with a compression switch, is designed so that it folds out into the wind while still airborne. This introduces a systematic mechanical vulnerability that is toler­ated solely through procedural timing (a known leaky mechanism against failure): first the gear, then the spoilers. In other words, "gear down and locked" is a mechanical prerequisite for spoiler arming, but the whole gear cycle can take longer than there is room in the procedures and the timing of events driving their application. The hydraulic system of the old jet does not pressurize as well: It can take up to 30 seconds for a landing gear to cycle out. The aircraft simulator, in contrast, does the same job inside of 10 seconds, leaving a subtle but substantive mechanical mismatch. One work sequence is introduced and rehearsed in training, whereas a delicately dif­ferent one is necessary for actual operations. Moreover, this aircraft has a system that warns if the spoilers are not armed on takeoff, but it does not have a system for warning that the spoilers are not armed on approach. Then there is the mechanical arrangement in the cockpit. The armed spoiler handle looks different from the unarmed one by only one inch and a small red square at the bottom. From the position of the right-seat pilot (who needs to confirm their arming), this red patch is obscured behind the power levers as these sit in the typical approach position. With so much me­chanical contribution going around (landing gear design, eroded hydrau­lic system, difference between simulator and real aircraft, cockpit lever ar­rangement, lack of spoiler warning system on approach, procedure timing) and a helping of scheduling stochastics (two captains on this flight), a whole lot more mechanical failure could be plugged into the equation to rebalance the human contribution. But that is still dualist. When reassembling the parts that we found among procedures, timing, mechanical erosion, design trade-offs, we can begin to wonder where mechanical contributions actually end, and where human contributions begin. The border is no longer so clear. The load im­posed by a wind of 180 knots on the nose wheel is transferred onto a flimsy procedure: first the gear, then the spoilers. The nose wheel, folding out into the wind and equipped with a compression switch, is incapable of car­rying that load and guaranteeing that spoilers will not extend, so a proce­dure gets to carry the load instead. The spoiler lever is placed in a way that makes verification difficult, and a warning system for unarmed spoilers is not installed. Again, the error is suspended, uneasily and unstably, between human intention and engineered hardware—it belongs to both and to nei­ther uniquely. And then there is this: The gradual wear of a hydraulic sys­tem is not something that was taken into account during the certification of the jet. An MD-80 with an anemic hydraulic system that takes more than half a minute to get the whole gear out, down, and locked, violating the original design requirement by a factor of three, is still considered airwor­thy. The worn hydraulic system cannot be considered a mechanical failure. It does not ground the jet. Neither do the hard-to-verify spoiler handle or lack of warning system on approach. The jet was once certified as airworthy with or without all of that. That there is no mechanical failure, in other words, is not because there are no mechanical issues. There is no mechani­cal failure because social systems, made up of manufacturers, regulators, and prospective operators—undoubtedly shaped by practical concerns and expressed through situated engineering judgment with uncertainty about future wear—decided that there would not be any (at least not related to the issues now identified in an MD-80 overrun). Where does mechanical failure end and human error begin? Dig just deeply enough and the ques­tion becomes impossible to answer. RES EXTENSA AND RES COGITANS, OLD AND NEW Separating res extensa away from res cogitans, like Descartes did, is artificial. It is not the result of natural processes or conditions, but rather an imposition of a worldview. This worldview, though initially accelerating scientific progress, is now beginning to seriously hamper our understanding. In modern accidents, machinistic and human causes are blurred. The disjunction be­tween material and mental worlds, and the requirement to describe them differently and separately, are debilitating our efforts to understand socio­technical success and failure. The distinction between the old and new views of human error, which was also made the earlier Field Guide to Human Error Investigations (Dekker, 2002) actually rides roughshod over these subtleties. Recall how the investi­gation into the runway overrun incident found "breakdowns in CRM" as a causal factor. This is old-view thinking. Somebody, in this case a pilot, or rather a crew of two pilots, forgot to arm the spoilers. This was a human er­ror, an omission. If they had not forgotten to arm the spoilers, the accident would not have happened, end of story. But such an analysis of failure does not probe below the immediately visible surface variables of a sequence of events. As Perrow (1984) puts it, it judges only where people should have zigged instead of zagged. The old view of human error is surprisingly common. In the old view, error—by any other name (e.g., complacency, omission, breakdown of CRM)—is accepted as a satisfactory explanation. This is what the new view of human error tries to avoid. It sees human error as a consequence, as a result of failings and problems deeper inside the systems in which people work. It resists seeing human error as the cause. Rather than judging people for not doing what they should have done, the new view presents tools for explaining why people did what they did. Human er­ror becomes a starting point, not a conclusion. In the spoiler case, the error is a result of design trade-offs, mechanical erosion, procedural vulnerabili­ties, and operational stochastics. Granted, the commitment of the new view is to resist neat, condensed versions in which a human choice or a failed mechanical part led the whole structure onto the road to perdition. The distinction between the old and new views is important and needed. Yet even in the new view the error is still an effect, and effects are the language of Newton. The new view implicitly acknowledges the existence, the reality of error. It sees error as something that is out there, in the world, and caused by something else, also out there in the world. As the next chapters show, such a (naively) realist position is perhaps untenable. Recall how the Newtonian-Cartesian universe consists of wholes that can be explained and controlled by breaking them down into constituent parts and their interconnections (e.g., humans and machines, blunt ends and sharp ends, safety cultures and blame cultures). Systems are made up of components, and of mechanical-like linkages between those components. This lies at the source of the choice between human and material cause (is it human error or mechanical failure?). It is Newtonian in that it seeks a cause for any observed effect, and Cartesian in its dualism. In fact, it expresses both Descartes' dualism (either mental or material: You cannot blend the two) and the notion of decomposition, where lower order properties and interactions completely determine all phenomena. They are enough; you need no more. Analyzing which building blocks go into the problem and how they add up is necessary and sufficient for understanding why the problem occurs. Equation 1 is a reflection of the assumed ex­planatory sufficiency of lower order properties. Throw in the individual contributions, and the answer to why the problem occurred rolls out. An aircraft runway overrun today can be understood by breaking the contributions down into human and machine causes, analyzing the properties and interactions of each and then reassembling it back into a whole. "Human error" turns up as the answer. If there are no material contributions, the human contribution is expected to carry the full explanatory load. As long as progress is made using this worldview, there is no reason to question it. In various corners of science, including human factors, many people still see no reason to do so. Indeed, there is no reason that struc­turalist models cannot be imposed on the messy interior of sociotechnical systems. That these systems, however, reveal machine-like properties (com­ponents and interconnections, layers and holes) when we open them up post mortem does not mean that they are machines, or that they, in life, grew and behaved like machines. As Leveson (2002) pointed out, analytic reduction assumes that the separation of a whole into constituent parts is feasible, that the subsystems operate independently, and that analysis results are not distorted by taking the whole apart. This in turn implies that the components are not subject to feedback loops and other nonlinear in­teractions and that they are essentially the same when examined singly as when they are playing their part in the whole. Moreover, it assumes that the principles governing the assembly of the components into the whole are straightforward; the interactions among components are simple enough that they can be considered separate from the behavior of the whole. Are these assumptions valid when we try to understand system accidents? The next chapters give us cause to think. Take for example the conundrum of drift into failure and the elusive nature of accidents that happen beyond a 10"7 safety level. Those accidents do not happen just because of compo­nent failures, yet our mechanistic models of organizational or human func­tioning can never capture the organic, relational processes that gradually nudge a sociotechnical system to the edge of breakdown. Looking at com­ponent failures, such as the "human errors" that many popular error cate­gorization methods look for, may be fraudulent in its illusion of what it tells us about safety and risk in complex systems. There is a growing consensus that our current efforts and models will be incapable of breaking the asymptote, the level-off, in our progress on safety beyond 10-7. Is the struc­turalist, mechanistic view of sociotechnical systems, where we see compo­nents and linkages and their failures, still appropriate for making real progress?