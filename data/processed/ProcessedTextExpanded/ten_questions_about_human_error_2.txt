Title: Ten Questions About Human Error A New View Of Human Factors And System Safety Chapter 2 Author(s): Sidney W. A. Dekker Category: Analysis, Human, Factors, Errors Tags: human, error, factors, accident Why Do Safe Systems Fail? Accidents actually do not happen very often. Most transportation systems in the developed world are safe or even ultra-safe. Their likelihood of a fatal accident is less than 10~7, which means a one-out-of-10,000,000 chance of death, serious loss of property or environmental or economic devastation (Amalberti, 2001). At the same time, this appears to be a magical frontier: No transportation system has figured out a way of becoming even safer. Progress on safety beyond 10~7 is elusive. As Rene Amalberti has pointed out, linear extensions of current safety efforts (incident reporting, safety and quality management, proficiency checking, standardization and proce­ duralization, more rules and regulations) seem of little use in breaking the asymptote, even if they are necessary to sustain the 10~7 safety level. More intriguingly still, the accidents that happen at this frontier appear to be of a type that is difficult to predict using the logic that governs safety thinking up to 10~7. It is here that the limitations of a structuralist vocabu­ lary become most apparent. Accident models that rely largely on failures, holes, violations, deficiencies, and flaws can have a difficult time accommo­ dating accidents that seem to emerge from (what looks to everybody like) normal people doing normal work in normal organizations. Yet the mystery is that in the hours, days, or even years leading up to an accident beyond 10~7, there may be few reportworthy failures or noteworthy organizational deficiencies. Regulators as well as insiders typically do not see people violat­ ing rules, nor do they discover other flaws that would give cause to shut down or seriously reconsider operations. If only it were that easy. And up to 10~7 it probably is. But when failures, serious failures, are no longer pre­ ceded by serious failures, predicting accidents becomes a lot more difficult. And modeling them with the help of mechanistic, structuralist notions may be of little help. The greatest residual risk in today's safe sociotechnical systems is the drift into failure. Drift into failure is about a slow, incremental movement of systems operations toward the edge of their safety envelope. Pressures of scarcity and competition typically fuel this drift. Uncertain technology and incomplete knowledge about where the boundaries actually are, re­ sult in people not stopping the drift or even seeing it. The 2000 Alaska Air­ lines 261 accident is highly instructive in this sense. The MD-80 crashed into the ocean off California after the trim system in its tail snapped. On the surface, the accident seems to fit a simple category that has come to dominate recent accident statistics: mechanical failures as a result of poor maintenance: A single component failed because people did not maintain it well. Indeed, there was a catastrophic failure of a single component. A mechanical failure, in other words. The break instantly rendered the air­ craft uncontrollable and sent it plummeting into the Pacific. But such ac­ cidents do not happen just because somebody suddenly errs or something suddenly breaks: There is supposed to be too much built-in protection against the effects of single failures. What if these protective structures themselves contribute to drift, in ways inadvertent, unforeseen, and hard to detect? What if the organized social complexity surrounding the tech­ nological operation, all the maintenance committees, working groups, regulatory interventions, approvals, and manufacturer inputs, that all in­ tended to protect the system from breakdown, actually helped to set its course to the edge of the envelope? Since Barry Turner's 1978 Man-Made Disasters, we know explicitly that ac­ cidents in complex, well-protected systems are incubated. The potential for an accident accumulates over time, but this accumulation, this steady slide into disaster, generally goes unrecognized by those on the inside and even those on the outside. So Alaska 261 is not just about a mechanical failure, even if that is what many people would like to see as the eventual outcome (and proximal cause of the accident). Alaska 261 is about uncertain tech­ nology, about gradual adaptations, about drift into failure. It is about the inseparable, mutual influences of mechanical and social worlds, and it puts the inadequacy of our current models in human factors and system safety on full display. JACKSCREWS AND MAINTENANCE JUDGMENTS In Alaska 261, the drift toward the accident that happened in 2000 had be­ gun decades earlier. It reaches back into the very first flights of the 1965 Douglas DC-9 that preceded the MD-80 type. Like (almost) all aircraft, this type has a horizontal stabilizer (or tailplane, a small wing) at the back that helps direct the lift created by wings. It is this little tailplane that keeps an aircraft's nose up: Without it, controlled flight is not possible (see Fig. 2.1). The tailplane itself can angle up or down in order to pitch the nose up or down (and consequently make the aircraft go up or down). In most aircraft, the trim system can be driven by the autopilot and by crew inputs. The tailplane is hinged at the back, whereas the front end arcs up or down (it also has control surfaces at the back that are connected to the crew's con­ trol column in the cockpit, but those are not the issue here). Pushing the front end of the horizontal stabilizer up or down is done through a rotating jackscrew and a nut. The whole assembly works a bit like a carjack used to lift a vehicle: For example when changing a tire. You swivel, and the jackscrew rotates, pulling the so-called acme nuts inward and pushing the car up (see Fig. 2.2). In the MD-80 trim system, the front part of the horizontal stabilizer is connected to a nut that drives up and down a vertical jackscrew. An electri­ cal trim motor rotates the jackscrew, which in turn drives the nut up or down. The nut then pushes the whole horizontal tail up or down. Adequate lubrication is critical for the functioning of a jackscrew and nut assembly. Without enough grease, the constant grinding will wear out the thread on either the nut or the screw (in this case the screw is deliberately made of harder material, wearing the nut out first). The thread actually carries the entire load that is imposed on the vertical tail during flight. This is a load of around 5000 pounds, similar to the weight of a whole family van hanging by the thread of a jackscrew and nut assembly. Were the thread to wear out on an MD-80, the nut would fail to catch the threads of the jackscrew. Aerody­ namic forces then push the horizontal tailplane (and the nut) to its stop way out of the normal range, rendering the aircraft uncontrollable in the pitch axis, which is essentially what happened to Alaska 261. Even the stop failed because of the pressure. A so-called torque tube runs through the jackscrew in order to provide redundancy (instead of having two jack­ screws, like in the preceding DC-8 model). But even the torque tube failed in Alaska 261. None of this is supposed to happen, of course. When it first launched the aircraft in the mid 1960s, Douglas recommended that operators lubricate the trim jackscrew assembly every 300 to 350 flight hours. For typical com­ mercial usage, that could mean grounding the airplane for such mainte­ nance every few weeks. Immediately, the sociotechnical, organizational sys­ tems surrounding the operation of the technology began to adapt, and set the system on its course to drift. Through a variety of changes and develop­ ments in maintenance guidance for the DC-9/MD-80 series aircraft, the lu­ brication interval was extended. As we see later, these extensions were hardly the product of manufacturer recommendations alone, if at all. A much more complex and constantly evolving web of committees with repre­ sentatives from regulators, manufacturers, subcontractors, and operators was at the heart of a fragmented, discontinuous development of mainte­ nance standards, documents, and specifications. Rationality for mainte- nance-interval decisions was produced relatively locally, relying on incom­ plete, emerging information about what was, for all its deceiving basicness, still uncertain technology. Although each decision was locally rational, making sense for decision makers in their time and place, the global pic­ ture became one of drift toward disaster, significant drift. Starting from a lubrication interval of 300 hours, the interval at the time of the Alaska 261 accident had moved up to 2,550 hours, almost an order of magnitude more. As is typical in the drift toward failure, this distance was not bridged in one leap. The slide was incremental: step by step, decision by decision. In 1985, jackscrew lubrication was to be accomplished every 700 hours, at every other so-called maintenance B check (which occurs every 350 flight hours). In 1987, the B-check interval itself was increased to 500 flight hours, pushing lubrication intervals to 1,000 hours. In 1988, B checks were eliminated altogether, and tasks to be accomplished were redistributed over A and C checks. The jackscrew assembly lubrication was to be done each eighth 125-hour A check: still every 1,000 flight hours. But in 1991, A- check intervals were extended to 150 flight hours, leaving a lubrication ev­ ery 1200 hours. Three years later, the A-check interval was extended again, this time to 200 hours. Lubrication would now happen every 1,600 flight hours. In 1996, the jackscrew-assembly lubrication task was removed from the A check and moved instead to a so-called task card that specified lubri­ cation every 8 months. There was no longer an accompanying flight-hour limit. For Alaska Airlines, 8 months translated to about 2,550 flight hours. The jackscrew recovered from the ocean floor, however, revealed no evi­ dence that there had been adequate lubrication at the previous interval at all. It might have been more than 5,000 hours since it had last received a coat of fresh grease (see Fig. 2.3). With as much lubrication as it originally recommended, Douglas thought it had no reason to worry about thread wear. So before 1967, the manufacturer provided or recommended no check of the wear of the jackscrew as­ sembly. The trim system was supposed to accumulate 30,000 flight hours be­ fore it would need replacement. But operational experience revealed a different picture. After only a year of DC-9 flying, Douglas received reports of thread wear significantly in excess of what had been predicted. In response, the manufacturer recommended that operators perform a so-called end-play check on the jackscrew assembly at every maintenance C check, or every 3,600 flight hours. The end-play check uses a restraining fixture that puts pressure on the jackscrew assembly, simulating the aerodynamic load during normal flight. The amount of play between nut and screw, gauged in thou­ sandths of an inch, can then be read off an instrument. The play is a direct measure of the amount of thread wear. From 1985 onward, end-play checks at Alaska became subject to the same kind of drift as the lubrication intervals. In 1985, end-play checks were scheduled every other C check, as the required C checks consistently continuing airworthines management exposition in around 2,500 hours, which was rather ahead of the recommended 3,600 flight hours, unnecessarily grounding aircraft. By scheduling an end-play test every other C check, though, the interval was extended to 5,000 hours. By 1988, C-check intervals themselves were extended to 13 months, with no accompanying flight-hour limit. End-play checks were now performed ev­ ery 26 months, or about every 6,400 flight hours. In 1996, C-check intervals were extended once again, this time to 15 months. This stretched the flight hours between end-play tests to about 9,550. The last end-play check of the accident airplane was conducted at the airline maintenance facility in Oak­ land, California in 1997. At that time, play between nut and screw was found to be exactly at the allowable limit of .040 inches. This introduced considerable uncertainty. With play at the allowable limit, what to do? Re­ lease the airplane and replace parts the next time, or replace the parts now? The rules were not clear. The so-called AOL 9-48A said that "jackscrew as­ semblies could remain in service as long as the end-play measurement re­ mained within the tolerances (between 0.003 and 0.040 inch)" (National Transportation Safety Board, or NTSB, 2002; p. 29). It was still 0.040 inches, so the aircraft could technically remain in service. Or could it? How quickly would the thread wear from there on? Six days, several shift changes and another, more favorable end-play check later, the airplane was released. No parts were replaced: They were not even in stock in Oakland. The airplane "departed 0300 local time. So far so good," the graveyard shift turnover plan noted (NTSB, 2002, p. 53). Three years later, the trim system snapped and the aircraft disappeared into the ocean not far away. Between 2,500 and 9,550 hours there had been more drift toward failure (see Fig. 2.4). Again, each extension made local sense, and was only an increment away from the previously established norm. No rules were violated, no laws bro­ ken. Even the regulator concurred with the changes in end-play check intervals. These were normal people doing normal work around seemingly normal, stable technology. The figures of drift into failure are easy to draw in hindsight. They are fascinating to look at, too. The realities they represent, however, were not similarly compelling to those on the inside of the system at the time. Why would these numbers, this numeric degeneration of double checking and servicing, be noteworthy? As an indication, MD-80 maintenance techni­ cians were never required to record or keep track of the end play on the trim systems they measured. Even the manufacturer had expressed no in­ terest in seeing these numbers or the slow, steady degeneration they may have revealed. If there was drift, in other words, no institutional or organi­ zational memory would know it. The pictures of drift reveal what. But they shed no light on why. Indeed, the greatest conundrum since Turner (1978) has to been to elucidate why the slide into disaster, so easy to see and depict in retrospect, is missed by those who inflict it on themselves. Judging, after the fact, that there was a failure of foresight is easy: All you need to do is plot the numbers and spot the slide into disaster. Standing amid the rubble, it is easy to marvel at how misguided or misinformed people must have been. But why is it that the conditions conducive to an accident were never acknowledged or acted on by those on the inside of the system—those whose job it was to not have such accidents happen? Foresight is not hindsight. There is a profound re­ vision of insight that turns on the present. It converts a once vague, unlikely future into an immediate, certain past. The future, said David Woods (2003), seems implausible before an accident ("No, that won't happen to us"). But after an accident, the past seems incredible ("How could we not have seen that this was going to happen to us!"). What is now seen as ex­ traordinary was once ordinary. The decisions, trade-offs, preferences and priorities that seem so out of the ordinary and immoral after an accident, were once normal and common sensical to those who contributed to its in­ cubation. BANALITY, CONFLICT, AND INCREMENTALISM Sociological research (e.g., Perrow, 1984; Snook, 2000; Vaughan, 1996; Weick, 1995), as well as prescient human factors work (Rasmussen & Sve­ dung, 2000) and research on system safety (Leveson, 2002), has begun to sketch the contours of answers to the why of drift. Though different in background, pedigree, and much substantive detail, these works converge on important commonalities about the drift into failure. The first is that ac­ cidents, and the drift that precedes them, are associated with normal peo­ ple doing normal work in normal organizations—not with miscreants en­ gaging in immoral deviance. We can call this the banality-of-accidents thesis. Second, most works have at their heart a conflictual model: Organizations that involve safety-critical work are essentially trying to reconcile irreconcil­ able goals (staying safe and staying in business). Third, drifting into failure is incremental. Accidents do not happen suddenly, nor are they preceded by monumentally bad decisions or bizarrely huge steps away from the rul­ ing norm. The banality-of-accidents thesis says that the potential for having an acci­ dent grows as a normal by-product of doing normal business under normal pressures of resource scarcity and competition. No system is immune to the pressures of scarcity and competition, well, almost none. The only transpor­ tation system that ever approximated working in a resource-unlimited uni­ verse was national aeronautics and space administration during the early Apollo years (a man had to be put on the moon, whatever the cost). There was plenty of money, and plenty of highly motivated talent. But even here technology was uncertain, faults and fail­ ures not uncommon, and budgetary constraints got imposed quickly and increasingly tightly. Human resources and talent started to drain away. In­ deed, even such noncommercial enterprises know resource scarcity: Gov­ ernment agencies such as national aeronautics and space administration or safety regulators may lack adequate fi­ nancing, personnel or capacity to do what they need to do. With respect to the Alaska 261 accident, for example, a new regulatory inspection program, called the Air Transporation Oversight System (ATOS), was put into use in 1998 (2 years prior). It drastically reduced the amount of time inspectors had for actual surveillance activities. A 1999 memo by a regulator field- office supervisor in Seattle offered some insight: We are not able to properly meet the workload demands. Alaska Airlines has expressed continued concern over our inability to serve it in a timely manner. Some program approvals have been delayed or accomplished in a rushed manner at the "eleventh hour," and we anticipate this problem will intensify with time. Also, many enforcement investigations . . . have been delayed as a result of resource shortages. [If the regulator] continues to operate with the existing limited number of airworthiness inspectors . . . diminished surveil­ lance is imminent and the risk of incidents or accidents at Alaska Airlines is heightened. (NTSB, 2002, p. 175) Adapting to resource pressure, approvals were delayed or rushed, surveil­ lance was reduced. Yet doing business under pressures of resource scarcity is normal: Scarcity and competition are part and parcel even of doing in­ spection work. Few regulators anywhere will ever claim that they have ade­ quate time and personnel resources to carry out their mandates. Yet the fact that resource pressure is normal does not mean that it has no conse­ quences. Of course the pressure finds ways out. Supervisors write memos, for example. Battles over resources are fought. Trade-offs are made. The pressure expresses itself in the common organizational, political wrangles over resources and primacy, in managerial preferences for certain activities and investments over others, and in almost all engineering and operational trade-offs between strength and cost, between efficiency and diligence. In fact, working successfully under pressures and resource constraints is a source of professional pride: Building something that is strong and light, for example, marks the expert in the aeronautical engineer. Procuring and nursing into existence a system that has both low development costs and low operational costs (these are typically each other's inverse) is the dream of most investors and many a manager. Being able to create a program that putatively allows better inspections with fewer inspectors may win a civil ser­ vant compliments and chances at promotion, while the negative side effects of the program are felt primarily in some far-away field office. Yet the major engine of drift hides somewhere in this conflict, in this ten­ sion between operating safely and operating at all, between building safely and building at all. This tension provides the energy behind the slow, steady disengagement of practice from earlier established norms or design con­ straints. This disengagement can eventually become drift into failure. As a system is taken into use, it learns, and as it learns, it adapts: Experience generates information that enables people to fine-tune their work: fine-tuning compensates for discovered problems and dangers, re­ moves redundancy, eliminates unnecessary expense, and expands capacities. Experience often enables people to operate a sociotechnical system for much lower cost or to obtain much greater output than the initial design assumed. (Starbuck & Milliken, 1988, p. 333) This fine-tuning drift toward operational safety margins is one testimony to the limits of the structuralist systems safety vocabulary in vogue today. We think of safety cultures as learning cultures: cultures that are oriented to­ ward learning from events and incidents. But learning cultures are neither unique (because every open system in a dynamic environment necessarily learns and adapts) nor necessarily positive: Starbuck and Milliken high­ lighted how an organization can learn to "safely" borrow from safety while achieving gains in other areas. Drift into failure could not happen without learning. Following this logic, systems that are bad at learning and bad at adapting may well be less likely to drift into failure. A critical ingredient of this learning is the apparent insensitivity to mounting evidence that, from the position of retrospective outsider, could have shown how bad the judgments and decisions actually are. This is how it looks from the position of retrospective outsider: The retrospective out­ sider sees a failure of foresight. From the inside, however, the abnormal is pretty normal, and making trade-offs in the direction of greater efficiency is nothing unusual. In making these trade-offs, however, there is a feedback imbalance. Information on whether a decision is costeffective or efficient can be relatively easy to get. An early arrival time is measurable and has im­ mediate, tangible benefits. How much is or was borrowed from safety in or­ designated engineering representative to achieve that goal, however, is much more difficult to quantify and compare. If it was followed by a safe landing, apparently it must have been a safe decision. Extending a lubrication interval similarly saves immediately measurable time and money, while borrowing from the future of an appar­ ently problem-free jackscrew assembly. Each consecutive empirical success (the early arrival time is still a safe landing; the jackscrew assembly is still op­ erational) seems to confirm that fine-tuning is working well: The system can operate equally safely, yet more efficiently. As Weick (1993) pointed out, however, safety in those cases may not at all be the result of the decisions that were or were not made, but rather an underlying stochastic variation that hinges on a host of other factors, many not easily within the control of those who engage in the fine-tuning process. Empirical success, in other words, is not proof of safety. Past success does not guarantee future safety. Borrowing more and more from safety may go well for a while, but you never know when you are going to hit. This moved Langewiesche (1998) to say that Murphy's law is wrong: Everything that can go wrong usually goes right, and then we draw the wrong conclusion. The nature of this dynamic, this fine-tuning, this adaptation, is incre­ mental. The organizational decisions that are seen as "bad decisions" after the accident (even though they seemed like perfectly good ideas at the time) are seldom big, risky, order-of-magnitude steps. Rather, there is a suc­ cession of increasingly bad decisions, a long and steady progression of small, incremental steps that unwittingly take an organization toward disaster. Each step away from the original norm that meets with empirical suc­ cess (and no obvious sacrifice of safety) is used as the next basis from which to depart just that little bit more again. It is this incrementalism that makes distinguishing the abnormal from the normal so difficult. If the difference between what "should be done" (or what was done successfully yesterday) and what is done successfully today is minute, then this slight departure from an earlier established norm is not worth remarking or reporting on. Incrementalism is about continued normalization: It allows normalization and rationalizes it. Drift Into Failure and Incident Reporting Can incident reporting not reveal a drift into failure? This would seem to be a natural role of incident reporting, but it is not so easy. The normalization that accompanies drift into failure (an end-play check every 9,550 hours is "normal," even approved by the regulator, no matter that the original inter­ val was 2,500 hours) severely challenges the ability of insiders to define inci­ dents. What is an incident? Before 1985, failing to perform an end-play check every 2,500 hours could be considered an incident, and supposing the organization had a means for reporting it, it may even have been con­ sidered as such. But by 1996, the same deviance was normal, regulated even. By 1996, the same failure was no longer an incident. And there was much more. Why report that lubricating the jackscrew assembly often has to be done at night, in the dark, outside the hanger, standing in the little basket of a lift truck at a soaring height above the ground, even in the rain? Why report that you, as a maintenance mechanic have to fumble your way through two tiny access panels that hardly allow room for one human hand—let alone space for eyes to see what is going on inside and what needs to be lubricated—if that is what you have to do all the time? In main­ tenance, this is normal work, it is the type of activity required to get the job done. The mechanic responsible for the last lubrication of the accident air­ plane told investigators that he had taken to wearing a battery-operated head lamp during night lubrication tasks, so that he had his hands free and could see at least something. These things are normal, they are not reportworthy. They do not qualify as incidents. Why report that the end­ play checks are performed with one restraining fixture (the only one in the entire airline, fabricated in-house, nowhere near the manufacturer's speci­ fications) , if that is what you use every time you do an end-play check? Why report that end-play checks, either on the airplane or on the bench, gener­ ate widely varying measures, if that is what they do all the time, and if that is what maintenance work is often about? It is normal, it is not an incident. Even if the airline had had a reporting culture, even if it had had a learning culture, even if it had had a just culture so that people would feel secure in sending in their reports without fear of retribution, these would not be inci­ dents that would turn up in the system. This is the banality of accidents the­ sis. These are not incidents. In 10~7 systems, incidents do not precede acci­ dents. Normal work does. In these systems: accidents are different in nature from those occurring in safe systems: in this case accidents usually occur in the absence of any serious breakdown or even of any serious error. They result from a combination of factors, none of which can alone cause an accident, or even a serious incident; therefore these com­ binations remain difficult to detect and to recover using traditional safety analysis logic. For the same reason, reporting becomes less relevant in pre­ dicting major disasters. (Amalberti, 2001, p. 112) Even if we were to direct greater analytic force onto our incident- reporting databases, this may still not yield any predictive value for acci­ dents beyond 1(H, simply because the data is not there. The databases do not contain, in any visible format, the ingredients of accidents that happen beyond 10~7. Learning from incidents to prevent accidents beyond 10~7 may well be impossible. Incidents are about independent failures and errors, noticed and noticeable by people on the inside. But these independent er­ rors and failures no longer make an appearance in the accidents that hap­ pen beyond 10~7. The failure to adequately see the part to be lubricated (that nonredundant, single-point, safety-critical part), the failure to ade­ quately and reliably perform an end-play check—none of this appears in in­ cident reports. But it is deemed "causal" or "contributory" in the accident report. The etiology of accidents in 10~7 systems, then, may well be funda­ mentally different from that of incidents, hidden instead in the residual risks of doing normal business under normal pressures of scarcity and com­ petition. This means that the so-called common-cause hypothesis (which holds that accidents and incidents have common causes and that incidents are qualitatively identical to accidents except for being just one step short) is probably wrong at 10~7 and beyond: . . . Reports from accidents such as Bhopal, Flixborough, Zeebrugge and Chernobyl demonstrate that they have not been caused by a coincidence of independent failures and human errors. They were the effect of a systematic migration of organizational behavior toward accident under the influence of pressure toward cost-effectiveness in an aggressive, competitive environment. (Rasmussen & Svedung, 2000, p. 14) Despite this insight, independent errors and failures are still the major re­ turn of any accident investigation today. The 2002 national transportation safety board report, following Newtonian-Cartesian logic, spoke of deficiencies in Alaska Airlines' maintenance program, of shortcomings in regulatory oversight, of responsibili­ ties not fulfilled, of flaws and failures and breakdowns. Of course, in hind­ sight they may well be just that. And finding faults and failures is fine because it gives the system something to fix. But why did nobody at the time see these oh-so apparent faults and failures for what they (in hind­ sight) were? This is where the structuralist vocabulary of traditional hu­ man factors and systems safety is most limited, and limiting. The holes found in the layers of defense (the regulator, the manufacturer, the oper­ ator, the maintenance facility and, finally, the technician) are easy to dis­ cover once the rubble is strewn before one's feet. Indeed, one common critique of structuralist models is that they are good at identifying defi­ ciencies, or latent failures, post-mortem. Yet these deficiencies and fail­ ures are not seen as such, nor easy to see as such, by those on the inside (or even those relatively on the outside, like the regulator!) before the acci­ dent happens. Indeed, structuralist models can capture the deficiencies that result from drift very well: They accurately identify latent failures or resident pathogens in organizations and can locate holes in layers of de­ fense. But the build-up of latent failures, if that is what you want to call them, is not modeled. The process of erosion, of attrition of safety norms, of drift toward margins, cannot be captured well by structuralist ap­ proaches, for those are inherently metaphors for resulting forms, not models oriented at processes of formation. Structuralist models are static. Although the structuralist models of the 1990s are often called "system models" or "systemic models," they are a far cry from what is actually consid­ ered systems thinking (e.g., Capra, 1982). The systems part of structuralist models has so far largely been limited to identifying, and providing a vocab­ ulary for, the upstream structures (blunt ends) behind the production of errors at the sharp end. The systems part of these models is a reminder that there is context, that we cannot understand errors without going into the organizational background from which they hail. All of this is necessary, of course, as errors are still all too often taken as the legitimate conclusion of an investigation (just look at the spoiler case with "breakdown in CRM" as cause). But reminding people of context is no substitute for beginning to explain the dynamics, the subtle, incremental processes that lead to, and normalize, the behavior eventually observed. This requires a different per­ spective for looking at the messy interior of organizations, and a different language to cast the observations in. It requires human factors and system safety to look for ways that move toward real systems thinking, where acci­ dents are seen as an emergent feature of organic, ecological, transactive processes, rather than just the end-point of a trajectory through holes in lay­ ers of defense. Structuralist approaches, and fixing the things they point us to, may not help much in making further progress on safety. We should be extremely sensitive to the limitations of known remedies. While good management and organizational design may reduce accidents in certain systems, they can never prevent them . . . The causal mechanisms in this case suggest that technical system failures may be more difficult to avoid than even the most pessimistic among us would have believed. The ef­ fect of unacknowledged and invisible social forces on information, interpre­ tation, knowledge, and—ultimately—action, are very difficult to identify and to control. (Vaughan, 1996, p. 416) Yet the retrospective explanatory power of structuralist models makes them the instruments of choice for those in charge of managing safety. In­ deed, the idea of a banality of accidents has not always easily found traction outside academic circles. For one thing, it is scary. It makes the potential for failure commonplace, or relentlessly inevitable (Vaughan, 1996). This can make accident models practically useless and managerially demoralizing. If the potential for failure is everywhere, in everything we do, then why try to avoid it? If an accident has no causes in the traditional sense, then why try to fix anything? Such questions are indeed nihilist, fatalist. It is not surprising, then, that resistance against the possible world lurking behind their an­ swers takes many forms. Pragmatic concerns are directed toward control, toward hunting down the broken parts, the bad guys, the violators, the in­ competent mechanics. Why did this one technician not perform the last lu­ brication of the accident airplane jackscrew as he should have? Pragmatic concerns are about finding the flaws, identifying the weak areas and trouble spots, and fixing them before they cause real problems. But those prag­ matic concerns find neither a sympathetic ear nor a constructive lexicon in the misers about drift into failure, for drift into failure is hard to spot, cer­ tainly from the inside. TOWARD SYSTEMS THINKING If we want to understand failures beyond 10~7, we have to stop looking for failures. It are no longer failures that go into creating these failures—it is normal work. Thus the banality of accidents makes their study philosophi­ cally philistine. It shifts the object of examination away from the darker sides of humanity and unethical corporate malgovernance, and toward pe­ destrian, everyday decisions of normal, everyday people under the influ­ ence of normal, everyday pressures. The study of accidents is rendered dra­ matic or fascinating only because of the potential outcome, not because of the processes that incubate it (which in itself can be fascinating, of course). Having studied the Challenger Space Shuttle disaster extensively, Diane Vaughan (1996) was forced to conclude that this type of accident is not caused by a series of component failures, even if component failures are the result. Instead, together with other sociologists, she pointed to an indige­ nousness of mistake, to mistakes and breakdown as systematic, normal by- products of an organization's work processes: Mistake, mishap, and disaster are socially organized and systematically pro­ duced by social structures. No extraordinary actions by individuals explain what happened: no intentional managerial wrongdoing, no rule violations, no conspiracy. These are mistakes embedded in the banality of organizational life and facilitated by environments of scarcity and competition, uncertain technology, incrementalism, patterns of information, routinization and or­ ganizational structures, (p. xiv) If we want to understand, and become able to prevent, failure beyond 10~7, this is where we need to look. Forget wrongdoing. Forget rule viola­ tions. Forget errors. Safety, and the lack of it, is an emergent property. What we need to study instead is patterns of information, the uncertainties in operating complex technology and the ever-evolving and imperfect sociotechnical systems surrounding it to make that operation happen, the influence of scarcity and competition on those systems, and how they set in motion an incrementalism (itself an expression of organizational learning or adaptation under those pressures). To understand safety, an organiza­ tion needs to capture the dynamics in the banality of its organizational life and begin to see how the emergent collective moves toward the boundaries of safe performance. Systems as Dynamic Relationships Capturing and describing the processes by which organizations drift into failure requires systems thinking. Systems thinking is about relationships and integration. It sees a sociotechnical system not as a structure consisting of constituent departments, blunt ends and sharp ends, deficiencies and flaws, but as a complex web of dynamic, evolving relationships and transac­ tions. Instead of building blocks, the systems approach emphasizes princi­ ples of organization. Understanding the whole is quite different from un­ derstanding an assembly of separate components. Instead of mechanical linkages between components (with a cause and an effect), it sees transac- tions—simultaneous and mutually interdependent interactions. Such emergent properties are destroyed when the system is dissected and studied as a bunch of isolated components (a manager, department, regulator, manufacturer, operator). Emergent properties do not exist at lower levels; they cannot even be described meaningfully with languages appropriate for those lower levels. Take the lengthy, multiple processes by which maintenance guidance was produced for the DC-9 and later the MD-80 series aircraft. Separate components (such as regulator, manufacturer, operator) are difficult to distinguish, and the interesting behavior, the kind of behavior that helps drive drift into failure, emerges only as a result of complex relationships and transactions. At first thought, the creation of maintenance guidance would seem a solved problem. You build a product, you get the regulator to certify it as safe to use, and then you tell the user how to maintain it in order to keep it safe. Even the second step (getting it certified as safe) is nowhere near a solved problem, and is deeply intertwined with the third. More about that later: First the maintenance guidance. Alaska 261 reveals a large gap between the production of a system and its operation. Inklings of the gap appeared in observations of jackscrew wear that was higher than what the manufacturer expected. Not long after the certification of the DC-9, people began work to try to bridge the gap. Assembling people from across the in­ dustry, a Maintenance Guidance Steering Group (MSG) was set up to de­ velop guidance documentation for maintaining large transport aircraft (NTSB, 2002), particularly the Boeing 747. Using this experience, another MSG developed a new guidance document in 1970, called MSG-2 (NTSB, 2002), which was intended to present a means for developing a mainte­ nance program acceptable to the regulator, the operator, and the manufac­ turer. The many discussions, negotiations, and interorganizational collabo­ rations underlying the development of an "acceptable maintenance program" showed that how to maintain a once certified piece of complex technology was not at all a solved problem. In fact, it was very much an emerging property: Technology proved less certain than it had seemed on the drawing board (e.g., the DC-9 jackscrew-wear rates were higher than predicted), and it was not before it hit the field of practice that deficiencies became apparent, if one knew where to look. In 1980, through combined efforts of the regulator, trade and industry groups and manufacturers of both aircraft and engines in the United States as well as Europe, a third guidance document was produced, called MSG-3 (NTSB, 2002). This document had to deconfound earlier confusions, for example, between "hard-time" maintenance, "on-condition" maintenance, "condition-monitoring" maintenance, and "overhaul" maintenance. Revi­ sions to MSG-3 were issued in 1988 and 1993. The MSG guidance docu­ ments and their revisions were accepted by the regulators, and used by so- called Maintenance Review Boards (MRB) that convene to develop guid­ ance for specific aircraft models. A Maintenance Review Board, or MRB, does not write guidance itself, however; this is done by industry steering committees, often headed by a regulator. These committees in turn direct various working groups. Through all of this, so-called on-aircraft maintenance planning (OAMP) documents get produced, as well as generic task cards that outline specific maintenance jobs. Both the lubrication interval and the end-play check for MD-80 trim jackscrews were the constantly changing products of these evolving webs of relationships between manufacturers, regulators, trade groups, and operators, who were operating off of continuously renewed op­ erational experience, and a perpetually incomplete knowledge base about the still uncertain technology (remember, end-play test results were not re­ corded or tracked). So what are the rules? What should the standards be? The introduction of a new piece of technology is followed by negotiation, by discovery, by the creation of new relationships and rationalities. "Techni­ cal systems turn into models for themselves," said Weingart (1991), "the ob­ servation of their functioning, and especially their malfunctioning, on a real scale is required as a basis for further technical development" (p. 8). Rules and standards do not exist as unequivocal, aboriginal markers against a tide of incoming operational data (and if they do, they are quickly proven useless or out of date). Rather, rules and standards are the constantly up­ dated products of the processes of conciliation, of give and take, of the de­ tection and rationalization of new data. As Brian Wynne (1988) said: Beneath a public image of rule-following behavior and the associated belief that accidents are due to deviation from those clear rules, experts are operat­ ing with far greater levels of ambiguity, needing to make expert judgments in less than clearly structured situations. The key point is that their judgments are not normally of a kind—how do we design, operate and maintain the sys­ tem according to "the" rules? Practices do not follow rules, rather, rules fol­ low evolving practices, (p. 153) Setting up the various teams, working groups, and committees was a way of bridging the gap between building and maintaining a system, between pro­ ducing it and operating it. Bridging the gap is about adaptation—adaptation to newly emerging data (e.g., surprising wear rates) about an uncertain tech­ nology. But adaptation can mean drift. And drift can mean breakdown. MODELING LIVE SOCIOTECHNICAL SYSTEMS What kind of safety model could capture such adaptation, and predict its eventual collapse? Structuralist models are limited. Of course, we could claim that the lengthy lubrication interval and the unreliable end-play check were structural deficiencies. Were they holes in layers of defense? Ab­ solutely. But such metaphors do not help us look for where the hole oc­ curred, or why. There is something complexly organic about MSGs, some­ thing ecological, that is lost when we model them as a layer of defense with a hole in it; when we see them as a mere deficiency or a latent failure. When we see systems instead as internally plastic, as flexible, as organic, their func­ tioning is controlled by dynamic relations and ecological adaptation, rather than by rigid mechanical structures. They also exhibit self-organization (from year to year, the makeup of MSGs was different) in response to envi­ ronmental changes, and self-transcendence: the ability to reach out beyond currently known boundaries and learn, develop and perhaps improve. What is needed is not yet another structural account of the end result of or­ ganizational deficiency. What is needed instead is a more functional ac­ count of living processes that coevolve with respect to a set of environmen­ tal conditions, and that maintain a dynamic and reciprocal relation with those conditions (see Heft, 2001). Such accounts need to capture what hap­ pens within an organization, with the gathering of knowledge and creation of rationality within workgroups, once a technology gets fielded. A func­ tional account could cover the organic organization of maintenance steer­ ing groups and committees, whose makeup, focus, problem definition, and understanding coevolved with emerging anomalies and growing knowl­ edge about an uncertain technology. A model that is sensitive to the creation of deficiencies, not just to their eventual presence, makes a sociotechnical system come alive. It must be a model of processes, not just one of structure. Extending a lineage of cyber­ netic and systems-engineering research, Nancy Leveson (2002) proposed that control models can fulfill part of this task. Control models use the ideas of hierarchies and constraints to represent the emergent interactions of a complex system. In their conceptualization, a sociotechnical system consists of different levels, where each superordinate level imposes constraints on (or controls what is going on in) subordinate levels. Control models are one way to begin to map the dynamic relationships between different levels within a system—a critical ingredient of moving toward true systems think­ ing (where dynamic relationships and transactions are dominant, not struc­ ture and components). Emergent behavior is associated with the limits or constraints on the degrees of freedom of a particular level. The division into hierarchical levels is an analytic artifact necessary to see how system behavior can emerge from those interactions and relationships. The resulting levels in a control model are of course a product of the ana­ lyst who maps the model onto the sociotechnical system. Rather than reflec­ tions of some reality out there, the patterns are constructions of a human mind looking for answers to particular questions. For example, a particular MSG would probably not see how it is superordinate to some level and im­ posing constraints on it, or subordinate to some other and thus subject to its constraints. In fact, a one-dimensional hierarchical representation (with only up and down along one direction) probably oversimplifies the dy­ namic web of relationships surrounding (and determining the functioning of) any such multiparty, evolving group as an MSG. But all models are sim­ plifications, and the levels analogy can be helpful for an analyst who has particular questions in mind (e.g., why did these people at this level or in this group make the decisions they did, and why did they see that as the only rational way to go?). Control among levels in a sociotechnical system is hardly ever perfect. In order to control effectively, any controller needs a good model of what it is supposed to control, and it requires feedback about the effectiveness of its control. But such internal models of the controllers easily become inconsis­ tent with, and no longer match, the system to be controlled (Leveson, 2002). Buggy control models are true especially with uncertain, emerging technology (including trim jackscrews) and the maintenance requirements surrounding them. Feedback about the effectiveness of control is incom­ plete and can be unreliable too. A lack of jackscrew-related incidents may provide the illusion that maintenance control is effective and that intervals can be extended, whereas the paucity of risk actually depends on factors quite outside the controller's scope. In this sense, the imposition of con­ straints on the degrees of freedom is mutual between levels and not just top down: If subordinate levels generate imperfect feedback about their func­ tioning, then higher order levels do not have adequate resources (degrees of freedom) to act as would be necessary. Thus the subordinate level im­ poses constraints on the superordinate level by not telling (or not being able to tell) what is really going on. Such a dynamic has been noted in vari­ ous cases of drift into failure, including the Challenger Space Shuttle disas­ ter (see Feynman, 1988). Drift Into Failure as Erosion of Constraints and Eventual Loss of Control Nested control loops can make a model of a sociotechnical system come alive more easily than a line of layers of defense. In order to model drift, it has to come alive. Control theory sees drift into failure as a gradual erosion of the quality or the enforcement of safety constraints on the behavior of subordinate levels. Drift results from either missing or inadequate con­ straints on what goes on at other levels. Modeling an accident as a sequence of events, in contrast, is really only modeling the end product of such ero­ sion and loss of control. If safety is seen as a control problem, then events (just like the holes in layers of defense) are the results of control problems, not the causes that drive a system into disaster. A sequence of events, in other words, is at best the starting point of modeling an accident, not the analytic conclusion. The processes that generate these weaknesses are in need of a model. One type of erosion of control occurs because original engineering con­ straints (e.g., 300-hour intervals) are loosened in response to the accumula­ tion of operational experience. A variety of Starbuck and Milliken's (1988) "fine-tuning," in other words. This does not mean that the kind of ecologi­ cal adaptation in system control is fully rational, or that it makes sense even from a global perspective on the overall evolution and eventual survival of the system. It does not. Adaptations occur, adjustments get made, and con­ straints get loosened in response to local concerns with limited time- horizons. They are all based on uncertain, incomplete knowledge. Often it is not even clear to insiders that constraints have become less tight as a re­ sult of their decisions in the first place, or that it matters if it is. And even when it is clear, the consequences may be hard to foresee, and judged to be a small potential loss in relation to the immediate gains. As Leveson (2002) put it, experts do their best to meet local conditions, and in the busy daily flow and complexity of activities they may be unaware of any potentially dangerous side effects of those decisions. It is only with the benefit of hind­ sight or omniscient oversight (which is Utopian) that these side effects can be linked to actual risk. Jensen (1996) describes it as such: We should not expect the experts to intervene, nor should we believe that they always know what they are doing. Often they have no idea, having been blinded to the situation in which they are involved. These days, it is not un­ usual for engineers and scientists working within systems to be so specialized that they have long given up trying to understand the system as a whole, with all its technical, political, financial, and social aspects, (p. 368) Being a member of a system, then, can make systems thinking all but im­ possible. Perrow (1984) made this argument very persuasively, and not just for the system's insiders. An increase in system complexity diminishes the system's transparency: Diverse elements interact in a greater variety of ways that are difficult to foresee, detect, or even comprehend. Influences from outside the technical knowledge base (those "political, financial, and social aspects" of Jensen, 1996, p. 368) exert a subtle but powerful pressure on the decisions and trade-offs that people make, and constrain what is seen as a rational decision or course of action at the time (Vaughan, 1996). Thus, even though experts may be well-educated and motivated, a "warning of an incomprehensible and unimaginable event cannot be seen, because it can­ not be believed" (Perrow, 1984, p. 23). How can experts and other decision makers inside organizational systems make sense of the available indicators of system safety performance? Making sure that experts and other decision makers are well informed is in itself an empty pursuit. What well informed really means in a complex organizational setting is infinitely negotiable, and clear criteria for what constitutes enough information are impossible to obtain. As a result, the effect of beliefs and premises on decision making and the creation of rationality can be considerable. Weick (1995, p. 87) pointed out that "seeing what one believes and not seeing that for which one has no beliefs are central to sensemaking. Warnings of the unbeliev­ able go unheeded." That which cannot be believed will not be seen. This confirms the earlier pessimism about the value of incident reporting be­ yond 10~7. Even if relevant events and warnings end up in the reporting sys­ tem (which is doubtful because they are not seen as warnings even by those who would do the reporting), it is even more generous to presume that fur­ ther expert analysis of such incident databases could succeed in coaxing the warnings into view. The difference, then, between expert insight at the time and hindsight (after an accident) is tremendous. With hindsight, the internal workings of the system may become lucid: The interactions and side effects are ren­ dered visible. And with hindsight, people know what to look for, where to dig around for the rot, the missing connections. Triggered by the Alaska 261 accident, the regulator launched a special probe into the maintenance- control system at Alaska Airlines. It found that procedures in place at the company were not followed, that controls in place were clearly not effec­ tive, that authority and responsibility were not well defined, that control of the maintenance-deferral systems was missing, and that quality-control and quality-assurance programs and departments were ineffective. It also found incomplete C-check paperwork, discrepancies of shelf-life expiration dates of parts, a lack of engineering approval of maintenance work-card modifi­ cations and inadequate tool calibrations. Maintenance manuals did not specify procedures or objectives for on-thejob training of mechanics, and key management positions (e.g., safety) were not filled or did not exist. In­ deed, constraints imposed on other organizational levels were nonexistent, dysfunctional, or eroded. But seeing holes and deficiencies in hindsight is not an explanation of the generation or continued existence of those deficiencies. It does not help predict or prevent failure. Instead, the processes by which such deci­ sions come about, and by which decision makers create their local rational­ ity, are one key to understanding how safety can erode on the inside of a complex, sociotechnical system. Why did these things make sense to organi­ zational decision makers at the time? Why was it all normal, why was it not reportworthy, not even for the regulator tasked with overseeing these proc­ esses? The questions hang in the air. Little evidence is available from the (already huge) national transportation safety board investigation on such interorganizational processes or how they produced a particular conceptualization of risk. The report, like others, is testimony to the structuralist, mechanistic tradition in acci­ dent probes to date, applied even to investigative forays into social-organi- zational territory. The Creation of Local Rationality The question is, how do insiders make those numerous little and larger trade-offs that together contribute to erosion, to drift? How is it that these seemingly harmless decisions can incrementally move a system to the edge of disaster? As indicated earlier, a critical aspect of this dynamic is that peo­ ple in decision-making roles on the inside of a sociotechnical system miss or underestimate the global side effects of their locally rational decisions. As an example, the MSG-3 MD-80 MRB (if you just lost it there, do not worry, other people must have too) considered the 3,600 hour jackscrew lubrica­ tion task change as part of the larger C-check package (NTSB, 2002). The review board did not consult the manufacturer's design engineers, nor did it make them aware of the extension. The manufacturer's initial OAMP document for the DC-9 and MD-80 lubrication, specifying an already ex­ tended 600- to 900-hour interval (departing from the 1964 recommenda­ tion for 300 hours), was also not considered in MSG-3. From a local per­ spective, with the pressure of time limits and constraints on available knowledge, the decision to extend the interval without adequate expert in­ put must have made sense. People consulted at the time must have been deemed adequate and sufficiently expert in order to feel comfortable enough to continue. The creation of rationality must have been seen as sat­ isfactory. Otherwise, it is hard to believe that MSG-3 would have proceeded as it did. But the eventual side effects of these smaller decisions were not foreseen. From the larger perspective, the gap between production and op­ eration, between making and maintaining a product, was once again al­ lowed to widen. A relationship that had been instrumental in helping bridge that gap (consulting with the original design engineers who make the aircraft, to inform those who maintain it), a relationship from history to (then) present, was severed. A transaction was not completed. If not foreseeing side effects made sense for MSG-3 MD-80 MRB (and this may well have been a banal result of the sheer complexity and paper load of the work mandated), it may make sense for participants in a next sociotechnical system too. These decisions are sound when set against local judgment criteria; given the time and budget pressures and short-term in­ centives that shape behavior. Given the knowledge, goals, and attentional focus of the decision makers and the nature of the data available to them at the time, it made sense. It is in these normal, day-to-day processes, where we can find the seeds of organizational failure and success. And it is these proc­ esses we must turn to in order to find leverage for making further progress on safety. As Rasmussen and Svedung (2000) put it: To plan for a proactive risk management strategy, we have to understand the mechanisms generating the actual behavior of decision-makers at all levels . . . an approach to proactive risk management involves the following analyses:• A study of normal activities of the actors who are preparing the land­ scape of accidents during their normal work, together with an analysis of the work features that shape their decision making behavior • A study of the present information environment of these actors and the information flow structure, analyzed from a control theoretic point of view. (p. 14) Reconstructing or studying the "information environment" in which ac­ tual decisions are shaped, in which local rationality is constructed, can help us penetrate processes of organizational sensemaking. These processes lie at the root of organizational learning and adaptation, and thereby at the source of drift into failure. The two Space Shuttle accidents (Challenger in 1996 and Columbia in 2002) are highly instructive here, if anything be­ cause the Columbia Accident Investigation Board (CAIB), as well as later analyses of the Challenger disaster (e.g., Vaughan, 1996) represent signifi­ cant (and, to date, rather unique) departures from the typical structuralist probes into such accidents. These analyses take normal organizational processes toward drift seriously, applying and even extending a language that helps us capture something essential about the continuous creation of local rationality by organizational decision makers. One critical feature of the information environment in which national aeronautics and space administration en­ gineers made decisions about safety and risk was "bullets." Richard Feyn­ man, who participated in the original Rogers Presidential Commission in­ vestigating the Challenger disaster, already fulminated against them and the way they collapsed engineeringjudgments into crack statements: "Then we learned about 'bullets'—little black circles in front of phrases that were supposed to summarize things. There was one after another of these little goddamn bullets in our briefing books and on the slides" (Feynman, 1988, p. 127). Eerily, "bullets" appear again as an outcropping in the 2003 Columbia accident investigation. With the proliferation of commercial software for making "bulletized" presentations since Challenger, bullets proliferated as well. This too may have been the result of locally rational (though largely unreflective) trade-offs to increase efficiency: Bulletized presentations col­ lapse data and conclusions and are dealt with more quickly than technical papers. But bullets filled up the information environment of national aeronautics and space administration engi­ neers and managers at the cost of other data and representations. They dominated technical discourse and, to an extent, dictated decision making, determining what would be considered as sufficient information for the is­ sue at hand. Bulletized presentations were central in creating local rational­ ity, and central in nudging that rationality ever further away from the actual risk brewing just below. Edward Tufte (CAIB, 2003) analyzed one Columbia slide in particular, from a presentation given to national aeronautics and space administration by a contractor in February 2003. The aim of the slide was to help national aeronautics and space administration consider the potential damage to heat tiles created by ice debris that had fallen from the main fuel tank. (Dam­ aged heat tiles triggered the destruction of Columbia on the way back into the earth's atmosphere, see Fig. 2.5.) The slide was used by the Debris As­ sessment Team in their presentation to the Mission Evaluation Room. It was entitled "Review of Test Data Indicates Conservatism for Tile Penetra­ tion," suggesting, in other words, that the damage done to the wing was not so bad (CAIB, 2003, p. 191). But actually, the title did not refer to predicted tile damage at all. Rather, it pointed to the choice of test models used to predict the damage. A more appropriate title, according to Tufte, would have been "Review of test data indicates irrelevance of two models." The reason was that the piece of ice debris that struck the Columbia was esti­ mated to be 640 times larger than the data used to calibrate the model on which engineers based their damage assessments (later analysis showed that the debris object was actually 400 times larger). So the calibration mod­ els were not of much use: They hugely underestimating the actual impact of the debris. The slide went on to say that "significant energy" would be re­ quired to have debris from the main tank penetrate the (supposedly harder) tile coating of the Shuttle wing, yet that test results showed that this was possible at sufficient mass and velocity, and that, once the tiles were penetrated, significant damage would be caused. As Tufte observed, the vaguely quantitative word "significant" or "significantly" was used five times on the one slide, but its meaning ranged all the way from the ability to see it using those irrelevant calibration tests, through a difference of 640-fold, to damage so great that everybody onboard would die. The same word, the same token on a slide, repeated five times, carried five profoundly (yes, sig­ nificantly) different meanings, yet none of those were really made explicit because of the condensed format of the slide. Similarly, damage to the pro­ tective heat tiles was obscured behind one little word, it, in a sentence that read "Test results show that it is possible at sufficient mass and velocity" (CAIB, 2003, p. 191). The slide weakened important material, and the life- threatening nature of the data on it was lost behind bullets and abbreviated statements. A decade and a half before, Feynman (1988) had discovered a similarly ambiguous slide about Challenger. In his case, the bullets had declared that the eroding seal in the field joints was "most critical" for flight safety, yet that "analysis of existing data indicates that it is safe to continue flying the existing design" (p. 137). The accident proved that it was not. Solid Rocket Boosters (or SRBs or SRMs) that help the Space Shuttle out of the earth's atmosphere are segmented, which makes ground transportation easier and has some other advantages. A problem that was discovered early in the Shuttle's operation, however, was that the solid rockets did not always prop­ erly seal at these segments, and that hot gases could leak through the rub­ ber O-rings in the seal, called blow-by. This eventually led to the explosion of Challenger in 1986. The pre-accident slide picked out by Feynman had de­ clared that while the lack of a secondary seal in a joint (of the solid rocket motor) was "most critical," it was still "safe to continue flying." At the same time, efforts needed to be "accelerated" to eliminate safety risk management seal erosion (1988, p. 137). During Columbia as well as Challenger, slides were not just used to support technical and operational decisions that led up to the acci­ dents. Even during both post-accident investigations, slides with bulletized presentations were offered as substitutes for technical analysis and data, causing the CAIB (2003), similar to Feynman years before, to grumble that: "The Board views the endemic use of PowerPoint briefing slides instead of technical papers as an illustration of the problematic methods of technical communication at NASA" (p. 191). The overuse of bullets and slides illustrates the problem of information environments and how studying them can help us understand something about the creation of local rationality in organizational decision making. NASA's bulletization shows how organizational decision makers are config­ ured in an "epistemic niche" (Hoven, 2001). That which decision makers can know is generated by other people, and gets distorted during transmis­ sion through a reductionist, abbreviationist medium. (This epistemic niche also has implications for how we can think about culpability, or blamewor­ thiness of decisions and decision makers—see chap. 10.) The narrowness and incompleteness of the niche in which decision makers find themselves can come across as disquieting to retrospective observers, including people inside and outside the organization. It was after the Columbia accident that the Mission Management Team "admitted that the analysis used to con­ tinue flying was, in a word, 'lousy.' This admission—that the rationale to fly was rubber-stamped—is, to say the least, unsettling" (CAIB, 2003, p. 190). "Unsettling" it may be, and probably is—in hindsight. But from the inside, people in organizations do not spend a professional life making "unset­ tling" decisions. Rather, they do mostly normal work. Again, how can a manager see a "lousy" process to evaluate flight safety as normal, as not something that is worthy reporting or repairing? How could this process be normal? The CAIB (2003) itself found clues to answers in pressures of scar­ city and competition: The Flight Readiness process is supposed to be shielded from outside influ­ ence, and is viewed as both rigorous and systematic. Yet the Shuttle Program is inevitably influenced by external factors, including, in the case of STS-107, schedule demands. Collectively, such factors shape how the Program estab­ lishes mission schedules and sets budget priorities, which affects safety over­ sight, workforce levels, facility maintenance, and contractor workloads. Ulti­ mately, external expectations and pressures impact even data collection, trend analysis, information development, and the reporting and disposition of anomalies. These realities contradict NASA's optimistic belief that pre­ flight reviews provide true safeguards against unacceptable hazards. (2003, p. 191) Perhaps there is no such thing as "rigorous and systematic" decision making based on technical expertise alone. Expectations and pressures, budget priorities and mission schedules, contractor workloads, and workforce levels all impact technical decision making. All these factors de­ termine and constrain what will be seen as possible and rational courses of action at the time. This dresses up the epistemic niche in which decision makers find themselves in hues and patterns quite a bit more varied than dry technical data alone. But suppose that some decision makers would see through all these dressings on the inside of their epistemic niche, and alert others to it. Tales of such whistleblowers exist. Even if the imperfection of an epistemic niche (the information environment) would be seen and ac­ knowledged from the inside at the time, that still does not mean that it war­ rants change or improvement. The niche, and the way in which people are configured in it, answers to other concerns and pressures that are active in the organization—efficiency and speed of briefings and decision-making processes, for example. The impact of this imperfect information, even if acknowledged, is underestimated because seeing the side effects, or the connections to real risk, quickly glides outside the computational capability of organizational decision makers and mechanisms at the time. Studying information environments, how they are created, sustained, and rationalized, and in turn how they help support and rationalize com­ plex and risky decisions, is one route to understanding organizational sensemaking. More will be said on these processes of sensemaking else­ where in this book. It is a way of making what sociologists call the macro- micro connection. How is it that global pressures of production and scarcity find their way into local decision niches, and how is it that they there exer­ cise their often invisible but powerful influence on what people think and prefer; what people then and there see as rational or unremarkable? Al­ though the intention was that NASA's flight safety evaluations be shielded from those external pressures, these pressures nonetheless seeped into even the collection of data, analysis of trends and reporting of anomalies. The information environments thus created for decision makers were con­ tinuously and insidiously tainted by pressures of production and scarcity (and in which organization are they not?), prerationally influencing the way people saw the world. Yet even this "lousy" process was considered "nor- mal"—normal or inevitable enough, in any case, to not warrant the expense of energy and political capital on trying to change it. Drift into failure can be the result. ENGINEERING RESILIENCE INTO ORGANIZATIONS All open systems are continually adrift inside their safety envelopes. Pres­ sures of scarcity and competition, the intransparency and size of complex systems, the patterns of information that surround decision makers, and the incrementalist nature of their decisions over time, can cause systems to drift into failure. Drift is generated by normal processes of reconciling dif­ ferential pressures on an organization (efficiency, capacity utilization, safety) against a background of uncertain technology and imperfect knowl­ edge. Drift is about incrementalism contributing to extraordinary events, about the transformation of pressures of scarcity and competition into or­ ganizational mandates, and about the normalization of signals of danger so that organizational goals and supposedly normal assessments and decisions become aligned. In safe systems, the very processes that normally guarantee safety and generate organizational success, can also be responsible for or­ ganizational demise. The same complex, intertwined sociotechnical life that surrounds the operation of successful technology, is to a large extent responsible for its potential failure. Because these processes are normal, be­ cause they are part and parcel of normal, functional organizational life, they are difficult to identify and disentangle. The role of these invisible and unacknowledged forces can be frightening. Harmful consequences can occur in organizations constructed to prevent them. Harmful consequences can occur even when everybody follows the rules (Vaughan, 1996). The direction in which drift pushes the operation of the technology can be hard to detect, also or perhaps especially for those on the inside. It can be even harder to stop. Given the diversity of forces (political, financial, and economic pressures, technical uncertainty, incomplete knowledge, fragmented problem-solving processes) both on the inside and outside, the large, complex sociotechnical systems that operate some of our most haz­ ardous technologies today seem capable of generating an obscure energy and drift of their own, relatively impervious to outside inspection or inside control. Recall that, in normal flight, the jackscrew assembly of an MD-80 is sup­ posed to carry a load of about 5,000 pounds. But in effect this load was borne by a leaky, porous, continuously changing system of ill-taught and impractical procedures delegated to operator level that routinely, but for­ ever unsuccessfully, tried to close the gap between production and opera­ tion, between making and maintaining. Five thousand pounds of load on a loose and varying collection of procedures and practices were slowly, incrementally grinding their way through the nut threads. It was the sociotechnical system designed to support and protect the uncertain tech­ nology, not the mechanical part, that had to carry the load. It gave. The ac­ cident report acknowledged that eliminating the risk of single catastrophic failures may not always be possible through design (as design is a reconcilia­ tion between irreconcilable constraints). It concluded that "when practica­ ble design alternatives do not exist, a comprehensive systemic maintenance and inspection process is necessary" (NTSB, 2002, p. 180). The conclusion, in other words, was to have a nonredundant system (the single jackscrew and torque tube) made redundant through an organizational-regulatory conglomerate of maintenance and airworthiness checking. The report was forced to conclude that the last resort should be a countermeasure that it had just spent 250 pages proving does not work. Drifting into failure poses a substantial risk for safe systems. Recognizing and redirecting drift is a challenge that lies before any organization at the 10~7 safety frontier. No transportation system in use today has yet broken through this barrier, and success in breaching the asymptote in progress on safety is not likely to come with extensions of current, mechanistic, structuralist approaches. Safety is an emergent property, and its erosion is not about the breakage or lack of quality of single components. This makes the conflation of quality and safety management counterproductive. Many organizations have quality and safety management wrapped up in one func­ tion or department. Yet managing quality is about single components, about seeing how they meet particular specifications, about removing or re­ pairing defective components. Managing safety has little to do anymore with single components. An entirely different level of understanding, an entirely different vocabulary is needed to understand safety, in contrast to quality. Drifting into failure is not so much about breakdowns or malfunctioning of components, as it is about an organization not adapting effectively to cope with the complexity of its own structure and environment. Organiza­ tional resilience is not a property, it is a capability: A capability to recognize the boundaries of safe operations, a capability to steer back from them in a controlled manner, a capability to recover from a loss of control if it does occur. This means that human factors and system safety must find new ways of engineering resilience into organizations, of equipping organizations with a capability to recognize, and recover from, a loss of control. How can an organization monitor its own adaptations (and how these bound the ra­ tionality of decision makers) to pressures of scarcity and competition, while dealing with imperfect knowledge and uncertain technology? How can an organization become aware, and remain aware, of its models of risk and danger? Organizational resilience is about finding means to invest in safety even under pressures of scarcity and competition, because that may be when such investments are needed most. Preventing a drift into failure re­ quires a different kind of organizational monitoring and learning. It means fixing on higher order variables, adding a new level of intelligence and analysis to the incident reporting and error counting that is done today. More will be said about this in the following chapters.