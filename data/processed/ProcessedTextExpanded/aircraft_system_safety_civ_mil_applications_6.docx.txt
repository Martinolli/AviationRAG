Title: Aircraft System Safety – Military and Civil Aeronautical Applications – Chapter 4 – 5 - 6 Author(s): Duane Kritzenger Category: System Safety Tags: Airworthiness, Aircraft, Safety, Chapter 4 Risk-based approach 4.1 Introduction Risk management is a concept applied as part of a decision-making process and can also be explained as follows: A process … Risk-based decision-making involves a series of basic steps. It can add value to almost any situation, especially when the possibility exists for serious (or catastrophic outcomes). The steps can be used at different levels of detail and with varying degrees of formality, depending on the situation. … that organizes information about the possibility for one or more unwanted outcomes … This information about the possibility for one or more unwanted outcomes separates risk-based decision-making from more traditional decision-making. These unwanted outcomes can be project, market, mission, and/or safety-related. … into a broad, orderly structure … Most decisions require information not only about risk but also about other things as well. This additional information can include things such as cost, schedule requirements, and public perception. In risk-based decision-making, all of the identifiable factors that affect a decision must be considered. The factors may have different levels of importance in the final decision. … that helps decision-makers… The only purpose of risk-based decision-making is to provide enough information to help someone make a more informed decision. The information must, therefore, be compiled and presented in a consistent (e.g., the safety criteria applied) and user-friendly fashion (e.g., a Hazard Log) to ensure that ‘apples are not compared with pears’. … make more informed management choices … The objective of risk-based decision-making is to help people make better, more logical choices without complicating their work or taking away their authority. The business world usually distinguishes between: Σ project risk (e.g., failure to meet specification/schedule) Σ marketing risk (e.g., inappropriate product, not being the dominant design) Σ business risk (e.g., financial risk of insufficient cash flow or legal risk of being sued) Σ insurance risk (e.g., risk of theft, damage to property or unexpected medical bills). Conventionally, risk deals with uncertainty in project appraisal, project management, and financial performance. However, when it comes to safety, we also have to add safety risk as another factor. Although it can be argued that it falls under project risks, it has a different slant. A product may meet the specifications and be on time and within cost, but that does not mean that it is safe. Note that safety is also closely connected with other sorts of risk (e.g., an accident can affect insurance and business risk), and the prudent manager thus needs to ensure that safety is given due attention. 4.2 Defining risk When it comes to safety, many interpretations exist when we use the term ‘risk.’ For instance: Σ risk is the chance of achieving a certain, usually negative, outcome; or Σ risk means the same as a hazard, or Σ risk is the consequence of failure, or Σ risk is the same as danger, or Σ risk is about taking chances. The concept of risk starts from the premise that perfect safety (i.e., complete freedom from harm) is not achievable for all but the simplest systems (David, 2002). Risk is the measure that allows different safety concerns to be compared according to how serious they are. But how do we make decisions regarding risk? For the purposes of establishing objective evaluation criteria, an appropriate definition of ‘risk’ is taken from ISO/IEC Guide 51 (1999): ‘Risk is the combination of the probability of occurrence of harm and the severity of that harm.’ This implies that risk can be expressed as the combined effect of the probability of occurrence of an undesirable event and the severity of the consequence of that event. This can be expressed mathematically as follows: R = S x P where R = risk, S = severity of the consequence, P = probability of occurrence of the consequence. Please note that risk is estimated on the probability of occurrence of the consequence. The consequence is the undesired event and is usually some sort of accident. In any accident, there is rarely only one single cause. Generally, there are a number of causes (e.g., failures) and events (i.e., pilot error) that combine like links in a chain to create an accident. This concept is illustrated in Fig. 4.1. So, to estimate risk, we are thus not only interested in the probability of a specific hazard and or failure but also in all the factors that lead to the undesired event.2 Risk, therefore, relates to accidents (i.e., the event causing the harm) rather than hazards (i.e., the situation with the potential to cause harm) or failures of any individual piece of equipment.3 Accidents happen when the characteristics of different factors (such as component failures, procedural shortcomings, and environmental effects) combine in an unexpected fashion. Manipulating any of the causes/hazards/events in the accident sequence can influence the risk. This includes the series of human failures (e.g. pilot error when exposed to increased stress) that may have contributed to the accident. 4.3 Assessing risk We have already stated that risk is a combination of accident likelihood (probability) and severity of the consequence. The risk increases with either the severity or the probability of the accident, as illustrated in Fig. 4.2. Different regulatory authorities use a variety of classification criteria in order to evaluate the acceptability of risk. Some of these are discussed in more detail in Appendix B, but the remainder of this paragraph will use the UK MoD criteria to illustrate the basic approach adopted by most. Σ Accident severity can be categorized in accordance with the impact on personnel as defined as Negligible at most a single minor injury or minor occupational illness; marginal – a single severe injury or occupational illness and/or multiple minor injuries or minor occupational illness; critical – a single death, and/or multiple severe injuries or severe occupational illnesses; catastrophic – multiple deaths. Σ Accident probability can be categorized during risk estimation in accordance with the definitions: Frequent Likely to be continually experienced < 10^-2; Probable Likely to occur often < 10^-4; Occasional Likely to occur several times < 10^-6; Remote Likely to occur some time < 10^–8; Improbable Unlikely, but may exceptionally occur < 10^–10; Incredible Extremely unlikely that the event will occur at all, given the assumptions recorded about the domain of the system < 10^–12. 4.4 As low as reasonably practicable (ALARP) The Health and Safety at Work Act (HSWA) affects product safety as well as workplace safety and has its basis in law (see Chapter 1). It requires us to determine if the risk is as low as reasonably possible (as low as reasonably practicable), where: Σ ‘practicable’ means what is possible to do, such as considering options to reduce the frequency of occurrence and/or the consequence of the event, Σ ‘reasonable’ means to balance costs, time, trouble against the risk. ‘As low as reasonably practicable’ means that risk in a particular activity/product can be balanced against the time, cost, and difficulty of taking measures to avoid the risk. The greater the risk to safety, the more likely it is that it is reasonable to make a substantial effort to reduce it.4 The UK HSE divides risk into three tiers - Intolerable (or unacceptable) region, within which the risk cannot be justified save in extraordinary circumstances. Risk reduction measures or design changes are considered essential, as well as the as low as reasonably possible or tolerability regions, where risk reduction is desirable. Risks are considered tolerable only if they are shown to be ALARP. This may require that risk reduction measures be evaluated by means of a cost-benefit analysis, a Negligible (or broadly acceptable) region within which the risk is generally tolerable and no risk reduction measures are needed. as low as reasonably possible is based on the legal standard of ‘as far as reasonably practicable.’ This standard has acquired its meaning in case law (i.e., the decisions made by judges in court) and has come to mean that the degree of risk of injury or adverse effect must be balanced against the cost in terms of money, time, and physical difficulty, of taking measures to reduce the risk. If the quantified risk of injury is insignificant compared with measures needed to mitigate the risk, then no action needs to be taken to satisfy the law. However, the greater the risk, the more likely it is to be reasonably practicable to go to substantial expense to do something about it. In the main, changes to the safety of modern aircraft are made only when a cost-benefit analysis has been done in which the cost of the new safety feature is balanced against a notional figure for the monetary value of life. If the cost of the measure exceeds the ‘value’ of the lives saved, then it will not be implemented.5 The regulation of safety in the United Kingdom is based upon the principle that risks must be reduced to a level that is ‘as low as reasonably practicable’ (ALARP). It is a philosophy applied to the reduction and acceptability of risk. There is little guidance from the courts as to what this means. The key case is Edwards vs. The National Coal Board, where the Court of Appeal considered whether or not it was reasonably practicable to make the roof and sides of a road in a mine secure. The Court of Appeal held that … in every case, it is the risk that has to be weighed against the measures necessary to eliminate the risk. The greater the risk, no doubt, the less will be the weight to be given to the factor of cost, and … reasonably practicable is a narrower term than physically possible and seems to me to imply that computation must be made by the owner in which the quantum of risk is placed on one scale and the sacrifice involved in the measures necessary for averting the risk (whether in money, time or trouble) is placed in the other, and that, if it is shown that there is a gross disproportion between them – the risk being insignificant in relation to the sacrifice – the defendants discharge the onus on them. Precedent in which as low as reasonably possible was found not to comply Stark vs. Post Office (March 2000) The Provision and Use of Work Equipment Regulations (PUWER) regulation 6(1) states: ‘Every employer shall ensure that work equipment is maintained in efficient working order and in good repair’ Mr Stark (a postman) was injured when a part of his bicycle broke. Counsel for Mr. Stark argued that Provision and Use of Work Equipment Regulations 6(1) does not say anything about ‘reasonably practicable.’ Therefore, the duty is absolute and applies at all times. The court found for Mr Stark. It was clear that the obligation of maintenance was an absolute one and applied at all times. It did not matter that the cause of some maintenance failure caused the accident. If it can be proved that some piece of working equipment has, in fact, failed, that was sufficient. 4.5 Managing the risk Risk management is defined as ‘the process whereby decisions are made to accept a known or assessed risk and/or the implementation of actions to reduce the consequences or probability of occurrence’ (British Standard 4778). A well-known cliché is ‘You cannot manage what you cannot measure.’ Therefore, a logical and systematic means is required to: 1. identify conditions and situations that may result in an unacceptable level of safety and 2. provide a measure of the technical airworthiness risk, which is defined as the chance of exposure to an unacceptable level of safety 3. control the implementation of risk reduction measures 4. accept the level of risk 5. track the risk to make sure it does not change. These five elements are illustrated in Fig. 4.6. The flowchart in Fig. 4.7 illustrates another way to consider a typical risk management process. When it comes to evaluating risk, the popular question ‘Is it safe?’ has to be rephrased: ‘Are the risks low enough for the public/authorities to tolerate?’ Safety management is not about the removal of all risks. Rather, the key principle is that risk is reduced to ‘as low as reasonably practicable’ (ALARP). Risk management of a system is not simply about reducing risk; it relates to striking a balance between the benefits from reduced risk and the expense of that reduction. However, some risks (such as a polio outbreak) may be completely unacceptable and not subject to balancing against the expense. Risk management strategies include: Σ Eliminate – get rid of the hazard that could cause the accident. Σ Spread out – spread the loss exposure responsibility out among different entities, across operations, or across time. Σ Transfer – make others accept loss exposure responsibility. Σ Accept – live with the current loss exposure level or responsibility. Σ Avoid – cancel or delay the activity that involves the risk or does not operate. equipment that involves the risk. Make processes inherently safer by eliminating hazards (e.g., eliminate energy sources such as pressure, heat, potential energy, kinetic energy, etc. Do not use hazardous materials and materials that can generate hazardous energy. Use other, less hazardous materials in place of more hazardous materials. Σ Reduce – do something to reduce the accident potential. Accidents can be well controlled at any point in the chain of events producing the accident. The goal is to get the most for your money by doing the things that are most effective. Options to consider include reducing the likelihood of initiating events (e.g., eliminate error-like situations that set people up for failure; make sure that sufficient competent people are assigned to operations and maintenance departments; improve design ratings and factors of safety). – provide multiple layers of safeguards, sometimes called layers of protection, in critical applications (e.g., add additional instrumentation, equipment, or safety interlocks, especially items with different designs and operations; make the operators perform more surveillance and checks during operations). – reduce the chance of safeguard failures (e.g., perform additional or more frequent inspections, tests, and preventive maintenance). – make processes inherently safer by reducing the severity of consequences (e.g., reduce energy stored or generated as pressure, heat, potential energy, kinetic energy, etc.; keep only small inventories of hazardous materials and materials that can generate hazardous energy; provide shutdown and alarm/response systems to limit consequences; protect people and other valuables from consequences by providing emergency response training, personal protective equipment, etc.). MIL-STD-882C (para 4.4) provides the following useful guidance when considering the order of precedence for resolving identified hazards: 1. Design for minimum risk. From the start, design to eliminate risk. If an identified hazard cannot be eliminated, the risks associated with that hazard should be controlled through design selection. 2. Safety devices. Hazards that cannot be eliminated or have their associated risks sufficiently mitigated through primary design selection will be controlled by the use of fixed, automatic, or other protective safety design features or devices. Provision will be made for periodic functional checks of safety devices. 3. Warning devices. When neither design nor safety devices can effectively eliminate or control an identified hazard, devices will be used to detect the condition and generate an adequate warning signal to correct the hazards or provide personnel evacuation. Warning signals and their application will be designed to minimize the probability of incorrect personnel reaction to the signals and will be standardized within like types of the system. 4. Procedures and training. Where it is impossible to eliminate a hazard or adequately control its associated risk through design selection or use of safety and warning devices, procedures and training will be used to mitigate the risk. Procedures may include the use of personal protective equipment or checklists. Precautionary notations must be standardized. Safety critical tasks and activities may require certification of personnel proficiency. However, too often, we find that hazards are only mitigated by procedures. Remember, if something can go wrong, then one day, it will go wrong. Remember the 50-50-90 rule: if at any time you have a 50-50 chance of getting something right, there’s a 90% probability you’ll get it wrong. 4.6 Summarising the risk-based approach, In essence, any risk-based safety assessment process is made up of basic steps: Σ identify the hazards Σ classify the severity of each accident Σ determine the probability of each accident occurring Σ assess the risks to people, property, and/or success of a mission or program, in terms of both probability of occurrence and severity of consequences Σ manage the risk. Typical questions a risk-based approach can answer are: Σ What is the likelihood that a particular unfavorable consequence (mission failure, loss of platform, program delay, etc.) will happen? Σ What are the most important drivers of overall risk (i.e., what factors should we concentrate our improvement effort on)? Σ Which alternative gives less risk? Σ Does a proposed action (e.g., modification, procedure, or limitation) reduce risk-related costs enough to justify its cost? Σ What risk factors have so much uncertainty that more testing or analysis is needed to define them better? Advantages of the risk-based approach include: Σ It provides clear guidance about the acceptability of the risk. It assists in identifying and prioritizing the factors (design, operations, maintenance, management, environment, etc.) that contribute to risk and evaluating the uncertainty that inevitably accompanies all estimates of risk. Σ It is most effective when applied to major accidents, where the chances of occurrence are relatively low and operating experience very high. Σ By expressing risk and costs in common units, the cost-benefit analysis becomes a useful decision-making airport information desk to project management. Σ Safety targets can be set for total systems (i.e., human + equipment + procedures + training). The safety targets are based on the consequences of an undesired event. Disadvantages/limitations of the risk-based approach include: Σ Appropriate risk criteria need to be set (there are no universally acceptable criteria to define whether or not risks are tolerable) and agreed upon with all stakeholders (including those exposed to the risk). Σ Be aware of the units of measurement employed during accident probability quantification. The term ‘operating hour’ (see Table 4.3) must be defined and consistently applied to ensure a ‘level playing field’ during risk comparison. Σ When we look at the acceptability of risk – acceptable to whom? The general public? The engineer? The company? Risk is affected by perception; for instance, scientists/engineers may trade risk against long-term benefits, while society will focus on the consequences versus the immediate benefits.6 Σ Not all hazards necessarily lead to accidents. Furthermore, any hazard can have many potential causes and consequences depending on the sequence of events. This is very dependent on how the hazard is defined (see example below, and also Chapter 6) Σ. Accident sequences have a large number of variables, and it is unrealistic always to consider them all. Risk assessment relies on judgemental decisions that integrate reliability, availability, and maintainability (RAM) engineering analysis, statistics, decision theory, systems engineering, quality engineering, conventional engineering analysis, and even cognitive psychology. Estimating the probability of occurrence of each event in the accident sequence can become very subjective – especially when evaluating the probability of human response, and human error is invariably present in the accident sequence.7 Even though quantitative assessments (see Appendix A) may look very objective, the core input data is often very subjective and/or predictive (e.g., see Ch. 10 para 6). Σ as low as reasonably possible requires cost-benefit analysis (i.e. simply meeting a risk target is not enough), which implies that a price needs to be put on the value of a human life in the explicit trade-off between safety and economics. Only the owner of the risk can make this decision. Σ as low as reasonably possible may not always be defensible in a court of law (see Section 4.4 above, and also Ch. 1). Σ Risk-based decision-making may hamper/stifle innovation. For instance, would cars – or even glass – be allowed if they were invented today? 4.7 Discussion Risk assessment seeks to answer relatively simple questions, although, like many engineering questions, these are far easier to pose than to respond properly. Table 4.4 summarises some of these questions and provides an indication of the processes needed to address them. The general principles of safety risk management are (Federal Aviation Administration System Safety Handbook (2000)): Σ All system operations represent some degree of risk. Recognize that human interaction with elements of the system entails some element of risk. Σ Keep hazards in proper perspective. Do not overreact to each identified risk, but make a conscious decision on how to deal with it. Dr Trevor Kletz is known to have said: ‘To maintain the balance in your risk exposure levels, when confronted by a new risk, just smoke 1 or 2 less cigarettes a day.’ Σ Weigh the risks and make judgments according to your own knowledge, inputs from subject matter experts, experience, and program needs. There may be no ‘single solution’ to a safety problem. There are usually a variety of directions to pursue. Each of these directions may produce varying degrees of risk reduction. A combination of approaches may provide the best solution. Σ Risks are reduced asymptotically. Thus, the closer to zero we get, the more effort is needed. We thus need to know when enough is enough. A good decision made quickly is much better than a perfect decision made too late. Also, a good decision does not always result in a good outcome. The best we can hope for is to equip intelligent decision-makers with good information based on a number of decision factors and the interests of stakeholders. On average, and over time, good decisions made through this process should provide the best outcomes. They will also provide logical explanations for decisions when the outcomes are not favorable. Finally, remember that producing a risk assessment is simple enough; the challenge lies in demonstrating that risks have been identified in a structured and systematic way and that the risks are managed throughout the product life-cycle, i.e., Σ Implement risk control. Risk-management activities have no effect on risk until the process of risk control is implemented to actually change the design, add safety protective features, or alter working practices. Σ Assess risk continuously. Assessment of technical risk depends on the quality and quantity of information available. There may be very little data available in the preliminary stage of a decision-making process. As the process progresses, the results of system safety analysis, failure mode, effects criticality analysis, and compliance testing may provide the necessary information and details required to refine the risk assessment. Ultimately, actual operational use and airworthiness-related occurrences will provide the most valuable information. Even then, the risk changes as the system ages or as operating or maintenance procedures (and personnel) alter. Chapter 5 Goal-based approach 5.1 Introduction An acceptable level of safety for aviation is normally defined in terms of an acceptable aircraft accident rate. There are two primary causes of aircraft accidents: Σ operational (such as pilot error, weather, and operating procedures) and Σ technical (such as design errors, manufacturing errors, maintenance errors, and part failures). When certifying a new (or modified) system, designers concentrate on the technical integrity of the system, which has been designed around an operational requirement. For a number of years, airplane systems were evaluated to specific requirements, to the ‘single fault’ criterion, or to the ‘fail-safe design’ concept (see Chapter 7). As later-generation airplanes were developed, more safety-critical functions were required to be performed. This generally resulted in an increase in the complexity of the systems designed to perform these functions. The likely hazards to the airplane and its occupants that could arise in the event of loss of one or more functions (provided by a system or that system’s malfunction) had to be considered, as also did the potential interaction between systems performing different functions. The application of the fail-safe concept thus had to be supplemented by some sort of safety target (i.e., goal) against which the integrity of the system architecture could be evaluated. 5.2 Probability targets vs. failure severity levels In assessing the acceptability of a design, it was recognized that rational failure probability values would have to be established. The civil regulatory authorities implemented the following logic (AMC25.1309 to CS25) for large commercial transport aircraft: Historical evidence indicated that the probability of a serious accident due to operational and airframe-related causes was approximately one per million hours of flight.1 Furthermore, about 10 percent of the total was attributed to failure conditions caused by the airplane’s systems. It seems reasonable that serious accidents caused by systems should not be allowed a higher probability than this in new airplane designs. It is reasonable to expect that the probability of a serious accident from all such failure conditions is not greater than one per ten million flight hours or 10^–7 per flight hour for a newly designed airplane. Most civilian airworthiness authorities have thus determined that an acceptable aircraft accident rate attributable to technical cause factors for large commercial transport aircraft is of the order of 1 per 10 million hours,2 provided the probability of occurrence does not vary from flight to flight.3 The difficulty with this is that it is not possible to say whether the target has been met until all the systems on the airplane are collectively analyzed numerically. A typical transport category aircraft type design has many individual systems that may influence the safe flight and landing of an aircraft. Without a full system safety analysis, it is difficult, if not impossible, to consider the contribution of each individual system to the overall accident rate. For most aircraft types, it is therefore assumed (refer, inter alia, ACJ25.1309 para 6.a) that as many as 100 individual system failure conditions may exist, which could prevent continued safe flight and landing. The target allowable probability of 10^– 7 is thus apportioned equally among these conditions, resulting in a probability allocation of not greater than 10^–9 per flight hour to each.4 This upper probability limit establishes an approximate probability value for the term ‘extremely improbable.’ Failure conditions having less severe effects could be relatively more likely to occur based on the principle that an inverse relationship should exist between the probability of an occurrence and the degree of hazard inherent in its effect.5 This led to the general principle that an inverse relationship should exist between the probability of loss of function(s) or malfunction(s) leading to a serious failure condition and the degree of hazard to the airplane and its occupants arising therefrom. This ‘degree of hazard’ is commonly referred to as the severity of the consequence. The civil aviation authorities use this inverse relationship between Consequence and Frequency to substantiate safety against prescribed hazard definitions and allocated probability targets. Failure effects are, therefore, regulated by requiring an inverse relationship between the severity of the failures and their frequency of occurrence. The broad intention is that effects of a catastrophic nature should virtually never occur in the fleet of a type of aircraft. Where the effects are less hazardous, they are permitted to occur more frequently. Each failure mode classification can thus be allocated a quantitative or a qualitative safety objective based on its level of criticality. Appropriate qualitative probability terms can then be defined and are ‘commonly accepted as airborne integrated data system to engineering judgment.’ The International Civil Aviation Organisation advises (ICAO Airworthiness Manual, page IIA-4h-I) that where it is necessary to use numerical assessments. These qualitative and quantitative objectives become safety requirements and provide a measure of performance against which the integrated product will be evaluated. The target probability is set to assist the assessor in minimizing the occurrences of hazards by applying a variety of defenses and design disciplines appropriate to the severity of the safety target. Typically, the objectives are accomplished through the application of the fail-safe concept, as discussed in Chapter 7. 5.3 Discussion The goal-based approach to safety is applied as follows: Σ identify potentially hazardous situations (e.g., failures or occurrences, not necessarily accidents) Σ assess their impact on the system Σ set safety targets according to the potential severity Σ prove that these targets are met. These safety targets can have an impact on all aspects of the design. If they are too severe, they will impact costs, capability, performance, etc. If not severe enough, the high failure rates experienced in service become unacceptable, resulting in loss of customer capability and resources, high damage costs, and loss of company reputation. The safety targets should, therefore, be agreed upon with the applicable airworthiness authority as early as possible within the product development lifecycle. In essence, any goal-based safety assessment process consists of three basic processes: 1. identifying the failure modes or hazardous situations/occurrences, 2. allocating safety objectives to these failure modes, and 3. proving safety objective accomplishment. Advantages of the goal-based approach include Σ Accidents do not just happen – they need a sequence of events to combine in a particular fashion and are thus difficult to predict. This is especially true if the assessor is not the operator and therefore cannot control the final mitigations (such as human factors associated with personnel competence or exposure levels) which make the distinction between an accident and an incident (see Fig. 5.2). Technical failures do happen and are easier to predict. Σ The goal-based approach provides clear guidance about the severity of a particular system failure and, in so doing, gives clear minimum safety objectives that need to be accomplished for that system to be satisfactory. These objectives can be determined quickly and efficiently (using Table 5.1) and allocated to responsible parties to accomplish. The goal-based approach is thus particularly useful for systems designers (see example on page 65). Σ The application of internationally accepted safety criteria provides for a level playing field in the integration and certification of projects with international participation/subcontractors and customers/operators. Limitations of the goal-based approach: Σ If ‘Historical evidence indicates that the risk of a serious accident due to operational and airframe-related causes is approximately 1 per million hours of flight’, then (with reference to Fig. 13.2), the rationale used for the goal allocation may no longer remain acceptable to society if there is to be one large aircraft accident every 7–10 days by the year 2010. For more on this, see Planning for Super Safety by R. Howard (2000). Σ It usually does not distinguish between different accident severities (i.e., the death of one person vs. the death of 100 people). It concentrates only on the probabilities of technical failures. Σ It is primarily used to consider failures and malfunctions in systems only. It does not consider operational hazards, nor does it include human errors. Only a minority of accidents can be attributed to system failure or malfunction only. Accidents seldom occur due to isolated events. More often than not, they are the result of a series of failures, events, and/or failing mitigations. Events are influencing external factors, such as lightning strikes, flying in VMC conditions, or being under enemy fire. Mitigations put in place with the intention to block the accident path can be divided into three main types: those that reduce the likelihood of the error taking place, those that airport information desk error detection, and those that airport information desk error recovery. Mitigations can involve the human (e.g., training and supervision), the machine (e.g., alert devices), or procedures (e.g., emergency procedures). Mitigations are not perfect in blocking the error, and when the holes line up, the error is allowed to continue along its path to an accident. Figure 5.2 illustrates this roulette of changing circumstances, which needs to align to result in an accident. So, although the goal-based approach provides a designer with acceptable levels of safety, which need to be accomplished in the design (as in Chapter 8), the user of the system will need to conduct further assessments (as in Chapter 9) to consider how the system is put into operational use and what risks said use will hold. 5.4 Combining the risk- and goal-based criteria A valid question would be to ask whether the risk-based8 and goal-based9 criteria can be combined in a single set of tables. The Federal Aviation Administration’s internal Safety Management System (ASD-100-SSE-1) combines the two approaches by classifying hazard severity as shown in Table 5.4: Hazard severity: Catastrophic Results in multiple fatalities; Hazardous Reduces the capability of the system or the operator's ability to cope with adverse conditions to the extent that there would be a large reduction in safety margin or functional capability. Crew physical distress/excessive workload is such that operators cannot be relied upon to perform required tasks accurately or completely. Serious or fatal injury to a small number of persons (other than flight crew); Major Reduces the capability of the system or the operators to cope with adverse operating conditions to the extent that there would be a significant reduction in safety margin or functional capability, a significant increase in operator workload and conditions impairing operator efficiency or creating significant discomfort or physical distress to occupants of aircraft (except operator) including injuries, Major occupational illness and/or major environmental damage, and/or major property damage; Minor Does not significantly reduce system safety. Actions required by operators are well within their capabilities. Includes a slight reduction in safety margin or functional capabilities, a slight increase in workload such as routine flight plan changes and some physical discomfort to occupants of aircraft (except operators), Minor occupational illness and/or minor environmental damage, and/or minor property damage; No safety effect Has no effect on safety; allocating consequence probability as shown in Consequence probability; Probable Qualitative: anticipated to occur one or more times during the entire system/operational life of an item. Quantitative: the probability of occurrence per operational hour is equal to or greater than 10^-5; Remote Qualitative: unlikely to occur to each item during its total life. It may occur several times in the life of an entire system or fleet. Quantitative: the probability of occurrence per operational hour is less than 10^–5 but greater than 10^–7; Extremely Qualitative: not anticipated to occur to each item during its total life. It may occur remotely a few times in the life of an entire system or fleet. Quantitative: the probability of occurrence per operational hour is less than 10^–7 but greater than10^–9. Extremely Qualitative: so unlikely that it is not anticipated to occur during the entire improbable operational life of an entire system or fleet. Quantitative: the probability of occurrence per operational hour is less than 10^–9. Author’s note: strictly speaking, these are not hazards but the worst-case consequences of a hazard (see Chapter 6). Furthermore, be aware that this approach may cause confusion (e.g., will a ‘hazardous’ condition reduce the capability or will it cause serious/fatal injuries? Surely the latter should be more severe?). It is thus shown that great care should be taken when following these two approaches. Personally, the author has found it far simpler and more efficient to consistently keep a clear distinction between ‘airworthiness/failure criteria’ and ‘accident criteria’ (as discussed in Chapter 8), with the key differentiating factor in the two approaches being that: Σ the goal-based approach emphasis the frequency of an event/failure (which is the nuclear and civil aviation industry approach), whilst Σ the risk-based approach looks at a large number of events to evaluate the probability of an accident occurring (which is the approach used by many operators and facilities. The UK MoD has also adopted this approach in DEF STAN 00-56). This differentiating approach has also been adopted by the Australian Defence Force. SAAP 7001.054((AM1), Section 2 Chapter 1)), which states: System safety objectives for aircraft acquisitions and modification projects are different from those for the management of in-service aircraft. During acquisition and modification projects, the system safety objective is to procure an aircraft with an acceptable level of safety … Once in service, the system safety objective is to ensure that the aircraft’s inherent level of safety is maintained. The goal-based and risk-based approaches can be combined in the same assessment by: Σ showing an inverse relationship between failure severity and failure probability (i.e., the goal-based approach) during system certification Σ from these causes/failures, identifying the hazards that could lead to an accident (i.e., the risk-based approach) during the operational application of the system. For instance, ten different causes/failures (e.g., components that could release toxic fumes) may all lead to one hazard (e.g., intoxication), which in turn could cause one (or more) types of accident (e.g., death of a technician, or death of all passengers). Σ assessing the probability of this hazard becoming an accident. If the hazard (and resulting accident) has a technical failure as a contributing cause, then the probability of said technical failure would need to be obtained from the system provider. Σ Determining the risk by combining the accident severity with its probability. Chapter 6 Hazards 6.1 Understanding hazards and their causes Safety is freedom from accidents. Accidents are caused by hazards. But what exactly do we understand the term ‘hazard’ to mean? It goes by many (often confusing) definitions, such as: Σ an accident (i.e. injury to personnel, damage to property or pollution of environment) waiting to happen Σ a physical condition of a platform that threatens the safety of personnel of the platform, i.e., that can lead to an accident, or has the potential to cause harm Σ a condition of the platform that, unless mitigated, can develop into an accident through a sequence of events and actions Σ natural events such as bird strikes, lightning, windshear, etc. Σ a potentially unsafe condition resulting from failures, malfunctions, external events, errors, or a combination thereof (ARP 4761) Σ a situation that could occur during the lifetime of a product, system or plant that has the potential for human injury, damage to property, damage to the environment or economic loss (British Standard 4778). Σ a situation with the potential for human injury, damage to property/assets or the environment (Rhys, 2002, page 4). Σ a set of conditions in the operation of a product with the potential for initiating or contributing to events that could result in personal injury, damage to property or harm to the environment Σ exposure of vulnerability to injury, loss, evil, etc. A thing likely to cause injury, etc. (Collins English Dictionary, 2003). Note that the presence of a hazard does not make an accident inevitable. From the discussions in this chapter, it is proposed that an all-encompassing definition might thus rather be: ‘A hazard is a prerequisite condition that can develop into an accident through a sequence of failures, events and actions in the process of meeting an objective.’ Example: making the distinction between hazards and their causes Is ‘aircraft brakes overheat’ a hazard? No, the real hazards are: Σ loss of braking (i.e. a functional hazard) Σ fire due to brakes overheating (i.e. a physical hazard (Fig. 6.1)) Σ any other direct consequence So, do not confuse causes with hazards (i.e. loss of brakes can be caused by brakes overheating). A Safety Assessment involves detailed predictions of the likely hazardous behaviour of a system, often before it enters service. Before such an assessment can be made it is necessary to understand the nature of hazards and how system failures/inadequacies contribute to accidents and incidents. There is a causal chain from causes to hazards to accidents. Rhys (2002, page 4) defines an accident as: ‘an unintended event or sequence of events which causes death, injury, environmental damage or material damage’. The accident is the undesired outcome, rather than the initiating event or any intermediate state or hazard. Understanding this model leads to better management of the term ‘hazard’ by separating (but not ignoring) the consequences and the causes. Note that any single hazard may lead to a variety of different outcomes, some of which will be accidents and some relatively unimportant. Example The release of toxic fumes (the hazard) may have several outcomes (accidents), such as Σ a few individuals becoming ill Σ death of a single maintenance person Σ multiple fatalities of crew and passengers. Similarly a particular hazard may have several possible causes, either acting alone or together. Example Consider the hazard ‘loss of engine power’. This could be caused by: Σ water in the fuel system Σ no fuel in the tank Σ crimped fuel line Σ loss of ignition Σ any other cause. The Federal Aviation Administration (ASD-100-SSE-1 revision 7D, Fig. 4.1-1) also make this distinction between hazards and their causes as illustrated in Fig. 6.3. The causes are events that lead to a hazard or hazardous condition. Causes can occur by themselves or in combinations and can be technical and procedural in nature. The hazard is the adverse event that occurs as a result of the cause(s). A hazard is defined as ‘anything real or potential, that could make possible or contribute to an accident. A condition that is a prerequisite to an accident.’ It is vital to link the hazards to the accidents they could cause because the risk assessment is applied to the accident outcome (see Chapter 4). Hazard control is concerned both with preventing the hazardous condition from happening and from stopping it from becoming an accident (e.g. by managed mitigations, as illustrated in Fig. 5.2). Note that once a hazard exists, it does not always turn into an accident and for any accident there is rarely only one single cause. Generally there are a number of causes and events which combine like links in a chain to create an accident. The illustrations in Figs 5.2, 6.1 and 6.2 are quite successfully demonstrated in the National Aeronautics and Space Administration Challenger accident: Example: the National Aeronautics and Space Administration Challenger accident In 1986, the space shuttle Challenger exploded 73 seconds after lift-off from the Kennedy Space Center in Florida. The following sections describe the chain of events involved in this catastrophic loss. Hazard Σ fuel (liquid hydrogen and liquid oxygen) tank ignition. Incident (initiating event) Σ lift-off of a shuttle when the ambient temperature was low. Accident Σ Flight 51-L explodes 73 seconds after lift-off. Consequences Σ loss of seven astronauts Σ loss of a multi-billion-dollar shuttle Σ suspension of the shuttle programme for almost three years Σ safety culture of National Aeronautics and Space Administration considered suspect. Direct causes Σ Solid rocket motor rubber O-ring failed to seal properly because of its reduced pliability from exposure to low temperature prior to launch. Σ Heavy wind shear during the last 45 seconds of the flight caused higher than normal bending of the joints of the solid rocket motor sealed by the rubber Oring. Σ High-pressure hot exhaust gases from the solid rocket motor eroded through the cold rubber O-ring (aided by the higher-than-normal bending of the joint) and contacted the external fuel tank. Systemic causes Σ ineffective management assessment of identified issues Σ temperature effects on O-rings not well understood by launch safety personnel Σ no definite operating envelope was set for O-rings Σ design specification did not include a temperature range Σ prior evidence of O-ring problems was not viewed as a problem Σ O-ring damage was observed on 15 of 25 missions Σ eventually, O-ring damage was viewed as acceptable. Safeguards not provided (causes) Σ effective O-ring design Σ timely communication of temperature limit for O-rings in this service. 6.2 Identifying hazards When we look at any system we can distinguish between two distinct groups of hazards: endogenous and exogenous – see Table 6.1 (which is a useful reminder to consider hazards resulting from causes outside the system’s boundary). Once a hazard is identified, we need to decide on the severity of the hazard (if we are using the goal-based approach) or the severity of the potential accident (if we are using the risk-based approach). The severity is determined by considering the effect (or harm) of the potential outcome of the hazard within the context of the system state. For this explanation to make sense, a few clarifying explanations are needed: Σ A system can be defined (ASD-100-SSE-1 revision 7D) as: ‘a composite of people, procedures, materials, tools, equipment, facilities, and software operating in a specific environment to perform a specific task or achieve a specific purpose, support, or mission requirement.’ Σ Hazards are properties of an entire system and may be defined at any system level.2 However, it is essential to select the right level: – A common mistake is to select it too low (e.g. within a Piece-Part FMECA, see Appendix A), which results in too many hazards, no system properties, expensive (impossible) to track and over-engineering. – If it is selected too high, then it is hard to ensure the identification and management of all hazards. Example of hazard levels To continue our braking example from paragraph 1 above, the hazards can be broken down into its constituent elements/subsystems as follows: 1. loss of controllability – Level A 1.1 braking 1.1.1 loss of braking – Level B Σ brake pipe ruptures – Level C Σ no brake fluid – Level C Σ brake booster failure – Level C 1.1.2 uncommanded braking – Level B 1.2 steering 1.2.1 loss of steering control – Level B 1.2.2 over-steer 1.2.3 etc. This example demonstrates that the Level B hazards would probably (but not necessarily) be the appropriate hazard level to manage, because: Level A might be too vague by not focusing on any specific system, and Level C is designated as contributing causes/failures to the level of hazard, whereas Level B directly leads to the accident. Σ The system state is an expression of the various conditions, characterised by quantities or qualities, in which the system can exist. For any given hazard, the system state can be described in any of the following terms: – operational/ procedural terms (e.g. air-to-air refuelling, instrument landing system (ILS) approach, etc.), – conditional terms (e.g. instrument (IMC) vs. visual meteorological conditions (VMC), low altitude, rough terrain, etc.) – physical terms (e.g. electromagnetic environment effects, precipitation, low rotor speed, low hydraulic pressure, high impedance, etc.). For any given hazard, not all system states result in equal severity ratings. Example Loss of one engine in a multi-engined aircraft at mid-altitude and airspeed, would not be likely to result in a catastrophic accident. However: Σ Loss of one engine at low airspeed, low altitude and high gross weight has the potential to result in loss of control or lift. In this system state, the end result of the hazard would be catastrophic. Σ Loss of an engine due to uncontained failure (e.g. loss of a propeller) at high altitude may cause explosive decompression if the fuselage is at a high pressure differential. In this system state, the end result of the hazard is likely to be hazardous. Most regulatory authorities expect the assessment to consider the worst-case system state. If desired, other system states may be considered, but only in addition to the worst case. The following sections will consider some of the causes which can lead to a hazardous situation. Thereafter we will briefly consider some of the safety assessment tools and techniques available to identify and assess these hazards and their causes. 6.3 Equipment failures and faults A failure can be classified as the inability of an item to perform its intended function within previously specified limits. The following section will attempt to distinguish between various types of failure, as well as provide some advice on how to mitigate them. 6.3.1 Active vs. passive failure Active failure An active failure is one that produces immediate adverse effect (e.g. loss or degraded engine functionality). This type of failure can be permanent or intermittent. Example: Antonov AN-28, Ulemiste Airport, Tallinn, Estonia, 10 Feb 2004 Witnesses reported a ‘loud noise’ coming from the aircraft as it was climbing through 130 ft after take-off in darkness with snow and sleet. It veered right, lost height and crashed about 1 km from the runway. Two fatalities. The flight engineer, injured in the accident, reported an engine ‘explosion’. Flight International, 20–26 Jan 2004 Passive/latent/dormant failure In some systems there can be a fault in one channel which leaves the system operating but the presence of the fault is undetected. This type of failure produces no immediately adverse effects and goes by unnoticed. Usually not harmful in isolation but can interact with other situations to become very active (e.g. failure of a back-up system). Can be permanent or intermittent. These are usually mitigated through the use of monitors or specific checks (such as during maintenance or via flight check-lists). Example: BAC1-11, Blossburg, 23 June 1967 The probable cause of this accident was the loss of integrity of the empennage pitch control due to a destructive undetected in-flight fire, which originated in the airframe plenum chamber. 6.3.2 Obvious vs. non-obvious failures These are a derivative of active/passive failure conditions, but with a novel twist brought in during the introduction of digital technology. Many computer-based information systems act in an advisory manner, where an obvious failure can be tolerated but a ‘plausible but wrong’ output is hazardous. Example: Boeing 747-200F, London Stansted, December 1999 Thirty-seven seconds after take-off the aircraft began a left turn as part of the departure routing. Eighteen seconds later, the aircraft was pitched at 40∞ nose down and banked left close to 90∞ just prior to impact with the ground. During the investigation the commander of the previous flight of the accident aircraft reported that the captain’s attitude indicator (ADI) was unreliable and would indicate wings level during turns in either direction. international civil aviation organization Journal Number 1, 2002, p. 14. Note that a fail-safe design (see Chapter 7) may include monitoring software running in parallel with the actual application, providing a sanity check on the outputs displayed. In this instance, a significant issue is how to avoid common cause or latent failures, such as the operating system failing to run the monitor. One way to address the latter is to introduce a ‘hardware watchdog’ to confirm the execution of the monitoring function. 6.3.3 Independent vs. dependent failure Independent failures Independent failures, which separately do not degrade safety significantly, may combine to produce a hazardous situation. There may be a combination of active failures and passive failures, such as dormant failure of a standby system before the main system fails or an undetected leak of flammable vapour followed by a spark caused by an electrical failure. Example: Antonov An-24, Ndjole, Gabon, 17 Jan 2004 The aircraft circled, apparently with navigation equipment failure following a total electrical failure. The crew failed to locate their airfield and eventually the aircraft ran out of fuel and hit a low hill. Seven fatalities (all on board). Flight International, 20–26 Jan 2004. Dependent failures These failures are those caused by common modes or events, or which have a cascading effect. High levels of safety needed from essential systems are usually achieved by some form of ‘fail-safe’ design as detailed in Chapter 7. However, in spite of these precautions, there are various threats to the independence of the channels of redundant systems which may lead to multiple failures at higher rates than would be forecast by calculating the multiple rates from the failure rates of the component channels alone (Lloyd and Tye, 1995). A common cause (or a common mode) failure concerns the possibility that system failure involving multiple item failure may occur due to a common cause, i.e., the loss (during some critical period) of multiple or redundant paths/components/parts/ functions due to an underlying common mechanisms/faults/phenomenon. A common mode failure is a failure which has the potential to fail more than one function and to possibly cause an initiating event, or other event(s), simultaneously. One of the most widely used assumptions in quantitative analyses is that failures of components or sub-systems are independent of any other failures. This assumption greatly simplifies the analysis and is therefore very convenient. Although most essential and critical systems employ some sort of redundant technique, closer scrutiny soon makes it apparent that many of these systems have a ‘single element’ (or ‘common point’), the failure of which will cause multiple channel failures. This means that any conclusions drawn from these results need to be evaluated for sensitivity to common cause failures. We need to constantly ask ourselves whether this assumption is realistic and, if it is not, whether the analyses need to be modified to take account of any common cause failures. Example: common part failure Three totally independent flying control systems may merge together in a common part – the pilot’s control column. A failure of this common part causes total system failure. Example: DC10, Paris, 5 March 1974 Defective closing mechanism of cargo door caused it to detatch in flight. Sudden depressurisation led to disruption of floor structure, causing six passengers and parts of the aircraft to be ejected, rendering no. 2 engine inoperative and impairing the flight controls (tail surfaces) so that it was impossible for the crew to regain control of the aircraft. Example: common cause failures Σ A fire in a compartment might destroy all the channels of a system running through that compartment. Σ Contaminated hydraulic fluid could cause all the channels of the hydraulic system to fail. Σ Mechanical failures in an electrical loom (due to chafing and then short-circuit). Σ Identical software in a dual redundant system will fail when exposed to the same inputs; jamming of a mechanical system (either due to failure or due to FOD); overheating of avionic equipment, etc. Protection can be provided by careful design, as well as through the use of disconnect devices. Example: cascade failure A single failure may overload the remaining channels, thereby increasing the probability of their failure. For example, In a two-channel system, each channel with a failure rate of 1 in 1000 hrs, the probability of any one of the channels failing is 3 ¥ 10–3. So in a period of a million hrs, there will be 2000 failures. The probability of two channels failing is (10–3)2, i.e., one double failure in a million hrs. However, if the failure of the first channel will cause a ten-fold increase in the probability of the second channel also failing, then the probability of total failure is (1/1000) x (1/100), i.e., ten such double failures in a million hrs. From this example it is therefore evident that the combined failure rates increase proportionally with increase of risk under the added load and hence, it is important to take this into account and preferably design channels to cope with the added load without materially worsening the failure rate (Lloyd and Tye, 1995). Using mean time between failures data alone to obtain the multiple failure probability is thus bound to be flawed, because the mean time between failures does not take account of the ‘overstrain’ condition. Example: Concorde SST, Flight 4590, Paris, 25 July 2005 An initial minor failure (e.g. a deflated tyre) causes a cascade of events. The Concorde caught fire shortly after takeoff from Charles de Gaulle Airport on a charter flight to New York. The pilots lost control and the plane crashed into a hotel restaurant. Subsequent investigation revealed that a metal strip left on the runway by another plane gashed one of the Concorde’s tyres which blew out sending a piece of rubber into the underside of the wing which sent a shockwave which ruptured a seam in the fuel tank. An electrical wire severed by another piece of rubber sparked and ignited leaking fuel that started an uncontrollable fire. Power was lost to the No. 1 and No. 2 engines which led to loss of control of the aircraft and subsequent crash. The essence of the problem is that we cannot actually construct and operate absolutely independent systems which are not vulnerable to some sort of common failure. The challenge lies in: Σ identifying those parts of a system which are vulnerable to common cause failures Σ identifying all reasonable foreseeable sources of common cause failure Σ accounting for the probability of common cause failure is our safety justification for the application of the following defences: – segregation (i.e. mechanically and electrically) of redundant systems – use of dissimilar redundancy3 (e.g. VC10 flying control system, where the elevators and ailerons are powered by the main electrical systems (i.e. electrohydraulic actuators) and the tailplane and spoilers are powered by the main hydraulic system). 6.3.4 Wear-out vs. random failures Wear-out Wear-out occurs at the end of useful life. These modes are reasonably well understood and their rate of occurrence is generally considered to have a ‘bathtub’ characteristic (i.e. relatively high failure rate during both the early and late phase, with a middle portion of useful life where the rate is relatively low and constant as illustrated in Fig. 10.2). Random Random failures occur, as the name suggests, randomly and are the result of degradation mechanisms within the system. Often evaluated by means of failure rates (e.g. failures per hour of operation) or due to physical causes involving a range of mechanisms (e.g. lighting or problems during manufacture, installation or maintenance). Generally it is possible to quantitatively predict, with reasonable accuracy, failure rates for this type of failure. 6.4 Hazards of a normal functioning system We can distinguish between normal functioning and degraded functioning. 6.4.1 Normal functioning system The safety issues associated with the system when it is working correctly should not be neglected. In this context, ‘working correctly’ means that the way hardware and software (which have not failed) perform together as a system (including the people involved) results in an unsafe situation. Typical examples of a normal functioning system causing hazardous conditions include a missile system that locks on to the wrong target, unwanted operation of stick-pusher near the ground, pilot error, etc.). Example: DC8, Toronto, 5 July 1970 Preliminary information from the flight recorder indicates that, during the approachto- land, the approved procedure for arming the ground spoilers for automatic touch-down was not followed. For an undetermined reason the ground spoilers were prematurely deployed, momentarily, resulting in a rapid descent, heavy impact with the runway causing 109 fatalities and structural damage to the aircraft. The subject ‘pilot error’ is sufficiently wide to justify several volumes. However, although many accidents are attributed to pilot error, the arrangements of equipment (e.g. displays, controls, levers, switches) and their method of operation is often such that, taking account of human fallibility (or Murphy’s Law4), the accident was one day bound to happen (Lloyd and Tye, 1995) (see example on page 82). Errors classifed in terms of the part of the human information processing system at which the fault occurs (Edwards, 1999) are given in Table 6.2. Table 6.2 Human processing errors: Failure to detect signal Input overload; adverse environment Incorrect identification of signal Lack of differential cues; inappropriate expectation. Incorrect signal recalled Confusion in short-term memory; distraction before task completed Incorrect assessment of priority Values inadequately defined; complex evaluation required Wrong action selected Consequences of action misjudged; correct action inhibited Incorrect execution of action Clumsiness due to excessive haste; selection of wrong control device. Example: 747-400, Jeddah, 7 April 1999 The pilots failed to switch on their pitot/static heating systems as the 747-400 entered icing conditions not long after leaving Jeddah. Frozen pitot and static ports ‘robbed’ the aircraft of airspeed and altitude information leading to crash and death of two pilots and four cabin crew members (no passengers on board). The reports cited inattention by the crew, who were talking to the cabin crew on the flight deck at the time the flight entered icing conditions. The general approach to understanding the hazards associated with a ‘normal functioning system’ is to achieve a good understanding of the way the system performs and hence build confidence in its safety integrity. It should be noted that no specific measurements or numerical probabilities are produced, it is simply confidence from clear understanding. It follows that the methods (e.g. SFD, block text diagrams, SLD, etc., see Appendix A) used to achieve this understanding are not strictly formal and vary with the system complexity. The basic steps are: Σ Identify the top events (i.e. the feared event). Σ Delineate the system functions (hardware, software and human interactions) for each scenario (the way the system is prepared and used) to achieve an understanding of how they relate to each top event in turn. Σ Deduce the following: – probability of an accident occurring as a result of normal operation – the benefits of safety redundancy – any operator or maintainer activities which are vital for safe operation. Where an analysis identifies some indication to, and/or action by, the flight crew, cabin crew or maintenance personnel, the following activities should be accomplished: (i) Verify that any identified indications are actually provided by the system. (ii) Verify that any identified indications will, in fact, be recognised. (iii) Verify that any actions required have a reasonable expectation of being accomplished in a reasonable manner. 6.4.2 Degraded functionality/performance The performance of a system is the degree of accuracy with which it performs its intended function. Performance can be affected by the following factors: Σ Failure-free operation. When operating without failure the factors affecting performance may be the variation of tolerances within the system itself; the variations of aircraft response; the effect of environmental conditions (e.g. turbulence, windshear, temperature, icing, runway surfaces, etc.); and the variances of other influencing systems (e.g. ground systems which affect approach, navigation and auto-landing systems) (Lloyd and Tye, 1995). Lloyd and Tye (p. 119) arbitrarily allocated different reasons for performance variations into three main groups: (i) Those which directly affect the physical make-up of the system (e.g. manufacturing tolerances, maintenance adjustments, etc.). These can lead to variations in the response of the system to particular stimuli. (ii) The basic competence of the system in carrying out the job it is designed to do. (iii) Those which indirectly affect the way the system responds in given circumstances. These are largely environmental (in that temperature, vibration, etc., are prime contributors to system performance) and also include characteristics of input supplies (e.g. voltages, hydraulic pressures, pilot action/inaction, etc.). Σ Failures. A failure can result in degraded performance. These failures could be active or passive (see also section 6.3.1): – Active failures would result in immediate performance deviations. For example, the ability to maintain control after the loss of one engine on a multi-engined platform. – Passive (or dormant) failures could result in degraded performance without giving a definitive indication to the crew. This could go undetected until discovery during maintenance checks, or until discovered too late at a time when functionality is required. An example of the latter would be a dormant failure in an auto-land system where accuracy of the instrument landing system centre line may be compromised. Accidents produced by a lack of system performance have usually been in the field of powerplant or control systems performance (e.g. lack of sufficient controllability to recover from flight upsets) or navigation systems (e.g. automatic landing systems degraded due to variations produced by the ground equipment, accuracy of instrument landing system receivers, auto-pilot accuracy during instrument landing system approach). Minimum performance standards have been developed for many of these systems, either by the regulatory authorities (e.g. Federal Aviation Administration), or by organisations such as the Radio Technical Commission for Aeronautics (RTCA). To a large extent, system performance is not an airworthiness concern unless it affects an essential or flight-critical function. This is even more applicable to circumstances where the cues for pilot detection are uncertain, or if there is insufficient time for the pilot to react and recover from a performance deficiency. Degraded performance may require a reduced operating envelope (e.g. reduced speed or altitude) or reduced demand on the system by shedding loads (e.g. electrical) or by altering the flight plan. Alternatively, it may be practical to demonstrate that the combined probability of the failure and that of other conditions (e.g. gusts) necessary to produce a hazardous situation is acceptably remote. A method of performance analysis involves establishing a statistical distribution for each critical performance parameter (when carrying out the tasks assigned to it, the ‘output’ of a system can be expressed as statistical distribution which describes the probabilities that the system output will reach or exceed any particular values). From such a distribution determine the probability that the performance of the system will be such as not to create an unacceptable hazard/risk. In taking account of all these variables, it may be unreasonable to assume that each variable is at its most disadvantageous limit, so that it is necessary to take account of the statistical distribution of the variables in order to arrive at sensible conclusions. For more information on evaluating system performance, see Lloyd and Tye (1995, Chapter 8, pp. 60–67). 6.4.3 Unwanted operation A system can operate, and in doing so, cause a hazard because it does so when not required or expected. For example, during low-level flight, unwanted operation of the ‘stick shaker’ could be hazardous. Likewise the unwanted operation of a warning system when there is nothing wrong with the monitored system. It could either add to crew workload, or lead to the crew not trusting the warning system (like the boy that cried ‘wolf’ once too often). Example: Delta Airlines 737, September 2004. Salt Lake City The co-pilot suffered a burned retina when a high-power laser ‘painted’ his aircraft on final approach. The aircraft landed safely and the US Transport Security Administration is now investigating the case. Sourced from Aerospace International, Nov. 2004, p. 8 Note: High-power lasers are being used in missile protection systems such as DIRCM (directional infra-red countermeasures). As shown by this example, these lasers can cause eye injuries to third parties. The safety assessment therefore needs to consider the implication of unwanted operation (for instance via the functional hazard assessment as a specific functional failure mode applied to specific flight phases). The usual design practice (to ensure high system integrity) is to design parallel multiplex systems. However, when it is important to avoid unwanted operation, items may be put in series to avoid unwanted operation (Lloyd and Tye, p. 47). The second solution above has its limitations. For instance, the series system would also mean that if one detector fails to function, then the system may fail to operate when required to do so. Practical judgement will be required to balance the probability of failure to operate when wanted against the probability of operating when not required. It may be more desirable to add a ‘comparator’, which compares the results of the two detectors, monitors correct functionality and then prompts the stick shaker to respond. However, bear in mind that the comparator also has failure modes (which could include a dormant failure). 6.5 Systemic failures5 Systemic failures are due to human errors (e.g. mistakes, misconceptions, miscommunications, omissions) in the specification, design, build, operation and/or maintenance of the system. Errors in this case are taken to include both mistakes and omissions. Errors can be introduced during any part of the lifecycle and errors are caused by failures in design, manufacture, installation or maintenance. Systematic failures occur whenever a set of particular conditions is met and are therefore repeatable (i.e. items subjected to the same set of conditions will fail consistently) and thus apply to both hardware and software. It is difficult to quantify the rate at which systemic failures will occur and a qualitative figure based on the robustness of the development/build process is normally used. The probability of systemic failures is often evaluated by means of safety integrity (or development assurance) levels. Systemic failures are often seen as indefensible (i.e. should not occur) but are hard to prevent. Any system is vulnerable to the vague fallibility of human beings. As the level of complexity increases, the proportion of systemic failures tends to increase. They play a major part in accidents and may in themselves lead to: Σ errors by the crew because of poor arrangement of controls or instruments or warning systems Σ errors by the maintenance staff because of a lack of proper information or because the design allows incorrect assembly (i.e. by cross-connection). Systemic failures may arise from: Σ a concept which is inherently flawed (i.e. a bad idea) Σ specification (i.e. designing the wrong thing, e.g. due to omissions in the specification (see TWA 800 example on page 86)) Σ design or manufacture (i.e. building the thing incorrectly). For instance, failure to re-establish proper restraint of electrical cable looms relative to moving parts (such as control cables) has not only produced serious electrical failures (see Fig. 6.5), but has also resulted in severance of the control cables. Σ Use and maintenance (i.e. mistakes, poor procedures, violating designers’ intentions (see faulty rigging example on page 86)). Example: TWA 800, New York July 1996 On 17 July 1996 a 25-year old Boeing Model 747 aircraft was involved in an inflight break-up after takeoff from Kennedy International Airport in New York, resulting in 230 casualties. The accident investigation conducted by the National Transportation and Safety Board (NTSB) indicated that the centre wing fuel tank exploded due to an unknown ignition source. Although the ignition source could not be determined with certainty, the national transportation safety board determined that the most likely source was a short circuit outside the centre wing fuel tank that allowed excessive voltages to enter the tank through electrical wiring associated with the fuel quantity indication (FQIS). Opening remarks at the TWA800 hearing included (SFAR88): ‘…This investigation and several others have brought to light some broader issues regarding aircraft certification. For example, there are questions about the adequacy of the risk analyses that are used as the basis for demonstrating compliance with many certification requirements.’ Example: micro-switch rigging An inadvertent stick-pusher operation was caused by the faulty rigging of duplicated micro-switches (the function of which was to change the datum settings of the system relative to incidence). Sources of design/development error include: Σ failure to account for all likely environmental conditions (e.g. temperature effects, icing, etc). Failures caused by environmental effects may be minimised by proper design and installation, by experience of the components selected, by production quality control, and by appropriate environmental testing. However, should environmental failures occur, they could be common to all channels of a system employing similar hardware. Σ the poor segregation of critical systems so that cascade or other multiple failures can occur Σ the mixing of flammable substances and sources of ignition Σ the location of electrical equipment below sources of contamination (e.g. toilets, galleys, etc.). Σ software errors. Software does not ‘fail’ in the traditional sense of the word. If it does not perform its intended function, then a design error exists which must have been present since the software was first created. Software cannot directly cause harm (it is not toxic, does not have high levels of energy, etc.). Software can, however, contribute to accidents by causing failures through systems it controls and by misleading operators. The risk is increasing due to its growing scale and complexity, as well as playing an increasingly important role (e.g. authority). All software ‘failures’ are systemic failures.6 Σ manufacturing errors are potentially a prime source of common mode failures, particularly with electrical and avionic equipment. One of the objectives of the Safety Assessment should be to identify critical parts so that the manufacturing techniques and controls can be clearly specified. Manufacturing errors are basically caused by: Σ insufficient information on drawings (e.g. critical tolerances, stress relieving) Σ inadequate control of quality (e.g. not conforming to the design) Σ contamination (e.g. oil/grease in oxygen supply components) Σ damage (e.g. static electricity damage to circuit boards). Σ maintenance errors have been the root cause of many accidents. Some of the following examples could have been avoided by design precautions, others by imposed procedures or labelling: Σ incorrect assembly (e.g. cross connection, fitting of wrong part, fitting valves the wrong way round, etc.) Example: Viscount 732, 2 December 1958 Elevator spring tab operated in the reversed sense. This caused involuntary manoeuvres which overstressed the aircraft and caused the wing to break off. Work done to the spring tab mechanism during overhaul had been carried out incorrectly and the inspectors failed to observe the faulty operation of the tab. Σ carrying forward defects or the incorrect diagnosis of defects Σ leaving loose objects (e.g. tools) in places where they can cause damage, electrical shortage, or inhibit control surface movement Σ putting wrong fluids into vital systems Σ to lack of good housekeeping when making modifications and repairs (e.g. the leaving of swarf and loose rivets in fuel tanks) Σ changing the maintenance schedule and/or philosophy (see MD83 example below) Σ Damage inflicted during maintenance (see DC10 example below) Example: MD83, Alaska Airlines, Jan 2000 At 28,000 feet, the crew reported that they were unable to control the pitch of the aircraft. Descending through 23,000 feet, the crew reported that they had regained control, declared an emergency, and received vectors to land. Shortly thereafter, control of the aircraft was lost and the MD-83 was seen ‘tumbling, spinning, nose down, continuous roll, corkscrewing and inverted’. The aircraft crashed off Point Mugu in 650 feet deep water with loss of 88 lives. Probable cause was found to be a loss of airplane pitch control resulting from the in-flight failure of the horizontal stabiliser trim system jackscrew assembly’s acme nut threads. The thread failure was caused by excessive wear resulting from Alaska Airlines’ insufficient lubrication of the jackscrew assembly. Contributing to the accident were Alaska Airlines’ extended lubrication interval, which increased the likelihood that a missed or inadequate lubrication would result in excessive wear of the acme nut threads. Alaska Airlines also extended the end play check interval, which allowed the excessive wear of the acme nut threads to progress to failure without the opportunity for detection. Information tailored from: Σ http://aviation-safety.net/database/record.php?id=20000131-0 Σ http://www.airdisaster.com/cgi-bin/ view_details.cgi?date=01312000&reg=N963AS&airline=Alaska+Airlines Example: DC10, Chicago, 25 May 1980 national transportation safety board determined that the probable cause of the accident was the asymmetrical stall and the ensuing roll of the aircraft because of the uncommanded retraction of the left wing outboard leading edge slats and the loss of stall warning and slat disagreement indication systems. This resulted from maintenance-induced damage and led to the separation of the no. 1 engine and pylon assembly at a critical point during take-off. The separation resulted from damage caused by improper maintenance procedures which led to the failure of the pylon structure. Contributing to the cause of the accident were: Σ the vulnerability of the design of the pylon attachment points to maintenance damage Σ the vulnerability of the design of the leading edge slat system which produced asymmetry Σ deficiencies in the Federal Aviation Administration surveillance and reporting system which failed to detect and prevent the use of improper maintenance procedures Σ deficiencies in the practices and communication among the operators, the manufacturer and the Federal Aviation Administration, which failed to determine and disseminate the particulars regarding previous maintenance damage incidents Σ the intolerance of the prescribed operation procedures to this unique emergency. The probability of systemic failure cannot be determined. These risks cannot be quantified because no scientific law exists to characterise the vague possibilities of human beings in their intellectual endeavour (Murphy, 1991). Hazards due to systemic failures are dependent upon conditions that will not follow any predictable model or time based distribution. If a design deficiency exists it will manifest itself only when the circumstances are ‘right’, but equally it will always manifest itself when the circumstances are again ‘right’ (i.e. those systems subjected to the same conditions will fail consistently). Failures can be replicated and are predictable but not accurate. Design deficiencies can occur at any stage of the design and development activities, from the early contract negotiation phase where general ideas and assumptions are being discussed, through to the detailed design and testing phases. Areas of particular risk are: Σ designs which are very complex and hence difficult for one person to understand and probably impossible to test in all circumstances Σ designs which are perceived to be very simple and attract very little effort or interest Σ the interface between design areas, which is often constrained by departmental/ company organisation and systems that incorporate items previously developed and accepted, but are not fully understood when integrated into the new systems and its new circumstances. Do not forget to consider the behaviour of passengers. Many events such as the one below, which was probably caused by a cigarette, are predictable and have to be contained by careful design. Example: Boeing 707, Paris, 11 July 1973 The probable cause of the accident was a fire, which appears to have started in the wash-basin unit of the aft toilet. It was detected because smoke had entered the adjacent left toilet. The difficulty in locating the fire made the actions of cabin personnel ineffective. The flight crew did not have the facilities to intervene usefully (from the cockpit) against the spread of fire and invasion of smoke. The lack of visibility in the cockpit prompted the crew to decide on a forced landing. At touch-down the fire was confined to the area of the aft toilets. The occupants of the passenger cabin were poisoned, to varying degrees, by carbon monoxide and other combustible products. Unfortunately, identifying all the circumstances leading to systemic failures is seldom feasible. Design deficiencies cannot be eliminated, but they may be minimised. As we cannot cope with design deficiencies by ‘analysis’ and ‘comparison’ with an acceptable requirement, a radical approach is necessary. For this reason the required level of protection against systemic failures uses the concepts of: Σ safety integrity level (SIL) (refer IEC 61508), or Σ development assurance levels (see RTCA-DO-178B), or Σ checklists (e.g. common cause analysis and PRA, see Annex A), and Σ regulatory requirements (see Chapter 3). Safety integrity levels (SILs) and design assurance levels (DALs) are allocated to systems commensurate with the significance of any residual malfunctions. Essentially we establish the safety significance of the design and initiate a commensurate amount of ‘design rigour’ to minimise the risk of malfunction and to concentrate effort where it is most needed. In accordance with IEC 61508, the required integrity level shall be inherited by (or passed down to) the components that implement the function. The initial SIL allocation shall consider: Σ the degree of control that the function has over the system Σ the independence of the other functions provided for the purpose of preventing the hazard occurring directly Σ the time allowed for independent safety systems to intervene and mitigate the hazard Σ the severity caused by the loss of function, degraded function, function provided when not required or the function provided inadvertently. Guidelines such as radio technical commission for aeronautics DO-178B and IEC 61508,7 contain much useful information which this book shall not attempt to duplicate, other than summarising that: Development assurance is a process involving specific planned and systematic actions that together provide confidence that errors or omissions in requirements or design have been identified and corrected to the degree that the system, as implemented, satisfies applicable certification requirements. All systems are vulnerable to systemic failures and hence any safety analysis process should take into account their occurrence. 6.6 Safety assessment tools and techniques There are a variety of tools and techniques available for assessing safety, which can broadly be classified into two categories. ‘Top-down’ analysis starts by identifying the accidents or failure conditions to be investigated, and then proceeds to derive the combination of failures and/or events which can produce them. ‘Bottom-up’ analysis starts with hardware failure modes which can occur and analyses the effects of these on the system and aircraft in order to determine the hazardous conditions which can occur. The objectives of these techniques fall into three broad categories: 1. Hazard identification techniques. 2. Causal techniques (looking back to see how hazards and accidents might possibly be caused). 3. Consequence techniques (looking forward to see what an event or situation might develop into). Some of the techniques available serve more than one purpose; they not only identify hazards but examine consequences too. Nevertheless, it is vital to choose the correct combination of techniques and to tailor them to the particular system being assessed. Why do we need all these assessment techniques? The reasons include: Σ first, highly integrated and complex systems present greater opportunities for development error and undesirable, unintended effects.8 It is generally not practical – and may not even be possible – to develop a finite test suite which conclusively demonstrates that there is no hazard residue Σ the second reason is due to the variety of hazards and their causes. This was explored in the previous paragraphs which clearly show that we need an arsenal of analytical techniques, each with their own strengths and weaknesses. There is bound to be some overlap but that should only strengthen the safety argument and should not lead to additional work if cross-referenced properly Σ finally, tools and techniques used should add value to the process by improving understanding of systems and its hazards. The table in Appendix A provides a brief synopsis of the tools and techniques available. It also includes an indication (often very subjective) of the main advantages and limitations of each. 6.7 Discussion A safety assessment is an iterative process within the overall development of the system. The techniques and approaches touched on in this section can be used to different depths at different stages in the development process. Different projects use a variety of safety tools/techniques in numerous combinations. There is much guidance material and many standards available on this subject (e.g. society of automotive engineers ARP4761, DEF STAN 00-56, MIL-STD-882, etc.). Unfortunately, safety assessment tools and techniques are not agreed upon before contract closure and are used ‘after the fact’ to satisfy safety questions, and not as a useful tool to influence and optimise the design. Tools and techniques used should add value to the process by improving understanding of systems and hazards. For this to occur we need to: Σ be clear what the output should be by identifying safety effects clearly so as to provide a set of meaningful, useful recommendations Σ pick the correct tools for the job, and use these tools at the appropriate stage in the process Σ avoid overcomplication and to try to be as consistent as possible Σ beware of increasing common mode failures with increased system complexity Σ apply considerable judgement. Do not regard it as a ‘write only’ exercise. An assessment to identify and classify failure conditions is necessarily qualitative. On the other hand, an assessment of the probability of a failure condition may be either qualitative or quantitative. The extent to which structured methods/tools/techniques are applied is a function of the system’s complexity and the system failure consequence, and will be more rigorous with increasing system complexity and severity of consequence (ACJ 25.1309 para 7.e). An analysis may range from a simple report that interprets test results or compares two similar systems to a detailed analysis that may (or may not) include estimated numerical probabilities. The depth and scope of an analysis depends on the types of functions performed by the system, the severities of failure conditions, and whether or not the system is complex. In considering the likely failure sequences, Lloyd and Tye (1995, p. 75) remind us to take account of the fact that, following a series of failures, the pilot himself will be under increased stress and may be more likely to make mistakes. Regardless of its type, an analysis should show that the system and its installation could tolerate hazards and failures to the extent that the applicable safety targets are accomplished in an auditable fashion. Be careful of using too many techniques, as this could cause conflicts and confusion. However, ensure that you have used a sufficient range of tools that will ensure that something is not missed or overlooked. Remember that it is not the identified hazard which is the problem. If you have identified it, you can measure it, you can fix it, and you can control it. It is the unidentified hazard that causes concern. A hazard not identified is a hazard not managed. Do not take it too far (i.e. too low in system decomposition), as this will produce lots of output with little extra understanding. It is better to do it well, with insight, at high level, than merely mechanically at more detailed level. Be careful of rigid, restrictive models and methods which can be counter-productive because they involve an inevitable simplification of the project domain and discourage subsequent free thought about the domain. Using a rigid model simply shifts the real analysis work backwards to the creation of the model itself. Sometimes exaggerated claims for the benefits of certain methods are made, but no particular method should ever be seen as a panacea. For example, a formal method may be just one of a number of techniques that, when applied judiciously, may result in a system of high integrity. Particular techniques and notations should not be applied merely as a means of demonstrating a company’s ability. Similarly, they should not be used to satisfy management whim, or merely as a result of peer pressure. Before a particular approach is applied, it should be determined whether it is really necessary. Potential reasons may be to: Σ increase confidence in the system Σ satisfy a particular standard required by procurers Σ airport information desk in tackling complexity, etc. The identification of the failures and hazards should be carried out by a crossfunctional team of pilots, engineers, logisticians and maintenance personnel, working in a series of facilitated brain-storming workshops. Ideally, all those involved must be current practitioners of the process under consideration, and involve a range of seniority and experience levels. Complementary methods should not be dismissed lightly. Despite the mathematical basis of formal methods, they have no guarantee of correctness; they are applied by humans, with all the potential for error that this brings. Since system development is essentially a human activity, most methods depend on the quality and suitability of the personnel involved in applying the techniques. All of these require considerable judgement, and the careful identification and application of assumptions.