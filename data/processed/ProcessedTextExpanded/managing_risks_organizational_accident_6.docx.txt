Title: Managing The Risks Of Organizational Accidents – Chapters 4 – 5 – 6 Author(s): James Reason Category: Management, Risks Tags: Risks, Organization, Accidents, Aircraft Chapter 4 The Human Contribution The Human Factor People design, build, operate, maintain, manage and defend hazardous technologies. It is hardly surprising, therefore, that the human factor plays a major part in both causing and preventing organizational accidents. This chapter explores the human contribution to the safety-or otherwise-of complex well, defended systems. It has become fashionable to claim that human error is implicated in 80-90 percent of all major accidents. While probably close to the truth, this statement adds very little to our understanding of how and why organizational accidents happen. In the first place, it could hardly be otherwise, given the range of human involvement in hazarduous systems. Second, the term 'human error' conveys the impression that all unsafe acts can be lumped into a single category. But errors take different forms, have different psychological origins, occur in different parts of the system and require different methods of management. In addition, this assertion fails to acknowledge that people's behavior within hazardous systems is far more constrained than it is in everyday life. The actions of pilots, ships' crews, control room operators and the like are tightly governed by managerial and regulatory controls. These administrative controls form a major part of any hazardous system's defenses and are of two main kinds.1 • External controls are made up of rules, regulations, and procedures that closely prescribe what actions may be performed and how they should be carried out. Such paper-based controls embody the system's collective wisdom on how the work should be done. • Internal controls derived from the knowledge and principles acquired through training and experience. External controls are written down and need to be close at hand when people are carrying out the prescribed activities. Internal controls, on the other hand, exist mainly within an individual's head. Any attempt to classify organizational behaviors must begin by considering how various combinations of administrative controls work to confine the natural variability of human action to safe and productive pathways. The Varieties of Administrative Controls Administrative controls range from the mainly prescriptive to the largely discretionary. The former depend primarily upon external procedures, while the latter are provided by internalized knowledge and experience-or, in a word, training. Between the two are various blends and mixtures. This continuum of control modes is summarized in Figure 4.1. A mainly prescriptive control mode is shown in Figure 4.2. Figure 4.3 illustrates the opposite end of the dimension, being predominantly discretionary. Figure 4.4 shows a mixture of control modes. It can be seen from Figures 4.2-4.4 that the different administrative control modes share three common features: • organizational standards and objectives that define the system's production and safety goals and also specify the standards of performance required to achieve those goals, • human performance, or the activities carried out by the system's human elements and • the process being controlled-this will obviously vary according to the nature of the technology involved. In Figure 4.2, human performance - and hence the process being controlled-is strictly standardized by rules and procedures. In the language of the control theorists, these are feedforward devices, with the behavioural means to achieve organizational goals being specified in advance and delivered through rules and procedures, regardless of the local conditions. In the realm of safety-related actions, however, there is an intermittent feedback loop. As discussed in Chapter 3, the response of many bureaucratic organizations to their occasional accidents and incidents is to 'write another procedure' prohibiting those actions implicated in the last event. The result is that the body of procedures increases in an additive fashion as shown in Figure 4.2. The feedback output control shown in Figure 4.3 defines the opposite end of the prescriptive-discretionary continuum. Here, it is presumed that the workforce understands the organizational goals and possesses the internalized knowledge, skills and abilities to achieve them, using a variety of means. Output measures are frequently compared with organizational objectives. Deviant performance is adjusted to reduce the discrepancy and congruent performance is reinforced. Within any given technology or system, the balance between feedforward and feedback control modes will depend on several factors: • the stage reached in the organization's life history, • the type of work or activities, • an individual's position within the hierarchy, and • the amount of training given to individuals. The Stage Reached in the Organization's Life History In the early stages of a technology or manufacturing process, work will probably be governed largely by trial and error, or by feedback- driven controls. During this initial period, however, managers and supervisors will be noting those occasions when actions and conditions yield precisely the desired results. In other words, they will be looking out for zero deviations between output measures and or- ganizational goals. When these are observed often enough, it is probable-assuming that the process is reasonably predictable-that they will become expressed as procedures. This will give rise to mixed controls of the kind summarized in Figure 4.4. As organizational learning proceeds and the work becomes increasingly standardized, so feedforward controls will come to predominate over feedback controls. It is, after all, much cheaper to have the work performed by a relatively untrained labour force con- trolled by procedures than it is to rely upon the judgement of experienced and highly trained individuals. In certain professions, the transition from feedback to feedforward may take a very long time. In medicine, for example, doctors have been trained for many centuries to function on a largely discretionary basis. Each patient's illness was thought to pose a unique set of clinical problems, requiring diagnosis and treatment to be determined by a combination of clinical experience and knowledge-based principles. For a variety of political, economic, legal and managerial reasons, however, this traditional approach is now changing to a system in which healthcare is increasingly being delivered on the basis of condition-specific procedures. It is curious that such a bastion of discretionary action as medicine should be moving towards a feedforward mode of control when many other hitherto rule-dominated domains-notably railways and oil exploration and production are shifting towards performance-based controls and away from prescriptive ones. Type of Activity Regardless of the stage of system development, the balance between process (feedforward) controls and output (feedback) controls will be largely shaped by the nature of the tasks that the organization undertakes. Certain activities readily lend themselves to being proceduralized; others do not. Charles Perrow identified two aspects of organizational activity that determine the degree to which it can be pre-programmed.3 • The number of 'exceptional cases': That is, the degree to which surprises, novel situations and unexpected events are likely to arise during the course of the work. • The nature of the 'search process' required to deal with problems. Some problems may be easily resolved by analysis and the application of rule-based solutions; others are little under- stood and require extensive knowledge-based processing. Figure 4.5 fleshes out the characteristics of these different kinds of organizational activity and Figure 4.6 gives examples of each of the four task types. Number of exceptional cases (i.e., new events, situations and problems) Nature of search for problem solutions: Easy – Few: Tasks routine, repetitive, well-structured and predictable. Pre-programmed process control by rules and procedures possible Easy – Many: Tasks non routine, but the many exceptional cases are relatively simple to analyze. Requires mixture of prescriptive and discretionary performance control. Hard – Few: Work routine, but problems are sometimes vague and poorly conceptualized. Requires a mix of prescriptive control by rules and procedures and discretionary performance by the individual. Hard – Many: Tasks non-routine, poorly structured and unpredictable. Rules and procedures not applicable. Task performance at the discretion of the individual. Number of exceptional cases (i.e., new events, situations and problems) Nature of search for problem solutions Easy – Few: Production lines Railways Postal services Construction Traditional banking Road haulage, etc. Easy – Many: Architecture Scientific research Project management Maintenance & repair Oil exploration Police work, etc. Hard – Few: Nuclear power plants Chemical process plants Modern aircraft Advanced manufacturing R&D organizations Anaesthesia Geriatric medicine, etc. Hard – Many: Modern military operations investment banking Macro-economics Politics Recovering beyond design basis accidents Crisis management, etc. Level within the Organization The higher an individual's position within the organization, the harder it is to proceduralize the job. The task of a train driver, for example, is to start, run and stop the train according to the dictates of the track signalling system and the timetable. How this should be done is clearly prescribed by the railway company's procedures. But there are no procedures setting out how the senior management team should achieve the company's strategic goals. In relatively bureaucratic organizations, therefore, one would expect top management to operate mainly by feedback-driven output controls, with the reverse being true for those at the 'sharp end'. Middle management and first-line supervisors would occupy intermediate positions along the prescriptive-discretionary continuum. The wide variation in the degree to which individual jobs can be proceduralized has implications not only for the type of controls that are applicable but also for the ease with which errors and violations may be identified or even defined. If a train driver passes a signal at danger, he or she has committed either an unwitting error or a deliberate violation. Both errors and violations are defined in terms of a deviation of action from some intended or required standard of performance. At the 'sharp end' of any industry, there is usually little doubt about the yardsticks from which such departures are gauged. But who is to say whether a top-level strategic decision is correct or not? Such matters can usually only be settled long after the fact, and even then the question of whether or not a given decision was wrong is often a matter of debate. The Trade-off Between Training and Procedures The longer and more intensive an individual's training, the less likely is that person to be governed by rigid feedforward controls, and conversely. There are, of course, exceptions. Airline pilots receive extensive training, both initially and throughout their flying careers, yet their activities are highly proceduralized and regulated. The same is true of most branches of the professional military. Military cultures differ in the autonomy granted to junior commanders and non-commissioned officers. The fighting quality of the German Army-from the Franco-Prussian War in 1870, through the First World War, to the defence of Germany in 1945-was legendary. German ground troops inflicted half as many more casualties than they received from the opposing Allied forces throughout the Second World War, even though the latter had, in later years, mas- sive air superiority over the battlefield. Against the Russians the Wehrmacht inflicted, on average, six times as many casualties as they received.4 Many authorities have speculated on the basis of this military superiority. Was it due to the courage, quality and tenacity of individual German soldiers, or did it stem from the character of their military institutions? While not denying the former possibility, most writers favour the organizational hypothesis. It has been convincingly argued that Germany created a more effective military organization than was achieved by her enemies. This effectiveness had many facets, but one is of particular relevance here. In its training of junior leaders, particularly NCOs, the Wehrmacht followed the longstanding principle of Auftragssystem (literally 'mission system'). Its essence was that a subordinate commander should be able to pursue the tactical goals of his superiors with or without orders. This presupposed a considerable degree of initiative and tac- tical understanding on the part of these junior leaders. As Alistair Horne observed: Far from being a horde of rigid and inflexible robots (which was always one of the most insidious of the Allied misconceptions about the Germans), the Wehrmacht thus had a far greater ability to react or to regain the initiative-especially in a moment of reverse-than was possessed particularly by the British Army of 1944.5 If one substitutes the title 'supervisor' for 'NCO' and considers safety rather than war, it is interesting to see whether the 'mission system' idea can be used to improve the guidance of safe behavior in hazardous technologies. We will return to this issue of decentralization in Chapter 9. 50 far, we have been looking at procedures from an organizational perspective. Now we need to take a more psychological stance and consider some of the ways in which rule-related actions can be distinguished. We begin by identifying three levels of human performance. Three Levels of Performance Figure 4.7 summarizes the main distinctions between three levels of human performance: the skill-based (5B), rule-based (RB) and knowledge-based (KB) levels. These performance levels, first introduced by Jens Rasmussen,6 are distinguished by both psychological and situational variables which, together, define an 'activity space' on to which the three performance levels can be mapped. Human beings control their actions through various combinations of two control modes-the conscious and the automatic. The conscious mode is restricted in capacity, slow, sequential, laborious, er- ror-prone, but potentially very smart. This is the mode we use for 'paying attention' to something. But attention is a limited resource, if it is focused upon one thing, it is necessarily withdrawn from other things. The automatic mode of control is the opposite in all respects. It is largely unconscious-we may be aware of the product (a word, an action, an idea, a perception), but not of the process that created it. The automatic mode is virtually limitless in its capacity-at least, no psychologist has yet discovered these limits. It is very fast and operates in parallel-that is, it does many things at once rather than one thing after another. It is effortless and essential for handling the recurrences of everyday life. But it is a highly specialized community of knowledge structures. It knows only what it knows; it is not a general problem solver like consciousness. Naturally, we prefer to operate in the automatic mode whenever possible. The second dimension defining the activity space is the nature of the immediate situation. The two extremes of this dimension are highly familiar everyday situations and entirely novel problems, with trained-for problems-or ones for which procedures have been writ- ten-lying in between. The three performance levels can be summarized as follows: • At the skill-based (SB) level, we carry out routine, highly-practiced tasks in a largely automatic fashion with occasional conscious checks on progress. This is what people are very good at most of the time. • We switch to the rule-based (RB) level when we notice a need to modify our largely preprogrammed behavior because we have to take account of some change in the situation. This problem is likely to be one that we have encountered before, or have been trained to deal with, or which is covered by the procedures. It is called the rule-based level because we apply memorized or written rules of the kind-if (this situation) then do (these actions). In applying these rules, we operate by automatically matching the signs and symptoms of the problem to some stored knowledge structure. We may then use conscious thinking to verify whether or not this solution is appropriate. • The knowledge-based (KB) level is something we come to very reluctantly. Only when we have repeatedly failed to find some pre-existing solution do we resort to the slow and effortful business of thinking things through on the spot. Given time and a forgiving environment to indulge in trial-and-error learning, we often come up with good solutions. But people are not usually at their best in an emergency-though there are some notable exceptions. Quite often, our understanding of the problem is patchy, inaccurate or both. Furthermore, consciousness is very limited in its capacity to hold information; it can store no more than two or three distinct items at a time. It tends to behave like a leaky sieve, allowing things to be lost as we turn our attention from one aspect of the problem to another. In addition, we can be plain scared, and fear like other strong emotions has a way of replacing reasoned action with 'knee jerk' or over-learned responses. This is represented in Figure 4.7 by the bottom right-hand corner of the activity space. It must be emphasized that these performance levels are not mutu- ally exclusive. All three can coexist at the same time. Consider the process of driving a car. Control of the speed and direction of the car is carried out at the SB level. Negotiating other drivers, pedestrians and the rules of the road occurs at the RB level. However, at the same time that both of these levels are in play, we could also be brooding about some new and difficult life problem at the KB level. Under- standing these performance levels helps us to classify the varieties of errors and violations, as discussed below. Errors and Successful Actions Human error can be defined as the failure of planned actions to achieve their desired ends-without the intervention of some unforeseeable event.7 The rider is important because it separates controllable action from either good or bad luck. An individual who is struck down by a piece of returning space debris will probably not achieve his or her imme- diate goal, but this could hardly be termed an error. Similarly, someone who slices a golf ball so that it hits a tree and then bounces fortui- tously on to the green may achieve their purpose, but their actions are still erroneous. There are three elements to this definition: a plan or intention that incorporates both the goal and the means to achieve it, a sequence of actions initiated by that plan, and the extent to which these actions are successful in achieving their purpose. Logically, actions may fail to achieve their goal for one or other of the following reasons: • The plan is adequate, but the actions fail to go as planned. These are unintended failures of execution and are commonly termed slips, lapses, trips or fumbles. Slips relate to observable actions and are commonly associated with attentional or perceptual failures. Lapses are more internal events and generally involve failures of memory. • The actions may conform exactly to the plan, but the plan is inadequate to achieve its intended outcome. Here, the failure lies at a higher level-with the mental processes involved in assessing the available information, planning, formulating intentions, and judging the likely consequences of the planned actions. These errors are termed mistakes, and have been further divided into two subcategories: rule-based mistakes and knowledge-based mistakes. Rule-based mistakes involve either the misapplication of normally good rules, the application of bad rules, or the failure to apply a good rule (a violation). Knowledge-based mistakes occur when we have run out of prepackaged solutions and have to think out problem solutions on line. This, as discussed above, is a highly error-prone business. The various error types are summarized in Figure 4.8. As mentioned earlier, all errors involve some kind of deviation. In the case of slips, lapses, trips and fumbles, actions deviate from the current intention. In the case of mistakes, however, the departure is from some adequate path towards the desired goal. Violations and Compliant Action Violations are deviations from safe operating procedures, standards or rules Such deviations can be either deliberate or erroneous (for example, speeding without being aware of either the speed or the local restriction). However, we are mostly interested in deliberate violations, where the actions-though not their possible bad consequences-are intended. These non-malevolent acts should be distinguished from sabotage in which both the act and the damaging outcome are intended. Three major categories of safety violation have been distinguished: routine, optimizing and necessary violations. In each case, the decision not to abide by safe operating procedures is shaped by both organizational and individual factors, though the balance of these influences varies from one type of violation to another. Routine violations typically involve corner-cutting at the skill-based level of performance-taking the path of least effort between two task- related points. These short-cuts can become an habitual part of the person's behavioural repertoire, particularly when the work environment is one that rarely sanctions violations or rewards compliance. Routine violations are also promoted by 'clumsy' procedures that direct actions along what seems to be a longer-than-necessary pathway. Optimizing violations-or violating for the thrill of it-reflect the fact that human actions serve a variety of motivational goals and that some of these are quite unrelated to the functional aspects of the task. Thus, a driver's functional goal is get from A to B, but in the process he or she can optimize the joy of speed or indulge aggressive instincts. Tendencies to optimize non-functional goals can become a part of an individual's performance style. They are also characteristic of particular demographic groups such as young male drivers. Whereas routine and optimizing violations are clearly linked to the attainment of personal goals (that is, least effort and thrills), necessary violations have their primary origins in particular work situations. Here, non-compliance is seen as essential in order to get the job done. Necessary violations are commonly provoked by organizational failings with regard to the site, tools or equipment-see the shunting example given in Chapter 3. In addition, they can also provide an easier way of working. The combined effect of these two factors often leads to these violations becoming routine rather than exceptional. Correct and Incorrect Actions Here, the terms' correct' and 'incorrect' relate to the accuracy of risk perception. A correct action is one taken on the basis of an accurate risk appraisal. An incorrect action is one based upon an inaccurate or inappropriate assessment of the associated risks. Successful actions-with respect to personal goals-need not necessarily be correct actions-with regard to the accuracy of the risk assessment. As indicated earlier, driving at 100 mph could achieve the personal goals of arriving sooner and of experiencing the thrill of speed, but it is clearly not 'correct' given the increased risks associated with a relatively unknown physical regime and its reduced error margins. Similarly, compliance is not automatically 'correct', nor a violation incorrect. It all depends on the local conditions and the adequacy of the procedures. What is 'correct' is often unknowable in advance, and frequently unknown at the time. In principle, however, it could be expressed in probabilistic terms. For example, the likelihood of being involved in a fatal accident while travelling within the speed limit on a British motorway can be assessed from data relating accident frequencies to miles of exposure.9 Similarly, the extent to which this risk is in- creased through exceeding the speed limit by varying amounts can also, in principle, be determined. The Quality of the Available Procedures So far, we have looked at rule-related behaviours from both an organizational and a psychological perspective. Now we will adopt a control theory perspective and consider the effectiveness of safety procedures. There are three possibilities: good rules, bad rules or non-existent rules-no rules. Here, 'good' entails both correctness and the furtherance of acceptable personal goals (that is, efficiency, least effort). Bad rules are either inappropriate for the particular circumstances, or incorrect, or both. Unlike the procedures governing productive activities, those writ- ten to ensure safe working suffer from a lack of requisite variety. In virtually all hazardous operations, the variety of possible unsafe behaviours is very much greater than the variety of required productive behaviours. Thus, while it may be appropriate to control productive performance through feed forward prescriptions (see again Figure 4.2), the requisite variety of the procedures necessary to gov- ern safe behavior will always be less than the possible variety of unsafe situations. As the control theorist, Ross Ashby, put it, 'only variety can destroy variety'. In other words, only variety in the con- trolling agency can reduce variety in the to-be-controlled outcome. We can illustrate this problem with a simple domestic example. The procedures necessary to guide the preparation of, say, minestrone soup can be conveyed in a few sentences. But the procedures necessary to guarantee that this task will be performed with absolute safety could fill several books, and even then it is unlikely that all the possible hazards and accident scenarios would have been covered. Of course, we can anticipate some of the probable hazards (harmful bacteria, sharp knives, scalding, electrocution, inflammable and poisonous gases and so on) and seek to regulate our cook's behavior with regard to these obvious dangers. And this is what organizations try to do in relation to specific operational hazards. But there is no way in which all the possible combinations of dangers and their related accident scenarios could be anticipated. In essence, therefore, wholly safe be- haviour can never be controlled entirely by feed forward prescriptions. There will be always be situations that are either not covered by the rules or in which they are locally inapplicable. In other words, there will always be 'bad rule' situations and 'no rule' situations. This does not mean, of course, that organizations should give up the attempt to formulate safety rules. Not only are such rules important for guiding safe behavior in relation to identified and understood hazards, they also constitute an important record of the organization's learning about its operational dangers. Since people change faster than jobs, such a record is crucial for the dissemination of safety knowledge throughout the system. But the requisite variety problem means that this collection of safe operating procedures will never be wholly comprehensive nor universally applicable. Six Kinds of Rule-related Behaviour On the basis of the various distinctions discussed above, we can now identify six types of rule-related behavior, as shown in Figure 4.9. Here, there are three kinds of rule-related situations and two kinds of performance. In the latter case, 'correct performance' refers to actions that are both correct (in regard to risk assessment) and successful (in achieving personal goals). 'Erroneous performance', on the other hand, refers to actions that are both incorrect and unsuccessful. The main characteristics of these six behaviours are summarized below, and real-life examples of these actions are given in the next section. • Correct compliance: correct (and safe) performance achieved through adhering to appropriate safety rules. • Correct violation: correct performance achieved by deviating from inappropriate rules or procedures. • Correct improvisation: a course of action taken in the absence of appropriate procedures that leads to a safe outcome. • Misvention: behavior that involves both a deviation from ap- propriate safety rules and error(s), leading to an unsafe outcome. • Mispliance: behavior that involves mistaken compliance with inappropriate or inaccurate operating procedures, leading to an unsafe outcome. • Mistake: an unsafe outcome resulting from an unsuitable plan of action carried out in the absence of appropriate procedures (that is, a knowledge-based mistake). There is one category of unsafe actions that is not captured by the logic of Figure 4.9, and that is the incorrect, but successful, violation of good rules. Here, personal goals are achieved as planned despite faulty hazard assessments. These successful violations-not the same as correct violations-are the breed stock from which occasional misventions emerge. Successful violations, though often inconsequential in themselves, create the conditions that promote dangerous misventions. These will include overconfidence in personal skills and a chronic underestimation of the hazards. What are the relative frequencies with which these various types of behavior are likely to occur? Since safety rules usually evolve by a process of natural selection, we can reasonably expect that good rules will outnumber bad rules, and that 'no rule' situations will be rare. It is also highly probable that compliance will occur more frequently than non compliance. Thus, correct compliance is likely to be the commonest mode of behavior with correct violations, misplaces and improvisations -- either successful or unsuccessful- as the least frequent types. The evidence from field studies indicates that successful, but incorrect, violations are relatively commonplace, although they number far fewer than correct compliances. Misventions would comprise a much smaller subset of successful but incorrect violations. Some Real-life Examples Misventions Chemobyl was an accident initiated almost entirely by misventions. Of the seven well-documented unsafe acts leading up the explosion of the Ukrainian RBMK nuclear reactor in April 1987, six were misventions of one kind or another. The first (and perhaps most critical) of these was the operators' mistaken decision to continue the testing of the voltage generator even though an initial operating error had caused the power level to fall to 7 per cent full power. The characteristics of the RBMK reactor were such that operations at these low-power regimes created a positive void coefficient in the reactor's core. This can lead to runaway reactivity. The station operating procedures strictly prohibited any operations below 20 percent full power (the initial intention was to run the experiment at 25 per cent full power). Subsequently, operators successively switched off various engineered safety systems in order to complete the experiment, and in so doing made the subsequent explosions inevitable. Here was a sad and remarkable case in which a group of highly motivated, prize-winning operators managed to destroy an elderly, but relatively well defended, reactor without the assistance of any technical failures. Successful Compliance and Mistakes Successful compliance is generally not recorded since it is rarely implicated in an accident. Recently, however, the US Nuclear Regulatory Commission sponsored a human factors inspection programme into the circumstances surrounding a number of potentially significant operational events in US nuclear power plants Of the 21 inspection reports, approximately half were designated as successful recoveries and the remainder as less successful recoveries. All events ended safely, but in 11 cases, the operators showed confusion or uncertainty as to the causal conditions and made errors in their attempts to restore the plant to a safe state. In contrast, the actions of the successful crews were exemplary and, on occasions, inspired. An important discriminator between the successful and less successful recoveries was the state of the plant when the off-normal conditions arose. In most of the successful recoveries, the plants were operating at or near full power. This is the 'typical' condition for a nuclear power plant-pushing out megawatts to the grid. It is for this condition that most of the emergency operating procedures are written. Nearly all successful recoveries were facilitated by these procedures and their associated training. The less successful recoveries, on the other hand, occurred mostly during low-power or shutdown conditions. These are perfectly normal plant states-necessary for maintenance, repairs and refuelling but they constitute a relatively small part of the total plant life history. Few procedures are available to cover emergencies arising in these atypical states. As a result, the operators had to 'wing it' and, on several occasions, made errors that exacerbated the orig- inal emergency and delayed recovery. The crews operated outside the rules because no rules were provided for handling emergencies in these still dangerous plant conditions. As a result, they were forced to improvise in situations they did not fully understand. While not ultimately unsuccessful (the plants were eventually restored to a safe condition), many of their actions along the way fitted the mistake category, identified in Figure 4.9. Mispliances and Successful Violations The Piper Alpha disaster on 6 July 1988 provided examples of both mispliances and successful violations. The emergency procedures required platform personnel to muster in the galley area of the accommodation. Ed Punchard describes their behavior: All over the rig, people were instinctively following their training and emergency instructions. In the absence of any form of announcement, most were trying to make their way to the galley to muster, have a head count, and take instructions. After all, that was what they were trained to do. Tragically, the accommodation was directly in the line of the fireball that erupted at 11.22 pm. Most of those who complied with the emergency procedures were burned or suffocated to death. At the time when the first explosion occurred (around 9.58 pm), Ed Punchard, the diving superintendent, was in a diving office on one of the lower decks of the platform with some of his diver colleagues. Their first action was run up the stairs to the 85-foot level, hoping to reach the lifeboat on the 107-foot level. On the way, they encountered thick smoke and heard the sound of escaping gas, so they started to descend and, with the help of a rope, reached the spider-deck on the north-west leg. From there, Punchard climbed down a steel ladder and jumped on to a rescue boat. Whether the manner of his escape was an act of deliberate violation or sheer necessity is not clear. What is evident, however, is that those people who deviated from the mustering instructions formed the majority of the survivors. Military history is rich in both mispliances and successful violations, though only the latter tend to be celebrated, such as Nelson's violation at Copenhagen when he chose to scan the flagged order to disengage from the enemy holding the telescope to his blind eye. Of course, had he subsequently gone on to lose the battle, history would have viewed such a misvention in a different light. Successful violations are often taken as the mark of the great commander, while mispliances are seen as characteristic of inferior ones. Sun Tzu, in his 2500-year-old treatise on 'The Art of War', recommended judicious violations: If fighting is sure to result in victory, then you must fight, even though the ruler forbid it; if the fighting will not result in victory, then you must not fight even at the ruler's bidding. One of the most dramatic examples of successful improvization occurred on 19 July 1989 aboard United 232, a McDonnell Douglas DC-10, commanded by Captain Alfred C. Haynes. While flying at cruise altitude, the fan rotor of the tail-mounted number two engine disintegrated and cut through all three of the aircraft's hydraulic systems. The probability of losing all three hydraulic systems was considered by the designers to be less than one-in-a-billion (10-9) and no emergency procedures were available to cover this almost un- thinkable possibility. Al Haynes described their plight as follows: That left us at 37 000 feet with no ailerons to control roll, no rudders to co-ordinate a tum, no elevators to control pitch, no leading edge de- vices to help us slow down, no trailing edge flaps to be used in landing, no spoilers on the wings to slow us down in flight or help braking on the ground, no nosewheel steering and no brakes. That did not leave us a great deal to work with.16 Forty-five minutes later, the aircraft crash-landed at Sioux City, Iowa. Of the 285 passengers and 11 crew members on board, 174 passengers and 10 crew survived. About 14 seconds into the emer- gency, the aircraft had reached 38 degrees of bank to the right, on its way to rolling over on its back and then falling out of the sky. At this point, Al Haynes took control of the aircraft, slammed the number one (left engine) throttle closed and opened the number three (right engine) throttle all the way, with the result that the right wing started to come back up. After that, the two pilots-plus a third flying as a passenger - learned to use the differential thrusts of the two remaining wing-mounted engines to skid the aircraft left or right in descending turns. In this way, they coaxed it into Sioux City and might have landed safely had the aircraft not been affected by un- controllable maneuvers (phugoids) while low on the approach. To save the lives of 184 out of a total of 296 people aboard-all of whom would have almost certainly died in an uncontrollable crash-was a remarkable feat of airmanship and crew resource management. Assembling the Big Picture This chapter has examined some of the behavioral options available to people working in complex technologies. Since all such behavior is governed by administrative controls, we began by looking at the principal means used by systems to limit people to safe and productive courses of action. These were represented along a dimension ranging from externalized feedforward procedures, that rigidly prescribe the allowable actions, to largely discretionary feedback controls, based upon internalized knowledge, skills and experience. Several factors influenced which of these two modes would predominate: the stage reached in the life history of the system; the type of operations carried out by the system; an individual's level within the organization; and the amount of training deemed necessary. The next part of the chapter discussed a number of ways in which human performance could be segmented. Three levels of performance were identified-the skill-based (SB), rule-based (RB) and knowledge-based (KB) levels-each defined by the interaction be- tween the modes of action control (conscious or automatic) and the nature of the immediate situation (routine or problematic). Three major error types, or unsuccessful actions, were linked to these performance levels: skill-based slips and lapses, rule-based mistakes and knowledge-based mistakes. We then identified three ways in which people can violate procedures: routine violations, optimizing violations and necessary violations. We also distinguished between correct and incorrect actions on the basis of whether or not they involved an appropriate assessment of the dangers involved in a planned course of action. Next, the point was made that safety procedures could never anticipate all the ways in which harm could come to people in the course of their work. They lacked the requisite variety to match these many dangers. As a result, work situations would fall into one of three groups: those covered by good procedures, those for which the available procedures were inappropriate or ill-matched, and those for which no procedures had been written. These three conditions- good rules, bad rules and no rules-were then combined with correct and erroneous performance to generate six kinds of rule-related be- haviour: correct compliance, correct violation, correct improvization, misvention (mistaken circumvention), mispliance (mistaken compliance) and mistakes (failures of improvization). Both organizations and human behavior are complex, so it is hardly surprising that this chapter has invoked a bewildering number of distinctions. Now the time has come to put them, quite literally, into one big picture. This is shown in Figure 4.10 which offers two principal routes for action: one that goes directly to correct and successful performance; and the other showing a variety of pathways to what are mostly unsafe behaviours. In both cases the starting point is the question: was the task (or situation) covered by procedures or training? Human actions can only be understood within a given context which, in this case, is a work-related task or situation. Al- though most activities within well established systems will have been anticipated in one way or another, sometimes totally novel situations arise in which people have to improvize a suitable course of action on the basis of knowledge-based processing. When the individuals concerned are both highly skilled and highly experienced (like Captain Al Haynes), there seems to be a 50:50 chance of coming up with the right answers. Mostly, however, the odds' are much lower. While procedures may have been written for a particular task, they may not necessarily be appropriate for, or applicable to, the local situation. In the nuclear power industry, for example, some 60 per cent of all human performance problems are associated with bad procedures. Wrong or inapplicable procedures are latent conditions that will cause the workforce to lose trust in the official procedures. They may either ignore them altogether or follow unofficial procedures of their devising-known in some industries as 'black books'. People faced with a bad procedure have two options: they can either follow the procedure (a mistaken compliance or mispliance) or not (a correct violation). The next level of Figure 4.10 poses the question: were the good procedures followed as intended? If the answer is 'yes', we have then completed all the stages necessary for correct and successful performance. As we have seen, there are a number of ways in which human actions can deviate from this desirable pathway. We can commit a rule-based mistake. For example, we can misread the situation, fail to recognize it as an exception, and apply a rule or procedure that is appropriate for most situations of this general kind but not this particular one. Second, we could follow some informal working practice and commit a mistaken violation (misvention). Finally, we could make a slip or suffer memory lapse so that the actions, although intended to follow the procedure, do not turn out as planned. We have now covered most of the human behaviours implicated in organizational accidents. In the next chapter, we will focus on a single type of error-maintenance omissions during installation or reassembly. As we shall see, the frequency with which this one error type crops up as a contributor to organizational accidents more than justifies such close attention. Chapter 5 Maintenance can Seriously Damage your System Close Encounters of a Risky Kind In Chapter 3 we discussed some of the problems created by automated systems in modern technologies. Distancing people from the processes they nominally control reduces the amount of 'hands on' contact, and hence the occurrence of slips, lapses, trips and fumbles, but it also increases the likelihood of certain kinds of mistakes-notably mode confusions-that may have more disastrous consequences than those the designers were seeking to avoid. In this chapter, we look at the opposite end of this 'hands on' spectrum, at maintenance-related activities in which the amount of direct contact between people and the system has certainly not diminished; indeed, in some areas it has actually increased. The term 'maintenance-related' includes unscheduled repairs, inspections, planned preventive maintenance, calibration and testing. It will be argued that these close encounters with the technical components of a system probably constitute the single largest human performance problem facing most hazardous technologies. By the same token, they also offer the greatest opportunity for human factors improvements. Errors by pilots, control room operators and other 'sharp-end' personnel may add the finishing touches to an organizational accident, but it is often latent conditions created by maintenance lapses that either set the accident sequence in motion or thwart its recovery. The chapter will present evidence to support the following assertions: • There have been many organizational accidents in which maintenance failures were either a significant cause or an exacerbating feature. • Of the three kinds of human activity that are universal in hazardous technologies- control under normal conditions, control under emergency conditions, and maintenance-related activities- the latter poses the largest human factors problem. • Of the two main elements of maintenance-the disassembly of components and their subsequent reassembly-the latter at-tracts by far the largest number of errors. • Of the various possible error types associated with the reassembly, installation or restoration of components, omissions-the failure to carry out necessary steps in the task--comprise the largest single error type. We will focus primarily on aviation and nuclear power generation because these two areas offer the best documented data on maintenance-related failures. But they are not special cases. There is every reason to suppose that similar patterns of occurrence are present in all hazardous technologies. Regardless of the domain, maintenance-related activities share a large number of common characteristics, particularly the taking apart and the putting back together of components. In addition, of course, they all involve close encounters with the human hand. And it is this frequency of contact, and hence greater error opportunity, rather than any lack of competence on the part of maintenance personnel that lies at the heart of the problem. The chapter then goes on to examine some of the ways in which these commonly occurring failures can be managed. A more fundamental remedy, however, is for designers and manufacturers to recognize that the risk of a 'weak' component failing due to lack of maintenance may now be considerably less than the risk of a 'healthy' component being damaged or omitted during maintenance. Organizational Accidents and Maintenance Failures It is neither possible nor necessary to provide a comprehensive list of all the organizational accidents in which maintenance-related failures have been implicated. For our purposes, it is sufficient to present some well documented examples from a range of hazardous technologies. Apollo 13 Shortly after the launch of Apollo 13 on 11 April 1970, an explosion occurred within the Service Module, blowing out its side.1 The explosion originated within a tank of liquid oxygen and had been initiated by the first activation by the crew of a set of rotating paddles designed to supplemental type inspection report the liquid oxygen at intervals throughout the mission. Just prior to the launch, the paddles had been bench-tested. But they had been connected to the wrong power supply. The effect of this error was to bum off the insulating material associated with the electrical supply to the paddles. This, in tum, converted the oxygen cylinder into a bomb waiting to be set off as soon as the paddles were switched on, at which point a spark ignited the liquid oxygen. The rest, as they say, is history. The spacecraft circled the Moon, with the crew-Astronauts Lovell, Haise and Swigert-using the life-support system of the Lunar Module during the extremely perilous re-entry into the Earth's atmosphere. This was one of many events in which the skills of the flight crew, acting as a last line of defence, saved the day following a maintenance-initiated emergency. Flixborough In 1974, a temporary pipe at the Nypro Factory in Flixborough failed, releasing 50 tons of hot cyclohexane.2 The cyclohexane mixed with the air and exploded, killing 28 people and destroying the plant. The production process normally used six reactor vessels, connected in series, each joined to the next by a short 28-inch pipe with expanding bellows. Two months before the disaster, one of the reactor vessels-number five in the series-developed a crack and had to be removed. It was replaced by an improvised 20-inch bypass pipe connecting vessels four and six. The bypass pipe was inadequately supported and, because it entered into the existing bellows at either end, it was free to move about or 'squirm' when the pressure rose above normal-causing the cyclohexane to be released into the atmosphere. Three Mile Island Two separate maintenance failures were implicated in the Three Mile Island nuclear power plant accident that occurred on 28 March 1979. One initiated the emergency; the other delayed its recovery. The first failure occurred when a maintenance crew was attempting to renew the resin for the treatment of the plant's water. A small volume of water found its way into the plant's instrument air system, tripping the feedwater pumps. This, in turn, cut the water flow to the steam generator and tripped the turbine, preventing the heat of the primary cooling system from being transferred to the cooler water in the secondary system. At this point, the emergency feedwater pumps continuing airworthines management exposition on automatically. However, the pipes from the emergency feedwater storage tanks were blocked by closed valves, erroneously left shut during maintenance two days earlier. With no heat removal, there was a rapid rise in core temperature and pressure which caused the reactor to 'scram'-the control rods were automatically lowered into the core, absorbing neutrons and stopping a chain reaction. However, because decaying radioactive materials still produce heat, the temperature and pressure within the core increased further, causing a pilot-operated relief valve to open. This was supposed to flip open and then close but it remained stuck in the open position. A hole was thus created in the primary cooling system through which radioactive water, under high pressure, poured into the containment area and then down into the basement. It took a further 16 hours to restore the plant to a safe state. American Flight 191, Chicago O'Hare On the afternoon of 25 May 1979, the day before the Memorial Day weekend, an American Airlines DC-I0 departed for Los Angeles. Just before it rotated into the takeoff attitude, pieces of the port engine fell away from the aircraft. Shortly afterwards, the entire engine and pylon assembly tore itself loose, flew over the wing and then smashed down on to the runway. The DC-10 continued to climb away, but at 300 feet it started to bank sharply to the left. The bank steepened and the nose began to fall. Within seconds, it crashed into the ground and exploded, killing all 271 people aboard. The accident investigators discovered fatigue cracking and fractures on the engine pylon. A crescent-shaped deformation on the upper flange of the pylon bulkhead strongly suggested that the flange had been cracked when the pylon was being removed from the wing or reinstalled during maintenance. The investigation revealed that the port engine and pylon had been removed eight weeks earlier to replace the self-aligning bearings in compliance with a McDonnell Douglas Service Bulletin. As a result of this finding, the Federal Aviation Agency took the unprecedented step of grounding all US-registered DC-10s for further investigation. These inspections revealed that six supposedly serviceable DC-10s had similar fractures in the upper flanges of their pylon rear bulkheads. Four of these aircraft belonged to American Airlines and two to Continental Airlines. It was later discovered that both American and Continental had devised special procedures for carrying out the replacement of the forward and rear bulkheads' self-aligning bearings, as required by the manufacturer's Service Bulletin. Although the manufacturer's Bulletin recommended that the engines be removed before the pylons, both airlines had devised what they believed to be a more efficient technique, namely removing the engine and the pylon as a single unit. The investigators concluded that the engine and pylon separation resulted from damage inflicted by these improper maintenance procedures. Bhopal On the night of 2-3 December 1984 a leak of methyl isocyanate from a small pesticide plant devastated the Indian city of Bhopal, killing 2500 people and injuring several thousands more.5 The immediate cause of the discharge was the influx of water into a methyl isocyanate storage tank. Its presence there involved a tangled story of botched maintenance, improvised bypass pipes, failed defenses, drought, and flawed decision-making on the part of both management and politicians. Among the contributing maintenance failures were a disconnected flare tower, an inoperable refrigeration plant and a failure to regularly clean pipes and valves. Japan Air Lines, Flight JL 123, Mount Osutaka On 12 August 1985, a JAL Boeing 747, on an internal flight, crashed into the side of Mount Osutaka, about 100 km west of Tokyo's Huneda Airport.6 The crash resulted in the highest death toll ever to occur in a single-aircraft accident. After a lengthy and painstaking investigation, it was established that the principal cause of the accident was a botched fuselage repair carried out more than seven years earlier. In splicing the repaired portion of the pressure bulkhead to the original, part of the splice had been wrongly assembled. Over a small section of the splice, two separate doubler plates, instead of one continuous one, were used as reinforcement. The gap in the doubler plating meant that the splice was joined by only a single row of rivets, instead of two rows. This reduced the assembly's resistance to fatigue by some 70 percent. These problems were not discovered in subsequent maintenance checks, mainly because the area in which the splicing had been carried out was inaccessible to visual inspections. Piper Alpha The Piper Alpha disaster, which occurred on the evening of 6 July 1988, resulted in the deaths of 165 of the 226 people on board the North Sea oil and gas platform, together with two crew members of a nearby rescue vessel The explosion was caused by a leak of condensate which occurred when members of the night shift attempted to restart a pump that had been shut down for maintenance. Unknown to them, a pressure safety valve had been removed from the relief line of the pump and a blank flange assembly, that had been fitted at the site of the valve, was not leak-tight. Their unawareness of the valve removal was the result of communication failures at the shift hand over earlier in the evening, together with a breakdown of the permit-to-work system relating to the valve maintenance. Clapham Junction At 0810 on Monday, 12 December 1988, a northbound commuter train ran into the back of a stationary train in a cutting just south of Clapham Junction station. A third train, going south, ran into the wreckage of the first train. Thirty-five people died and 500 were injured. The immediate cause of the crash was a signal that failed with a green aspect, concealing the presence of the stationary train from the driver of the northbound commuter train until it was too late. This failure was directly due to the working practices of a technician engaged in rewiring the signal on the previous day. Rather than cutting off or tying back the old wires, the technician merely pushed them aside. It was also his practice to re-use old insulating tape, though on this occasion, no tape at all was wrapped around the bare ends of the wire. As a result, the wire continuing airworthines management exposition into contact with nearby equipment, causing a 'wrong side' signal failure. Phillips 66 Company At 1300 on 23 October 1989 an explosion and fire ripped through the Phillips 66 Company Houston Chemical Complex in Pasadena, Texas.Twenty-three workers were killed and more than 130 injured. Property damage amounted to nearly a billion dollars. The accident was caused by the release of extremely flammable gases that occurred during regular maintenance operations on one of the plant's polyethylene reactors. In less than two minutes, the vapor cloud continuing airworthines management exposition into contact with an ignition source and exploded with a force of 2.4 tons of TNT. Contract maintenance personnel were engaged in removing 'plugs' of solidified polyethylene from the settling legs. There was a single-ball valve at the point where the legs joined the reactor pipes. These valves were kept open during production, so that the polyethylene particles could settle into the leg, and closed during maintenance operations. On this occasion, the valve was opened in error. An air hose that supplied the air pressure to open or close the valve had been connected the wrong way round (the two ends were identical). This allowed the valve to open even when the actuator switch was in the closed position. The flammable gases escaped through the open valve. In these brief accounts of maintenance-induced organizational accidents, we have deliberately sacrificed detail for scope. The intention was to convey something of the extent to which maintenance errors have been implicated in a wide range of accidents across different domains. But it must be emphazised that maintenance errors-like human failures in any other sphere-are not just isolated causes; they are themselves the consequences of upstream organizational factors or latent conditions. Activities and their Relative Likelihood of Performance Problems There are many ways of classifying human performance and its associated errors, each one having its uses and limitations. In Chapter 4, for example, we identified three main classes of human error: skill-based slips and lapses, rule-based mistakes and knowledge-based mistakes. Such a causal taxonomy is helpful in locating the underlying mental processes, but it is difficult for non-specialists to apply. Left to their own devices, engineers and quality inspectors are much more likely to classify human performance problems according to their consequences for the system (for example, missing fastenings, improper installation, tools left behind and the like). Although it may obscure the underlying cognitive mechanisms, this kind of information is both widely available and easy to interpret. One of the advantages of these consequential classifications is that there is usually little doubt as to what the person was doing when the error occurred. Such breakdowns of performance problems by activity can be very revealing, as will be shown below. It is useful, when considering the varieties of human performance, to start with a set of activities that are carried out within all hazardous technologies. A preliminary list is set out below: • Control under normal conditions • Control under abnormal or emergency conditions • Maintenance, calibration and testing. To this list we should properly add the preparation and application of procedures, documentation, rules, regulations and administrative controls but, as these were considered in Chapter 4, we will limit our attention here to the three activities identified above. From this list of universal human activities it is possible to make a preliminary assessment of their relative likelihood of yielding human performance problems. To do this, we need to ask three questions. • The 'hands on' question. What activities involve the most direct human contact with the system and thus offer the greatest opportunity for active failures (errors and violations) to have an adverse effect upon the system? • The criticality question. What activities, if performed less than adequately, pose the greatest risks to the safety of the system? • The frequency question. How often are these activities performed in the day-to-day operation of the system as a whole? It would be reasonable to expect that an activity scoring 'high' on all three of these questions is the one most likely to be associated with human performance problems. The results of this analysis are summarized in Table 5.1 Maintenance-related work-scoring 'high' on all three criteria-emerges as the activity most likely to generate human performance problems of one kind or another. To what extent is this prediction borne out by the available evidence? The most relevant data come from nuclear power operations. Table 5.2 shows a compilation of the results of three surveys: two carried out by the Institute of Nuclear Power Operations (INPO) in Atlanta, and one conducted by the Central Research Institute for the Electrical Power Industry (CRIEPI) in Tokyo. The inputs for the INPO investigation were significant event reports filed by US nuclear utilities. In the first INPO study, 87 significant event reports yielded 182 root causes.10 In the second INPO study, 387 root causes were identified from 180 significant event reports. The data for the Japanese study continuing airworthines management exposition from 104 standardized event reports from the CRIEPI-associated utilities. These data bear out the earlier analysis. In all three studies more than half of all the identified performance problems were associated with maintenance, testing and calibration activities. The Vulnerability of Installation The next question concerns which aspect of maintenance, testing and calibration is most likely to be associated with less-than-adequate human performance. Regardless of the domain, all maintenance-related activities require the removal of fastenings and the disassembly of components, followed by their reassembly and installation. Thus, a large part of maintenance-related activity falls into the categories of either disassembly or installation. Once again, there are a priori grounds for identifying one of these tasks-installation-as being the most likely to attract human performance problems. The reasons for this are made clearer by reference to Figure 5.1, showing a bolt with eight marked nuts attached to it. This represents the maintenance task in miniature. Here, the requirement is to remove the nuts and to replace them in some predetermined order. For the most part, there is only one way to remove the nuts, with each step being naturally cued by the preceding one. The task is one where all the necessary knowledge is 'in the world' rather than 'in the head' In the case of installation, however, there are over 40 000 ways in which the nuts can be reassembled in the wrong order (factorial 8). And this takes no account of any possible omissions. Moreover, the likelihood of error is further compounded by the fact that many possible omissions and mis orderings may be concealed during the reassembly process. Thus, the probability of making errors during installation is very much greater than during disassembly, while the chances of detecting and correcting them are very much less. The available evidence supports the prediction that installation will be especially vulnerable to errors. Listed below are the top seven causes of 276 inflight engine shutdowns (IFSDs) in Boeing aircraft. • Incomplete installation (33%) • Damaged on installation (14.5%) • Improper installation (11%) • Equipment not installed or missing (11%) • Foreign object damage (6.5%) • Improper fault isolation, inspection, test (6%) • Equipment not activated or deactivated (4%). These data show that various forms of faulty installation were the top four most frequent causal categories, together comprising over 70 per cent of all contributing factors. Comparable findings were obtained by Pratt and Whitney in their 1992 survey of 120 in-flight shutdowns occurring on Boeing 747s in 1991.15 Here, the top three contributing factors were missing parts, incorrect parts and incorrect installation. In a UK Civil Aviation Authority survey of maintenance deficiencies of all kinds, the most frequent problem was the incorrect installation of components, followed by the fitting of wrong parts, electrical wiring discrepancies, and loose objects (tools and so on) left in the aircraft. The Prevalence of Omissions What type of error is most likely to occur during maintenance-related activities and most especially during the installation task? As noted earlier, the answer is omissions: the failure to carry out necessary parts of the task. Omissions can involve either the failure to replace some component or the failure to remove foreign objects (tools, rags and the like) before leaving the job. Jens Rasmussen17 analysed 200 significant event reports published in Nuclear Power Experience and identified the top four error types as follows. • Omission of functionally isolated acts (34%) • Other omissions (9%) • Side-effect(s) not considered (8%) • Manual variability, lack of precision (5%). He also identified the activities most often associated with omissions, as listed below. • Repair and modification (41%) • Test and calibration (33%) • Inventory control (9%) • Manual operation and control (6%). The INPO investigation, mentioned earlier, found that 60 per cent of all human performance root causes involved omissions and that 64.5 per cent of the errors in maintenance-related activities were omissions. This study also observed that 96 per cent of deficient procedures involved omissions of one kind or another. I analysed the reports of 122 maintenance lapses occurring within a major airline over a three-year period. A preliminary classification yielded the following proportions of error types: • Omissions (56%) • Incorrect installations (30%) • Wrong parts (8%) • Other (6%). What gets omitted? A closer analysis of the omission errors produced the following results: • Fastenings undone/incomplete (22%) • Items left locked/pins not removed (13%) • Caps loose or missing (11%) • Items left loose or disconnected (10%) • Items missing (10%) • Tools/spare fastenings not removed (10%) • Lack oflubrication (7%) • Panels left off (3%). It seems unnecessary to labor the point any further. Omissions represent the largest category of maintenance-related errors, and maintenance-related errors constitute the largest class of human performance problems in nuclear power plants, and probably in aviation as well-although there are no comparable data to support this view as yet. Omission-prone Task Features From an analytical point of view there are at least two approaches towards a better understanding of maintenance omissions, one seeking to identify the underlying cognitive mechanisms, the other trying to determine what aspects of a task cause it to be especially omission-prone. The former route is made difficult by the fact that an omission can arise within a number of cognitive processes concerned with planning and executing an action, as summarized in Table 5.3. Even when the omission is one's own, the underlying mechanisms are not easy to establish, but when the omission is made by another person at some time in the past, the underlying reasons may be impossible to discover. The task analysis route, on the other hand, is more promising. The act of duplicating a looseleaf document on a simple photo-copying machine gives an everyday illustration of omission-prone task steps (see Figure 5.2). There is strong evidence to show that the most likely omission is to leave the last page of the original under the lid when departing with the copy and the remainder of the original pages. There are at least four distinct features of the photocopying task that combine to make this omission highly likely-irrespective of who is carrying out the task. • The step is functionally isolated from the preceding actions. Before, the act of removing the previously copied page had been cued by the need to replace it with the next page. In this instance, there is no next page. • The need to remove the last page of the original occurs after the main goal of the activity has been achieved-obtaining a complete copy of the document-but before the task itself is complete. • The step occurs close to the end of the task. Studies of absentminded slips in everyday life have shown that such 'premature exits' are a common form of omission which can be prompted by a preoccupation with the next task. However, in maintenance work organized around an eight- or twelve-hour shift pattern, there is no guarantee that the individual who starts upon a job will be the one to complete it. And even when the same person performs the whole task, there is always the possibility that he or she may be called away or distracted before the task is finished. • The last page of the original is concealed under the lid of the photocopier-the out-of-sight-out-of-mind phenomenon. To this list can be added several other features which, if present within a given task step, can combine to increase the probability that the step will be omitted. Other omission-provoking features include the following: • Steps involving actions or items not required in other very similar tasks. • Steps involving recently introduced changes to previous practice. • Steps involving recursions of previous actions, depending upon local conditions. • Steps involving the installation of multiple items (for example, fastenings, bushes, washes, spacers and so on.) • Steps that are dependent upon some former action, condition or state. • Steps that are not always required in the performance of this particular task. Maintenance activities are highly proceduralized. It is therefore possible, in principle, to identify in advance those steps most vulnerable to omissions by establishing the number of omission provoking features that each discrete step possesses. Having identified error-prone steps, remedial actions can then be taken to reduce the likelihood of these steps being left out. The Characteristics of a Good Reminder Although there are a variety of cognitive processes that could contribute to an omission, and their precise nature is often hidden from both the actor and the outside observer, the means of limiting their future occurrence can be relatively straightforward and easy to apply once the error-prone steps have been identified. The simplest countermeasure is an appropriate reminder. What characteristics should a good reminder possess? Some suggestions are listed below. • It should be able to attract the actor's attention at the critical time (conspicuous). • It should be located as closely as possible in both time and distance to the to-be-remembered (TBR) task step (contiguous). • It should provide sufficient information about when and where the TBR step should be carried out (context). • It should inform the actor about what has to be done (content). • It should allow the actor to check off the number of discrete actions or items that should be included in the correct performance of the task (check). These five characteristics are universal criteria for a good reminder. They are applicable in virtually all situations. There are, however, a number of secondary criteria that could also apply in many situations: • It should work effectively for a wide range of TBR steps (comprehensive). • It should (when warranted or possible) block further progress until a necessary prior step has been completed (compel). • It should help the actor to establish that the necessary steps have been completed. In other words, it should continue to exist and be visible for some time after the performance of the step has passed (confirm). • It should be readily removable once the time for the action and its checking have passed. Does not, for example, want to send more than one Christmas card to the same person (conclude). The presence of reminders is not a guaranteed solution to the omission problem. But-in the spirit of kaizen -it will certainly help to bring about a substantial reduction in their numbers. Consider, for example, what the impact of the reminder shown in Figure 5.3 might be upon the likelihood of you leaving behind the last page of the original In maintenance, any such reduction in the largest single category of human error could have substantial benefits, both in lives and in costs. According to a Boeing analysis, maintenance and inspection failures ranked second only to controlled flight into terrain in the list of factors contributing to onboard fatalities, and caused the deaths of 1481 people in 47 accidents between 1982-91.20 However, the costs of maintenance failures are more likely to be counted in terms of money rather than casualties. Such losses can be very high. One major air-line has reckoned its annual losses due to maintenance lapses at around $38 million dollars.21 It has been estimated that an inflight engine shutdown, for example, can cost up to $500 000 in lost income and repairs; each flight cancellation can cost up to $50 000; each hour of delay on the ramp can cost $10 000. It should be noted that the reminders described above are not a permanent solution to the omission problem. They are, at best, 'first aid' measures to cope with the difficulties experienced in the present generation of hazardous systems-whose working lives will run for many years into the future. A more lasting solution would be to design components so that they can only be installed in the correct way. Another would be to make the system disable itself automatically when it detected the presence of missing parts. A third and more fundamental solution would be to design out the need for 'hands on' human contact during maintenance inspections. As a first step in the direction of the last possibility, we will review some of the engineering and economic reasons why maintenance is carried out in the first place. The Rationale for Maintenance Why not design systems for zero maintenance? Why not have all the constituent parts of a system lasting for exactly the same length of time-a period equal to the planned life of the system as a whole? The standard answer is that this would be uneconomical, given the complexity and expense of a modern technological system.23 Thus, many of the components will have been designed with a useful life that is greater than the longest production cycle but less than the planned lifetime of the total system. These 'weak' components will have been identified at the design stage and made accessible and replaceable. This is termed the 'expected maintenance load'. Yet other components will fail in an unplanned way as the result of design mistakes or operational mishandling. This is the 'unexpected maintenance load'. The two are interrelated. Failure to deal with the expected maintenance load will generate a larger and more costly investment of maintenance resources to cope with unexpected failures. This interaction is summarized in Figure 5.4. Once the 'resource elbow' is passed, the costs of restoring the unit and hence the system to an acceptable level accelerate dramatically. Working in the preventive maintenance zone has several advantages: it is more economical, it takes care of the expected maintenance load, and-by forestalling failures-it greatly reduces the unexpected load as well. This is the orthodox engineering view. It is summarized in Figure 5.5 that shows how the optimum level of preventive maintenance can be determined by summing the costs of both corrective and preventive maintenance, and then selecting the level coinciding with the lowest overall maintenance cost. Unfortunately, this orthodoxy presumes that all-or at least most- maintenance activities are essentially benign. But suppose preventive maintenance did not always prevent failure and that corrective maintenance did not always correct it. Suppose that both of these activities actually had the potential for doing serious harm, rendering previously reliable components inoperable or simply removing them altogether. Figure 5.6 looks at the maintenance issue from a broader perspective-one that includes human as well as technical factors. Here are plotted (in a very speculative fashion) the risks to the system posed by (a) neglected maintenance, and (b) by the likelihood of errors being committed during either preventive or corrective maintenance. The latter plot is based on the assumption that the likelihood of error will increase as a direct linear function of the amount of maintenance activity. Since only a relatively small proportion of human actions are erroneous, the human failure risk will never rise above a fairly low value. But, as we shall see below, it is not the absolute value that matters, but the relative proportions of the maintenance neglect and maintenance error risks. It is also assumed that these error risks will not change in any systematic fashion over time. Technology may advance, but human fallibility stays the same. In sharp contrast, however, the risks due to maintenance neglect are likely to diminish steadily as manufacturing techniques and the intrinsic reliability of materials improve with technological developments. This is indicated in Figure 5.6 by the set of dotted diagonals advancing towards the lower left-hand corner of the graph, where each dotted line represents a stage in the improvement of the technology. It is clear that if a given level of maintenance-determined by the economic and engineering considerations discussed above-remains relatively constant over time, then a point will soon be reached when the dangers to the system come to be dominated by even a relatively low error rate. The data reported earlier on the causes of inflight engine shutdowns show that all of the most common contributing factors are associated with human rather than 'unaided' technical failures. Of course, it could be argued that the advent of non-destructive testing and other advanced diagnostic techniques allow aircraft engineers to identify potential technical failures before they happen inflight, thus leaving human errors as the main residual category of failure. This may well be true, but it does not alter the fact that regular human contact with the 4-6 million removable parts on a modern aircraft poses an unacceptable level of risk. Ironically, one of the pressures that sustains this high level of maintenance contact is the safety-criticality of these systems. A catastrophic breakdown is unacceptable in commercial aviation, nuclear power generation or chemical process plants. Everything must be done-and be seen to be done-to preserve the integrity and reliability of these systems. But, as we have seen, the maintainer's touch can harm as well as heal, and the point seems to have been reached in some modern systems when the risks of the former may outweigh the benefits of the latter. Conclusions Rapid technological advances in hazardous systems have not only brought about the replacement of human control by computers, they have also led to very substantial improvements in the reliability of equipment and components. This has been achieved by the use of better manufacturing processes and materials, as well as through the widespread availability of sophisticated diagnostic techniques. But the maintenance schedule for a modern aircraft or nuclear power plant still demands the repeated disassembly, inspection and replacement of millions of removable parts over the long working life of the system. Thirty or even twenty years ago, these inspections would probably have resulted in the frequent detection and replacement of failed components. Then, the risks of failure due to intrinsic engineering defects almost certainly exceeded the dangers created by allowing legions of fallible people direct access to the vulnerable entrails of the system. But now the balance has tipped the other way. The greatest hazard facing modern technologies comes from people, and most particularly from the well intentioned, but often unnecessary, physical contact demanded by outdated maintenance schedules. We urgently need a greater awareness on the part of system designers and manufacturers of the varieties of human fallibility and the error-provoking nature of large parts of the maintenance task---especially during installation or reassembly. Most of all, they must appreciate that maintenance can be a serious hazard as well as a necessary defence. Until systems are designed and built with these issues in mind, good maintenance personnel will go on contributing to bad organizational accidents. Chapter 6 Navigating the Safety Space Assessing Safety This chapter deals with the principles underlying the measurement of safety (the actual measures and their application will be discussed in the next chapter). In particular, it is about assessing the 'safety health' of complex technological systems that-by virtue of their many-layered defences-have relatively few bad accidents. We will also be returning to a distinction first introduced in Chapter 4-that between process and outcome. When it comes to restricting human behaviour to safe and productive pathways, most organizations favour process controls, based upon rules, regulations and procedures (see Chapter 4). But, in the management of system safety, the reverse is the norm. Most organizations involved in hazardous operations rely heavily upon outcome measures, or, more specifically, upon negative outcome measures that record the occurrence of adverse events such as fatalities and lost time injuries. Unfortunately, such outcome data provide an unreliable indication of a system's intrinsic safety. This is especially the case when the number of adverse events has fallen to some low asymptotic value around which the small fluctuations from one accounting period to the next are more 'noise' than 'signal'. In many well defended systems, these data are too few and too late to guide effective safety management. Indeed, some modem technologies have become the victims of their own success: they have largely eliminated the conventional outcome measures by which they were accustomed to steer system safety. In commercial aviation, for example, the accident risk has remained fairly constant over the last 25 years at an average worldwide value of one passenger fatality for every million air miles' If we are to make progress, we need to reconsider the nature of safety. Dictionary definitions are of little help. Most equate safety with freedom from danger or risk, but both are ever-present in hazardous technologies. The most widely used indicator is the number of negative outcomes, but it is only helpful when the accident rates are high enough, and even then we are left with the problem of chance-the fortuitous combination of causal elements at a particular place and time. Only if the managers of a system had complete control over all possible accident-producing factors could negative outcome data provide a valid index of its intrinsic safety. Only then could accident rates be linked directly to the quality of safety management. But no hazardous technology can ever achieve this total control. Natural hazards can be defended against, unsafe acts can be moderated to some degree, but neither can be eliminated altogether. Latent conditions, or pathogens, will always be present. The likelihood of their adverse conjunction is always greater than zero. The large random component in accident causation means that 'safe' organizations can still have bad accidents, and 'unsafe' organizations can escape them for long periods. Bad luck can bring down the deserving, while good luck can protect the unworthy. One way out of this impasse is through the use of process measures of safety. To apply such measures effectively, we need to recognize that safety has two faces, a negative one and a positive one. As with health, the occasional absences of safety are easier to quantify than its more enduring presence. In order to get closer to the positive face of safety, we need to think about horse kicks. Counting Horse Kicks In the early part of the nineteenth century, Simeon Poisson, the great French mathematician, counted the number of horse kicks received by Prussian cavalrymen over a given time period.2 The exact numbers do not matter, but Figure 6.1 gives an approximate idea of what he found. By far the largest proportion of the regiment suffered no kicks at all. A small number were kicked once, an even smaller number twice and still fewer were kicked three or more times. On the basis of these and similar data, Poisson developed a theoretical model-the Poisson distribution-for determining the chance probability of an accident among a group of people sharing equal exposure to a particular hazard. The Poisson distribution addresses the question: how many people would we expect to find with 0, 1,2 or more, accidents when there is no special reason why one person should have more than any other? Even in the relatively straightforward matter of horse kicks, it is very unlikely that liability to these painful encounters would have been the same for every person. It can be seen from Figure 6.1, that most of the regiment fell into the 'zero kick' category. However, there could be many different reasons for this. Some of the unkicked cavalrymen could have been just lucky. Some could have established a good relationship with their mounts. Others could have exercised greater caution. Still others could have been both kind to their horses and wary in the kicking zone. Similar, but opposite, reasons could explain why some people were kicked more often than others. We can therefore think of a continuum running from the kick-proof to the kick-prone, or from relatively safe to relatively unsafe individuals. Many different factors act to locate an individual's position along this safe-unsafe dimension. In some cases, it will simply be a question of good or bad luck. But in others it will be due to the success of deliberate countermeasures. The most kick-resistant cavalrymen will be those who employ the best protective measures (kindness, caution and so on) in the most sustained and effective way. In short, we can discriminate degrees of safety as well as 'unsafety', as shown in Figure 6.2. The horse kicks exercise demonstrates that outcome data can only reveal the negative face of safety. Such measures record moments of vulnerability; they cannot discriminate the more enduring ingredients of resistance. As we shall see later, these need to be assessed directly using process measures-indices that gauge the general' safety health' of the system as a whole by sampling various vital signs on a regular basis. Only in this way can we navigate the safety space (see below) in a principled manner rather than remaining at the mercy of the prevailing currents. Introducing the Safety Space The safety space is a natural extension of the resistance-vulnerability continuum introduced in the previous section. It is a notional space within which we can represent the current resistance or vulnerability of an individual or an organization. As shown in Figure 6.3, it is cigar-shaped, with extreme resistance located at the left-hand end and extreme vulnerability at the right-hand end. The shape of the space acknowledges that most people or organizations will occupy some intermediate point within this space. Hereafter, we will focus upon organizations rather than individuals. An organization's position within the safety space is determined by the quality of the processes used to combat its operational hazards. In other words, its location on the resistance-vulnerability dimension will be a function of the extent and integrity of its defences at anyone point in time. But there is no such thing as absolute safety. So long as natural hazards, human fallibility, latent conditions and the possibility of chance conjunctions of these accident-producing factors continue to exist, then even the most intrinsically resistant organizations-those at the extreme left-hand end-can still have accidents. By the same token, 'lucky' but unsafe organizations at the extreme right-hand end of the space can still escape accidents for quite long periods of time. Organizations can, of course, make their own luck to some degree-but never completely. This has two implications for outcome measures. First, there will be some correspondence between an organization's position within the safety space and the number of negative outcomes that befall it. Organizations at the resistant end of the space are likely to suffer fewer bad events within a given sampling period than those at the vulnerable end. But this correlation will never be perfect due to unforeseeable interventions of happy and unhappy chance. Second, when accident rates within a particular domain fall to very low levels, as they have in aviation and nuclear power generation, the occurrence or not of negative out-comes (within a particular accounting period) reveal very little about an organization's position within the safety space-yet such differences in location will continue to exist. Currents within the Safety Space Very few organizations occupy fixed positions within the safety space. Most of them are in continuous motion, either being actively driven towards the resistant end of the space by the energetic implementation of effective safety measures, or by being allowed to drift passively towards the unsafe end. Naturally occurring currents are running in opposite directions within the space, their force becoming stronger the nearer an organization approaches either end. These currents are shown in Figure 6.4. The closer an organization drifts towards the unsafe end of the space, the more likely it is to suffer accidents. These, together with public and regulatory pressures, provide a powerful impetus for enhanced safety measures. Although accidents are relatively unreliable indicators of intrinsic safety, they nevertheless succeed, temporarily at least, in concentrating the minds of top management upon protection rather than production issues. This raises the question to be discussed at a later point: Do organizations need bad accidents in order to survive? Contrary forces will come into playas the organization moves into the more resistant regions of the space. Safety initiatives run out of steam and yield diminishing returns. Managers once again forget to be afraid and begin to divert more of their attention and resources to production goals. And, as discussed in Chapter 1, new and improved defences are used for furthering productive, rather than protective, goals. In other words, organizations become accustomed to their apparently safe state and allow themselves to drift-like the unrocked boat-into regions of greater vulnerability. If they were to rely entirely on outcome measures, organizations would probably move passively to and fro across the central zone like pieces of flotsam. Effective safety management means actively navigating the safety space in order to reach and then remain within the zone of maximum resistance. To do this, managers must under-stand the nature of the forces acting upon the organization, as well as the kinds of information needed to fix their current position. To reach the target region and then stay there, two things are necessary: an internal 'engine' to drive the organization in the right direction, and 'navigational aids' to plot their progress. What Fuels the 'Safety Engine/? Three ingredients are vital for driving the safety engine, all of them the province of top management-or what the organizational theorist, Mintzberg, has termed the strategic apex of the system. These driving forces are: commitment, competence and cognisance-'the three Cs'. Commitment has two main components: motivation and resources. The motivational issue relates to whether an organization seeks to be the domain model for good safety practices or whether it is simply content to keep one step ahead of the regulators-the difference noted in Chapter 2 between generative and pathological organizations. High levels of commitment are comparatively rare and hard to sustain. This is why the organization's safety culture is so important. Top management come and go. More organizational leaders are appointed to revive sagging commercial fortunes than to improve indifferent safety records. A good safety culture, on the other hand, is something that endures beyond these palace revolutions and so provides the necessary driving force irrespective of the inclinations of the latest CEO. The second issue concerns the resources allocated to the achievement of safety goals. This is not just a matter of money. It concerns quality as well as quantity, and has to do with the calibre and status of the people assigned to direct the management of system safety. Within some organizations, safety jobs are seen as being in the fast lane of career advancement. In all too many companies, however, they are regarded as long-term parking areas for under-powered or burned-out executives. But commitment alone is not enough. The organization must also possess the technical competence necessary to achieve its safety goals. Competence is very closely related to the quality of the organization's safety information system. Does it collect the right information? Does it disseminate it? Does it act upon it? Paired comparison studies-examining pairs of companies matched in all respects except for safety performance-have shown that the two characteristics most likely to distinguish safe organizations from less safe ones are, firstly, top-level commitment and, secondly, the possession of an adequate safety information system.Neither commitment nor competence will suffice unless the organization has a correct awareness-or cognisance-of the dangers that threaten its operations. Two features are symptomatic of organizations lacking this necessary level of cognisance. The first is the positional paradox-where those at the top of the organization, possessing the largest degree of decisional autonomy, blame most of their safety problems on the personal shortcomings of those at the sharp end who, for the most part, simply follow procedures and work with the equipment provided. The second symptom is the tick-off phenomenon. This is something that can afflict technical managers assigned to safety jobs. They treat safety measures like pieces of equipment. They put them in place, then tick them off as another job done. Most pieces of equipment do what they are supposed to do. Switch them on and they function as specified. But safety measures involve both a product and a process. Simply implementing them is not enough. They have to be watched, worried about, tuned and adjusted. Items of equipment are nearly all product and very little process. However, safety measures are more like religion-there is a great deal of praying (process), but few miracles (product). Cognisant organizations understand the true nature of the 'safety war'. They see it for what it really is-a long guerilla struggle with no final conclusive victory. For them, a lengthy period without a bad accident does not signal the coming of peace. They see it, correctly, as a period of heightened danger and so reform and strengthen their defences accordingly. In any case, since entropy wins in the end, a more appropriate metaphor for the conclusion of the safety war might be likened to the last helicopter out of Saigon rather than a decisive Yorktown, Waterloo or Appomatox. Setting the Right Safety Goals The key to navigating the safety space lies in appreciating what is manageable and what is not. Many organizations treat safety management as a negative production process. They set reduced negative outcome targets for the coming accounting period (e.g., 'Next year, we'll reduce our lost-time accidents by half'.). But accidents, by their nature, are not directly controllable. So much of their causal variance lies outside the organization's sphere of influence. The organization can only defend against hazards, it cannot remove or avoid them-and still stay in business. Similarly, an organization can only strive to minimize unsafe acts, it cannot eliminate them altogether. Effective safety management is more like a long-term fitness programme than negative production. Rather than struggling vainly to exercise direct control over incidents and accidents, managers should regularly measure and improve those processes-design, hardware, training, procedures, maintenance, planning, budgeting, communication, goal conflicts, and the like-that are known to be implicated in the occurrence of organizational accidents. These are the manageable processes determining a system's safety health. They are, in any case, the processes that managers are hired to manage. In this way, safety management is not an add-on, but an essential part of the system's core business. As indicated earlier, the only attainable goal for safety management is not zero accidents, but to reach that region of the safety space associated with maximum resistance-and then staying there. Simply moving in the of greater safety is not difficult. But sustaining these improvements is very hard. To hold such a position against the strong countervailing currents requires navigational aids. More specifically, it requires a safety information system that is not only capable of revealing the right conclusions about past events (reactive measures), but that also facilitates regular 'health checks' of the basic organizational processes (proactive measures) which are then used to guide targeted remedial actions. The navigational airborne integrated data system are shown in Figure 6.5 and discussed further in succeeding sections. A Test to Destruction In case the ideas of resistance or vulnerability to hazards seem too abstract, consider the results of the following study. One way of determining the relative resilience of different systems is to establish how many things went wrong before they sustained a serious accident. In effect, this is the equivalent of an engineering test-to-destruction. The systems in this case were types of aircraft: helicopters, light aircraft (general aviation) and large commercial jets. The data source was 90 fatal accident investigations carried out by the UK Air Accident Investigation Branch between the 1970s and the 1990s.5 All such investigations were conducted and reported in the standardized fashion laid down by the International Civil Aviation Organization's guidelines for accident investigators. Sixteen possible contributing factors were identified in advance: engine problems, airframe problems, system problems, fuel problems, pilot hours on aircraft type, rest periods, pilot error, operator issues, change of plan, time of day/night, icing conditions, visibility, wind, precipitation, air traffic control and radio communications. The number of such problems was established for each fatal accident, and the results are summarised in Table 6.1. Not surprisingly, helicopters were the most vulnerable aircraft type. In contrast, it required more than twice the number of contributing factors to bring about a fatal accident in the better defended commercial jet airliners. An Overview of the Navigational airborne integrated data system Navigational airborne integrated data system fall into two main categories: reactive measures that can only be applied after the occurrence of an event, and proactive measures that can be used before an event to assess the 'safety health' of the system as a whole. Effective safety management requires the use of both of these measures. We need to learn the right lessons from past events, and then translate this knowledge into enhanced resistance. At the same time, we must make visible to those who manage and operate the system the latent conditions and resident pathogens that are an inevitable part of any complex technology, no matter how well managed, and that experience has shown can provide the ingredients for future organizational accidents. Used properly, both reactive and proactive measures can give valuable information about the state of the underlying organizational processes. The accident causation model presented in Figure 1.6 directs our attention to two important issues: the local and organizational conditions that promote unsafe acts, and the barriers, safeguards and defences that keep hazards and potential losses apart. One of the commonest misuses of reactive measures is to focus too narrowly upon single events. This leads to countermeasures aimed chiefly at preventing the recurrence of individual failures, particularly human ones. But organizational accidents have multiple causes. No one factor is necessarily more important than any other. It was their combination that caused the event. To learn the right lessons from the past, it is best to analyse several domain-related events using a common classificatory framework. This reveals patterns of cause and effect that are rarely evident in single-case investigations. Moreover, these patterns also indicate which of the local or organizational factors is playing a regular part in contributing to adverse events. This, in turn, guides the selection of which organizational processes to sample proactively on a regular basis. There are many possible candidates for regular assessment, but few organizations have the resources to sample them all, nor would it be appropriate to do so. The analysis of multiple events tells us which of many possible processes are the most likely to provide a valid and cost-effective measure of current safety health. An effective reactive technique should give an accurate picture of the weaknesses and absences that existed in the defences. These 'snapshots' reveal the often improbable ways that even the most elaborate defences can be defeated. Once again, analyses of several events from the same domain can bring to light recurrent, and often surprising, patterns of defensive weakness. The most important navigational aids, however, lie on the right-hand side of Table 6.1. They are the proactive process measures that can be applied before a bad event. Their purpose is to provide regular checks both on the system's defences and on its 'vital signs' at the workplace and organizational levels. Just as in medicine, there is no single comprehensive measure of safety health. It involves sampling a subset of a potentially larger collection of indices reflecting the current state of various organizational processes. The number of such diagnostic checks ranges typically from around eight to 16 and will vary from one type of system to another and their purpose is to identify those two or three processes that are in most urgent need of attention. The frequency with which these checks are carried out will depend upon the rate at which the process in question is likely to change. Workplace factors change more rapidly than organizational ones, and so need to be monitored at more frequent intervals. We will return to these procedures at a later point, and a practical guide to their selection and application will be given in the next Chapter. In the meantime, we must look briefly at near-miss and incident reporting, a type of safety measurement that is not only developing rapidly in a variety of domains, but. which also yields both reactive and proactive data. Near-miss and Incident Reporting Schemes For some people, the terms 'nearmiss' and 'incident' have distinct meanings, but since this is not a universal practice, we will use 'nearmiss' to cover all such events. A near-miss is any event that could have had bad consequences, but did not. Near-misses can range from a partial penetration of the defences to situations in which all the available safeguards were defeated but no actual losses were sus-tained. In other words, they span the gamut from benign events in which one or more of the defences prevented a potentially bad out-come as planned, to ones that missed being catastrophic by only a hair's breadth. The former provide useful proactive information about system resilience, while the latter are indistinguishable from fully-fledged accidents in all but outcome, and so fall squarely into the reactive camp. The advantages of collecting and analysing near-misses are clear. In Exxon's nice phrase, they provide 'free lessons'. • If the right conclusions are drawn and acted upon, they can work like 'vaccines' to mobilize the system's defences against some more serious occurrence in the future-and, like good vaccines, they do this without damaging anyone or anything in the process. • They provide qualitative insights into how small defensive failures can line up to create large disasters. • Because they occur more frequently than bad outcomes, they yield the numbers required for more penetrating quantitative analyses. • And, perhaps most importantly, they provide a powerful reminder of the hazards confronting the system and so slow down the process of forgetting to be afraid. But, for this to occur, the data need to be disseminated widely, particularly among the bean counters in the upper echelons of the organization. The latter have been known to become especially alert when the information relating to each event includes a realistic estimate of its potential financial cost to the organization. There are, of course, a number of problems facing near-miss reporting schemes, not least that they all depend upon the willingness of individuals to report events in which they themselves may have played a significant part. The factors contributing to a 'reporting culture' are discussed at length in Chapter 9. An informant may be willing to report an event, but not be able to give a sufficiently detailed account of the contributing factors. Sometimes reporters are not aware of the upstream precursors. On other occasions, they may not appreciate the significance of the local workplace factors. If they have been accustomed to working with substandard equipment, they may not report this as a contributing factor. If they habitually perform a task that should have been supervised but was not, they may not recognize the lack of supervision as a problem-and so on. Together, the willingness and the ability issues are likely to have two effects: not all near-misses will be reported, and the quality of information for anyone event may be insufficient to identify the critical precursors. But the very considerable successes of the best schemes (discussed in Chapter 7) indicate that the advantages greatly outweigh these difficulties. There seems little doubt that near-miss reporting schemes can provide an invaluable navigational aid. Proactive Process Measurement: The Priorities The rationale for making regular assessments of the organizational processes underlying both safety and quality has already been given above. Our purpose here is to look more closely at the things that could be measured and the likely returns on investment for each of the possible areas. The areas in question are those outlined in the triangular region at the bottom of Figure 1.6, and shown separately in Figure 6.6. They are unsafe acts, local workplace factors and or-ganizational factors. All three exist independently of events. Defences are not considered as a separate entity because they are so closely intermingled with the human, local and organizational factors already identified in Figure 6.6. The relative merits of each type of proactive process measurement are considered below: • Unsafe acts. Unsafe acts are the stuff of which accidents are made. The numerous 'iceberg' theories of accident causation presume various-widely differing-ratios between fatalities, injuries and unsafe acts. But there are serious measurement problems. The actual numbers of unsafe acts committed are almost impossible to determine. What is certain is that these numbers are large. Of greater value than the risk achievement worth numbers-more or less unobtainable in any case-is information relating to the nature and variety of unsafe acts, particularly when it is linked to the type of activity being performed (see Chapter 5). Different unsafe acts require different kinds of management (see Chapter 7). Errors are essentially information-processing problems and require the provision of better information, either in the person's head or in the workplace. Violations, on the other hand, have their origins in motivational, attitudinal, group and cultural factors, and need to be tackled by counter-measures aimed more at the heart rather than the head. • Local workplace factors. One stage upstream from unsafe acts are their immediate mental and physical precursors-such things as poor workplace design, clumsy automation, inadequate tools and equipment, unworkable procedures, the absence of effective supervision, high workload, time pressure, inadequate training and experience, unsociable hours, unsuitable shift patterns, poor job planning, undermanning, badly calibrated hazard perception, inadequate personal protective equipment, poor teamwork, leadership shortcomings and the like. They are likely to be fewer in number than the unsafe acts they breed. As such, they are more easily managed than the human condition. But they are still only the local expressions of higher-level organizational problems. • Organizational factors. Only in the upper levels of the system can we begin to get to grips with the 'parent' failure types-the processes that create the downstream 'problem children'. If these remain unchanged, then efforts to improve things at the workplace and worker level will be largely in vain. The damaging effects of certain kinds of unsafe act may be reduced and specific conditions within the workplace improved, but the continuing existence of the 'parent' failures in the upper echelons of the organization will ensure their rapid replacement by other kinds of human and workplace problems. Clearly, then, organizational factors represent the priority area for process measurement. But how do we select the processes to be measured? Organizations are made up of many elements. If each element were wholly independent of the others, it would only be possible to assess a company's overall safety health by measuring all the elements individually. Alternatively, if all the elements were closely related one to another, then the state of anyone of them should provide a global indication of the organization's intrinsic safety. The reality probably lies somewhere between these two extremes, with the individual elements being clustered in an overlapping and modular fashion. A recent review of a number of safety process measures identified five broad clusters, as listed below: • safety-specific factors (for example, incident and accident reporting, safety policy, emergency resources and procedures, off-the-job safety and so on) • management factors (for example, management of change, leadership and administration, communication, hiring and placement, purchasing controls, incompatibilities between pro- duction and protection and so on) • technical factors (for example, maintenance management, levels of automation, human-system interfaces, engineering controls, design, hardware and so on) • procedural factors (for example, standards, rules, administrative controls, operating procedures and so on) • training (for example, formal versus informal methods, presence of a training department, skills and competencies required to perform tasks and so on). At the core of these clusters and pervading all of them is the issue of culture. For the present, we can link cultural factors to the three Cs, discussed earlier: commitment, competence and cognisance-but as they exist within the organization as a whole, rather than in the mind of anyone senior manager. This composite picture of the main dimensions of process measurement is summarized in Figure 6.7. The message of Figure 6.7 is straightforward. There will be wide variation in the kinds of process that could be measured proactively from one domain to the next. This is not a problem so long as sufficient and even-handed attention is given to the main subsystems underpinning organizational safety. There are many possible ways in which one could assess the quality of the training, the procedures, the engineered safety features, and so on. What matters is that a principled attempt is made to sample each of the six main dimensions identified above: culture, training, management, safety-related issues, procedures and technical factors. The purpose of each measurement exercise is twofold. First, to identify those processes giving the greatest cause for concern at that time. Second, to track the effectiveness of previous remedial measures. Measurements that are not used to guide the enhancement of system 'fitness' are not worth taking. Are Accidents Really Necessary? History shows that the cause of safety flourishes in the aftermath of disaster, albeit briefly. Must we conclude that it takes catastrophes of the magnitude of Piper Alpha, Chernobyl, or Bhopal before politicians and top managers understand that investment in safety is good business? Do organizations have to fall over the edge before they know where it is? Are accidents, even the more frequent small ones, necessary to calibrate the effectiveness of safety measures? Are companies doomed to fighting the last fire or trying to prevent the last crash? The answer must be yes-if complex hazardous organizations continue to rely principally on outcome measures in order to navigate the safety space. This chapter has outlined a workable alternative-the regular assessment of the organizational processes that are common to both quality and safety. Latent accident-producing conditions are present now. It is not necessary to wait for bad events to find out what they are. But we cannot expect to remedy them all at once. Systems need principled ways of identifying their most urgent process problems in order to deploy their limited remedial resources in the most efficient and timely manner. We have discussed some of the principles by which cost-effective safety measurement can be achieved. Making and acting upon proactive assessments of the system's vital signs together with the intelligent application of near-miss reporting will not guarantee freedom from accidents, but it will take an organization closer to the only achievable safety goal-acquiring the maximum degree of intrinsic resistance to its local hazards and then sustaining it. The last being the hardest part. .In the next chapter, we move from guiding principles to real-world practices. Human errors pose the greatest single threat to hazardous technologies. Chapter 7 offers a practical guide to error management and reviews some of the measuring instruments and tools currently available for this purpose.