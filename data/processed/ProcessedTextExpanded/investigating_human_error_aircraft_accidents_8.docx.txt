Title: Investigating Human Error Incidents, Accidents, and Complex Systems – Chapter(s) 7 - 8 Author(s): Barry Strauch Category: Safety Tags: Human, Error, Investigation, Accidents, Incidents, System 7 The Regulator President Park Geun-hye of South Korea vowed on Monday to disband her country’s Coast Guard, saying that South Korea owed “reform and a great transformation” to hundreds of high school students who died in a ferry disaster last month. Choe, 2014 New York Times Introduction In one form or another, regulators are essential components of complex systems. They provide a degree of assurance to the public that the complex systems it depends upon are safe, and they regulate the systems to ensure that minimum standards of safe operation are maintained. Regardless of the complexity of a particular system or a system with operational safety measures with which people may be unfamiliar, a government agency is typically responsible for overseeing the safety of the system. Whether one boards an airplane or a ferry or turns on an electrical appliance that receives its electrical power from nuclear energy, an agency of some kind regulates the particular industry. Throughout the world, some level of independent supervision and inspection of complex systems has become expected. In the United States, with some exceptions, federal agencies carry out the oversight. In Australia, individual states oversee the railroads. The level of regulator oversight over a company’s operations varies among countries, industries, and regulators. Some oversee relatively minute aspects of company operations, while others play a less active role in the systems they oversee. In general, the more consequential the potential adverse effects of a system accident, the more likely that a regulator will be involved in overseeing the system. The importance of the regulator to system safety can be seen in the marine environment. Ferry accidents, in which hundreds of people have been killed, have occurred in countries with large inter-island ferry transportation systems but with relatively weak regulator oversight. For example, the April 14, 128 2014, ferry accident in South Korea involving the ferry Sewol, referred to at the beginning of this chapter, killed over 300 passengers. After the accident, allegations arose over lax oversight of the ferry. The agency responsible for overseeing the safety of the vessel was reported to have failed to recognize how changes to the vessel’s structural design could affect its stability, which, if true, is a critical regulatory oversight error. Depending on the industry, regulators establish standards for operator licensing and training, maintenance and inspection, operating procedures, operator medical standards, and in some industries, even organizational structure, activities that parallel many of those of organizations and companies. A weak or ineffective regulator communicates to the industry it oversees and to the operators of that industry that deficient performance, whether deliberate or inadvertent, will be overlooked. Given the similarity of the regulator oversight tasks to those of companies, investigators can assess the adequacy and effectiveness of regulator performance and of regulator antecedents to error in ways that are similar to those conducted for companies by examining the role of many of the antecedents previously outlined. For example, the effectiveness of rules governing equipment design features can be gauged by the standards of data conspicuity, interpretability, and other characteristics discussed in the preceding chapter. What Regulators Do Regulators primarily perform two functions that are critical to system safety? They establish rules ensuring the safety of system operation and equipment design, and they enforce compliance with those rules. The rules are designed to provide a minimum level of safety to the systems they oversee. Organizations that operate at that level would be expected to operate safely. Of course, exceeding those levels of safety would be encouraged, but not required, by the regulator. Regulator antecedents are either a function of inadequate rules, poor oversight, or both, and investigators must examine whether regulator oversight, in either capacity, led to errors that caused accidents. Regulators are expected to establish sufficiently rigorous rules, methods, and standards to provide a minimally acceptable standard of public safety. Yet, at the same time, overly restrictive regulations may inhibit operations and, thus, a company’s ability to operate profitably. As a result, regulators face pressures unlike other system elements. They are asked to oversee the safety of systems with potentially catastrophic consequences to system malfunction without restricting the freedom of companies to operate their systems profitably. 129 Reason (1997) refers to “the regulators’ unhappy lot,” in which they are caught between demands for absolute system safety, resistance to what may be considered excessive oversight, and a public that is often unwilling to provide regulators the resources necessary to enable them to carry out their missions effectively. As a result, regulators, whose work is effective with continuous system safety, rarely attract public attention. Reason (1997) points out that the public may consequently become unwilling to provide the resources regulators need to enable them to conduct the oversight needed to effectively monitor the safety of the systems that they are tasked with overseeing. Since the benefits of effective regulatory oversight are mainly seen in their absence, such as after an accident has occurred, reducing the funding of regulators may appear to be a relatively painless way to reduce the expenditure of public funds while at the same time not inhibiting perceptible levels of public safety. As Reason (1997) notes, this leads to less than satisfactory consequences. In an effort to work around these obstacles, regulators tend to become dependent upon the regulated companies to help them acquire and interpret information. Such interdependence can undermine the regulatory process in various ways. The regulator’s knowledge of the nature and severity of a safety problem can be manipulated by what the registered organization chooses to communicate and how this material is presented. (p. 174) Further, regulators also face pressure to maintain their expertise in the industries they oversee in the face of continued technological advances. As the pace of technological change increases, regulators find it increasingly difficult to effectively evaluate systems and the procedures needed to operate them. Yet, as new hardware and software are introduced into service, regulators are expected to anticipate potential operating errors in the use of the new systems, even if they lack the technical expertise to evaluate them. As Reason (1997) explains, Regulators are in an impossible position. They are being asked to prevent organizational accidents in high-technology domains when the aetiology of these rare and complex events is still little understood. (p. 171) Regulator Activities Given the two functions of regulators, enacting rules and enforcing those rules, regulator antecedents to error fall into one of those categories. As with other antecedents, investigators must work backward from the error through the system to identify the regulator antecedents to the error. The nature of the antecedent determines the resultant error. 130 Enacting Rules There tend to be fewer antecedents from regulator shortcomings in the rules that govern the industry than in those related to rule enforcement because, over time, as regulatory shortcomings become recognized, often through accidents, regulators tend to correct the shortcomings rectified by enacting rules that address particular deficiencies. For example, in the 1970s and 1980s, as aviation accidents involving well-designed and well-maintained aircraft due to crew error continued to occur, regulators recognized that pilots needed to be trained in CRM. In response, in 1998, the U.S. Federal Aviation Administration required pilots of air transport aircraft to complete cockpit resource management training (Federal Aviation Administration, 2004). This type of regulatory deficiency could be seen in a ferry accident that occurred in New York City in 2003 (National Transportation Safety Board, 2005). The vessel operator experienced what investigators termed an “unexplained incapacitation” as he was about to dock the vessel. As a result, he did not slow the vessel as it neared the dock, and it crashed into the dock, killing 11 passengers. The investigation found that the operator had been taking a prescribed pain medication, and one of its side effects included seizures, a possible factor in explaining his incapacitation. Yet, the Coast Guard, the federal agency that regulates U.S. Marine operations, had no prohibition in place to warn mariners against using the medication. Nonetheless, fearing suspension of his license, the mariner did not report using the medication to the Coast Guard. As a result, investigators identified shortcomings in the Coast Guard’s medical oversight system, but they did not consider those to have played a part in the accident. Investigators identified these shortcomings as safety concerns in their report and recommended that the Coast Guard upgrade its system of medical oversight of mariners to bring it in line with the medical oversight that other federal transportation regulators (such as the Federal Aviation Administration) conducted. To its credit, the Coast Guard agreed and, over several years, considerably upgraded its system of medical oversight of mariners to a level considered equivalent to that of other transportation regulators. In some systems, such as aviation and marine, both domestic and international regulators establish rules governing system operations through agencies that are entities of the United Nations. In aviation, this is carried out by the ICOA, and in marine, it is carried out by the IMO. In both transportation modes, their rules, once adopted, are enforced by countries on behalf of the international regulators, which have no enforcement authority themselves. When investigating the role of the regulator in an accident, document the applicable regulations and determine whether the errors identified in the accident had been addressed in the existing regulations. Be aware of rules that are so general as to be of little value in actually ensuring safety. For example, pilots in the United States, after an accident due to human error, 131 can be charged with violating rule 14 Code of Federal Regulations 91.13 for so-called “careless and reckless” operations. The rule states: a. Aircraft operations for the purpose of air navigation. No person may operate an aircraft in a careless or reckless manner so as to endanger the life or property of another. Because the rule does not distinguish between an error due to equipment design or training, oversight, and so on, a pilot involved in an accident in which error played a part can, in effect, be charged with violating this rule. Enforcing Rules Antecedents to shortcomings in regulator performance tend to fall in this general category, typically in accidents in which the regulator had information or should have recognized that an entity’s operational safety was deficient. For example, when identifying errors resulting from ineffective training, for example, training that was required to meet certain regulatory standards, investigators should determine whether the regulator training should have recognized and addressed the shortcomings and, if so, why they were not addressed. Investigators of a marine accident that occurred 4 years after the previously discussed ferry accident similarly identified an operator whose use of prescription medications with impairing side effects played a role in the cause of the accident (National Transportation Safety Board, 2009). Unlike the previous accident, the mariner had informed the Coast Guard of some, but not all, of the drugs he had been taking, but the Coast Guard, which by this time had upgraded its medical oversight system, failed to follow up on the mariner’s drug use to determine whether the prescribed medications he had provided information on to the regulator adversely affected his performance (they did). Because the regulator, the Coast Guard, had information about this mar- miner’s use of impairing prescription medications, and because it had been cited previously for its deficient oversight of the mariner's medical status but still permitted the mariner to operate while using impairing medications, investigators determined that the regulator had played a role in the cause of the accident. As they wrote: “Also contributing to the accident was the U.S. Coast Guard’s failure to provide adequate medical oversight of the pilot in view of the medical and medication information that the pilot had reported to the Coast Guard” (National Transportation Safety Board 2009, p. 136). This accident also demonstrates that in an accident, a regulator rarely directly causes an error that leads to an accident, as it would, for example, with a company that operated a system unsafely. Regulators do not cause 132 accidents, but by not preventing companies from operating unsafely, regulators can be considered to contribute to accidents rather than cause them. Alternatively, by failing to ensure compliance with its regulations, regulators can be considered to have permitted companies to cause accidents through their own actions (or inactions). Case Study Regulators play a role in overseeing the safety of many systems in addition to the ones that we typically consider. For example, in one famous incident, the regulator of the U.S. financial industry failed to act on information that it had, suggesting the need for a higher level of enforcement than what it had been providing. On December 10, 2008, Bernard Madoff, a well-respected financier who had headed a leading electronic securities exchange, confessed to his sons that the investment firm that he headed, Bernard L. Madoff Investment Securities, was a Ponzi scheme. A Ponzi scheme is an illegal enterprise in which the operator takes money from one investor and gives it to another, pocketing some of the money for himself or herself by promising the “investor” high returns on the money provided. The sons alerted the U.S. federal regulator of the financial industry, the Securities and Exchange Commission (SEC), of their father’s activities, and the next day, Madoff was arrested for securities fraud, among other criminal charges. Madoff pled guilty to multiple counts of securities fraud 7 months later, at the age of 76, and was sentenced to 150 years in prison. Bernard Madoff’s fraud was a classic Ponzi scheme, a type of fraud named for Charles Ponzi, who, according to the SEC, defrauded thousands of persons in New England in a phony postage system-theoretic accident model and processes investment scheme (SEC, 2009). Newspaper accounts said that investors lost an estimated $17 billion in the money that they had given Madoff to invest. Counting the paper losses from the fraud, that is, the cash losses with the loss of the income Madoff’s fund was alleged to have generated, the total loss that investors sustained was estimated to exceed $60 billion. Among the defrauded investors were close Madoff friends, neighbors, relatives, and the late Nobel Prize winner, Elie Wiesel, who lost his life savings and whose charitable organization lost over $15 million (Strom, 2009) in the scheme. The SEC establishes financial reporting requirements for the governance of publicly owned corporations and the trading of equities in those corporations, among its other responsibilities. Its role is vital to the integrity of the U.S. financial system. Ineffective SEC oversight by a regulator that was established by the federal government during the Great Depression to oversee an industry whose fraudulence had led to widespread financial losses of duped 133 investors could lead to financial catastrophe, the financial equivalent of a catastrophic accident in a complex system. After Madoff’s arrest, it was learned that the SEC had, on multiple occasions, been informed of suspicions regarding Madoff’s trades. Not only had several investors reported their suspicions to the Commission, one had filed three separate, signed complaints and then met with SEC officials to explain why Madoff’s purported returns, which he was required to regularly report to the SEC and to his investors, could not have been legally obtained. Further, two periodicals, one, Barron’s, a widely read and well-respected U.S. business publication, published articles in May 2001, 7 years before Madoff’s arrest, suggesting improprieties with Madoff’s securities. In addition to the suspicions raised by complainants to the SEC and by financial publications, numerous “red flags” or suspicions regarding the nature of Madoff’s alleged investments should have been evident to the regulator in its oversight of the Madoff securities company. As investigators of the SEC’s failure, its inspector general reported, • The returns on Madoff’s investments were consistent and largely unrelated to actual market performance over a 14-year period, a highly unusual result through several market downturns • Madoff did not charge fees per trade, as was the Wall Street practice • His timing of trades, that is, selling before a downturn and buying before the market turned up, was consistently successful, something that is also highly unusual • The outside auditor of his fund (an SEC requirement analogous to an independent auditor of an safety management system system) was his brother-in-law and not a major public accounting firm • Several investors who considered investing in Madoff’s funds, and who then examined the funds closely (conducting “due diligence”) using publicly available information, became suspicious of the funds and would not invest in them • The alleged financial strategy that the Madoff fund employed could not be duplicated by other investors In response to both the complaints it received and the publication of the articles about Madoff’s fund, the SEC conducted at least two examinations of Madoff’s securities, one in 2004 and one in 2005. Neither uncovered the Ponzi scheme. How, then, did the federal regulator responsible for overseeing the U.S. financial industry miss discovering Madoff’s fraud, one that was relatively unsophisticated (as are all Ponzi schemes) and one that private investors became suspicious of using publicly available information? To answer this question, the SEC’s Office of the Inspector General (OIG), an independent monitor established by the federal government to provide 134 impartial examinations of the performance of federal agencies, conducted an investigation into the SEC’s failures with regard to its oversight of Madoff’s securities. What made the SEC’s failure particularly troublesome was its history and role in U.S. financial oversight. The SEC had • Over 60 years of experience in overseeing the financial industry • Considerable in-house expertise at detecting fraud in the industry • Promulgated the rules that Madoff was accused of violating. Underlying the OIG investigation was the knowledge that had the fraud been discovered sooner, for example, at the time of its own investigations, investors would have saved billions of dollars in losses because Madoff’s activities would have been terminated, and many investigators would not have lost their life savings. The OIG report (SEC, 2009) describes, in considerable detail, the errors of a regulator tasked with considerable responsibility to oversee a vast and complex system. These included: • Failing to properly oversee those SEC officials charged with examining Madoff’s securities • Selecting examiners who lacked the necessary expertise to address the allegations against Madoff • Failing to follow up on overt discrepancies in Madoff’s statements to examiners • Failing to understand the nature of Madoff’s alleged financial crimes and not making an effort to understand them • Failing to recognize the significance of the suspicions raised against Madoff’s funds • Ineffectively communicating findings of one investigation to those conducting the second, resulting in considerable duplicated efforts • Failing to verify information Madoff provided with readily obtainable information from outside entities that would have demonstrated the deceptiveness of Madoff’s claims The report cited a litany of mistakes and misjudgments that individuals within the agency committed regarding Madoff, including repeated errors by the same persons. Although the report cited errors that, as with many errors, may appear to be inexplicable in hindsight, implicit in the report is the sense that some SEC managers were trying to meet the agency’s many responsibilities in the face of numerous mandated tasks, with limited resources. Investigators also determined that some SEC personnel, as many outside the 135 agency, were impeded in their efforts by their difficulty in believing that someone as well-regarded in the financial industry as Bernard Madoff, with many prominent investors among his clients, could have been the perpetrator of a simple Ponzi scheme. Regulator personnel did not cause the Madoff Ponzi scheme, but they failed to uncover it, despite what was shown to have been considerable available information suggesting the scheme and, in one instance, outlining it. This failure was due to bureaucratic shortcomings in the performance of the agency, managers who chose people for the Madoff investigation who lacked the necessary expertise and thereafter provided them with little guidance and follow-up. Further, these managers also failed to understand the nature of the scheme and avoided opportunities to understand it. The failure of the SEC to recognize the fraud, after suspicions regarding Madoff were first raised in the years before the scheme collapsed, cost investors billions of dollars, including many who lost their life savings. The regulator did not cause the fraud, but its failure contributed to its severity. Summary Regulators play critical roles in the safety of complex systems. They establish the rules governing the operations of the systems they oversee, and they enforce compliance with those rules to ensure system safety. Regulators may establish and enforce rules governing system design, system operation, maintenance, and personnel qualifications. Many of the antecedents relating to equipment design and organizational antecedents apply to regulator antecedents as well. Regulator antecedents to error do not cause accidents directly but, by not preventing organizations from operating unsafe systems, allow errors that can lead to accidents occurring. Typically, regulator shortcomings arise from failures to enforce rules rather than from not enacting them. However, in some instances, accidents have occurred in which regulators were shown to have failed to enact adequate rules. In one accident, the regulator overhauled its oversight system and tightened its rules governing medical oversight, but inadequate enforcement allowed violations of its rules to lead to a subsequent accident. DOCUMENTING REGULATOR ANTECEDENTS • Examine regulator inspector selection criteria and inspection training and determine their competence in assessing the safety of a company’s operations. • Apply the presentation and control design standards outlined in Chapter 4 to assess the quality of the regulator’s oversight and approval of the design of equipment used in the system being overseen. • Determine the extent to which the regulator effectively assessed the technology incorporated in the equipment. • Evaluate the extent to which the regulator’s operator-licens- ing requirements provided an acceptable level of safe system operation. • Examine the effectiveness of regulator approval of operating rules and procedures as applied to the circumstances of the event. • Determine the number of inspections of a company and the thoroughness of those inspections to determine the extent to which the regulator met the oversight standards it established. • Identify information the regulator had, such as previous company incidents, accidents, and rule violations, indicating the need for additional oversight and determine whether additional oversight was conducted. • Determine how responsive the regulator was in addressing safety deficiencies that it had identified. 8 Culture At Korean Air, “such teamwork has been nearly impossible,” says Park Jae Hyun, a former captain and Ministry of Transportation flight inspector. Its cockpits have operated under an “obey or else” code, he says. Co-pilots “couldn’t express themselves if they found something wrong with a captain’s piloting skills. Carley and Pasztor, 1999 Wall Street Journal Introduction On August 6, 1997, a Korean Air Boeing 747 crashed into a hill several miles short of the runway in Guam, killing 228 passengers and crewmembers (see Figure 8.1). This accident was the latest in a string of major accidents that investigators had attributed, at least in part, to errors that the airline’s pilots had committed (National Transportation Safety Board, 1999). In the 16 years before this accident, the airline experienced one of the highest accident rates of any airline. These included the following: • August 1983: A Boeing 747 deviated more than 300 miles off course into Soviet territory before the Soviet Air Force shot it down • December 1983: A DC-10 crashed in Anchorage after the pilots attempted to take off from the wrong runway • July 1989: A DC-10 crashed in Libya after the crew mishandled an instrument approach • August 1994: An Airbus A300 crashed in Cheju, Korea, after the crew landed at an excessive airspeed Even after the Guam accident, the most serious event the airline experienced since the Soviet Air Force shot down its Boeing 747, pilot error-related accidents continued to plague the airline. These included the following: 138 • August 1998: A Boeing 747 crashed after the captain misused the thrust reverser while landing at Seoul • September 1998: An MD-80 ran off the end of the runway at Ulsan, Korea • March 1999: An MD-80 ran off the end of the runway at Pohang, Korea • April 1999: An MD-11 freighter crashed in Shanghai In 1999, the Wall Street Journal implied that factors rooted in Korean society and culture affected the airline and its safety record (Carley & Pasztor, 1999). The newspaper stated that Korean Air’s history emphasizes hierarchy. It is easy to discern the hierarchy: former [Korean] Air Force pilots, then fliers from other military services, and Cheju men [civilians that the airline trained] at the bottom. Red-stone rings worn by Korean Air Force Academy graduates command instant respect. And ex-military men, while training co-pilots in simulators or during check rides, sometimes slap or hit the co-pilots for mistakes. In the cockpit, friction, and intimidation can cause trouble. For a civilian co-pilot to challenge a military-trained captain “would mean loss of face for the captain,” says Mr. Ludwin…[a] former Pan Am captain. For the co-pilot, he adds, “it’s more honorable to die, and sometimes they do.” (Carley and Pasztor, 1999, pp. A1, A2) The newspaper raised a critical issue by suggesting that specific Korean cultural factors adversely affected the airline’s safety. Was it correct in its implication? Can cultural influences serve as antecedents to errors? This chapter will examine cultural factors in complex systems and discuss the relationship of cultural factors to operator performance. National Culture Cultural influences affect and are manifested in the behavior of people who work in the same companies, live in the same regions, and belong to the same ethnic groups. Some have suggested that cultural factors affect opera- tor performance (e.g., Orasanu, Fischer, and Davison, 1997) and hence, system safety. “Performance of a plant,” Moray (2000) notes, “is as much affected… by the expectations of society, as by the engineering characteristics of the design and the ergonomics of individual work and the design of communi- cation within and between groups and teams” (p. 860). Schein (1990, 1996) defines a culture as, (a) a pattern of basic assumptions, (b) invented, discovered, or developed by a given group, (c) as it learns to cope with its problems of external adaptation and internal integration, (d) that has worked well enough to be considered valid and, therefore (e) is to be taught to new members as the (f) correct way to perceive, think, and feel in relation to those prob- lems. The strength and degree of internal consistency of a culture are, therefore, a function of the stability of the group, the length of time the group has existed, the intensity of the group’s experiences of learning, the mechanisms by which the learning has taken place, and the strength and clarity of the assumptions held by the founders and leaders of the group. (1990, p. 111) The assumptions that Schein refers to are commonly known as norms, ways that people act, perceive, and interpret the values that they share, that may be unspoken but are nevertheless perceived, felt, and practiced by mem- bers of a group. Cultural norms are recognized by members of their culture. They help group members understand expectations, customs, and beliefs so that they can facilitate integration, acceptance, and beliefs regarding behav- iors. For those outside the culture, they can serve to demarcate exclusion. Cultural norms help travelers understand how to act when meeting mem- bers of a different culture. Cultures can differ in childrearing practices, courtship behavior, and deference to the elderly, for example, and those from different cultures may be uncomfortable with those differences. Avoiding behaviors considered offensive or insulting in different cultures is key to harmonious relations with members of those cultures. 140 Much of the recent work relating cultural factors to system safety was influenced by Hofstede (1980, 1991), and his study of a multi-national cor- poration, IBM, then with offices in 66 countries. In the 1960s and 1970s he administered a Likert-type questionnaire to company employees in their offices across the globe. With this type of questionnaire respondents are given statements and asked to agree or disagree with the statements on a scale generally of one, strongly disagree, to five, strongly agree. He found differences on several dimensions of behavior among the employees of the different cultures. One, he termed “power distance,” refers to the extent to which people perceive difference in status or power between themselves and their subordinates and superiors. In cultures with high power distance, subordinates and supervisors perceive the differences between them to be greater than do those in cultures that score low on power distance. In those cultures, subordinates would be less willing to confront a superior, or call a superior’s attention to an error that he or she may have committed, than would their counterparts in countries with low power distance. A second dimension, “individualism-collectivism,” characterizes the degree to which individuals accept and pursue the goals of the group to which they belong, relative to their own individual goals. An individually oriented person is more self-sufficient and derives more satisfaction from pursuing personal goals than from pursuing group goals. Collectivist- oriented persons identify more with the companies that employ them than do individually oriented persons. They tend to view errors as reflections of the company or group as much as of themselves as individuals. “Uncertainty avoidance,” a third dimension, refers to the willingness or ability of people to contend with uncertain or ambiguous situations. People in cultures with high uncertainty avoidance generally find it difficult to deal with ambiguous or unclear situations that have few applicable procedures. They would be expected to respect and adhere to rules more readily than would their counterparts in cultures with low uncertainty avoidance. Those in cultures that are low in uncertainty avoidance feel comfortable respond- ing to uncertain or novel situations that they had not experienced before, or to situations to which few rules and procedures apply. He labeled a fourth dimension masculinity–femininity. Masculine cultural traits refer to asser- tiveness and toughness, and are focused on material success. Feminine traits are considered to be more modest and tender, and are concerned with the quality of life. When Hofstede initially conducted his research China was relatively unin- volved in the global economy and IBM had no offices in China. Since then China has become fully integrated into the world’s economy and Hofstede applied his inventory to people in China. The result was a fifth dimension, long-term and short-term orientation (Hofstede and McCrae, 2004). Long- term cultural traits stress thrift and perseverance while short-term orien- tation, reflecting traditional Asian or Confucian traits, emphasizes social obligations and tradition. 141 Some researchers have corroborated Hofstede’s findings in a variety of settings and identified additional differentiating cultural characteristics (e.g., Helmreich and Merritt, 1998). Maurino (1994) described five cultural dimensions among operators in aviation that are related to those Hofstede identified. These include adherence to authority compared to a participative and democratic approach, inquiry in education and learning as opposed to rote learning, identification with the group rather than identification with the individual, calm and reflective temperament compared to a volatile and reactive one, and free expression and individual assertiveness compared to deference to experience and age. Helmreich and his colleagues argued that the inability of early cockpit resource management programs to substantially impact the quality of operations was due to the influence of cultural factors and the application of a Western crew model of cockpit resource management to non-Western cultures. As a result, factors such as the critical role of junior officers in safety, a fundamental precept of CRM, would be difficult to accept in some non-Western cultures that stress rank and status. Helmreich, Merritt, and Wilhelm (1999) and Helmreich, Wilhelm, Klinect, and Merritt (2001) contend that these programs were developed in the United States, but when applied to countries in other cultures difficulties developed. Unlike the United States where cockpit resource management programs had originated, organizations found that when cockpit resource management programs were implemented in countries where employees scored high on power distance, the junior operators resisted efforts to be more assertive in dealing with their superiors, and senior operators did not accept their subordinates as fully contributing team members, thus negating many of the perceived benefits of cockpit resource management programs. Researchers have applied Hofstede’s work to a complex system, military aviation, to assess the relationship between cultural factors and safety. Soeters and Boer (2000) examined the safety records of members the North Atlantic Treaty Organization (NATO), and the air forces of 14 of its member countries. Many north atlantic treat organisation pilots operate the same aircraft, and instructors from two of its member countries train almost all of its pilots. The air forces, although distinct, often practice together, follow the same procedures, and use similar criteria to select their pilots. They found a significant relationship between the accident rate of each country and the country’s score on Hofstede’s cultural dimensions, particularly indi- vidualism–collectivism. Countries with cultures that scored high on indi- vidualistic traits had lower accident rates than countries considered more group oriented. Since Hofstede published his findings, his work has come under criti- cism. Tayeb (1994), criticized the methodology Hofstede employed, while Chen (2008) and Heine, Lehman, Peng, and Greenholtz (2002) criticized both the methodology and the interpretations Hofstede drew from the results of his questionnaire administrations. McSweeney (2002) was perhaps, most detailed in criticism of Hofstede’s work. For example, among his criticisms he writes, 142 Having assumed that the pertinent response differences were caused by national values, Hofstede then supposes that the questionnaire response differences are decipherable manifestations of culture (cf. Kreweras, 1982; Smucker, 1982; d’Iribarne, 1991). Despite the criticisms above of (this) assumption…let us temporarily assume it to be correct. It requires another analytical leap to assert that the cause may be identified through its assumed consequences. Disregarding this problem, Hofstede obfus- cates the questionnaire response differences with national culture. (p. 104) Hofstede (2002) responded to McSweeney’s criticism point by point, going as far as to label him an “accountant” as his academic appointment was in a business college. Overall, the criticisms of Hofstede’s work essentially address the datedness of the research, noting how much society and cultures have changed since the 1960s and 1970s when Hofstede collected much of his data, and the difficulties inherent to ascribing cultural dimensions to the results of Likert questionnaires. In truth, the criticisms have some validity. It is difficult to conceive of a researcher today who would label a cultural dimension as masculinity–fem- ininity and ascribe to the feminine side such traits as modesty and tender- ness. Moreover, to ethnographers and anthropologists, who spend extensive time inhabiting the cultures they study in order to observe, identify, and document norms and cultural traits, identifying cultural traits based on the results of a quickly completed questionnaire is difficult to accept. Thus, Hofstede’s dimensions can be criticized because they have remained static while cultures have continued to evolve, and because of the difficulties in the method used to derive them. Nonetheless, his work is still widely accepted by researchers and accident investigators who may intuitively accept the dimensions because of their simplicity and the virtue of their ready applica- tion to common observations of cultural differences. Ultimately, Sondergaard (1994) applied what may be an effective way to determine the validity of Hofstede’s work. “The widespread usage of Hofstede’s culture types beyond (the number of) citation(s),” he writes (p. 447), indicates “…validation of the dimensions by empirical research.” National Cultural Antecedents and Operator Error Regardless of what one thinks of Hofstede’s work and that of others who have employed questionnaires to identify cultural factors, the presence of cultural differences is widely accepted (e.g., Morris and Peng, 1994; Nisbett, Choi, Peng, and Norenzayan, 2001; Klein, 2005). The influence of cultural factors on system operations are evident; they influence a variety 143 of operator–equipment interactions, and can make the difference between effective and erroneous performance. Nonetheless, identifying cultural factors as antecedents of operator error is difficult. For one, language dif- ferences, and difficulty in an operator’s communicating in a language dif- ferent from his or her mother tongue, may account for shortcomings in training and oversight. Further, linking an error to a cultural factor, which may have been subject to considerable criticism for the manner in which it was derived, is not easily accomplished. The factor has to be established by reputable research and be widely accepted. Further, because of the potential presence of other factors that can serve as error antecedents, the difficulty of ascribing errors to cultural factors is made even more difficult. Korean Air may have sustained a high accident rate because of cultural factors for example, but shortcomings in training and oversight, which may have had little to do with Korean cultural factors, may have adversely operator per- formance as well. Strauch (2010) raised an additional difficulty with ascribing error causa- tion to cultural factors. “The very potential of sociotechnical systems for intense time pressure and severe consequences from errors,” he writes (p. 249), “distinguishes system operators from respondents of the bulk of cul- tural research.” Administering questionnaires to office workers may result in identifiable cultural traits, but those traits may not hold true among those who operate complex systems. Office workers do not face potentially severe consequences from error, and they rarely make immediate decisions based on their recognition of the circumstances they are encountering. Strauch (2010) argues that to establish culture as an antecedent to error the factor must meet two initial requirements • The factor must be strong enough to influence behavior • The cultural trait must be sufficiently influential to affect the par- ticular trait In identifying national culture as an antecedent to error, investigators must have sufficient support in the literature to identify the cultural trait in ques- tion as both attributable to a particular culture and sufficiently influential to affect an operator’s performance. If these criteria can be met, investigators must then meet a third criterion, excluding other potential antecedents to error. As described previously, language difficulties or training shortcom- ings, for example, may be incorrectly attributed to cultural factors. In sum, the difficulty of establishing cultural factors as antecedents to error are considerable, and despite strong beliefs on an investigator’s part as to their influence, the challenges to investigators in making such identifications may be formidable. Antecedents to error may well be culturally determined, but ascribing error to such factors in a way that meets the requirements of investigative logic may be difficult to achieve. 144 Organizational Culture Companies, as tribes, religious groups, and nations, can influence their employee’s behavior through the norms that they develop, norms that can be as powerful an influence on employee behavior as can cultural norms (e.g., Schein, 1990, 1996). Numerous illustrations of organizational practices, even among companies seemingly dedicated to enhancing operational safety, demonstrate the potentially adverse effects of norms on safety practices. For example, the New York Times described poor organizational practices in the National Aeronautics and Space Administration after it had experienced sev- eral major project failures, years after the 1986 accident of its Space Shuttle Challenger. The article reported that, In candid reports assessing recent problems with the National Aeronautics and Space Administration’s (NASA) programs to explore Mars, two panels concluded that pressures to conform to the agency’s recent credo of “faster, cheaper, better” ended up compromising ambi- tious projects. To meet the new constraints, the reports said, project managers sacrificed needed testing and realistic assessments of the risks of failure. (Leary, 2000) Interestingly, the author indicated that national aeronautics and space administration management had been criti- cized for many of the same management practices and norms demonstrated after the agency sustained the 1986 Space Shuttle Challenger accident. As with national cultures, corporate cultural factors can affect safety, and become antecedents to error or mitigate opportunities for error. Moreover, employee groups within companies develop their own norms based on com- monly held professional standards and beliefs. Vaughan (1996) examined the influence of the cultures at NASA, its primary space shuttle contractor, Morton Thiokol, and the shared engineer culture of both, on the Challenger accident. She suggested that engineers and their supervisors at both Morton Thiokol and national aeronautics and space administration had developed techniques of responding to the risky technology involved in space operations that minimized the perception of and appreciation for the risks inherent to the mission. Despite consider- able evidence suggesting that the low outside temperatures at the time of the launch could seriously degrade the integrity of the system, officials at both organizations agreed to the launch, which proceeded with disastrous results. Safety Culture Recent years have witnessed increased calls for companies to enhance their “safety cultures” as a means to improve the safety of their system opera- tions. The term safety culture itself is largely credited to the International 145 Atomic Energy Agency which found that the safety culture of the Soviet Chernobyl nuclear facility created the circumstances that led to the accident. In response the International Nuclear Safety Advisory Group or INSAG, of the International Atomic Energy Agency, developed protocols for nuclear power facilities to enhance their safety culture (International Atomic Energy Agency, 1991). In the United States, two federal agencies have promulgated policies calling for the companies they regulate to implement programs to enhance their safety culture. One, the Nuclear Regulatory Commission, wrote in a policy statement: In the United States, incidents involving the civilian uses of radioac- tive materials have not been confined to a particular type of licensee or certificate holder, as they have occurred at nuclear power plants and fuel cycle facilities and during medical and industrial activities involv- ing regulated materials. Assessments of these incidents revealed that weaknesses in the regulated entities’ safety cultures were an underlying cause of the incidents or increased the severity of the incidents. (Nuclear Regulatory Commission, 2011, p. 34774) What is safety culture and why are regulators endorsing the concept and encouraging the companies they regulate to adopt safety cultures? According to another U.S. federal agency, the Bureau of Safety and Environmental Enforcement, safety culture is defined “as the core values and behaviors of all members of an organization that reflect a commitment to conduct busi- ness in a manner that protects people and the environment” (Bureau of Safety and Environmental Enforcement, 2013). Although the Bureau of Safety and Environmental Enforcement has defined the term, there is little uniform agreement as to what safety culture refers. This is largely because safety culture calls for the definition of two distinct terms, safety and culture, both of which are difficult to define and/ or, are defined differently according to one’s perspective or field of study. As a result, according to Guldenmund (2000), “the concepts of safety cul- ture and safety climate are still ill-defined and not worked out well; there is considerable confusion about the cause, the content and the consequence of safety culture and climate, and the consequences of safety culture and climate are seldom discussed” (p. 247). Company Practices Aspects of a company’s culture are revealed in its selection policies, oper- ating procedures, and operational oversight, all of which can affect perfor- mance. Companies that operate complex systems are required to perform these tasks, but companies that are especially safety oriented will perform them more thoroughly, and at a higher level, than would be expected of others. Practices that encourage operator responsibility, professionalism, 146 and participation in safety matters can enhance operator attention to safety details; punitive practices do not. A company’s culture can also be reflected in its definitions of and response to employee transgressions. Companies that require extensive documentation of occasional and infrequent medical absences, for example, encourage their employees to report to work when ill, increasing the likelihood of errors. Managerial instability and frequent changes in supervisory personnel, supervisory practices, and operating procedures may reflect instability, an indication of a corporate cultural factor that could affect safety. Instability can adversely affect operator performance by leading to frequent changes in interpretations and enforcement of policies and procedures. One supervi- sor may interpret procedures literally and expect the same interpretation and compliance from operators. Another may interpret the procedures dif- ferently, and permit operators to comply with those he or she considers most important, while ignoring those that are perceived to have little or no influ- ence on system operations. Instability can also signal dissatisfaction with the company and its practices. Previous company incidents and accidents can also reveal much about cor- porate commitment to safety. Numerous incidents and accidents relative to those of comparable companies suggest deficiencies in company practices, standards, and oversight. Similar issues found in multiple events may indi- cate an unwillingness to identify and address potential system safety haz- ards. On the other hand, thorough company investigations of incidents and accidents and sincere efforts to address identified safety deficiencies reveal aspects of a positive corporate culture. Investigators noted the adverse effects on safety of some organiza- tional norms in a January 1996 rail accident outside of Washington, D.C. (National Transportation Safety Board, 1996). A Washington Metropolitan Area Transit Authority subway train was unable to stop and struck a train stopped ahead on the same track, killing the moving train’s operator. The track had received a large amount of snow that had fallen throughout the day, reducing friction on the exposed tracks. Several times before the acci- dent the train operator had requested authorization to disengage automatic train control and operate the train manually to better control braking on the slippery track. However, the Authority’s director of operations had prohib- ited manual train control under any circumstances, in an effort to reduce train wheel wear. Supervisors were reluctant to violate his order and grant the train operator’s request, despite their awareness of the slippery track conditions. Not one of the supervisors believed that he had the authority to countermand the policy, even though all knew that adhering to it posed a threat to system safety. The lessons of this accident apply to others in a variety of settings. Corporate norms that encourage unquestioning acceptance of rules risk jeopardizing safety when they no longer apply, and companies that manage through fear will, over time, increase the probability of unsafe operations. High Reliability Companies Companies that operate complex systems can establish practices that promote safe operations. After studying high-risk systems, Rochlin (1999) described what he calls “high reliability organizations.” Expecting to focus on avoid- ing errors and risk management, he found that some organizations tended to anticipate and plan for, rather than react to, unexpected events. They attended to safety while efficiently operating complex systems, rewarded error reporting, and assumed responsibility for error rather than assign- ing fault. These companies actively sought to learn from previous errors by maintaining detailed records of past events and applying the lessons of those events to system operations. As Rochlin writes, Maintenance of a high degree of operational safety depends on more than a set of observable rules or procedures, externally imposed training or management skills, or easily recognized behavioural scripts. While much of what the operators do can be formally described one step at a time, a great deal of how they operate, and more important, how they operate safely, is “holistic,” in the sense that it is a property of the inter- actions, rituals, and myths of the social structure and beliefs of the entire organization, or at least of a large segment of it. (p. 1557) Rochlin argues that an organization’s “interactions, rituals, and myths”— essentially its norms—can either help to create antecedents to error, or can anticipate and minimize their presence. An organization with a “good” cul- ture encourages safety, even at the expense of production. It fosters commu- nication among its employees and can proactively uncover problems in its operations. Westrum and Adamski (1998) offered techniques for companies to enhance safety through internal communications and error reporting. Reason (1990) describes the benefits of programs that allow employees to report mistakes without retribution. Industries in several countries have implemented such self-reporting programs, which have provided consider- able information about potential antecedents to error, before the antecedents could affect operator performance. The aviation industry has implemented a number of these programs, such as ASRS—Aviation Safety Reporting System in the United States, and CAIR—Confidential Aviation Incident Reporting, in Australia. Organizational Cultural Antecedents and Operator Error Identifying organizational cultural antecedents to an operator’s error calls for the same steps used to identify organizational factors as error anteced- ents outlined in Chapter 6. Consequently, investigators need to determine 148 whether the company (1) acted, decided, or made decisions improperly in the face of information alerting them to the need for different actions or deci- sions, (2) acted or decided improperly in the face of self-evident information of the need for corrective action, or (3) took no action or made no decision when an action and/or decision was warranted. Summary Cultures develop norms that influence the values, beliefs, expectations, behaviors, and perceptions of their group members. Recent studies have identified several cultural factors, including power distance, the perceived differences between superiors and subordinates, individualism–collectiv- ism, the extent to which people accept their own goals relative to those of their group’s, and uncertainty avoidance, the willingness to deal with uncer- tain situations, that can distinguish among members of different cultures and influence system safety. Although there is disagreement about these particular cultural factors, most researchers agree on the influence of culture on individual behaviors. However, establishing a link between cultural fac- tors and an operator’s error is difficult. Companies also develop norms through their actions, statements, prac- tices, and policies. Hiring criteria, training programs, operating procedures, and oversight reflect these norms. Some companies actively encourage safety by recognizing and addressing potential operational hazards. Differences between “good” and “bad” organizational cultures are suggested. DOCUMENTING CULTURAL ANTECEDENTS NATIONAL CULTURE • Refer to existing research to determine the effects of norms that are believed to influence safety (e.g., Hofstede, 1980, 1991; Helmreich and Merritt, 1998; Soeters and Boer, 2000; Helmreich et al., 2001), when national cultural issues need to be examined. ORGANIZATIONAL CULTURE • Identify organizational cultural factors and assess their effects on system safety by interviewing employees at all pertinent company levels, and examining written documentation such as memos, organizational policies, and procedures. • Document the extent to which company policies are enforced, and the extent to which operator expectations regarding com- pany enforcement practices are met. • Identify company recognized transgressions and the penalties it administers to operators who transgress. • Evaluate the comprehensiveness of selection practices, training programs, maintenance practices, and operational oversight and, if possible, compare them to other companies in the same industry. • Document managerial and operator turnover each year for a period of several years from the time of the event, and the rea- sons given for employees leaving the company. • Determine the number of incidents and accidents the company has experienced over several years from the time of the event, assess the comprehensiveness of the organizational investiga- tion of the events, identify common issues that may be present, and the remediation strategies the company has implemented to prevent future events. • Document the resources that companies have devoted to pro- grams that directly affect system safety, such as self-reporting error programs, rewards for suggestions to enhance safety, and efforts to remain current with industry and government safety programs.