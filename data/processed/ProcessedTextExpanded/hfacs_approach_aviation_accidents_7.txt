Title: A Human Error Approach to Aviation Accident Analysis The Human Factors Analysis and Classification System Chapters 4 - Author(s): Douglas A. Wiegmann, Scott A. Shappell Category: Human, Error, Analysis Tags: Human, Error, human factor analysis and classification sytems 4 Aviation Case Studies using human factor analysis and classification sytems To illustrate how human factor analysis and classification sytems can be used as an investigative as well as an analytical tool, we have chosen three U.S. commercial aviation accidents as case studies. For each, the final report made public by the National Transportation Safety Board (NTSB) was used as a resource. Often referred to as "blue covers" because of the color of ink used by the national transportation safety board on their aviation accident report covers, these documents are quite detailed and represent the official findings, analyses, conclusions and recommendations of the NTSB. The interested reader can find a variety of these accident reports on the official national transportation safety board web site (www.ntsb.gov) while many others (like the ones in this chapter) are available upon request. Interested readers can request the accidents described in this chapter and numerous others by writing to the national transportation safety board at: National Transportation Safety Board, Public Inquiries Section, RE-51, 490 L'Enfant Plaza, S.W., Washington, D.C. 20594. Recently, Embry-Riddle Aeronautical University in Daytona Beach, Florida has scanned all the reports since 1967 and posted them in PDF format on their web site at (http://amelia.db.erau). Before we begin, a word of caution is in order. It is possible that some readers may have more information regarding one or more of the accidents described below, or might even disagree with the findings of the NTSB, our analysis, or both. Nevertheless, our goal was not to reinvestigate the accident. To do so would be presumptuous and only infuse unwanted opinion, conjecture, and guesswork into the analysis process, since we were not privy to all the facts and findings of the case. Instead, we used only those causal factors determined by the NTSB, as well as the analyses contained within the report, when examining the accidents that follow. It is also important to note that these case studies are presented for illustrative purposes only. While we have attempted to maintain the spirit and accuracy of the NTSB's analyses by citing specific findings and analyses (excerpts have been identified with italics and page number citations), in the interest of time we have only presented those details necessary to support the causes and contributing factors associated with each accident. Other information relevant to a thorough investigation (e.g., statements of fact), but not the cause of the accident, was excluded. 72 Sometimes Experience does Count On a clear night in February of 1995, a crew of three positioned their DC-8 freighter with only three operable engines on the runway for what was intended to be a ferry flight from Kansas City, Missouri to Chicopee, Massachusetts for repairs. Just moments earlier the crew had aborted their initial takeoff attempt after losing directional control of the aircraft. Shortly after beginning their second attempt at the three-engine takeoff, the crew once again lost directional control and began to veer off the runway. This time however, rather than abort the takeoff as they had before, the captain elected to continue and rotated the aircraft early. As the aircraft briefly became airborne, it began an uncommanded roll to the left and crashed into the ground. Tragically, all three crewmembers were killed as the aircraft was destroyed (NTSB, 1995a). The accident aircraft had arrived in Kansas City the day before as a regularly scheduled cargo flight from Denver, Colorado. The plan was to load it with new cargo and fly it to Toledo, Ohio later that day. However, during the engine start-up sequence, a failure in the gearbox prevented the No. 1 engine from starting. As luck would have it, the gearbox could not be repaired locally, so a decision was made to unload the cargo from the aircraft and fly it using its three good engines to Chicopee, Massachusetts the next day where repairs could be made. Meanwhile, the accident aircrew was off-duty in Dover, Delaware having just completed a demanding flight schedule the previous two days. Their schedule called for them to spend the night in Dover, then ferry another aircraft to Kansas City the next afternoon. Because the crew would be in Kansas City, it was decided that they would be assigned the three-engine ferry flight to Chicopee – a somewhat surprising decision given that other crews, with more experience in three-engine takeoffs, would also be available. Nevertheless, the decision was made, and later that night the chief pilot contacted the captain of the accident crew to discuss the three-engine ferry flight. Among the things discussed, were the weather forecast and a landing curfew of 2300 at Chicopee. Absent from the conversation however, was a review of three-engine takeoff procedures with the captain. After an uneventful ferry flight from Dover to Kansas City the next day, the crew prepared for the three-engine ferry flight to Chicopee. Because the flight was expected to take a little over 2 hours, it was determined that they would have to depart Kansas City before 8 PM CST that night to arrive at Chicopee before the airfield closed at 11 o'clock eastern standard time (note that there is a 1 hour time difference between Kansas City and Chicopee). Despite the curfew however, the engines were not even started until just past 8 PM CST, and then only after being interrupted by several procedural errors committed by the crew. So already running a little bit late, the captain began to taxi the aircraft out to the runway and informed the crew that once airborne, they would need to fly as direct as possible to arrive at Chicopee before the 11 PM eastern standard time curfew. He also briefed the three-engine takeoff procedure; a brief that was characterized by a poor understanding of both three-engine throttle technique and minimum controllable airspeed during the takeoff ground run (Vmcg). After moving into position onto the runway and performing a static run-up of the three operable engines, the aircraft finally began its takeoff roll at roughly 8:20 PM CST (some 20 minutes behind schedule). However, during the takeoff roll, the power on the asymmetrical engine (in this case the number 4 engine, directly opposite the malfunctioning number 1 engine) was increased too rapidly, resulting in asymmetric thrust, causing the aircraft to veer to the left (Figure 4.1). It was at this point that the captain elected to abort the takeoff and taxi clear of the runway. While taxiing back into position on the runway for another takeoff attempt, the crew discussed the directional control problems and its relationship to Vmcg. In particular, the crew focused on the rate at which power was to be applied to the asymmetrical (number 4) engine. Once again, the discussion was marked by confusion among the aircrew. This time however, the captain elected to depart from existing company procedures and let the flight engineer, rather than himself, control the power on the asymmetrical engine during the next takeoff attempt. This was done presumably so the captain could devote his attention to maintaining directional control of the aircraft. Now, nearly 30 minutes behind schedule, the aircraft was once more positioned on the runway and cleared for takeoff. So, with the flight engineer at the throttles, the aircraft began its takeoff roll. However, this time the power was increased on the asymmetrical engine even faster than before. As a result, shortly after the first officer called "airspeed alive" at about 60 knots, the aircraft started an abrupt turn to the left followed quickly by a correction to the right. Then, almost immediately after the first officer called "90 knots,"' the aircraft once again started to veer to the left. However, unlike the initial takeoff attempt, the captain elected to continue, rather than abort, the takeoff Seeing that the aircraft was going to depart the runway, the captain pulled back on the yoke and attempted to rotate the aircraft early (roughly 20 knots below the necessary speed to successfully takeoff). As the first officer called "we're off the runway," the aircraft briefly became airborne, yawed, and entered a slow 90-degree roll until it impacted the ground. Human Factors Analysis using human factor analysis and classification sytems There are many ways to conduct an human factor analysis and classification sytems analysis using national transportation safety board reports. However, we have found that it is usually best to begin as investigators in the field do and work backward in time from the accident. In a sense then, we will roll the videotape backwards and conduct our analysis systematically. Using this approach, it was the captain's decision to continue the takeoff and rotate the aircraft some 20 knots below the computed rotation speed that ultimately sealed the crew's fate. That is to say, by continuing, rather than aborting the takeoff, the crew experienced a further loss of control, leading to a collision with the terrain. Hindsight being 20/20, what the captain should have done was abort the second takeoff as he had earlier, and made yet another attempt. Perhaps even better, the captain could have cancelled the flight altogether since it was unlikely that they would have arrived at Chicopee before the 2300 curfew anyway. So how did we classify the fact that the pilot elected to "continue [rather than abort] the takeoff' (NTSB, 1995a, p. 79) using HFACS? The classification process is really a two, or three-step process, depending on which level (i.e., unsafe acts, preconditions for unsafe acts, unsafe supervision, or organizational influences) you are working with (Figure 4.2). For instance, in this case, the decision to continue the takeoff was clearly an unsafe act. The next step involves determining whether the unsafe act was an error or violation. While certainly unwise, we have no evidence that the captain's decision violated any rules or regulations (in which case it would have been considered a violation). We therefore, classified it as an error. Next, one has to determine which type of error (skill-based error, decision error, or perceptual error) was committed. Using definitions found in the previous chapter, it is unlikely that this particular error was the result of an illusion or spatial disorientation; hence, it was not considered a perceptual error. Nor was it likely the result of an automatized behavior/skill, in which case we would have classified it a skill-based error. Rather, the choice to continue the takeoff was a conscious decision on the part of the captain and therefore classified as a decision error, using the human factor analysis and classification sytems framework. Continuing with the analysis,/ the next question one needs to ask is, "Why did the crew twice lose control of the aircraft on the takeoff roll?" The textbook answer is rather straightforward. By increasing the thrust on the asymmetric engine too quickly (i.e., more thrust from the engines attached to the right wing than the left), the captain on the first attempt and the flight engineer on the second, inadvertently caused the aircraft to drift left resulting in a loss of control. In both instances, an unsafe act was committed, in particular, an error: but, what type? The answer may reside with the crew's inexperience conducting three-engine takeoffs. It turns out that while all three crewmembers had completed simulator training on three-engine takeoffs the previous year, only the captain had actually performed one in an aircraft – and then only a couple of times as the first officer. When that inexperience was coupled with no real sense of how the airplane was responding to the increased thrust, an error was almost inevitable. Therefore, it was concluded that the manner in which the flight engineer applied thrust to the asymmetric engine led to the "loss of directional control ... during the takeoff roll" (NTSB, 1995a, p. 79). This was due in large part to a lack of experience with three-engine takeoffs. The flight engineer's poor technique when advancing the throttles was therefore classified as a skill-based error. While the rate at which the throttles were increased was critical to this accident, perhaps a better question is, "Why was the flight engineer at the throttles in the first place?" Recall that this was not the crew's first attempt at a three-engine takeoff that evening. In fact, after the first aborted attempt, the flight engineer suggested to the captain that "...if you want to try it again I can try addin' the power..." (NTSB, 1995a, p. 10), to which the captain agreed. However, the company's operating manual quite clearly states that only the captain, not any other crewmember, is to "smoothly advance power on the asymmetrical engine during the acceleration to Vmcg" (NTSB, 1995a, p. 45). In fact, by permitting someone other than the flying pilot to apply asymmetric thrust, the controls became isolated between two individuals with little, or no, opportunity for feedback. In a sense, it was like having someone press on the accelerator of the family car while you steer — certainly not a very good idea in a car, much less a DC-8 with only three operable engines. Given that company procedures prohibit anyone other than the captain to control the throttles during three-engine takeoffs, "[the crews] decision to modify those procedures" (NTSB, 1995a, p. 79) and allow the flight engineer to apply power is considered a violation using HFACS. Furthermore, such violations are arguably rare, and not condoned by management. As a result, this particular unsafe act was further classified as an exceptional violation. Still, this was not the crew's first attempt at a three-engine takeoff that evening. In fact, on their initial attempt, the captain was also unable to maintain directional control because he too had applied power to the asymmetrical engine too quickly. One would think that after losing control on the first takeoff that the conversation during the taxi back into position for the next attempt would largely center on the correct takeoff parameters and three-engine procedures. Instead, more confusion and a continued misunderstanding of three-engine Vmcg and rudder authority ensued, resulting in a second uncoordinated takeoff attempt. In effect, the confusion and "lack of understanding of the three-engine takeoff procedures" (NTSB, 1995a, p. 79) exhibited by the crew moments before, and after, the first aborted takeoff, contributed to a general lack of coordination in the cockpit. Consequently, this causal factor was classified as a failure of crew resource management. While it is easy to understand how the confusion with three-engine takeoffs contributed to the accident, it may have been a much less obvious and seemingly insignificant error that ultimately sealed the crew's fate. Even before taxiing out to the runway, during preflight calculations, "the flight engineer [miscalculated] the Vmcg speed, resulting in a value that was 9 knots too low" (NTSB, 1995a, p. 77). It appears that instead of using temperature in degrees Celsius, the flight engineer used degrees Fahrenheit when making his preflight performance calculations — an error that is not particularly surprising given that most of the company's performance charts use degrees Fahrenheit, rather than Celsius. Nevertheless, this understandably small error yielded a Vmcg value some nine knots below what was actually required, and may have contributed to the crew's early application of takeoff power to the asymmetrical engine. So yet another error was committed by the crew. However, unlike the conscious decision errors described above, this error was likely one of habit – automatized behavior if you will. After all, the flight engineer was accustomed to using degrees Fahrenheit, not Celsius when making his preflight calculations. It was therefore classified as a skill-based error.2 It is always easy to point the finger at the aircrew and rattle off a number of unsafe acts that were committed, but the larger, and perhaps more difficult question to answer is, "Why did the errors occur in the first place?" At least part of the answer may lie in the mental state of the crew at the time of the accident. For instance, it was evident from the cockpit voice recorder (CVR) that the crew "was operating under self-induced pressure to make a landing curfew at [Chicopee]" (NTSB, 1995a, p. 77). Comments by the captain like the need to get "as much direct as we can" (referring to the need to fly as direct as possible to Chicopee) and "...we got two hours to make it..." as well as those by the first officer such as "...boy it's gettin' tight..." and "...[hey] we did our best..." all indicate a keen awareness of the impending curfew and only served to exacerbate the stress of the situation (NTSB, 1995a, pp. 3, 14, and 97). Although we will never really know for certain, it is conceivable that this self-induced time pressure likely influenced the crew's decisions during those critical moments before the accident – particularly those of the captain when deciding to continue rather than abort the takeoff. Using the human factor analysis and classification sytems framework, we classified the self-induced pressure of the aircrew as a precondition for unsafe acts, specifically an adverse mental state. In addition to self-induced pressure, it appears that the crew, especially the captain, may have suffered from mental fatigue as well. As noted earlier, during the previous two days, the crew had flown a very demanding schedule. Specifically, they had flown a 6.5-hour check ride from Dover, Delaware to Ramstein, Germany, followed by a little less than 10 hours rest. This was followed by a 9.5-hour flight from Ramstein to Dover via Gander, Newfoundland. Then, after arriving in Dover, the crew checked into a hotel at 0240 for what was to have been 16 hours of crew rest before they could legally be assigned another commercial (revenue generating) flight. However, rather than get a good night's sleep, the captain's scheduled rest was disrupted several times throughout the night by the company's operations department and chief pilot to discuss the additional ferry flight the next day. In fact, telephone records indicate that the captain's longest opportunity for uninterrupted rest was less than five hours. So after only about 12 hours of rest (but not necessarily sleep) the captain and crew departed Dover for their first flight of the day, arriving in Kansas City shortly after 5:30 pm. Unfortunately, fatigue is often a nebulous and difficult state to verify, particularly when determining the extent to which it contributes to an accident. That being said, the poor decisions made by the aircrew suggest that at a minimum, the captain, and perhaps the entire crew, "were suffering from fatigue as a result of the limited opportunities for rest, disruption to their circadian rhythms, and lack of sleep in the days before the accident" (NTSB, 1995a, p. 76), all resulting in an adverse mental state using the human factor analysis and classification sytems framework. Up to this point we have focused solely on the aircrew. But, what makes human factor analysis and classification sytems particularly useful in accident investigation is that it provides for the identification of causal factors higher in the system, at the supervisory and organizational levels. For instance, one might question why this particular aircrew was selected for the ferry flight in the first place; especially when you take into consideration that other, more experienced crews, were available. As you may recall, there was no record of the captain having previously performed a three-engine takeoff as pilot-in-command, nor had any of the crewmembers even assisted in one. Even so, company policy permitted all of their DC-8 captains to perform this procedure regardless of experience or training. As a result, the crew was "qualified" for the flight by company standards; but were they by industry standards? In fact, of the nine other cargo operators contacted by the national transportation safety board after this accident, only two used line flight crews (like the accident crew) for three-engine takeoffs. Instead, most cargo operators used check airmen or special maintenance ferry crews exclusively for these flights. Of the two that did not, one restricted three-engine takeoffs to only the most experienced crews. Regardless of the rationale, the assignment of this crew to the three-engine ferry flight when "another, more experienced, flight crew was available" (NTSB, 1995a, p. 77) was considered an instance of unsafe supervision, in particular, planned inappropriate operations. So, why did the company choose not to assign its best crew to the three-engine ferry flight? Perhaps the company's decision was driven in part by current U.S. Federal Air Regulations that require pilots flying revenue generating operations (i.e., carrying passengers or cargo) to get at least 16 hours of rest before flying another revenue flight, but places no such restrictions on pilots conducting non-revenue flights. As a result, the accident crew was permitted to fly the non-revenue ferry flight from Dover to Kansas City with only about 9.5 hours of rest and could then fly the non-revenue, three-engine ferry flight to Chicopee. Because of the Federal regulations however, they could not legally fly the revenue generating flight scheduled to fly from Kansas City to Toledo that same day. In fact, another crew in the area had more experience flying three-engine ferry flights, but they were also eligible under Federal guidelines to fly revenue-generating flights. So rather than use a crew with more experience in three-engine ferry flights for the trip to Chicopee and delay the revenue flight to Toledo, the company chose to use a crew with less experience and get the revenue flight out on time. It can be argued then, that the decision to assign the ferry flight to the accident crew was influenced more by monetary concerns than safety within the company. Within the human factor analysis and classification sytems framework, such organizational influences are best considered breakdowns in resource management. Remarkably, the FAA, due to a previous DC-8 crash, had known about the hazards of flight and duty time regulations that "permitted a substantially reduced crew rest period when conducting [a] non-revenue ferry flight [under 14 code of federal regulations Part 91]" (NTSB, 1995a, p. 77). In fact, the national transportation safety board had previously recommended that the federal aviation administration make appropriate changes to the regulations, but they had not yet been implemented. So, where do we capture causal factors external to the organization within the human factor analysis and classification sytems framework? You may recall from earlier chapters that causal factors such as these are not captured within the human factor analysis and classification sytems framework per se, because they are typically not within an organization's sphere of influence (i.e., the organization has little or no control over Federal rulemaking). Instead, they are considered outside influences (in this case, loopholes within Federal Air Regulations) that have the potential to contribute to an accident. Still, some may argue that the crew had been trained for three-engine ferry flights, the same training that the company had provided to all their captains. As such, they were "qualified" to fly the mission. But was that training sufficient? The knee-jerk reaction is obviously 'no' given the tragic outcome. But some digging by the national transportation safety board revealed an even larger problem. It appears that the simulator used by the company did not properly simulate the yaw effect during a three-engine takeoff – the very problem the crew experienced during both takeoff attempts. Indeed, a post-accident test of the simulator revealed that with only three of the four engines brought up to takeoff power, the runway centerline could be easily maintained regardless of the airspeed achieved, something that clearly cannot be done in the actual aircraft. Regrettably, this lack of realism went unnoticed by the company's training department. As a result, the lack of "adequate, realistic training in three-engine takeoff techniques or procedures" (NTSB, 1995a, p. 76) within the company training program was viewed as yet another precondition for unsafe acts (environmental factor), in particular a failure within the technological environment.4 In addition to the limitations inherent within the simulator itself, the three-engine takeoff procedure description in the airplane operating manual was confusing (NTSB, 1995a, p. 76). Unfortunately, the dynamics of a three-engine takeoff are such that even the proper application of asymmetric throttles leaves little margin for error. Any confusion at all with the three-engine procedures would only exacerbate an already difficult task. To their credit however, the company did provide regular training of the three-engine takeoff procedure, but the poor description of the maneuver in the operations manual, and inaccurate simulator portrayal, lessened its effectiveness. Therefore, we did not consider this an organizational process issue. Instead, the procedural shortcomings in the company's operating manual, like the trouble with the simulator, were considered a failure of the technological environment. Finally, the lack of government oversight on occasion will affect the conduct of operations. That was once again the case in this accident as the national transportation safety board found that the federal aviation administration was not effectively monitoring the company's domestic crew training and international operations. A post-mishap interview with the FAA's principal operations inspector (POI) for the company revealed a general lack of familiarity with their cockpit resource management training program, crew pairing policies, and several aspects of the company's ground training program at Denver. This lack of oversight was likely the result of manning cuts and funding issues that restricted the POI's ability to travel to Denver from his base in Little Rock, Arkansas. The lack of government oversight is yet another example of the type of outside influence that can affect operations and, like the issues regarding Federal regulations guiding crew rest above, is included here for completeness. Summary As with most aviation accidents, indeed most accidents in general, this one could have been prevented at many levels (Figure 4.3). While the lack of governmental oversight of the company's operations and the loopholes contained within the FAA's crew rest guidelines may have laid the foundation for the tragic sequence of events to follow, the failures within the organization itself were far more pervasive. In a sense, the crew was "set-up" by a lack of realistic training (recall the problems with the simulator), a confusing description of three-engine takeoffs in the manual, and perhaps most important, poor crew manning practices. Nevertheless, the crew was also responsible. Clearly the decisions made in the cockpit that night were marred by the failure to thoroughly understand the intricacies of three-engine takeoffs which was exacerbated by poor crew coordination, mental fatigue, and a sense of urgency to get underway. Even so, had the captain aborted the takeoff, as he had done previously when the aircraft lost directional control, the accident would have been averted. Instead, he chose to continue the takeoff by initiating rotation below computed rotation speed, leading to a further loss of control and collision with the terrain. A World Cup Soccer Game They would Never See Shortly before midnight, in June of 1994, a Learjet 25D operated by Transportes Aereos Ejecutivos, S. A. (TAESA) departed Mexico City with 10 passengers bound for a World Cup Soccer game in Washington, DC. Flying through the night, the chartered flight contacted Washington Dulles Approach Control just before six o'clock in the morning and was informed that conditions at the field were calm with a low ceiling and restricted visibility due to fog. Moments later, using the instrument landing system (ILS), the crew made their first landing attempt. Having d culty maintaining a proper glideslope, the crew was forced to fly a missed approach and try again. Sadly, on their second attempt, they again had difficulty maintaining a proper glideslope, only this time they fell below the prescribed glide path and impacted the trees short of the runway, fatally injuring everyone on board (NTSB, 1995b). The crew's preparation for the flight began normally as both crewmembers reported for duty around 10:00 pm appearing well rested and in good spirits. In fact, according to his wife, the captain had been off duty for three days and even took a 3-hour nap before reporting that evening. There was no reason to believe that the first officer was fatigued either, although it remains unclear how much sleep he actually got since there were no witnesses. With the necessary preflight activities completed and the passengers loaded, the crew departed Mexico City shortly after 11:00 pm for what was to be a chartered flight to Washington, DC. After pausing briefly for a planned refueling stop in New Orleans, the flight continued uneventfully until they entered the Washington, DC area. There, after a brief delay for an aircraft with a declared emergency, the crew contacted Dulles Approach Control and was advised that due to low visibility (approximately 1/2 mile) and ground fog at the field, a straight-in, ILS-coupled5 approach (referred to as a Category III approach) would be required. So, with the onset of daybreak, the crew began their first ILS-coupled approach to runway 19R. The approach started out well as the aircraft remained within prescribed localizer parameters (i.e., they had not deviated too far left or right of their assigned path) approximately 14 nautical miles (nm) from the runway threshold. However, the aircraft was never fully stabilized on the glideslope, flying erratically (up and down) along the approach path. Evidence from the Air Traffic Control radar track had the aircraft briefly leveling out at 600 feet for a few seconds, perhaps in an attempt to establish visual contact with the airport — something that, if true, was clearly not authorized. In fact, with the runway obscured by fog, company procedures and U.S. Federal regulations required that the captain abort the landing and follow the published go-around procedures for another attempt or abandon the landing altogether. Nevertheless, it was only after the air traffic controller inquired about their intentions that the crew broke off their approach and accepted vectors for another try. The second approach was initially more stable than the first as can be seen from Figure 4.4. However, it quickly resembled the first, and from the outer marker (roughly 6.5 miles from the runway threshold) to time of impact, the aircraft flew consistently below the published glideslope. Eventually the aircraft impacted the trees and crashed a little less than one mile short of the runway threshold. Human Factors Analysis using human factor analysis and classification sytems As with the previous case study we will work backwards from the aircraft's impact with the terrain. In this case, what ultimately led to the accident was the captain's "failure to adhere to acceptable standards of airmanship during two unstabilized approaches" (NTSB, 1995b, p. 46). What we do know from the factual record is that during both approaches to Dulles, the TAESA aircraft flew well below the published glideslope. However, what remains unclear is whether or not the captain intended to go below the glideslope in an effort to establish visual contact with the airport while flying in a thick fog. As described above, there appears to be some evidence from the radar track that the pilot tried to obtain a visual fix on the runway during the first approach. Regrettably, the practice of flying below cloud layers and other adverse weather in an attempt to get a view of the ground is not altogether uncommon in aviation, just extremely dangerous and inconsistent with published Category III procedures. However, unlike the first approach, there appears to be no clear indication that the pilot intended to go below the glideslope on the second attempt. Actually, it is probably more plausible that the captain simply did not possess the necessary skill to safely perform an instrument approach. As we will see later in the analysis, the captain was not particularly adept at these types of approaches. This, coupled with his possible desire to gain visual contact with the ground, may have led to a breakdown in instrument scan and quite possibly explains why the captain was unaware that he was fatally below the glideslope. In the end, one thing is certain, the crew clearly did not know how dangerously close to the terrain they were; something an active instrument scan would have told them. Therefore, the failure of the aircrew to properly fly the instrument landing system approach can be largely attributed to a breakdown in instrument scan and the inability of the pilot to fly in instrument conditions, in both cases a skill-based error using HFACS. With the weather conditions at Dulles rapidly deteriorating, one could question why the second approach was even attempted in the first place, particularly given that category III operations were in effect. These operations require that aircraft fly a straight-in, ILS-coupled, approach to reduced visual minimums and require special certifications of the crew, runway, and equipment. In this case, however, the crew was only authorized to perform a category I approach that establishes a 200 foot ceiling and at least takeoff safety speed mile (2400 feet) visibility at the runway for an approach to be attempted. Often referred to as runway visual range (RVR), the visibility at the runway had deteriorated well below the 2400-foot minimum at the time of the accident. In fact, just moments before impact the local controller confirmed that the aircraft was on the instrument landing system and advised them that runway visual range at touchdown and at the midpoint of the runway was 600 feet. Nevertheless, the captain continued the approach; one that "he was not authorized to attempt" (NTSB, 1995b, p. 46); a clear violation of the Federal regulations in effect at the time of the accident. While we can be certain that a violation was committed, determining what type (routine or exceptional) is much less clear. In reality, making this determination is often difficult using national transportation safety board or other investigative reports, primarily because many investigators do not ask the right questions nor do they dig deep enough to reliably classify the type of violation. As a result, we are usually left only with the observation that a violation was committed. So, without supporting evidence to the contrary, we have not classified this particular violation as either routine or exceptional; instead, we simply stopped with the overarching category of violation. But, why would the crew attempt an approach beyond their legal limits –particularly given the first unsuccessful instrument landing system attempt? After all, it is not as if the crew had no options. For instance, they could have waited for the weather to improve. Or perhaps they could have elected to switch runways from 19R to 19L where visibility was noticeably better. In fact, another commercial flight was offered that option earlier, but elected to divert to Pittsburgh instead. Indeed, that may have been the most prudent alternative. The TAESA crew could simply have diverted to their designated alternate (Baltimore International Airport), about a hundred miles northeast of Dulles. Unfortunately, rather than "[hold] for improvements in weather, [request] a different runway (19L), or [proceed] to the designated alternate airfield" (NTSB, 1995b, p. 46), the crew elected to continue the approach — perhaps in the hope that a window of visibility would present itself. Such a decision, in light of safer alternatives, was classified a decision error using HFACS. While it is easy to identify what the crew did wrong, to truly understand how this tragedy could happen, one needs to explore the preconditions that existed at the time of the accident. First, and perhaps foremost, "the crew may have been experiencing the effects of fatigue following an all-night flight" (NTSB, 1995b, p. 47). It is a well-known scientific fact that performance degrades and errors, particularly decision errors, increase between 10:00 pm and 06:00 am for individuals entrained to the normal light-dark cycle (Figure 4.5). Otherwise known as the circadian trough, the majority of the flight took place during this time. In fact, when the accident occurred (just after 04:00 am Mexico City time), the crew was likely performing during a low point (nadir) in the circadian rhythm and experiencing the adverse effects of mental fatigue. When this is coupled with the fact that the crew had been flying for several hours with only a short stop in New Orleans for fuel, it is reasonable to assume that they were tired, an adverse mental state using the human factor analysis and classification sytems framework. But fatigue was not the only thing that affected the decisions of the crew that morning. Also contributing to the accident was the "relative inexperience of the captain for an approach under these conditions" (NTSB, 1995b, p. 46). Among major air carriers in the U.S., upgrades to captain usually do not occur until the pilot has amassed somewhere between 4,000 and 5,000 flight hours. In some countries however, where the number of qualified pilots may be limited, it is not at all unusual for candidates to have around 2,300 hours when upgrading. Even so, this captain had actually less experience than that, having accumulated just over 1,700 total flight hours, of which less than 100 were as pilot-in-command (PIC). Compounding the problem, it had been noted during the captain's recurrent training that his crew management and decision-making skills, while adequate under normal conditions, needed improvement when he was placed under stress or in emergency situations. As a result, it was recommended that he fly with a strong training captain or first officer during his upgrade. Unfortunately, he could not rely on the first officer either since he was also relatively inexperienced, having accumulated just over 850 total flying hours, only half of which were in the Learjet. Ultimately then, it can be argued that the captain's lack of experience as PIC, as well as his difficulties under stressful conditions like those that prevailed at the airfield, limited his ability to make sound decisions, and was therefore considered a physica/mental limitation. This begs the question, "How could a pilot with such limitations be upgraded to captain?" The answer, it seems, resides with the "ineffective communications between TAESA and ... the contract training facility regarding the pilot's skills" (NTSB, 1995b, p. 46). The official evaluation provided to TAESA by the training facility described the captain as "focused' and "serious," a "smooth pilot" and a "polished first officer." This description was in stark contrast to what was actually printed in the instructor's notes that described him as a pilot with only satisfactory flying skills during normal flying conditions and documented problems with his instrument scan and crew coordination. Furthermore, it was noted that "although he flew non precision approaches well ... his instrument approaches definitely did not meet airline transport pilot [airline transport pilot] standards" (NTSB, 1995b, p. 7). Unfortunately, only the "official" evaluation reached TAESA. As a result, permissive language contained in the report opened the door for TAESA officials to interpret the document as approval of their applicant. Had the instructor's notes, which painted a clearer picture of the captain's below average performance, been made available to TAESA, or had the evaluation been worded more clearly, the training facilities intent may have been more plainly communicated and a delay in this pilot's upgrade to captain might have occurred. In fact, it was only after repeated requests by the Director of Operations that a letter was finally received describing the captain's need "to improve his airmanship and command skills" as well as the need for "situational awareness training under high workload" (NTSB, 1995b, p. 8). Clearly, there was a miscommunication between TAESA and the contract facility, which led to the decision to upgrade a captain with limited experience. Recognizing that both organizations were responsible for the miscommunication (on an organizational level), we classified this causal factor as a failure of resource management. That being said, "oversight of the accident flight by TAESA was [also] inadequate." It turns out that the operations specifications in use by TAESA at the time of the accident failed to address which visibility value, runway visual range or prevailing, takes precedence in establishing a minimum for landing. Had this been a simple oversight of one particular aspect of the operational specifications, we might have chosen to classify this as a failure within the technological environment (i.e., inadequate documentation). However, a review of TAESA's operation specifications revealed that some of the pages were as much as 20 years old and none addressed the precedence of runway visual range or prevailing visibility on landing. We therefore classified the failure of TAESA to address this provision within the company's operations manual constitutes as a failure of operational processes. Finally, the national transportation safety board suggested that an operating ground proximity warning system (GPWS) might have prevented the accident. Although the failure to equip the aircraft with a functional GPWS is not a violation per se, it was considered a factor in this accident. TAESA was operating under 14 CFR, part 129 of the U.S. Federal Code of Regulations, which did not require installation of a GPWS for this aircraft. However, a GPWS aboard the aircraft likely would have provided a continuous warning to the crew for the last 64 seconds of the flight and may have prevented the accident. Given that controlled flight into terrain continues to plague aviation, a GPWS seems a reasonable fix in many instances. Although not a violation of any established rule or regulation, operating aircraft without such safety devices is typically a decision made at the highest levels of an organization. We therefore classified this factor as a failure of resource management. Summary As with most accidents, this accident unfolded with errors at several levels (Figure 4.6). At the company level, a decision to install a ground proximity warning system in all turbojet aircraft may have prevented this accident. The company leadership also showed poor planning by placing a very inexperienced captain with an even less experienced first officer. Not only was it questionable that the captain was even certified at the PIC level; but, if he was required to fly as a captain, he should have been paired with a very experienced First Officer to offset his lack of experience. Wisely, the captain took a nap the afternoon before the departure at Mexico City. Unfortunately, one nap may not have been enough to overcome the increase in fatigue and performance degradation associated with the circadian trough. In the final analysis, these breakdowns certainly contributed to the captain's decision to continue the instrument landing system approach, one that he was incapable of flying safely that morning. he Volcano Special On the afternoon of April 22, 1992, a Beech Model E18S, took off from Hilo, Hawaii with a group of tourists for the last leg of a sight-seeing tour of the Hawaiian Islands chain. Marketed as the "Volcano Special," the aircraft was to fly over the Islands of Molokini, Lanai, and Molokai on the western edge of the Hawaiian Island Chain. However, this day the young pilot inexplicably flew off course and into a layer of haze and clouds that obscured much of the Island of Maui and Mount Haleakala that lay just North of his planned route. Moments later, the aircraft crashed into the side of the Volcano, killing all eight tourists and the pilot (NTSB, 1993). The day began innocently enough, as eight tourists boarded Scenic Air Tours (SAT) Flight 22 for a day of sightseeing along the Hawaiian Island chain. The first part of the tour included sights along the north coast of Molokai and Maui, culminating with a flight over the Kilauea Volcano on the "Big Island" of Hawaii. After landing at Hilo Airport shortly after nine in the morning, the passengers were shuttled off for a 6-hour ground tour of sights around the Island of Hawaii. Later that afternoon, the tourists returned to the airport and boarded SAT Flight 22 for a 3:20 departure to Honolulu via the western edge of the Hawaiian Island chain. Tragically, they never made it past Maui. Unfortunately for the tourists, the weather that afternoon was not particularly favorable to sightseeing over the interior of the islands as haze and rain showers had settled in, lowering the visibility to three miles in some areas. Still, the weather, in and of itself, did not present a problem for the pilot as the normal flight path for SAT flights returning to Honolulu did not include flying over the interior of the islands. Instead, the typical flight to Honolulu included sights along the northern coast of Hawaii to Upola Point, at which time the aircraft would fly northwest over the village of Makena on the southern shore of Maui (Figure 4.7). Passing over Lanai, the flight would then normally continue to Honolulu and land at the airport. The return flight that day however, was anything but normal. Shortly after takeoff, the pilot of Flight 22 called the Honolulu Flight Service Station to inquire about the status of a restricted area over the island of Kahoolawe, just south of the company's normal route. When advised that the area was closed between the surface and 5000 ft mean sea level (msl), he advised air traffic control (ATC) that he would be flying over the top at 6500 feet. As can be seen from Figure 4.8, this slight deviation from his original flight plan would require that he fly a magnetic compass heading of 287° from the Upola Point vhf ominidirectional range (very high omnidirectional radio range), located on the northwest point of the island of Hawaii. The normal route flown by company pilots would typically have them flying a heading of 294° from Upola Point along the western edge of Maui. However, this day, the pilot of SAT Flight 22 flew neither the 294° nor the 287° heading. Instead, he inexplicably flew a 310° heading directly into the interior of Maui and multiple layers of clouds that had formed, obscuring Mount Haleakala from view. Passing the shoreline of Maui at roughly 8100 ft., SAT Flight 22 continued to ascend until its last recorded altitude of 8500 ft., just prior to impacting the volcano. A witness in the area at the time of the accident reported that while he did not see the impact due to heavy, rolling clouds in the area, he did hear what he believed to be a multi-engine plane until the sound stopped abruptly. Little did he know, the silence was the result of a tragic plane crash that took the lives of the pilot and his eight passengers. Human Factors Analysis using human factor analysis and classification sytems Working backwards from the impact, it does appear that in the final seconds before the accident that the captain did try to avoid the volcano. Regrettably however, he did not see the rising terrain of Mount Haleakala until the final seconds of flight because the cloud cover obscured it. In fact, by the time he realized the tragic error he had made, his fate and that of the passengers was likely already sealed. But why did Flight 22 enter the clouds in the first place? Perhaps he thought that he would simply pass harmlessly through the clouds and continue the tour on the other side. But any flight into the clouds is seemingly beyond comprehension when you consider that this was a "sight-seeing" flight and passengers certainly cannot see much in the clouds. What's more, SAT was a VFR-only operation. In other words, according to federal aviation administration regulations, all flight operations were required to be flown in visual meteorological conditions (VMC) while under visual flight rules (VFR), thereby prohibiting flight into the clouds and weather. Although we will never know exactly why the captain chose to fly into the clouds, what we do know is that contrary to regulations that required SAT flights "to be conducted under VFR, the captain chose to continue visual flight into the instrument meteorological conditions that prevailed along the eastern and southern slope of Mount Haleakala on the Island of Maui" (NTSB, 1993, p. 46) — a clear violation of existing company and federal aviation administration regulations. What is less clear is what type of violation was committed. Therefore, as with the preceding case study, we chose not to classify this causal factor further. It can only be assumed that the captain was aware of the rules regarding staying clear of weather and the clouds and the dangers associated with disregarding them. This only makes it more difficult to understand why he would elect to fly into the clouds rather than circumnavigate them. One possibility is that he did not see the clouds until after he was in them. This seems a bit improbable since he was told by personnel at the local Flight Service Station that VFR-flight was not recommended along the interior of the islands because of haze and moderate rain showers. What is more likely is that he did not realize that the upsloping cloud layer in front of him was produced by Mount Haleakala. Even this hypothesis is a bit surprising since pilots knowledgeable of weather patterns in the Hawaiian Islands would realize that only a landmass like Mount Haleakala could generate the orographic lifting of clouds at the altitudes that Flight 22 encountered them. Perhaps this is part of the problem since, as we shall see, this particular pilot had limited experience flying among the Hawaiian Islands. As a result, we chose to classify the fact that the "...captain did not evaluate the significance of an upsloping cloud layer that was produced by [the] orographic lifting phenomenon of Mount Haleakala" (NTSB, 1993, p. 46) as a knowledge-based decision error. Still, the accident might not have occurred at all had the captain not deviated from his intended course of 287°. Recall that he reported to the Honolulu Flight Service Station that he would be flying over Kahoolawe, considerably south of the 310° course that he actually flew. How then, could such a tragic error be committed? The national transportation safety board considered many plausible reasons including the possibility that the captain might have wanted to show the Mount Haleakala volcano crater to the passengers. However, because of existing weather and schedule considerations, the national transportation safety board ultimately ruled out any intentional deviation. So, what could have led to such a tragic error? While we will never really know, perhaps the most plausible reason may be the failure of the captain to refer to aeronautical references for navigation information. Indeed, three Hawaiian island visual flight rules sectional charts were found folded among the wreckage inside the captain's flight bag. It would appear then that the captain "did not use his navigation charts to confirm the correct heading and radial outbound from Upolu Point" (NTSB, 1993, p. 47). By not using all available information at his disposal, the captain did not practice good crew resource management. But, this still does not explain how the aircraft ended up on the 310°, vice 287° radial. One explanation lies with the omni-bearing selector, or OBS as it is called. The OBS is a dial used to select the desired radial one would like to fly on, in this case the 287° radial. Given the aircraft's flight path, it is quite possible that the captain failed to turn the omni-bearing selector (OBS) to the 287° radial while tracking outbound from the Upola vhf ominidirectional range (Figure 4.8). Curiously, the bearing the aircraft actually flew (310°) is identical to the radial SAT pilots typically fly when outbound from Hilo Airport in order to fly past some popular attractions along the north shore of Hawaii. In fact, the same 310° radial is the initial flight track for the northern route to Honolulu via Hana, a route that the captain had flown four times in the five days prior to the accident. It seems entirely plausible that after tuning in the Upola VOR, that the captain either simply forgot to select the 287° radial using the OBS or selected the 310° radial out of habit. In either case, a skill-based error was committed. Furthermore, his "navigation error went undetected because he failed to adequately cross-check [his] progress ... using navigation airborne integrated data system available to him" (NTSB, 1993, p. 47). This seemingly simple breakdown in instrument scan is yet another skill-based error committed by the captain. As with the other two case studies, there was more to this tragic story as well. It turns out that the captain was a former van driver with SAT – not once, but twice. In fact, he had been employed by nine different employers (twice with SAT) in the five years before the accident. Why this is important is that five of those employers had fired him because of "misrepresentation of qualifications and experience, failure to report for duty, disciplinary action, poor training performance, and work performance that was below standards" (NTSB, 1993, p. 14). This is particularly relevant here, because the captain had once again misrepresented his credentials when applying for the pilot job with SAT in the summer of 1991. Upon employment, the captain indicated that he had roughly 3,400 flight hours of which 3,200 were as PIC including 1,450 hours in twin-engine aircraft and roughly 400 hours of instrument time. However, using federal aviation administration records, he actually had fewer than 1,600 hours and less than 400 of those were in multiengine aircraft. Even the most liberal account had the captain with less than 2,100 hours, still well short of the 2,500 hours of actual flight time (including 1,000 hours of multiengine experience) required by SAT for employment. In fact, he had not met those requirements by the time the accident occurred! Clearly, this deceitful act contributed to the captain's ability to safely fly tourists around Hawaii; but, where do you classify the fact that "the captain falsified [his] ... employment application and resume when he applied for a pilot position at Scenic Air Tours" (NTSB, 1995b, p. 46) within the human factor analysis and classification sytems framework? Clearly, it was not an unsafe act violation, because it did not have immediate consequences, nor did it happen in the cockpit. What it did do was did set the stage for the unsafe acts to follow and affected the captain's overall readiness to perform. Consequently, it was considered a failure of personal readiness on the part of the pilot Perhaps even more difficult to understand is how a pilot with an employment and flight history as checkered as the captain's could be hired in the first place. After all, didn't SAT conduct a preemployment background check of the captain's employment and aeronautical experience? Recall that even at the time of the accident "the pilot did not possess the minimum hours of experience stipulated in the company operations manual to qualms as a captain" (NTSB, 1993, p. 46). Indeed, several potential employers had either fired or refused to hire him when it was determined that his experience did not meet their standards and at least one airline rejected the captain's application when it was determined that he had misrepresented his credentials. The fact that "SAT was unaware of the captain's falsified employment application because they did not [conduct a] ... substantive preemployment background check" (NTSB, 1993, p. 46) is therefore considered a classic failure of human resource management. But how could this happen? Aren't operations like SAT's required to conduct a standard background check? Actually, at the time of the accident, "the federal aviation administration did not require commercial operators to conduct substantive pilot preemployment screening" (NTSB, 1993, p. 47). This was somewhat surprising since the national transportation safety board had previously identified problems with preemployment screening, recommending that commercial operators be required to conduct substantive background checks of pilot applicants. Unfortunately, while the federal aviation administration agreed with the intent of the recommendations, it did not believe that the benefits would outweigh the costs of promulgating and enforcing them (NTSB, 1993, p. 39). Consequently, the intentional inaction by the federal aviation administration is considered an outside influence. Summary As with the other two case studies, this tragic accident involved more than the unsafe acts of the pilot alone. While there is no denying that the decision to fly into instrument meteorological conditions and the navigational errors were attributable to the captain, the fact that he was flying passengers in that situation was the responsibility of SAT. After all, had the company done an adequate background check, as at least one other airline had, they would have realized that the captain had misrepresented his experience and likely would have uncovered an employment history that might have influenced their decision to hire him. Certainly, the federal aviation administration bears some responsibility as well, since there were no regulations in place that required commercial operators to conduct substantive pilot preemployment screening. The good news is that this has since been remedied by the FAA.' Conclusion The preceding case studies demonstrate how human factor analysis and classification sytems can be used to classify existing human causal factors contained within national transportation safety board and other accident reports. In fact, we have used similar civilian and military cases within our own workshops and training seminars to demonstrate the ease with which human factor analysis and classification sytems can be used to analyze accident data. Although it is impossible to demonstrate in a book, we hope that the reader can see how a similar process could be used to identify human casual factors in the field during an actual accident investigation. The U.S. Naval Safety Center, for example, has improved the richness of the human error data significantly since human factor analysis and classification sytems was introduced to the aviation safety community. What remains to be answered however, is what can be done with the data once it is collected. For answers to that and other questions, we will turn to the next chapter. 5 Exposing the Face of Human Error After spending the weekend visiting with the pilot's parents, the crew of a U.S. Navy F-14 arrived at the hangar for a scheduled flight later that morning. But today, unlike previous missions, the pilot's parents and a family friend accompanied the crew as they busily prepared for the flight. After completing their pre-flight checks and pausing briefly for some pictures, the pilot and his radar intercept officer (RIO) strapped themselves into their multi-million dollar jet in preparation for their flight home. Then, with a sharp salute to his father, the pilot taxied his jet into position and waited as his parents hurried off to proudly watch the F-14 depart. Little did they realize that they would soon witness the tragic loss of their son. The pilot was a second tour fleet aviator known more for his abilities and performance as a Naval Officer than for his skills as a pilot. Indeed, he had always been a marginal aviator, struggling to prove his worth — especially after an accident less than one year earlier in which he lost control of his aircraft and ejected from an unrecoverable flat spin. But that morning, as he sat ready to launch in his Navy Tomcat, he was the pride of the family. With his parents watching from a restaurant just beyond the end of the runway, he began his takeoff roll. Reaching nearly 300 knots he rapidly pulled the nose of his jet up, zorching for the cloud layer less than 2,500 feet above. Never before had he taken off with such an extreme pitch attitude nor was he authorized to do so, particularly in poor weather conditions. But this was no ordinary day. Although he had been cleared by air traffic control for an unrestricted climb just moments earlier, the pilot began to level off after entering the cloud layer —perhaps believing that the altitude restriction of 5,000 feet was still in effect. Unfortunately, by transitioning to level flight in the clouds after a high g-force takeoff, the crew rapidly fell prey to the disorienting effects of the climb. Presumably flying by feel rather than instruments, the pilot continued to drop the nose of his jet until they were roughly 60° nose down and heading straight for the ground a few thousand feet below. Just seconds later, the aircraft exited the cloud layer still some 60-80° nose down with an airspeed in excess of 300 knots. Seeing the world fill his windscreen rather than the blue 99 sky he expected, the pilot abruptly pulled back on the stick and selected afterburner in a futile attempt to avoid hitting the terrain. In the end, he lost control of the aircraft as it crashed into a residential area killing himself, his RIO, and three unsuspecting civilians. For obvious reasons, impromptu air shows like this one for family and friends are prohibited and arguably rare within the U.S. military. On the other hand, when they do occur they often make front page news and understandably leave American taxpayers questioning the leadership and professionalism of our armed forces. But, are military pilots the unbridled risk takers portrayed in Hollywood movies like Top Gun or are they a class of highly educated, elite warriors involved in a high stakes occupation worthy of our praise and admiration? While the latter is surely the case, those at the highest levels of the military are often at a loss when explaining how a responsible Naval aviator could find himself in a no-win situation like the one described above. Conventional wisdom has historically been our only means for addressing this issue. Even the most senior aviators may only be personally familiar with a handful of accidents, of which few, if any were associated with the willful disregard for the rules. So when asked very pointed questions such as, "Admiral, how many accidents in the U.S. Navy are due to violations of the rules?" they very honestly reply, "Very few, if any." In fact, even the most experienced analysts at premiere safety organizations like the U.S. Naval Safety Center have traditionally had little more to offer. Rather than answer the question directly, they have often resorted to parading out a series of charts revealing only the total number of accidents due to aircrew error in very general terms. This is not to imply that the leadership of the U.S. Navy is evasive or that the analysts are incompetent. Rather, the numbers simply did not exist in a form that allowed such questions to be answered directly —at least, that is, until HFACS. Shortly after the accident described above, the U.S. Navy began questioning the extent to which aviators in the fleet were involved in aviation accidents due to human error, particularly violations of the rules. Coincidentally, around that same time we had begun exploring the use of human factor analysis and classification sytems as a data analysis tool within the U.S. Navy/Marine Corps. As part of that effort, we systematically examined all of the human causal factors associated with Naval (both the U.S. Navy/Marine Corps) tactical aircraft (TACAIR)8 and helicopter accidents that had occurred since 1991 (Shappell, et al., 1999). Little did we know that the results of our analyses would be the impetus behind fundamental changes that would soon take place within Naval aviation. During our investigation, we examined a variety of aircrew, supervisory, and organizational factors contained within 151 U.S. Naval Class A9 aviation accident reports using a panel of experts including aerospace psychologists, flight surgeons, and Naval aviators. The experts were instructed to classify within HFACS, only those causal factors identified by the investigative board. In other words, they were not to "reinvestigate" the accident or second-guess the investigators and chain of command. To classify anything other than the official causal factors of the accidents would not only be presumptuous, but would only infuse opinion, conjecture, and guesswork into the analysis process and in so doing, threaten the credibility of the findings. To many, the results of our analyses were alarming because we had discovered that roughly 1/3 of the Naval aviation accidents we examined were associated with at least one violation of the rules (Figure 5.1). Regardless of whether one looked at the percentage of accidents associated with violations, or the rate (number of accidents associated with at least one violation per 100,000 flight hours) at which they occurred, the findings were the same. To make matters worse, the percentage and rate had remained relatively stable across the seven years of data we examined. For obvious reasons, these findings did not sit well with the U.S. Navy/Marine Corps, and predictably, some within Naval leadership questioned the validity of our findings. After all, how could things be so bad? As a result, they sent their own experts to re-examine the data and much to their surprise, they got the same answer we did. There was no denying it now, the U.S. Navy/Marine Corps had a problem. Faced with explaining how such a large percentage of accidents could be attributable, at least in part, to the willful disregard for the rules, some simply argued that this was the natural by-product of selecting people for military duty. That is, we intentionally recruit pilots who are willing to push the envelope of human capabilities to a point where they may ultimately be asked to lay down their lives for their country. Put simply, military aviation is filled with risks, and those who aspire to be Naval aviators are naturally risk-takers who, on occasion, may "bend" or even break the rules. While certainly a concern within the U.S. Navy/Marine Corps, those who were willing to accept this explanation argued that the U.S. Army and Air Force had the same problem. Unfortunately, views like this were difficult, if not impossible to contest because the different branches of the military were using distinctly different investigative and archival systems, rather than a common human error framework. As a result, analysts were typically left with comparing little more than overall accident rates for each branch of the Armed Services rather than specific types of human errors. But with the development of HFACS, the U.S. military had a framework that would allow us to do just that! We therefore set out to examine U.S. Army and Air Force Class A accidents using the human factor analysis and classification sytems framework (Wiegmann et al., 2002). The results of these analyses were very surprising not only to us, but to others in the Fleet as well, because we found that the percentage of accidents attributable to violations of the rules was not the same among the Services (Figure 5.2). Indeed, during roughly the same time frame, a little over one quarter of the Army and less than 10 percent of the Air Force accidents were associated with violations. Furthermore, these differences had nothing to do with the fact that unlike the U.S. Navy/Marine Corps, the Army almost exclusively flies helicopters and the Air Force tends to fly more point-to-point cargo and resupply missions. Keenly aware of these potential confounds, we intentionally compared "apples-to-apples" and "oranges-to- oranges." That is, we compared data from Army helicopter accidents with Naval helicopter accidents, and those involving Air Force TACAIR aircraft with their counterparts in the U.S. Navy/Marine Corps. Having run out of explanations and still faced with this previously unknown threat to aviation safety, senior leadership within the U.S. Navy/Marine Corps knew they had to do something — but what? How could Naval aviators be so much different than their counterparts in the other services? It turns out that the answer was right under our nose, or should we say right before our eyes on the television. For over a generation, Gene Roddenberry, the creator of Star Trek, entertained millions with the adventures of Captain Kirk and the crew of the starship Enterprise. Why that is relevant to our discussion is that he chose to model his futuristic Starfleet after the U.S. Navy for one simple reason. For nearly two centuries, the U.S. Navy had empowered their front line combatants (whether they were captains of seafaring vessels or pilots of aircraft) with the ability to make tactical decisions on their own. After all, when the U.S. Navy was first created, seafaring Captains could not simply "phone home" for permission to engage the enemy. Likewise, Gene Roddenberry, when creating his popular television series, wanted Captain Kirk to be able to engage the Klingons and other enemies of the Federation without having to call home for permission every time. The Navy referred to this as "flexibility" and in many ways encouraged their officers to do what was necessary to get the job done and "beg forgiveness in the morning" if problems arose. It could even be argued that this "can-do" culture led to an attitude that if the rules did not explicitly say something could not be done it meant that you could do it. In stark contrast, within the U.S. Army and Air Force, if the rules did not say you could do something, it meant that you could not do it. As a result, if an aviator was caught "bending the rules" in the U.S. Navy/Marine Corps he would likely be taken behind the proverbial wood shed and spanked (figuratively, of course), but would then be allowed to return to tell everyone his story and fly another day. In the U.S. Army and Air Force, however, if you broke the rules you were immediately removed from duty. When all was said and done, it looked to even the most skeptical observer that the U.S. Navy/Marine Corps had a problem whose roots were imbedded deep within the culture and traditions of the organization — at least where Naval aviation was concerned. Armed with this information, the U.S. Navy/Marine Corps developed and implemented a three-prong, data-driven intervention strategy that included a sense of professionalism, increased accountability, and enforcement of the rules. To some, professionalism might sound a bit corny, but when you tell Naval Aviators that they are worse than their Army or Air Force counterparts, you quickly get their attention. Pilots, particularly those within the U.S. Navy/Marine Corps, are a very proud group. Any suggestion that they are anything less than professional, strikes at their very core, and in many ways provided the foundation for the other interventions that followed. Now that they had the full attention of the Fleet, senior leadership began building a sense of accountability among their aircrews beyond what had been in place before. In other words, if a pilot willfully broke the rules, regardless of rank, there would be consequences, and in some instances removal from flight status. It did not take long before a couple of fighter pilots were caught violating established regulations and as a result, they were summarily removed from flight status. News, in the fleet travels quickly and soon everyone knew that senior leadership was serious. Not only that, but if it was discovered that the Commanding Officer or other squadron management were not enforcing the rules, they too would be held accountable. And soon, a couple of senior officers were relieved of duty as well. Although some saw this new enforcement of the rules as Draconian, it was now clear to everyone that senior leadership was serious about reducing the number of accidents due to violations of the rules and regulations. The question was whether the interventions had any positive effect on the accident rate. Here again is where human factor analysis and classification sytems can help. Not only can human factor analysis and classification sytems identify human error trends in accident and incident data, but it is particularly useful for tracking the effectiveness of selective interventions as well. Before the development of HFACS, the only way an organization could determine the effectiveness of a given intervention was to examine the accident rate without regard for specific types of human error. Unfortunately, overall accident rates are dependent on many things of which the targeted behavior is but one piece. Consequently, if the overall accident rate did not decline, one might be tempted to abandon the intervention, without knowing whether it actually worked. After all, if accidents due to violations decline, but some other factor is on the increase, the overall accident rate may not change appreciably. But with HFACS, we can monitor selective types of human error, not just an overall accident rate. So, with that in mind, Figure 5.3 is submitted as evidence that the U.S. Navy/Marine Corps has indeed made tremendous gains. That is, by 1998, the percentage of accidents associated with violations had been reduced to Army levels, and by 2000, they had nearly reached those seen in the U.S. Air Force. Proof positive that data-driven interventions implemented by the U.S. Navy/Marine Corps worked! So, by using HFACS, the U.S. Navy/Marine Corps identified a threat to aviation safety, developed interventions, tracked their effectiveness, and "solved the problem" — or did they? As any psychologist or parent will tell you, the rules will be adhered to only as long as there is memory for the consequences. That being said, safety personnel within Naval aviation are keenly aware that as the current generation of pilots moves on, a new generation will take their place and we may once again see accidents due to violations increase. The good news is that we can track violations using HFACS, and if accidents due to this particular unsafe act begin to creep back up, the intervention can be re-evaluated and perhaps modified or reinforced. Quantifying Proficiency within the Fleet Throughout the last century, mankind has witnessed extraordinary advances within the world of aviation as propeller driven biplanes made of cloth and wood have been replaced by today's advanced turboprops and jets. Yet, improvements in aircraft design tell only part of the story as aviators have been forced to adapt with each innovation introduced to the cockpit. Gone are the celebrated days of the barnstorming pilots who flew as much by feel and instinct as they did by skill. Those pioneering aviators have been replaced by a generation of highly-educated technicians raised on Nintendo and computer games, flying state-of-the-art systems the likes of which the Wright Brothers could only dream of. Yet, at what price has this apparent evolution among aviators taken place? Within the military, some have argued that while aircrew today are perhaps smarter and may even be better decision-makers, their basic flight skills cannot compare with previous generations of pilots. This is not to say that today's military aviators are poor pilots. On the contrary, they are extremely bright and uniquely talented. Still, when comparing them to their predecessors of even 20 years ago, today's military aircrews seem to rely as much on automation and advanced flight systems as they do on their own stick-and-rudder skills. Are pilots today really that much different from those of generations past, or are such views simply the jaded opinions of those longing for the "good-old days" of aviation? Perhaps one way to answer this question is to examine the accident record for evidence that basic flight skills (otherwise known as proficiency) have eroded among today's aviators. That is, to the extent that accidents accurately portray the current state of the Fleet (and some may want to argue that point), one should be able to inspect the accident record for any evidence that would defend assertions that proficiency has degraded among military aircrews. More to the point, if proficiency has declined, the percentage of accidents associated with skill-based errors should naturally increase. With this in mind, we examined the same U.S. Naval aviation accidents described above using the human factor analysis and classification sytems framework (Shappell and Wiegmann, 2000b). What we found is that over half (110 of 199 accidents, or 55 percent) were associated with skill-based errors. It is important to remember that we are not talking about complex decisions or misperceptions that may be easier to justify. No, these were simply breakdowns in so-called "monkey skills", those stick-and-rudder skills that we all take for granted. Even more disturbing is that the percentage of accidents associated with these errors has increased steadily since 1991 when just under half (43 percent) of the accidents were associated with skill-based errors. Yet, by the year 2000, an alarming 80% of the accidents were, at least in part, attributable to a breakdown in basic flight skills (Figure 5.4). Given findings such as these, it would appear that there is at least some truth to the claim that aircrew proficiency has eroded over the decade of the 1990s – something that was little more than speculation before HFACS. So how could this happen to what many feel is the premiere military in the world? When searching for the answer, perhaps the best place to begin is with the pilots themselves. Indeed, if you ask most pilots, they will tell you that the erosion in basic flight skills was the direct result of a systematic reduction in flight hours after the Cold War. After all, flying, as with any skill, will begin to deteriorate if it is not practiced. Indeed, even the best athletes in the world cannot stay at the top of their game if they do not play regularly. It makes sense then that as flight hours decline, so to would basic flight skills. So, as any pilot will tell you, the solution is simple, provide more flight hours and proficiency will naturally improve. Unfortunately, the answer may not be that straightforward. Some experts have suggested that the reduction in aircrew proficiency is directly related to the complexity of today's modern aircraft, making the issue of flight hours all the more important to the process of maintaining basic flight skills. For example, it has been said that it takes one and a half pilots to fly an F/A-18 Hornet, the U.S. Navy/Marine Corps' front line fighter aircraft. Recognizing this, the Marine Corps has chosen to fly the two-seat model and added a weapons systems operator to help manage the workload. The problem is, the Navy chose to use the single-seat version meaning that something had to give– but what? An examination of the accident data revealed that together, single-seat and dual-seat Hornets account for roughly one quarter of all the aircrew-related accidents in the U.S. Navy/Marine Corps. Of these, just over 60 percent of the dual-seat Hornets have been associated with skill-based errors, similar to the percentage we have seen with other fleet aircraft. But what is particularly telling is that nearly 80 percent of the accidents involving single-seat Hornets have been attributed to skill-based errors. This finding lends some support to the belief that the complexity of modern fighter aircraft is a driving force behind the erosion of proficiency observed within Naval aviation. At a minimum, these data suggest that the single-seat version of the F/A-18 Hornet is likely to be the most sensitive to a reduction in flight hours. While a reduction in flight time and the increasing complexity of military aircraft make for compelling arguments, others have proposed that over-reliance on automation may also be responsible for the erosion of basic flight skills seen among today's modern fighter pilots. Consider first that modern TACAIR aircraft are all equipped with autopilots that will maintain a variety of flight parameters (e.g., altitude, heading, and airspeed) at the push of a button. Now consider that a large part of the flight regime is flown using these sophisticated avionics, thus providing little opportunity for honing one's flight skills, particularly as new automated systems replace more and more phases of flight. It should come as no surprise then that basic flight skills learned very early in training would erode. In contrast to modern fighter aircraft, most military helicopters still rely on manual flight controls where automation only serves to dampen or smooth out the inputs rather than maintain altitude, heading, or airspeed. Logically then, if one were to compare TACAIR pilots to helicopter pilots, marked differences among their basic flight skills should emerge. Not only that, but these differences should be consistent across the different branches of the Armed Forces. Again, the accident data may provide some support for this view as well. Because we now have a common framework for examining human error within U.S. military aviation, we can directly compare the percentage of skill-based errors associated with TACAIR and helicopter accidents over the last several years. As can be seen in Figure 5.5, significantly more TACAIR than helicopter accidents are associated with skill-based errors. Furthermore, this trend was consistent across the different Services as the percentage of accidents associated with skill-based errors was the same for Naval and Air Force TACAIR, as were those associated with Naval and Army helicopters. Obviously, differences other that just automation exist between helicopters and TACAIR aircraft; but, regardless of the reasons, there is no denying that a smaller percentage of helicopter accidents are attributable to skill-based errors. While it is easy to see how the reduction in flight hours, combined with the increased complexity of aircraft and even automation, may have led to the erosion in proficiency among Naval aviators, there may still be other explanations. Consider this opinion offered by a senior Naval aviator when comparing pilots today with those of his generation some 20-30 years earlier. Pilots today are better educated than we were, make better tactical decisions, and fly the most advanced aircraft known to man. But while we may not have been as smart, or have flown sexy aircraft, we could fly circles around these guys. There may be some truth to this point of view when you consider the emphasis placed on tactical decision-making during training. Much of the curriculum throughout the 1990s emphasized pilot decision-making, perhaps to the detriment of basic flight skills. Even today's modern flight simulators emphasize tactical simulations over the fundamental ability to fly the aircraft. The question is whether there is any support for this view in the accident data. Indeed, there appears to be some evidence that the emphasis on decision-making has paid some dividends, at least where accidents are concerned (Figure 5.6). The percentage of accidents associated with decision errors has declined since 1991, and even more if we only consider 1994 to 2000. Recall that it was during these same years that we saw the increase in skill-based 10 errors. So, it does appear that while there has been a modest decline in the percentage of accidents associated with decision errors, it may have been at the expense of basic flight skills. So, where does this leave us with the issue we began this section with, the erosion of proficiency among aircrew, in particular U.S. Naval aircrews? To the extent that accident data accurately reflects the state of Naval Aviation, it would appear that indeed proficiency has begun to erode. Although the debate continues over the exact causes of this troubling trend, everyone agrees that greater emphasis needs to be placed on simply flying the aircraft. In response, the U.S. Navy/Marine Corps has embarked on a concerted effort to improve proficiency among its' aviators. For instance, they have recently instituted a "back-to-the-basics" approach that focuses on such issues as re-emphasizing the need for an efficient instrument scan, prioritizing attention, and refining basic flight skills. In addition, there are efforts underway to develop low-cost simulators that focus on basic stick-and-rudder skills and issues of proficiency. While such PC-based aviation training devices have been shown to be useful within the civilian sector, their use in military aviation is only now being explored. In the end, only time and the accident record will tell whether any of these interventions will prove successful in reversing this threat to Naval Aviation. Crew Resource Management Training: Success or Failure? With the pioneering work of Clay Foushee, Bob Helmreich, and Eduardo Salas, the role of aircrew coordination in aviation safety has taken center stage within the airline industry. Not surprising then, when Naval leadership discovered that several of their accidents were attributable to breakdowns in crew resource management (CRM), an approach for integrating cockpit resource management training into the Fleet was created. Introduced to a limited number of squadrons in the late 1980s, aircrew coordination training (ACT), as it continuing airworthines management exposition to be known, was based largely on leadership and assertiveness training developed by the airlines to address their own concerns with CRM. By the early 1990s, ACT had become fully integrated into both initial and recurrent Naval training and was expanded to cover such things as workload management and communication skills. But was the U.S. Navy/Marine Corps' new ACT program effective at reducing the spate of accidents associated with cockpit resource management failures? Initial assessments of the program in 1992 were encouraging, particularly within those communities in which ACT was first introduced (Yacavone, 1993). As a result, it was widely believed that the program within the U.S. Navy/Marine Corps had been a huge success. So much so, that when faced with budget cuts after the Cold War, some officials suggested that funds allocated for ACT be redirected to other priorities like buying jet fuel or aircraft parts. After all, everyone had been trained on the principles of cockpit resource management and to paraphrase one senior Naval Officer, "we've got no [stinkin'] cockpit resource management problem in the Navy, at least not anymore." Unfortunately, follow-up analyses had not been conducted to assess the long-term impact of ACT on Naval aviation safety. This was due in large part to the difficulty of tracking cockpit resource management failures within the accident data and the growing belief that whatever cockpit resource management problems existed before, had been solved with ACT. With the implementation of human factor analysis and classification sytems in the late 1990s, the Navy and Marine Corps were able to quickly and objectively assess the success of the ACT program. Regrettably, the results did not support early optimism. As illustrated in Figure 5.7, roughly 60 percent of Naval aviation accidents were found to be associated with a breakdown in CRM. Even more troubling, this percentage was virtually identical to the proportion seen in 1990, one year before the fleet-wide implementation of ACT. Arguably, there was a slight dip in 1992, which may explain the early enthusiasm regarding the flagship program. Since then however, the percentage of cockpit resource management accidents has fluctuated up and down, but ultimately has not declined, as many believed was the case. Clearly, to even the most ardent supporter, the U.S. Navy's ACT program had not done what its original developers had hoped. Nevertheless, it did not make sense to shut down a program targeted at what appeared to be a large (60 percent of the accidents) and persistent (over 10 years) threat to aviation safety. On the other hand, to simply pour more money into an effort that yielded little improvement did not make much sense either. Instead, what the U.S. Navy/Marine Corps needed was a thorough evaluation of the ACT program so informed decisions could be made regarding the future of cockpit resource management training. But, where does one start? After all, how could ACT have been so ineffective given the touted success of similar programs within commercial aviation? Therein lies at least part of the problem. After all, have commercial cockpit resource management programs really been successful? Or, are such claims based simply on anecdotal and other subjective evidence presented by those too close to the issue to remain objective? With the transition of the human factor analysis and classification sytems framework from the military to the civilian sector, we were in a unique position to directly address this question. Using scheduled air carrier11 accidents occurring between 1991 and 1997, we found that roughly 30 percent of all air carrier accidents were associated with a breakdown in CRM, a proportion much lower than that seen in the U.S. Navy/Marine Corps (Figure 5.8). At first glance, this might lead some to conclude that the commercial program had been a success, at least when compared with Naval aviation. However, a closer inspection of the graph revealed that like the Navy and Marine Corps, the percentage of accidents associated with cockpit resource management failures has remained largely unchanged since 1991. Worse yet, the percentage may have even increased slightly. Notably, we were not the only ones that that identified this disturbing trend within the accident data. The Government Accounting Office (GAO) had initiated a major review of the commercial cockpit resource management program and concluded essentially the same thing (GAO, 1997). That is, commercial cockpit resource management programs have been largely ineffective at reducing accidents due to cockpit resource management failures. Thus, it would appear that our analyses had been validated (but, one thing is certain; given our current salaries, the human factor analysis and classification sytems analyses were much cheaper to perform). Now, before our colleagues start sending us hate mail for criticizing their cockpit resource management programs, we admit that cockpit resource management has changed considerably since it was first introduced to commercial aviation nearly two decades ago. In fact, those who pioneered the field were the first to recognize the limitations of the initial strategy for teaching cockpit resource management (for a brief review see Salas et al., 2003). For instance, many of the early programs amounted to little more than personality assessment and classroom discussions of high profile case studies with a little bit of science thrown in for good measure. Yet, even the best classroom instruction is limited if participants are not required to demonstrate the principles they have learned. In other words, simply making people aware that they need to communicate better without compelling them to practice in a real-world situation (e.g., in the aircraft or simulators) is tantamount to telling student pilots how to fly an airplane and expecting them to be able to do it the first time without practice! To address this limitation, programs like line-oriented flight training (LOFT) were developed in the early 1990s, requiring pilots to demonstrate, in a flight simulator, the skills they learned in the classroom. More recently, programs like the Advanced Qualification Program (AQP), have integrated cockpit resource management training and evaluations into the actual cockpit. A voluntary program employed by nearly all major air carriers in the U.S. and a large portion of regional airlines, AQP requires that both initial and recurrent qualifications include inflight evaluations of specific technical and cockpit resource management skills. Although these new training programs are promising, whether they improve cockpit resource management and reduce accidents remains to be seen. Recognizing the potential benefit of new programs like LOFT, the U.S. Navy began experimenting with their own version in 1997. In particular, a few S-3 Viking squadrons began using videotape feedback and simulators as part of their recurrent training program. Preliminary results (albeit subjective in nature) have been positive; but whether the program will be expanded to include the entire Fleet has yet to be determined. Perhaps more important in the near term, was the realization that the ACT curriculum had not been tailored to meet the specific needs of the targeted community. Indeed, much of the curriculum had yet to evolve beyond a few "classic" examples of civilian aviation accidents involving cockpit resource management failures. In essence, instructors were teaching combat pilots lessons learned from accidents like the Eastern Air Lines L-1011 that crashed into the Florida Everglades (NTSB, 1973). While the movies may have been informative, it is hard for an F/A-18 Hornet pilot to make the connection between a burned out light bulb on a commercial airliner and cockpit resource management failures in one of the most advanced fighter jets in the world today. On top of everything else, most of the examples were outdated, narrow in scope, and did not capture the factors that contribute to cockpit resource management failures in Naval aviation. After realizing the folly of their ways, it was no surprise to Naval leadership that ACT has had little or no impact in the Fleet. As a result, a concerted effort was undertaken to identify platform-specific cockpit resource management accidents as teaching tools for future cockpit resource management training. That is, the U.S. Navy/Marine Corps redesigned the program to teach F/A-18 cockpit resource management to F/A-18 pilots and F-14 cockpit resource management to F-14 pilots. Using specific examples relevant to each community, the goal was to identify those aspects that are unique to single-seat versus multi-seat aircraft, fighter versus attack, fixed-wing versus helicopter, and any other nuances that are relevant to today's modern Naval aviator. With HFACS, Naval leadership will be able to accurately and objectively assess those aspects that work to reduce cockpit resource management related accidents and those that do not. The Redheaded Stepchild of Aviation Understandably, a great deal of effort has been expended over the last several decades to improve safety in both military and commercial aviation. Yet, even though hundreds of people have died and millions of dollars in assets have been lost in these operations, the numbers pale by comparison to those suffered every year within general aviation (GA). Consider the decade of the 90's. For every commercial or military accident that occurred in the U.S., roughly nine GA aircraft crashed (Table 5.1). More alarming, nearly one in five GA accidents (roughly 400 per year) involved fatalities — resulting in a staggering 7,074 deaths! Since 1990, no other form of aviation has taken more lives. Why then has general aviation received so little attention? Perhaps it has something to do with the fact that flying has become relatively commonplace, as literally millions of travelers board aircraft daily to get from point A to point B. Not surprising then, when a commercial airliner does go down, it instantly becomes headline news, shaking the confidence of the flying public. To avoid this, the government has focused the bulk of their limited aviation resources on improving commercial aviation safety. But does the commercial accident record warrant the lion's share of the attention it has received? Well, if you consider the data in Table 5.1, there are about 130 commercial aircraft accidents per year. However, of these 130 so-called "accidents," many are simply injuries in the cabin due to turbulence or involve small, on-demand air taxis. Thankfully, very few are on the scale of TWA Flight 800, the Boeing 747 that crashed off the coast of New York in July of 1996 killing all 230 passengers. In fact, of the 1,309 commercial airline accidents that occurred in the 1990s, only a handful involved major air carriers and fewer yet were associated with fatalities. On the other hand, in the time it took you to read this book there have probably been 20 GA accidents in the U.S. alone, of which four involved deaths. But did you hear about any of them on the primetime evening news, or read about them on the front page of USA Today. Probably not, after all they happen in isolated places, involving only a couple of hapless souls at a time. In fact, unless the plane crashed into a school, church, or some other public venue, or involved a famous person, it is very unlikely that anyone outside the government or those intimately involved with the accident even knew it happened. Although GA safety may not be on the cusp of public consciousness, a number of studies of GA accidents have been conducted in an attempt to understand their causes. Unfortunately, most of these efforts have focused on contextual factors or pilot demographics rather than the underlying cause of the accident. While no one disagrees that contextual factors like weather (e.g., instrument meteorological conditions versus VMC), lighting (e.g., day versus night), and terrain (e.g., mountainous versus featureless) contribute to accidents, pilots have little, if any, control over them. Likewise, knowing a pilot's gender, age, occupation, or flight experience, contributes little to our ability to prevent GA accidents. After all, just because males may have a higher accident rate than females, are we now going to prohibit men from flying? Or how about this well publicized bit of trivia: pilots with fewer that 500 flight hours have a higher risk of accidents. What are we as safety professionals to do, wave a magic wand in the air and poof; a 300-hour pilot can now fly with the prowess of someone with 1000 flight hours under their belt? Truth be told, this information has provided little in the way of preventing accidents apart from identifying target audiences for the dissemination of safety information. In fact, even when human error has been addressed, it is often simply to report the percentage of accidents associated with aircrew error in general or to identify those where alcohol or drug use occurred. Recently however, we examined over 14,500 GA accidents using five independent raters (all were certified flight instructors with over 3,500 flight hours) and the human factor analysis and classification sytems framework. What we found was quite revealing, as previously unknown error trends among general aviation were identified. Let us first look at the roughly 3,200 fatal GA accidents associated with aircrew error. From the graph in Figure 5.9, some important observations can be made. For instance, it may surprise some that skill-based errors, not decision errors, were the number one type of human error associated with fatal GA accidents. In fact, accidents associated with skill-based errors (averaging roughly 82 percent across the years of the study) more than doubled those seen with decision errors (36 percent) and violations of the rules (32 percent). Even perceptual errors, the focus of a great deal of interest over the years, were associated with less than 12 percent of all fatal accidents. Also noteworthy is the observation that the trend lines are essentially flat. This would seem to suggest that safety efforts directed at general aviation over the last several years have had little impact on any specific type of human error. If anything, there may have been a general, across-the-board effect, although this seems unlikely given the safety initiatives employed. The only exceptions seemed to be a small dip in the percentage of accidents associated with decision errors in 1994 and 1995 and a gradual decline in those associated with violations between 1991-94. In both cases however, the trends quickly re-established themselves at levels consistent with the overall average. While this is certainly important information, some may wonder how these findings compare with the nearly 11,000 non-fatal accidents. As can be seen in Figure 5.10, the results were strikingly similar to those associated with fatalities. Again, the trends across the years were relatively flat and as with fatal accidents, skill-based errors were associated with more non-fatal accidents than any other error form, followed by decision errors, violations, and perceptual errors respectively. While the similarities are interesting, it was the differences, or should we say, the difference, that was arguably the most important finding. When the error trends are plotted together for fatal and non-fatal GA accidents, as they are in Figure 5.11, it is readily apparent that the proportion of accidents associated with violations was considerably less for non-fatal than fatal GA accidents. In fact, using a common estimate of risk known as the odds ratio, fatal accidents were more than four times more likely to be associated with violations than non-fatal accidents (odds ratio = 4.314; 95 percent confidence interval = 3.919 to 4.749, Mantzel-Haenszel test for homogeneity = 985.199, p<.001). Put simply, if a violation of the rules resulting in an accident occurs, you are considerably more likely to die or kill someone else than get up and walk away. So, what does all this mean? For the first time ever, we can talk about more than just the fact that nearly 80 percent of all general aviation accidents are attributable to "human error." After all, would you continue see a physician who only confirmed that you were "sick" without telling you what you what was wrong or what was needed to make you better? Probably not. The good news is that we now know "what is wrong" with general aviation –at least from a human error point of view. Specifically, the vast majority of GA accidents, regardless of severity, are due to skill-based errors. Also evident from our analyses, was the observation that one-third of all fatal accidents are due to violations of the rules and they are much less common in non-fatal accidents. All of this leads to the inevitable question, "what can be done now that the face of human error has been exposed within general aviation?" Well, the data does suggest some possible avenues for preventing accidents. For example, there is a need to address the large percentage of accidents associated with skill-based errors. Perhaps placing an increased emphasis on refining basic flight skills during initial and recurrent flight training could possibly be effective in reducing skill-based errors. However, if the goal is to reduce fatal accidents, then greater emphasis must also be placed on reducing the number of violations through improved flight training, safety awareness, and enforcement of the rules. Still, before such interventions can be effectively applied, several other questions concerning the nature and role of human error in aviation accidents need to be addressed. Conclusion Using a human error framework like human factor analysis and classification sytems allows safety professionals and analysts alike to get beyond simply discussing accidents in terms of the percentage and rate of human error in general. Instead, we can now talk about specific types of human error, thereby increasing the likelihood that meaningful and successful intervention strategies can be developed, implemented, and tracked. Imagine where we would be today if all we were still talking in generalities about mechanical failures without describing what part of the aircraft failed and why. Indeed, we might still be losing aircraft and aircrew at rates seen in the 1950s! Why then should we expect any less from human error investigations and analyses? The question remains, with so many human error frameworks available, how do you know which one is best for your organization. After all, we don't know of any developers or academicians who believe that their error framework is of little use in the lab or in the field. They all feel good about their frameworks. But as consumers, safety professionals should not have to rely on the developers for confirmation of a particular error framework's worth. Ideally, we would have some criteria that can be used to help us decide. With that in mind, let us turn to the next chapter. 6 Beyond Gut Feelings... Clearly, human factor analysis and classification sytems or any other framework adds little to an already long list of human error taxonomies if it does not prove useful in the operational setting. For that reason, we have made a concerted effort throughout the development process to ensure that it would have utility not only as a data analysis tool, but as a structure for accident investigation as well. The last thing we wanted was for human factor analysis and classification sytems to be merely an academic exercise applauded by those perched in their ivory towers, but of little use in the real world. In a sense then, we were serving two masters when we began our work. On the one hand, as accident investigators ourselves, we wanted to ensure that our colleagues in the field could use human factor analysis and classification sytems to investigate the human factors associated with aviation incidents and accidents. On the other hand, as academic wolves in sheep's clothing, we wanted to make certain that it could withstand the scientific scrutiny that would inevitably come. While we would like to think that we did a reasonable job straddling the fence as it were, how does one really know? Is there anything other than opinions upon which to base an evaluation of HFACS, or for that matter, any other framework? After all, error analysis systems like human factor analysis and classification sytems were designed to eliminate intuition and "gut feelings" from the accident investigation process. To turn around and evaluate their worth using those same gut feelings seems hypocritical at best and intellectually dishonest at worst. No, what we needed was to get beyond gut feelings. The good news is that human factor analysis and classification sytems was developed using an explicit set of design criteria well known within the research community, but perhaps not as familiar elsewhere (Wiegmann and Shappell, 2001a). The purpose of this chapter therefore is to describe those criteria within the context of the development of HFACS' development to illustrate our points. Our hope is that safety professionals and others will consider these criteria when deciding which error framework to use within their own organizations. Before we begin, however, we want to warn the reader that this chapter is by far the most academic (a code word for boring) in the book. So, if you are not the type that really enjoys reading scientific articles or talking statistics, then this is definitely a chapter for you to skim. On the other hand, if you are interested in learning more about the scientific rigor involved in developing and testing error analysis systems dike HFACS, you may want to read a bit more closely. We will catch up with, or perhaps wake up, the rest of you in Chapter 7. Validity of a Framework The utility of any human error framework centers about its validity. That is to say, what exactly does the taxonomy measure, and how well does it do so (Anastasi, 1988). Yet, as important as validity is, surprisingly little work has been done to examine or compare the validity of different error models in an applied context. This may be due, in part, to the fact that assessing the validity of a framework can be a very difficult and overwhelming task for even the most scholarly of us. Indeed, several methodological approaches for testing the validity of analytical techniques and measurement tools have been proposed (Anastasi, 1988). But, while there are many types of validity (Figure 6.1), and even more ways to measure it, three types (content, face, and construct validity) are essential if an error taxonomy is going to be useful in the field. The first, content validity refers to whether a given framework adequately covers the error domain to be measured (Carmines and Zeller, 1979). That is, does the system capture the multitude of ways a human can err, or does it leave some important factors out. As we discussed in Chapter 2, many error taxonomies do not address the important precursors to human error. For instance, some focus entirely on the operator, ignoring the role played by supervisors and the organization in the genesis of human error. These so-called "blame-and-train" systems suffer from a lack of content validity in addition to a variety of other, perhaps more obvious, problems. Face validity, on the other hand, refers to whether a taxonomy "looks valid" to those who would use it or decide whether or not to adopt its use within their organization (Carmines and Zeller, 1979). For example, does a particular framework seem like a reasonable approach for identifying the human factors associated with aviation accidents? More specifically, does it appear well designed, and will it work reliably when employed in the field by all levels of investigators? If the answer to any of these questions is "no," then the system is said to lack face validity. Finally, construct validity, seeks to bridge the gap between a theoretical concept (e.g., human error) and a particular measuring device or procedure like human factor analysis and classification sytems (Carmines and Zeller, 1979). In other words, to what extent does a framework tap into the underlying causes of errors and accidents? Does the system really address why accidents occur, or does it simply describe what happened or restate the facts. Hopefully, this sounds familiar since it is the same argument that we made in earlier chapters against merely describing what occurred (e.g., the pilot failed to lower the landing gear), without identifying why (e.g., mental fatigue, distraction, etc.). In many ways then, construct validity, although difficult to evaluate, is the most important form of validity associated with error taxonomies. Factors Affecting Validity A number of theorists have proposed objective criteria for inferring the validity of error frameworks in applied settings (Hollnagel, 1998; O'Connor and Hardiman, 1996). However, these criteria vary widely across researchers and domains. That being said, a recent review of the literature (Wiegmann and Shappell, 2001a) suggests that at least four factors need to be considered when evaluating an error framework. As illustrated in Figure 6.2, these include: reliability, comprehensiveness, diagnosticity, and usability. We will address each of these in turn over the next several pages. Reliability If an error framework is going to be practical, users must be able to identify similar causal factors and reach the same conclusions during the course of an accident/incident investigation (O'Connor and Hardiman, 1996). Unfortunately, complete agreement among investigators has proven virtually impossible using existing accident investigation schemes. As a result, developers of human error frameworks are continually striving to improve the reliability of their systems. Assessing reliability. As illustrated in Figure 6.3, the practice of assessing and improving the reliability of a classification system is an iterative process involving several steps. To begin, one must first decide on what level of agreement (i.e., inter-rater reliability) is acceptable. Ideally, that would be 100 percent agreement. However, as we just mentioned, reality dictates that something less will be required since very few, if any, classification systems will yield 100 percent agreement all of the time — particularly when human error is involved. After deciding upon a suitable level of inter-rater reliability, the investigation/classification system can then be tested using independent raters and a sample data set to determine if the error framework produces the requisite level of inter-rater reliability. If not, modifications may need to be made to the error categories, definitions, instructions, or other areas to improve agreement among the raters. After the necessary changes have been made, the new (presumably improved) taxonomy can then be applied to another sample of accident data, and inter-rater reliability reassessed. If the reliability has reached acceptable levels, the process stops. If not, the iterative process will continue until the target levels are eventually attained. Measuring reliability. While the process seems simple on the surface, let us assure you it is not. Even deciding upon an adequate measure of inter-rater reliability can be difficult since there are many ways to quantify it. Perhaps the most common method is to simply calculate the percentage of agreement among independent raters for a given classification task. For instance, if two raters agreed on 170 out of 200 classifications, they would have agreed 170/200 = 0.85 or 85 percent of the time. The problem is that people will agree some of the time simply by chance. Consider, for example, a classification system with just two categories (e.g., errors of omission and errors of commission). For any given causal factor, there is a 25 percent chance that you would find agreement even if both raters simply guessed. Let us explain. The probability that Rater A classified a given causal factor as an "error of omission" by chance alone is 1 in 2 or 0.50. Likewise, the probability that Rater B classified the same causal factor as an "error of omission" by chance is 0.50, as well. To obtain the probability that both raters agreed simply by chance, you just multiply the two probabilities (0.50 x 0.50) and get 0.25. In other words, 25 percent of the time Raters A and B would be expected to agree by chance alone. To control for this, a more conservative statistical measure of inter-rater reliability, known as Cohen's Kappa, is typically used (Primavara et al., 1996). Cohen's Kappa measures the level of agreement between raters in excess of the agreement that would have been obtained simply by chance. The value of the kappa coefficient ranges from one, if there is perfect agreement, to zero, if all agreements occurred by chance alone. In general, a Kappa value of 0.60 to 0.74 is considered "good" with values in excess of 0.75 viewed as "excellent" levels of agreement (Fleiss, 1981). At a minimum then, the goal of any classification system should be 0.60 or better. To illustrate how testing and improving reliability can perfect an error framework, we will walk you through our efforts during the development of HFACS. You may recall that our first attempt at designing a human error framework resulted in the Taxonomy of Unsafe Operations12(Figure 6.4), a system based largely on the approach described by Reason (1990). While causal factors from actual Naval aviation accidents served as "seed data" throughout the development process, our intentions were largely academic, with little thought given to the taxonomy's potential use by accident investigators (Shappell and Wiegmann, 1995; Wiegmann and Shappell, 1995). Yet, by 1996, it became apparent that our work had possibilities beyond simply analyzing existing accident data. Indeed, the U.S. Navy/Marine Corps had expressed interest in using the taxonomy in the field. But, could it be reliably used by non-academics since most accident investigators in the field do not possess Ph.D.s in human factors? To answer this question, Walker (1996) and Rabbe (1996) examined 93 U.S. Navy/Marine Corps controlled flight into terrain (CFIT) accidents and 79 U.S. Air Force F-16 accidents respectively, using the Taxonomy of Unsafe Operations. Employing pilots as raters, rather than academicians, the two studies classified over 700 causal factors associated with a combined 172 accidents. Their findings revealed that while the overall reliability among the raters was considered "good" using Cohen's Kappa (Table 6.1), inter-rater reliability was best for causal categories within the preconditions for unsafe acts, with slightly lower results for categories within the unsafe acts and unsafe supervision tiers. Based on these findings, a number of modifications were made to the taxonomy using input from accident investigators and the scientific literature (Figure 6.5). Initially, our focus was on the unsafe acts of operators. In many ways, we were forced to go back to the drawing board when it became clear that the concept of intended and unintended acts was lost on Walker and Rabbe's pilot-raters. For those that may not be familiar with Reason's (1990) description of intended and unintended actions, he felt that any description of unsafe acts must first consider the intentions of those committing them. In other words, "Did the action proceed as planned?" If so, then the behavior is considered intentional; if not, it is considered unintentional. Unfortunately, trying to determine the intentions of aircrew involved in accidents (particularly those that perished in the crash) proved extremely difficult as evident from the modest levels of agreement between the pilot-raters. Therefore, after discussions with several pilots and aviation accident investigators, what seemed to make more sense was to distinguish between errors and violations as described in Chapter 3. We therefore restructured the unsafe acts around those two overarching categories. But, the concept of intended and unintended actions was not the only problem that the pilot-raters had with the original Taxonomy of Unsafe Operations. The distinction between slips and lapses was also difficult for the pilot-raters to understand and manage. Slips, as described by Reason (1990), are characteristic of attention failures and take the form of inadvertent activations, interference errors, omissions following interruptions, and order reversals, among others. In contrast, lapses typically arise from memory failures and include errors such as omitted items in a checklist, place losing, and forgotten intentions. If all this sounds familiar, it should, since both slips and lapses comprise a large part of the skill-based errors category described in Chapter 3, and certainly made more sense to our investigators. We therefore abandoned the categories of slips and lapses and adopted skill-based errors as one of two types of errors committed by operators. The other type, mistakes, was kept in its entirety. We also separated the category of violations into routine and exceptional violations as described in Chapter 3 and made some minor adjustments to the definitions of the remaining categories. Finally, we combined inadequate design with unrecognized unsafe operations since both were considered "unknown/unrecognized" by supervisors or those within middle management. Two additional studies were then conducted to determine what, if any, impact the changes had on inter-rater reliabilities. Using the revised taxonomy, Ranger (1997) and Plourde (1997) examined causal factors associated with 132 U.S. Navy TACAIR and rotary wing accidents and 41 B-1, B52, F-111, and F-4 accidents, respectively. Findings from those studies revealed sizeable increases in agreement over those found by Walker (1996) and Rabbe (1996) the previous year (Table 6.1). Nevertheless, while we were close, we still felt that there was room for improvement. We therefore made additional modifications to the framework as illustrated in Figure 6.6. Essentially, we added the category of perceptual errors and renamed mistakes to decision errors within the unsafe acts tier. In addition, we deleted the category of mistakes/misjudgment from the preconditions for unsafe acts because it was often confused and not utilized by the pilot-raters. Likewise, the unforeseen supervisory failures were deleted in large part because the Navy felt that if something was unforeseen, it was difficult to hold supervisors culpable since even Navy leaders cannot know everything. Yet another reliability study was then conducted using what was renamed the Failure Analysis and Classification System (FACS)13 and 77 U.S. Air Force A-10 accidents (Johnson, 1997). Overall, pair-wise reliabilities were found to be excellent (Kappa = 0.94) and consistent across all levels (Table 6.1). So, it would appear that by the end of 1997, we had developed a highly reliable error framework for use in the field as well as an analytical tool. But, would it be useful outside the military – that remained to be seen. Early in 1998, we were interested in whether reliability would suffer if human factor analysis and classification sytems were used with civilian aviation accidents, given that our error framework was originally developed for use within the military. We were also curious whether the newly developed organizational influences tier would enjoy the same degree of inter-rater reliability as the rest of HFACS. We therefore conducted two additional reliability studies using commercial aviation accident data (Wiegmann et al., 2000). The first of these studies involved 44 scheduled air carrier accidents occurring between January 1990 and December 1997. The overall reliability of the human factor analysis and classification sytems coding system was again assessed by calculating inter-rater reliability between our pilot-raters. Overall, the independent raters agreed 73 percent of the time yielding a Kappa value of 0.65. As expected, the highest level of inter-rater agreement was found among the unsafe acts and preconditions for unsafe acts, while the lowest level of agreement was within the supervisory level and the newly developed organizational tier. Post-analysis discussions with raters suggested that the definitions and examples used to describe human factor analysis and classification sytems were too closely tied to military aviation and therefore somewhat ambiguous to those without a military background. For example, the concept of "flat-hatting," while commonly used within the U.S. Navy/Marine Corps to describe a Naval aviator who recklessly violates the rules to impress family and friends, was foreign to our civilian raters. To remedy this, the tables and checklists were modified to include more civilian examples, and yet another study was conducted to reassess the reliability of the human factor analysis and classification sytems system. The follow-up study involved a new set of 79 commercial aviation accidents. This time, the two independent raters agreed roughly 80% of the time for a Kappa value of 0.75, considered "excellent" by conventional standards. Still, not satisfied by such a small sample size, we subjected the human factor analysis and classification sytems framework to the largest reliability test yet. Using over 2,500 general aviation accidents associated with more than 6,000 causal factors, five independent pilot-raters agreed more than 80 percent of the time yielding a Kappa value of 0.72 (Wiegmann and Shappell, 2001c). Given the fact that most safety professionals agree that the data associated with GA accidents is sparse and often cryptic, we were quite surprised and obviously pleased with the findings. Furthermore, these percentages varied only slightly across the years examined in this study (the actual range was 77 percent to 83 percent agreement). To summarize then, over the course of nearly five years of research and development, human factor analysis and classification sytems evolved from little more than an academic exercise to a full-fledged accident investigation and analysis tool used by both military and civilian organizations. What's more, throughout the entire development process, we utilized the power of statistics and data analysis to ensure that inter-rater reliability was maximized knowing that a wide variety of individuals, with disparate educations and backgrounds would use HFACS. Comprehensiveness Reliability is only one of four criteria essential to the validity of any error framework. Comprehensiveness, or the extent to which a framework captures all the key information surrounding an accident, is important to the validity of any error classification system as well (O'Connor and Hardiman, 1996). However, this definition may be a bit misleading since a framework's comprehensiveness is really in the eye of the beholder. For instance, if one's interest is only in aircrew error, a cognitive framework may suffice. After all, when it comes to identifying and classifying information processing errors (e.g., decision errors, perceptual errors, attention errors, etc.), cognitive models tend to be very detailed and complete. Unfortunately, if your interest extends beyond operator error to include other contextual and organizational factors, these models tend to fall short, which may explain why they are rarely ever used exclusively. Indeed, most organizations are interested in the breadth of possible error causal factors (e.g., supervisory, working conditions, policies, procedures, etc.), in addition to operator error. For that reason, organizational frameworks may be more suitable. Regrettably, as discussed in Chapter 2, these frameworks are not without their limitations either. Often, they sacrifice the detail seen with cognitive models making them more comprehensive globally, but less so on any one dimension. In a perfect world though, one would not have to choose between breadth and depth in an error framework. A truly comprehensive system should be able to capture all the relevant variables that a given organization is interested in pursuing and perhaps even some that it may not. After all, it is hard to predict what aspects of human error an organization will actually be interested in tracking from year-to-year, much less 10 to 20 years from now. No one wants to find out that the framework they have invested years of effort and thousands of dollars in lacks the detail necessary to answer the safety questions at hand. This brings us to yet another issue regarding error frameworks: comprehensive does not necessarily mean large. One has to distinguish between a "database" that is primarily used for archival purposes, and an error framework that is used for accident investigation and data analysis. Consider, for example, databases like ICAO's ADREP-2000. While it is certainly comprehensive, its overwhelming size makes it of little use for data analysis (Cacciabue, 2000). In fact, some have even quipped that the number 2000 in ADREP's title stands for the number of causal factors in the database, rather than the year it was created! In contrast, error frameworks like human factor analysis and classification sytems are not databases per se, but are theoretically based tools for investigating and analyzing the causes of human error. Therefore, they must be comprehensive too, yet they must also maintain a level of simplicity if meaningful analyses are to be performed. Assessing comprehensiveness. So how do you know if the framework you are looking at is parsimonious, yet comprehensive enough to meet your needs today and in the future? To answer this question, one has to first decide when and where to stop looking for the causes of errors and accidents. Nearly everyone agrees that to gain a complete understanding of the causes of many accidents requires an examination well beyond the cockpit, and should include supervisors and individuals highly placed within the organization. But why stop there? As Reason (1990) and others have pointed out, there are often influences outside the organization (e.g., regulatory, economic, societal, and even cultural factors) that can affect behavior and consequently the genesis of human error. Still, if this line of reasoning is taken to its illogical extreme, the cause of every error and accident could be traced all the way back to the birth of those who erred. After all, if the individual had never been born, the error might never have occurred in the first place. Even some, in their zeal to find the "root cause," might find themselves tracing the sequence of events back to the dawn of creation. While this seems ridiculous (and it is), stopping at any point along the sequence of events is at best, a judgment call and thus is subject to the interpretation and wishes of those investigating the accident/incident. Therefore, to circumvent the arbitrary and capricious nature of when and where to stop an investigation, many theorists have adopted the strategy of searching for "remediable causes." A remediable cause is defined as one that is readily and effectively curable, "the remedy of which will go farthest towards removing the possibility of repetition" (DeBlois, 1926, p. 48). Ideally, a comprehensive framework would be capable of capturing all the errors and their sources that, if corrected, would render the system more tolerant to subsequent encounters with conditions that produced the original error event (Reason, 1990). With that being said, the question remains, "how do you know if the framework you are working with is truly comprehensive?' Regrettably, there are no fancy statistics or pretty diagrams to illustrate the process like there were for reliability. For all intents and purposes, it is simply a matter of mapping a given error framework onto an organization's existing accident database to see if any human causal factors remain unaccounted for. If everything is accommodated, the framework is said to be comprehensive – at least for initial analysis purposes. All of this brings us to our initial attempt at improving the way the U.S. Navy/Marine Corps analyzes the human factors surrounding an incident/accident. Initially, we tested several "off-the-shelf' error frameworks described in the literature using the Naval aviation accident database (Shappell and Wiegmann, 1995; Wiegmann and Shappell, 1995, 1997). Much to our chagrin however, most of those frameworks focused largely on the information processing or unsafe acts level of operator performance. As a result, they did not capture several key human factors considered causal to many Naval aviation accidents. It quickly became clear that we would need to develop a new error framework – a prospect that we did not plan for when we began our work in the area. Truth be told, our academic exercise had reached a huge roadblock until we continuing airworthines management exposition across James Reason's (1990) seminal work, Human Error. Using Reason's ideas as a starting point, we began by trying to fit the human causal factors identified within the U.S. Navy/Marine Corps accident database into the model of active and latent failures (known as the "Swiss-cheese" model of human error) described in Chapter 3. The problem was that we had to accommodate more than 700 human factors spread across more than 250 different personnel classifications (OPNAV Instruction 3750.6R). To give you a sense of the magnitude of the task, it might be helpful to describe how the Naval Safety Center's accident database is organized. As with many accident databases, the Naval aviation database is organized around who committed the error, what occurred, and why. A very small part of the database has been reproduced in Tables 6.2, 6.3, and 6.4 for illustrative purposes. While we were able to map a number of the accident causal factors directly onto Reason's description of unsafe acts (i.e., intended and unintended behavior as described above), no pre-existing framework for the preconditions for unsafe acts and supervisory failures was available. For those two tiers we had to look for natural clusters within the U.S. Naval database. Using subject matter experts within the scientific and operational community, we were able to identify several naturally occurring categories within the database. As illustrated earlier (Figure 6.4) we identified six categories of preconditions for unsafe acts (three categories of substandard conditions and three categories of substandard practices) and seven categories of supervisory failures (three unforeseen and four known supervisory failures). Furthermore, because we used the existing Naval aviation accident database as the basis of our clustering, our newly developed taxonomy was capable of accounting for all the causal factors. Well sort of ... As described above, the Taxonomy of Unsafe Operations did not contain an organizational tier. This was done intentionally since very few organizational failures had been identified by U.S. Navy/Marine Corps accident investigators. For that reason, or perhaps because we were both junior officers within the U.S. Navy with career aspirations beyond our current ranks, we chose not to develop an organizational tier within our original taxonomy. However, with the landmark symposium on organizational factors and corporate culture hosted by the U.S. national transportation safety board in 1997, it quickly became clear that organizational failures would have to be addressed within our framework if it was to be truly comprehensive. We therefore added an organizational tier in 1998, and with the other changes noted above, renamed the framework to the Human Factors Analysis and Classification System (Shappell and Wiegmann, 1999, 2000a; 2001). The human factor analysis and classification sytems framework was once again mapped onto the Naval aviation accident database resulting in a complete capture of the human-causal factors contributing to Naval aviation accidents (Shappell et al., 1999). Since then, evaluations of the comprehensiveness of human factor analysis and classification sytems have been performed using data obtained from over 20,000 commercial and general aviation accidents (Wiegmann and Shappell, 2001c; Shappell and Wiegmann, 2003; in press). As you can imagine, we were delighted to find out that the human factor analysis and classification sytems framework was able to accommodate all the human causal factors associated with these accidents, suggesting that the error categories within human factor analysis and classification sytems that were originally developed for use in the military were also appropriate for civil aviation as well. Nevertheless, while all the causal factors were accounted for, instances of some error categories within human factor analysis and classification sytems were not contained in the commercial and general aviation accident databases. As would be expected, there were very few organizational or supervisory causal factors associated with general aviation accidents. The one noted exception were accidents associated with flight training where the aircraft was owned and maintained by a flight school and the instructors acted as both an instructor and supervisor. Most other general aviation accidents involved owner/operators, so there was little need for either the supervisory or organizational tiers in those cases. It was also interesting to note that there were no instances of organizational climate or personal readiness observed in the commercial aviation accident database, nor were there very many instances of supervisory factors. One explanation for this finding might be that contrary to Reason's model of latent and active failures, supervisory and organizational factors do not play as large a role in the etiology of commercial aviation accidents as they do in the military. On the other hand, these factors may contribute to civilian accidents, but are rarely identified using existing accident investigation processes. Based on our experience in the field, as well as those of other safety professionals within commercial aviation, the latter explanation seems more likely to be the case. Regardless, our extensive studies of HFACS' comprehensiveness indicate that it is capable of capturing the existing human causal factors contained in U.S. civil and military aviation databases. Diagnosticity For a framework to be effective, it must also be able to identify the interrelationships between errors and reveal previously unforeseen trends and their causes (O'Connor and Hardiman, 1996). Referred to as diagnosticity, frameworks with this quality allow analysts to identify those areas ripe for intervention, rather than relying solely on intuition and conjecture. Better yet, once trends have been identified, a truly diagnostic error framework will ensure that errors can be tracked and changes detected so that the efficacy of interventions can be monitored and assessed. Assessing diagnosticity. Regrettably, like comprehensiveness, there are no fancy statistical measures devoted solely to the evaluation of diagnosticity. Instead, the proof is really in the proverbial pudding. Our discussion in the previous chapter of how the U.S. Navy/Marine Corps used human factor analysis and classification sytems to not only identify that violations were a problem, but also to develop an intervention strategy and then track its effectiveness, is a good example. But diagnosticity does not necessarily stop there. For a framework to be diagnostic, it must also be able to detect differences and unique patterns of errors within and among accident databases. For instance, one would expect that while there is much in common between military and civilian aviation, differences also exist, particularly when it comes to the types of operations and the way in which accidents occur. Likewise, one might expect that different types of accidents would yield unique patterns of error. For instance, one of the most inexplicable ways to crash an aircraft is to fly a fully functioning plane into the ground. These so-called controlled flight into terrain (CFIT) accidents continue to afflict both civilian and military aviation. In fact, the U.S. Navy/Marine Corps alone lost an average of ten aircraft per year to CFIT between 1983 and 1995 (Shappell and Wiegmann, 1995, 1997b). Likewise, between 1990 and 1999, 25 percent of all fatal airline accidents and 32 percent of worldwide airline fatalities (2,111 lives lost) have been attributed to CFIT (Boeing, 2000). In fact, since 1990, no other type of accident has taken more lives within military or civilian aviation. With this in mind, we examined 144 U.S. Navy/Marine Corps Class A accidents using an early version of human factor analysis and classification sytems (the Taxonomy of Unsafe Operations; Shappell and Wiegmann, 1997b). Like others working in the area, we found that many of the Naval CFIT accidents were associated with adverse physiological (e.g., spatial disorientation) and mental states (e.g., fatigue and the loss of situational awareness). In fact, to the extent that any human factors can be considered characteristic of a particular type of accident, it would be these two with CFIT. Even more interesting however, were the differences observed in the pattern of errors associated with CFIT that occurred during the day and those that occurred at night or in the weather. As it turns out, nearly half of all CFIT accidents occur in broad daylight during VMC – a significant finding in, and of itself. After all, it had been generally felt that most, if not all, CFIT accidents occurred during the night or when visual conditions were otherwise limited, such as during IMC. Naturally then, we examined whether any differences existed in the pattern of human error associated with CFIT within these two distinctly different environments. To no ones great surprise, there were. For instance, it is well known that when visual cues are limited, aircrew coordination, both internal and external to the cockpit, is even more critical than usual. Predictably, our analyses revealed that the incidence of cockpit resource management failures associated with CFIT was significantly higher during visually impoverished conditions. With the lack of visual cues, the proportion of adverse physiological and mental states that occurred at night or in instrument meteorological conditions were also more prevalent than what was observed during daytime VMC. While these findings seemed to make sense, a much more important question remained, "why would a pilot fly a perfectly good aircraft into the ground in broad daylight?" After all, to the surprise of many within the Navy hierarchy, roughly half of all CFIT occurs in daytime VMC. Again, our new error framework provided some clues as to how this could happen. It seems that many daytime CFIT accidents involved some sort of violation of the rules or regulations. What's more, these violations were often the seminal cause (if there is such a thing) in the tragic chain of events that followed regardless of whether they involved personal readiness (e.g., self-medicating or simply violating crew rest requirements) or unsafe act violations. This latter finding was particularly important to Naval leadership since many of the interventions that had been proposed to prevent CFIT involved terrain avoidance and ground proximity warning systems (GPWS). While such technology would obviously pay dividends in preventing CFIT accidents that occur during the night or in the weather, these measures would presumably be of little help during daytime VMC – particularly, if aircrew were willing to violate established safety practices. In fact, it could be argued that the introduction of a reliable GPWS or other terrain avoidance systems might actually increase, rather than decrease, the likelihood that aircrew would push altitude limits in an attempt to get an edge in training or combat. Certainly, it is no stretch to envision a military pilot using an enhanced GPWS to fly even closer to the ground in the belief that the system would bail him out if a collision was imminent. But, CFIT is not unique to military aviation. As we mentioned above, civil aviation, in particular general aviation, has been plagued by the same problem for years. For that reason, we have recently analyzed nine years of GA accidents (1990-98) using human factor analysis and classification sytems in an effort to better understand the cause of CFIT in civilian operations (Shappell and Wiegmann, 2003). Our analysis included over 14,000 GA accidents, of which roughly 10 percent (1,407) were classified as CFIT by our pilot-raters. Consistent with what we saw within the U.S. Navy/Marine Corps, almost one-third of all CFIT accidents were associated with violations of the rules – nearly three times more than what was seen with non-CFIT accidents (Table 6.5). Likewise, personal readiness failures, arguably another type of violation that occurs external to the cockpit, were over four times more likely during CFIT. As expected, adverse mental states and perceptual errors14 were also more prevalent during CFIT than non-CFIT accidents.Given our findings from the U.S. Navy/Marine Corps study, it seemed reasonable to explore whether similar differences existed between GA CFIT accidents occurring in broad daylight and those that occurred in visually impoverished conditions. Like the military data, GA CFIT accidents were evenly split between those that occurred in clear daytime conditions and those that occurred at night, or in IMC. Those that occurred during visually impoverished conditions were often associated with adverse physiological states, physical/mental limitations, and poor cockpit resource management (Table 6.6). Furthermore, CFIT accidents were six times more likely to involve a violation of the rules if they occurred in visually impoverished conditions. Indeed, it is not hard to envision a crew that fails to obtain a weather update prior to takeoff (crew resource management) and consequently encounters adverse weather enroute. Then, after choosing to continue into instrument meteorological conditions while visual flight rules only (violation), they end up spatially disoriented (adverse physiological state) and collide with the terrain. As easy as it may be to rationalize how GA pilots could fly their aircraft into the ground at night or in IMC, trying to comprehend why a pilot would collide with terrain in clear daytime conditions is considerably more perplexing. Unlike what we have seen within the Naval aviation database, it does not seem to be simply a function of an overconfident pilot pushing the envelope to gain an advantage over an adversary or target. No, violations are considerably less prevalent among GA CFIT accidents that occur in clear daytime conditions. The only variable that was more prevalent was skill-based errors, suggesting that CFIT in daytime VMC may be the result of inattention, a breakdown in visual scan, distraction, or simply stick-and-rudder skills. The fact that one half of all GA CFIT occur in these seemingly benign conditions suggest that further work is required to fully understand this type of accident. In a sense then, human factor analysis and classification sytems has provided us a unique look into the causes of CFIT and has illuminated (diagnosed) potential areas in need of intervention. However, often lost in the discussion, is the benefit of a common error framework when it comes to identifying trends between quantifiably different databases as unique as those within military and civilian aviation sectors. In fact, human factor analysis and classification sytems has now been implemented within all four branches of the U.S. military, as well as both commercial and general aviation. As a result, we can now compare error trends among these different organizations. But, will human factor analysis and classification sytems be diagnostic enough to detect any differences that would be expected to exist and even some that we had not anticipated? Let us take a look at a couple of error categories to find out. The percentage of U.S. military and civil aviation accidents (1990-98) associated with at least one perceptual error is presented in Figure 6.7. As expected, a larger percentage of military accidents involve perceptual errors than do civil aviation accidents. This is not surprising given that military pilots typically engage in more dynamic and aggressive flight, resulting in unusual attitudes that often wreak havoc with one's vestibular system causing spatial disorientation and perceptual errors. Even military helicopter pilots periodically perform low-level flights at relatively high speeds at night while donning night vision goggles, all of which increase the opportunity for perceptual errors. Such findings have implications for the development of intervention strategies, as well as for gauging the amount of time and resources that should be invested in addressing perceptual errors in aviation. Obviously for military aviation, this issue will require a great deal more attention since preventing accidents due to perceptual errors could substantially improve safety. Within civil aviation, however, considerably fewer resources should likely be invested, given that the problem of perceptual errors is relatively small. Albeit, preventing all types of accidents is important, but when resources are limited and there are bigger problems to address, one must be judicious, ensuring that the problems having the biggest impact on safety are given priority. But what is the "big" problem? Well, many have suggested that it is decision-making or even CRM. Indeed, as we mentioned in the previous chapter, a great deal of time and money has already been invested in developing programs to address these issues, based on the assumption that these are the major causes of accidents. However, the results from our human factor analysis and classification sytems analysis actually suggest that rather than decision-making or CRM, skill-based errors are associated with the largest percentage of aviation accidents – particularly, but not exclusively, within the civil aviation sector. As illustrated in Figure 6.8, more than half of all civil aviation accidents are associated with skill-based errors. The observation that the largest percentage was within general aviation is not surprising given the amount of flight time, training, and the sophistication of the aircraft within the GA community relative to the other aviation domains. While it can certainly be argued that there are exceptions, most would agree that the average GA pilot does not receive the same degree of recurrent training and annual flight hours that the typical commercial or military pilot does. Likewise, we would all agree that most GA aircraft are not nearly as sophisticated as an F-14 Tomcat or Boeing 777. Given the nature of non-scheduled air carrier operations and the relative experience of the aircrew, it was somewhat expected that the next highest percentage of accidents associated with skill-based errors was within the category of on-demand, non-scheduled air carriers. This is not to say that these so-called "air-taxi" pilots are unskilled or inexperienced, just that relative to their counterparts within the military and major air-carriers, they generally have less training and experience. In many instances, the aircraft they fly are less sophisticated as well. What is perhaps more interesting, and even somewhat surprising, is that nearly 60 percent of all accidents involving scheduled air carriers were associated with at least one skill-based error. Apparently, even the most highly trained, experienced, and skilled pilots continue to have difficulty with such issues as managing their attention and memory resources as well as handling the workload required to fly modern commercial aircraft. Even more surprising is the finding that commercial pilots were nearly as susceptible to skill-based errors as military pilots flying tactical fighter aircraft were. While we are only now exploring this issue, there is clearly much to be learned from findings such as these. In wrapping up this section, we want to reiterate that there really is no good measure of a framework's diagnosticity. That being said, we have made an earnest attempt to illustrate how it might be inferred based on research findings like the ones presented here. Indeed, others may have a different approach or method, but they will have to write their own book... Usability The acceptance of an error analysis approach is often determined by how easy it is to use. In reality, it matters very little whether a system is reliable, comprehensive, or diagnostic if investigators and analysts never use it. Therefore, the usability of a framework, or the ease with which it can be turned into a practical methodology within an operational setting, cannot be ignored (Hollnagel, 1998). Since its inception, the acceptability of human factor analysis and classification sytems and its predecessor frameworks has been repeatedly assessed and improved, based on feedback from those attending our training sessions, as well as from operators who have been using human factor analysis and classification sytems in the field. Over the years, input has come from pilots, flight surgeons, aviation safety officers, and other safety personnel within both military and non-military organizations from around the world. Some changes that we have made to improve the acceptability and usability of human factor analysis and classification sytems have included the rephrasing of technical or psychological terminology (e.g., slips, lapses and mistakes), to create terms that aviators would better understand (e.g. skill-based and decision errors). Other changes in the nomenclature were also made. For example, our use of the term taxonomy often drew blank and/or quizzical stares, as our students often thought we were going to teach them how to stuff animals. By changing the name of our error framework from the Taxonomy of Unsafe Operations to the Human Factors Analysis and Classification System or HFACS, we not only made the system more palatable to potential users, but we were no longer confused with taxidermists! The clearest evidence of HFACS' usability however, is that large organizations like the U.S. Navy/Marine Corps and the U.S. Army have adopted human factor analysis and classification sytems as an accident investigation and data analysis tool. In addition, human factor analysis and classification sytems is currently being utilized within other organizations such as the federal aviation administration and national aeronautics and space administration as a supplement to their preexisting systems (Ford et al., 1999). Perhaps the main reason why these organizations have embraced human factor analysis and classification sytems is that the system is highly malleable and adaptable. In other words, human factor analysis and classification sytems can be easily modified to accommodate the particular needs of an organization. For example, the role that environmental factors play in the etiology of human error was not incorporated into the version of human factor analysis and classification sytems developed for the U.S. Navy/Marine Corps, primarily because factors such as weather or terrain are uncontrollable and hence not considered remedial causes of an accident by the U.S. Naval Safety Center. Still, in some contexts, environmental factors may be controllable, such as in a maintenance facility or an air traffic control center. Therefore, these organizations may view the role of environmental factors differently or weigh them more heavily. In addition, some have argued that working conditions and equipment design problems are not only an organizational resource management issue, but also a precondition of unsafe acts. Therefore, such factors need to be addressed separately during the analysis of an accident. Given all of this, the version of human factor analysis and classification sytems that we have presented in this book includes these additional environmental and technological categories for those organizations that may find them useful. Additional evidence of HFACS' adaptability comes from organizations that have taken the liberty of modifying the framework themselves, successfully tailoring it to suit their own needs. For example, the Canadian Forces has developed what they call CF-HFACS, which is presented in Figure 6.9. As a minor modification to its depiction, they have chosen to place the unsafe acts tier at the top and organizational influences at the bottom in order to represent the typical analysis process that often starts with the identification of an unsafe act. They have also chosen to remove the negative wording of the categories, perhaps to reduce the appearance of apportioning blame to those individuals whose actions were causal to the accident. Less cosmetic, is the addition of categories at both the unsafe acts and preconditions level. As can be seen, they have chosen to separate technique errors from skill-based errors, which are now labeled "attention/memory." Also added is a "knowledge information" category, which is a type of error that occurs when knowledge or the information available to complete a task is incorrect, impractical or absent. Other changes at the pre-conditions level include the addition of categories such as "qualifications" and "training." Although this information is already included in HFACS, creating separate "bins" for such factors highlights their importance placed on them by this organization. Other variations of the original human factor analysis and classification sytems framework exist. For example, there is a version of human factor analysis and classification sytems for air traffic control (HFACS-ATC), aircraft maintenance (HFACS-ME), and even one for medicine (HFACS-MD). Descriptions of all of these frameworks can be found in the open literature (Pounds et al., 2000; Schmidt et al., 1998; Wiegmann et al., in press). Although on the surface, each of these derivatives may appear different from the parent HFACS, they all have human factor analysis and classification sytems as their core. Changes that were made to specific category descriptors were done primarily to accommodate the idiosyncrasies of their target audience. For example, since air traffic controllers tend to bristle at the term "violations," HFACS-ATC has replaced the term with the word "contraventions." Although anyone with a thesaurus can tell you, "contraventions" means the same thing as violations, the term is apparently much more palatable to its users. Perhaps this is because no one actually knows what contravention really means! However, as Shakespeare once wrote, "a rose by any other name is still a rose." But seriously, we welcome and even encourage others to modify human factor analysis and classification sytems to meet their needs. Safety is much more important to us than protecting our egos. Still, we hope that all who use or modify the system will let us know, because we often learn from it and actually appreciate it when others do our work for us! Conclusion First of all, let us congratulate you for making it through this entire chapter. Hopefully you did not get so bored that you jumped right to this summary section. But as we warned you from the outset, this chapter is by far the most academic in the book. Our purpose in writing it was to provide an overview of the criteria that can be used for evaluating error frameworks including HFACS. Unfortunately, some may view this chapter as simply self-serving or a way of touting human factor analysis and classification sytems as being the best. Please, believe us, this was not our intention. Rather, we have taken the opportunity to use our firsthand knowledge of HFACS' development to help describe these criteria and illustrate the evaluation process. We also wanted to provide the requisite information that potential users of human factor analysis and classification sytems will need to objectively evaluate human factor analysis and classification sytems for themselves. Admittedly, we are not the first to propose evaluation criteria or use them in the development of an error analysis system. However, all too often we have witnessed people haphazardly put together frameworks and then market them as being the "state of the art" without any validation whatsoever. As a result, the users of such systems are left to their own intuition or "gut feelings" when deciding on the adequacy of a given error framework. Indeed, often times such systems are accepted as valid simply based upon the reputation of the individual or organization who developed it. Hopefully, our readers will now be able to get beyond gut feelings when evaluating an error framework by asking its developer very pointed questions, such as: "What is the inter-rater reliability of your system?", "Will the system accommodate all the human factors information in my database?", "Will the framework allow me to identify unique trends in my error data so that interventions can be targeted at key problems and their efficacy subsequently evaluated?", "Can your framework be understood and used universally by our investigators and safety personnel?", and "Can it be tailored to meet my organization's specific needs?". These are but a few questions that an educated consumer should ask when shopping around for an error framework. If its developer cannot answer these or other related questions to your satisfaction, then you should move on until you find an approach that meets your expectations. Speaking of moving on, however, that is what we plan to do now. It is time to catch up to the others who skipped this chapter and went straight to chapter 7. But What About...? We would love to say that everyone who has ever heard or read about human factor analysis and classification sytems has immediately become an unwavering believer in the system. But, like any framework or method of accident investigation, human factor analysis and classification sytems too has its critics. There have been several occasions during our workshops or conference presentations, or while reviewing one of our manuscripts, that others in the field have voiced their questions or concerns. Some of these critiques have been due to differences in basic philosophical views concerning the fundamental nature of human error and accident causation. While such debates have helped refine our thinking, they have often led to an unavoidable impasse, ultimately boiling down to one's belief as to whether or not human error really even exists at all. Others, however, have offered very insightful critiques that have led to significant improvements in the present version of HFACS. We have always tried not to take any of these comments and critiques personally. Yet, as any proud parent gets upset when others criticize their children, we too have had our moments. Nevertheless, if any framework of accident investigation is to live beyond the personalities of those who created it, it must be flexible yet robust enough to withstand concerns and questions presented by critics. Admittedly, human factor analysis and classification sytems is not perfect and our academic training and consciences compel us to be "honest brokers" in our presentation of the system. After all, no one chooses a career in safety if they want to make a lot of money. In fact, human factor analysis and classification sytems has been published in the open scientific literature and can be used by anyone, free of charge. We do not make any money from "selling it." As anyone who has ever published a textbook knows, you do not get much in royalties either. Albeit, we might earn enough to buy ourselves a cup of coffee, but we're certainly not going to be able to fund our children's college education from the sale of this book! So, in all fairness to those who have offered their critiques of HFACS, we have decided to dedicate this last chapter to the systematic presentation of the comments and criticisms we have encountered over the years. After all, if one person has raised the question, there may be others with similar concerns. We have named this chapter "But What About...?" because it represents a type of "frequently asked questions" or FAQ document that might be found in an human factor analysis and classification sytems user's guide. We have chosen to state these questions and concerns directly, sometimes phrasing them in the rather blunt fashion that we originally received them, and then to briefly address each one in turn. Our hope is that in reading this chapter you too will have several of your questions or concerns answered and that ultimately you will be better able to determine the potential utility of human factor analysis and classification sytems for your organization. Isn't there more to human error than just labels? Some critics of human factor analysis and classification sytems have argued that the framework simply relies on labels such as "decision errors" or "crew resource mismanagement" to give investigators access to the psychological life beneath an error. What is needed is more guidance beyond the cursory analysis presented here, if one is to truly understand the genesis of human error. From an academic perspective, this is indeed a legitimate concern. However, we have intentionally restrained ourselves from writing an academic treatise on human error, as several titles already exist including James Reason's (1990) book "Human Error," upon which human factor analysis and classification sytems is based. Rather, we have chosen to develop a framework and to write a book that is aimed at practitioners. Our intent was not to describe all the theoretical underpinnings associated with each of the categories within HFACS. The end result would have taken on encyclopedic form. Rather, our goal was to provide a much-needed, rudimentary tool for applying a systematic human error approach to accident investigation. As such, human factor analysis and classification sytems provides a "down-to-earth," practical framework for helping investigators identify the need to further investigate the possible error forms that potentially contributed to an accident. Information for conducting these additional analyses can be readily found in the literature or by consulting experts in a particular area. Frameworks or checklists only limit the scope of the investigation. While we appreciate the possible drawbacks of using checklists during accident investigations, we do not view human factor analysis and classification sytems as a framework that confines investigators, or encourages them to consider only a limited number of possibilities. Rather, human factor analysis and classification sytems suggests multiple avenues of inquiry when analyzing human error and encourages investigators to go beyond what a pilot may have done wrong to discovering why the error occurred. Indeed, our discussions with those who actually use human factor analysis and classification sytems in the field indicate that human factor analysis and classification sytems actually helps them expand their investigation beyond just the facts about an accident. As we have stated previously in other chapters, most investigators are not formally trained in human factors and therefore they have often stopped at the level of "pilot error" when investigating accidents. Consequently, without frameworks like HFACS, the majority of investigations have historically left large gaps in the information collected (such as information about preconditions, and supervisory and organizational issues). human factor analysis and classification sytems has therefore actually increased both the quantity and quality of the human factors data collected during aviation accident investigations. What is really missing in human factor analysis and classification sytems is an emphasis on error-producing factors related to equipment design. This is by far the most common criticism that we have received about human factor analysis and classification sytems over the years. Indeed, our initial focus was, and continues to be, on those responsible for the design, specification, and procurement of equipment rather than on the equipment itself, since only humans can effect change. For example, if specifications for equipment were poor or if the organization chose to go a cheaper route and purchased sub-standard equipment, these causal factors would be examples of organizational failures, specifically those involving "resource management" using HFACS. In essence then, our focus is on the human aspects, not necessarily engineering or "knobs and dials" approaches. Nonetheless, we have listened to our critics and expanded human factor analysis and classification sytems in this book to include error producing factors such as equipment design and other environmental factors to address some of the issues that admittedly do not fall completely under the organizational umbrella. However, we remain convinced that unless there is some form of human responsibility attached to such problems, they will never get changed within the system. A poor computer interface cannot change itself no matter how many times you cite it as a cause to an accident. human factor analysis and classification sytems oversimplifies the translation from "finding a hole" to "plugging it up" and these are definitely not the same thing. We agree that there is much more to accident prevention and mitigation than HFACS. In fact, human factor analysis and classification sytems is only the first step in the risk management process– i.e., the identification of the problem. Too often, organizations embark on intervention strategies based on anecdotes and gut feelings rather than objective data that indicates what the problem really is. As we have often pointed out, many organizations subscribe to the "mishap-of-the-month" club. That is, if a particularly noteworthy accident involves cockpit resource management failures, all of the sudden millions of dollars are spent on cockpit resource management training. Then next month, it may be an accident involving violations of the rules. So, they move the dollars to procedural enforcement strategies and accountability only to find out that next month they have a controlled flight into terrain accident and the dollars are once again moved to prevent or mitigate causal factors associated with these types of accidents. In essence, these organizations are left doing little more than chasing their tails. However, the human factor analysis and classification sytems framework has proven useful to the U.S. Navy/Marine Corps and other organizations in providing data-driven efforts based on a series of accidents/incidents, rather than isolated cases that may have little to do with the more pervasive problems within the system. The approach also provides a way of monitoring the effectiveness of interventions so that they can be revamped or reinforced to improve safety. The analysis of supervisory failures in human factor analysis and classification sytems has a large problem of hindsight. After the fact, it is always easy to identify where supervisors or management should have paid more attention. Such a statement implies that the criterion for determining whether a factor was causal to an accident is whether it was identifiable prior to its occurrence. This seems a little silly and would likely result in a great deal of human and engineering data to be dismissed prematurely during an accident investigation. Such a statement also appears to be based more on a concern about the issue of blame rather than accident prevention (we'll address the blame issue later in this chapter). Furthermore, it is no big secret, and therefore no major revelation, that all accident investigations involve "hindsight" to some extent. Yet, to simply ignore data because it was discovered "after the fact" would be nothing short of malpractice. After all, much of the learning process involves changing behavior once an error has occurred. From a safety perspective, it is not criminal to make an error, but is inexcusable if you don't learn from it. We designed human factor analysis and classification sytems as a tool to help investigators ask the right questions, so that supervisory and organizational issues are at least considered. human factor analysis and classification sytems also provides a tool to examine and track trends in human error and accident data. As a result, intervention strategies can be developed that are not based on hindsight, anecdotes or "shoot from the hip" ideas, but rather on specified needs and objective data. Furthermore, researchers at the FAA's Civil Aerospace Medical Institute recently reported on a safety survey that they developed based on human factor analysis and classification sytems (Bailey et al., 2000). The survey was used with regional air carriers in Alaska to identify those human error areas that discriminated between carriers that had experienced an accident in the last three years and those that did not. This initial effort to develop a proactive survey to identify hazardous supervisory issues before they cause accidents, has shown considerable promise. It is not difficult to develop a human factors evaluation framework that utilizes under-specified labels and subsequently achieve high inter-rater reliabilities. From our experience, this comment does not appear to be true. In fact, we have found quite the opposite regarding "under-specified labels" and inter-rater reliabilities. As a matter of fact, until we refined the causal categories associated with the "Taxonomy of Unsafe Operations" (as the human factor analysis and classification sytems framework was originally called), and provided crisp definitions that were mutually exclusive, our inter-rater reliabilities were not nearly as high as those we enjoy today. This is exactly where many error analysis frameworks fall short. Indeed, other authors have presented similar systems, professing them to be user friendly and reliable without any appreciable data to back it up. This is perhaps one of HFACS' strongest suits. That is, human factor analysis and classification sytems is based on several years of extensive testing using thousands of military and civil aviation accidents and consequently there is considerable empirical data supporting its utility. When it comes to evaluating a framework like HFACS, it may be more meaningful to compare it to other similar tools. Another good point, however, we really do not want to engage in any downplaying of other frameworks directly. Rather, our aim was to put some evaluation criteria on the table and let the reader do his or her own comparisons. However, the fact that several organizations around the world have compared human factor analysis and classification sytems to their own systems and have chosen to adopt human factor analysis and classification sytems is clear evidence of its utility. Nevertheless, there are often redeeming qualities associated with almost all error frameworks. As a result, some organizations like the federal aviation administration and EUROCONTROL have chosen to merge human factor analysis and classification sytems with other systems (like the Human Error Reliability Analysis (HERA)), to capitalize on the benefits of both systems when investigating and analyzing air traffic control operational errors. Still others have chosen to keep their existing systems, and have employed human factor analysis and classification sytems as an adjunct to their pre-existing systems. Regardless of how it is employed, our hope is that those who are in need of a better human error system will find human factor analysis and classification sytems helpful in improving safety within their organizations. If I adopt HFACS, won't I lose all the accident data that I already collected with my old system? Since the human factor analysis and classification sytems framework is causal-based (e.g., skill-based error) rather than descriptive-based (e.g., failed to lower landing gear), it is "transparent" to the original causal factor identification in most databases. That is, application of the human factor analysis and classification sytems framework does not require a reinvestigation of the accident or changes to be made in the original accident causal factors; they can simply be regrouped into more manageable categories. This means that existing accidents can be classified post hoc, allowing for analysis of trends now, rather than years down the road, when sufficient numbers of accidents are investigated using a new framework. Implicit in the principle of a transparent framework is that the integrity of the existing database is preserved. After all, existing database formats have proven particularly useful in the management and reduction of mechanical failures. To simply replace existing database structures with new human factors databases would be like "throwing the baby out with the bath water." By superimposing the human factor analysis and classification sytems framework onto existing databases, you can preserve the benefits realized by existing frameworks while adding the obvious advantages of a human factors database. This is exactly what organizations like the U.S. Navy/Marine Corps have done. Numerous theories of accident causation have come and gone over the years. Therefore, since human factor analysis and classification sytems is tied to Reason's theory of latent and active failures, it is as fad-based as the analysis tools the authors wanted to get away from. James Reason's 1990 book "Human Error" started a revolution of sorts that breathed new life into accident and incident investigation, both in the fields of nuclear power and aviation. human factor analysis and classification sytems capitalizes on this work by providing a framework for applying Reason's ideas and theory. The fact that the book "Human Error" was first published over a decade ago and continues to be one of the most widely cited and respected works in this area speaks for itself. Isn't human factor analysis and classification sytems just the same old "blame-and-train" game? We chose to end our book addressing this question because it reflects what we believe is a growing chasm between two major schools of thought within the field of human factors. Furthermore, where one falls on either side of this chasm often determines whether human factor analysis and classification sytems is embraced or discarded. Therefore, we will spend a little more time elaborating upon this issue and then discuss what we think it really means for the utilization of HFACS. Consider the following scenarios: An automobile driver gets into an accident after running a stoplight and blames it on the yellow caution light, which varies in duration across signals in the community. A shopper hurts his back while loading groceries into the trunk of a car and blames it on the bags, which were designed to hold too many groceries. A student over-sleeps and misses an exam then blames it on his alarm clock that failed to go off at the pre-programmed time. A citizen fails to pay income taxes and blames it on the forms, which are too complex to understand. Such anecdotes illustrate the dilemma we often face as human factors professionals in attributing causes to accidents and errors in the workplace. But how do we (or should we) view operator error in light of this dilemma? As discussed in Chapter 2, some theories of human error focus on expanding models of human information processing to help explain the types of errors commonly made by operators in different contexts. These theories have helped raise awareness of human limitations, in terms of attention and cognitive processing abilities, and have encouraged a more thorough analysis of human error during accident investigations. Although these approaches have contributed greatly to our understanding of human error, an unwanted side-effect can emerge, which is the tendency to assign blame to operators directly involved in the unsafe act, particularly by those who have vested interests in the outcome of such analyses. This can lead to a "blame-and-train" mentality. Unfortunately, however, such adverse consequences have been generalized to this entire error analysis approach, so that some refer to all such models as "bad apple" theories of human performance. Another common approach to human error analysis is the systems or ergonomics approach. This is an "alternate" view of human error that considers the interaction between human and technical factors when exploring the causes of errors and accidents. Indeed, most students of human factors are likely taught this approach. There is no doubt that this approach has helped highlight the affects that system design can have on operator performance. It has also led to a greater appreciation of design-induced errors and an understanding of how to create systems that are more error-tolerant. However, as mentioned in Chapter 2, this approach often fails to consider intricate aspects of the human side of the error, relative to the information processing or cognitive approaches. Furthermore, it has led some in the field of human factors to conclude that human error does not really exist. Rather, when an accident occurs, it is the "system" that has erred, not the human. Although many human factors professionals appreciate the strengths and weakness of these various approaches, we must ask whether some in the field have taken the concept of "designed-induced" error too far? Indeed, there does appear to be some who hold the opinion that operators are simply "victims" of inadequate design – a type of "the designer made me do it" mentality. Even more disturbing, however, is that such extremism appears to be becoming increasingly more pervasive. But, is this perspective really correct? Should we never attribute at least some responsibility to operators who err? Consider for example an airplane that crashes while taking off down a closed runway and the surviving aircrew blame it on the poor runway signage and taxi lights. But then how should we interpret that fact that aircrew of 10 other aircraft successfully navigated the same route just before the accident occurred and were able to take-off on the correct runway? A more personal example occurred to a colleague who was helping review papers for a professional human factors conference. An author who was late submitting a proposal for the conference contacted him, blaming the tardiness of his paper on the instructions, which were too confusing. When the reviewer replied that 20 other presenters had interpreted the instructions correctly and had mailed their proposals in on time, the author stated that the reviewer was "blaming the victim" and that his "bad attitude was surprising for someone in human factors." Was it a really a bad attitude, or was the reviewer too just a "victim" of the constraints imposed on him by the organization for which he was reviewing proposals? Seriously, how should we view all those internal performance-shaping factors outlined in HFACS, such as operator expectancies, biases, complacency, over-confidence, fatigue, technique, time-management, aptitude, attitudes, and personality? Do these not count? When an error does occur, can we say only that the engineers failed to design a system that was adaptable to all levels of size, shape, knowledge, skill, and experience of operators? Is taking the time to read instructions, ask questions, plan activities, communicate intentions, and coordinate actions no longer the responsibility of operators? If such things are considered in analyzing operator error, is one practicing "bad" human factors? Obviously, we don't think so, but there are some who would disagree. They argue that we are using human factor analysis and classification sytems to push the pendulum too far back toward the "blame-and-train" direction. However, we believe that we are simply pushing it back toward middle ground, where both equipment design and operator characteristics are considered together. But then again, maybe we are just a couple of bad apples who are in need of some good company!