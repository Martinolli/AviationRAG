Title: Managing The Risks Of Organizational Accidents – Chapters 1 – 2 – 3 Author(s): James Reason Category: Management, Risks Tags: Risks, Organization, Accidents, Aircraft Preface This book is not meant for an academic readership, although I hope that academics and students might read it. It is aimed at 'real people' and especially those whose daily business is to think about and manage or regulate the risks of hazardous technologies. My imagined reader is someone with a technical background rather than one in human factors. To this end, I have tried- not always successfully- to keep the writing as jargon-free as possible. The book is not targeted at anyone's domain. Rather, it tries to identify general principles and tools that are applicable to all organizations facing dangers of one sort or another. This includes banks and insurance companies just as much as nuclear power plants, oil exploration and production, chemical process plants, and air, sea, and rail transport. The more one moves towards the upper reaches of such systems, the more similar their organizational processes and weaknesses become. In a book of this type, the 'big bang' examples inevitably tend to predominate, but although I have used case study examples to illustrate points, this is not intended to be yet another catalog of accident case studies. My emphasis is upon principles and practicalities- the two must work hand-in-hand. However, the real test is whether or not these ideas can eventually be translated into some improvement in the resistance of complex, well-defended systems to rare, but usually catastrophic, 'organizational accidents'. James Reason Chapter 1 Hazards, Defenses, and Losses Individual and Organizational Accidents There are two kinds of accidents: those that happen to individuals and those that happen to organizations. Individual accidents are by far the larger in number, but they are not the main concern of this book. Our focus will be on organizational accidents. These are the comparatively rare but often catastrophic events that occur within complex modern technologies such as nuclear power plants, commercial aviation, the petrochemical industry, chemical process plants, marine, and rail transport, banks, and stadiums. Organizational accidents have multiple causes involving many people operating at different levels of their respective companies. By contrast, individual accidents are ones in which a specific person or group is often both the agent and the victim of the accident.1 The consequences to the people concerned may be great, but their spread is limited. Organizational accidents, on the other hand, can have devastating effects on uninvolved populations, assets, and the environment. Whereas the nature (though not necessarily the frequency) of individual accidents has remained relatively unchanged over the years, organizational accidents are a product of recent times or, more specifically, a product of technological innovations that have radically altered the relationship between systems and their human elements. Finding the Right Level of Explanation Organizational accidents are difficult events to understand and control. They occur very rarely and are hard to predict or foresee. To the people on the spot, they happen 'out of the blue.' Difficult though they may be to model, we have to struggle to find some way of understanding the development of organizational accidents if we are to achieve any further gains in limiting their occurrence. Quite apart from the human costs in deaths and injuries, there are very few commercial organizations that can survive the fallout from a major accident of this kind. It has been said that nothing in logic is accidental. But does the reverse hold true? Is there nothing logical about accidents? Are there no underlying principles of accident causation? This book is written in the belief that such principles do exist. Organizational accidents may be truly accidental in the way in which the various contributing factors combine to cause the bad outcome, but there is nothing accidental about the existence of these precursors, nor in the conditions that created them. The difficulty, however, lies in finding the appropriate level of description. If we consider only the surface of the kind of information that is reported in press accounts- organizational accidents are dauntingly diverse. They involve a variety of systems in widely differing locations. Each accident has its own very individual pattern of cause and effect. Apart from the fact that they are all bad news, this level of description seems to defy generalization and implies that we clearly need to investigate more deeply into some common underlying structure and process to find the right level of explanation. On the other hand, it can be claimed that all organizational accidents involve the unplanned release of destructive agencies such as mass, energy, chemicals, and the like. This is indeed a generalization, but it does not take us very far. However, like gunners, we have bracketed the target. The appropriate level of understanding has to lie somewhere between the highly idiosyncratic superficial details and the vagueness of this overly broad definition. The aim is to find ideas that could be applied equally well to a wide range of low-risk, high-hazard domains. The basic thesis of this book is that the framework illustrated in Figure 1.1 will serve this purpose well. Figure 1.1 shows the relationship between the three elements that make up the title of this chapter: hazards, defenses, and losses. All organizational accidents entail the breaching of the barriers and safeguards that separate damaging and injurious hazards from vulnerable people or assets, which are collectively termed 'losses.' This is in sharp contrast to individual accidents where such defenses are often either inadequate or lacking. Figure 1.1 directs our attention to the central question in all accident investigations: By what means are the defenses breached? Three sets of factors are likely to be implicated- human, technical, and organizational. All three will be governed by two processes common to all technological organizations: production and protection. Production and Protection: Two Universals All technological organizations produce something-manufactured goods, the transportation of people, financial or other services, the extraction of risk achievement worth materials, and the like. However, to the extent that productive operations expose people and assets to danger, all organizations (and the larger systems within which they are embedded) require various forms of protection to intervene between the local hazards and their possible victims and lost assets. While the productive aspects of an organization are fairly well understood and their associated processes relatively transparent, the protective functions are both more varied and more subtle. Figure 1.2 introduces some of the issues involved in the complex relationship between production and protection. In an ideal world, the level of protection should match the hazards of the productive operations- the parity zone.2 The more extensive the productive operations, the greater the hazard exposure, and so is the need for corresponding protection. However, there are different types of production, hence different organizations in terms of the severity of their operational hazards. Thus, low-hazard ventures will require less protection per productive unit than high-hazard ventures. In other words, the former can operate in the region below the parity zone, whereas the latter must operate above it. This broad operating zone (the lightly shaded area in Figure 1.2) is bounded by two dangerous extremes. In the top left-hand corner lies the region in which the protection far exceeds the dangers posed by the productive hazards. Since protection consumes productive resources such as people, money, and materials, grossly overprotected, organizations would probably soon go out of business. At the other extreme, in the bottom right-hand corner, the available protection falls far short of that needed for productive safety, and organizations operating in this zone face a very high risk of suffering a catastrophic accident (which probably also means going out of business). These obviously dangerous zones are generally avoided, if only because they are unacceptable to both the regulators and the shareholders. Our main concern is with how organizations navigate the space bounded by these two extremes. Despite frequent protestations to the contrary, the partnership between production and protection is rarely equal, and one of these processes will predominate, depending on the local circumstances. Since production creates the resources that make protection possible, its needs will generally have priority throughout most of an organization's lifetime. This is partly because those who manage the organization possess productive rather than protective skills and partly because the information relating to production is direct, continuous, and readily understood. By contrast, successful protection is indicated by the absence of negative outcomes. The associated information is indirect and discontinuous. The measures involved are hard to interpret and often misleading. It is only after a bad accident or a frightening near miss that protection comes for a short period- uppermost in the minds of those who manage an organization. All rational managers accept the need for some degree of protection. Many are committed to the view that production and protection necessarily go hand-in-hand in the long term. Conflicts occur in the short term. Almost every day, line managers and supervisors have to choose whether or not to cut safety corners in order to meet deadlines or other operational demands. For the most part, such shortcuts have no bad effects and can become a habitual part of routine work practices. Unfortunately, this gradual reduction in the system's safety margins renders it increasingly vulnerable to particular combinations of accident-causing factors. Figure 1.3-the main purpose of which is to introduce the two important features of organizational life described below-plots the unhappy progress of one hypothetical organization through the pro-duction-protection space. The history starts towards the bottom left-hand corner of the space, where the organization begins production with a reasonable safety margin. (The organization's progress between events is indicated by the black dots.) As time passes, the safety margin is gradually diminished until a low-cost accident occurs. The event leads to an improvement in protection, but this is then traded off for productive advantage until another, more serious, accident occurs. Again, the level of protection is increased, but this is gradually eroded by an event-free period. The life history ends with a catastrophe. Trading off Added Protection for Improved Production Improvements in protection are often put in place during the period immediately following a bad event. Although the aim is to avoid a repetition of an accident, it is soon appreciated that the improved defenses confer productive advantages. Mine owners in the early nineteenth century, for example, quickly realized that the invention of the Davy lamp permitted coal to be extracted from areas previously considered too dangerous because of the presence of combustible gases. Ship owners soon discovered that marine radar allowed their merchant vessels to travel at greater speed through crowded or confined seaways. In short, protective gains are frequently converted into productive advantages, leaving the organization with the same inadequate protection that prevailed before the event or with something even worse. The incidence of mine explosions in- increased dramatically in the years following the introduction of the Davy lamp, and the history of marine accidents is littered with radar- to name but two of the many examples of accidents brought about by sacrificing protective benefits for productive gains. This process has been termed 'risk compensation' or 'risk homeostasis'.3 The Dangers of the 'Unrocked Boat '4 There is plentiful evidence to show that a lengthy period without a serious accident can lead to the steady erosion of protection as productive demands gain the upper hand in this already unequal relationship. It is easy to forget to fear things that rarely happen, particularly in the face of productive imperatives such as growth, profit, and market share. As a result, investment in more effective protection falls off and the care and maintenance necessary to preserve the integrity of existing defenses declines. Furthermore, productive growth is regarded as commercially essential in most organizations. Simply increasing production without the corresponding provision of new or extended defenses will also erode the available safety margins. The consequence of both processes neglecting existing defenses and failing to provide new ones is a much-increased risk of a catastrophic, and sometimes terminal, accident. We will return to the interplay between production and protection later, but for now, we need to focus on protecting the layers of defenses, barriers, and safeguards that are erected to withstand both natural and manmade hazards. The one sure fact about an accident is that the defenses must have been breached or bypassed. Identifying how these breakdowns can occur is the first step in understanding the processes common to all organizational accidents. Just as production can involve many different activities, so protection can be achieved in a variety of ways. In the remainder of this book, we will reserve the term 'protection' for the general goal of ensuring the safety of people and assets, and we will use the term 'defenses' to refer to the various means by which this goal can be achieved. At this point, it would be convenient to focus on the various ways by which defenses may be described or classified. The Nature and Variety of Defenses Defenses can be categorized both according to the various functions they serve and by the ways in which these functions are achieved. Although defensive functions are universal, their modes of application will vary between organizations, depending on their operating hazards. All defenses are designed to serve one or more of the following functions: • to create understanding and awareness of the local hazards • to give clear guidance on how to operate safely • to provide alarms and warnings when danger is imminent • to restore the system to a safe state in an off-normal situation • to interpose safety barriers between the hazards and the potential losses • to contain and eliminate the hazards should they escape this barrier • to provide the means of escape and rescue should hazard containment fail. Implicit in the ordering of this list is the idea of 'defenses-in-depth'- successive layers of protection, one behind the other, each guarding against the possible breakdown of the one in front. When understanding, awareness, and procedural guidance fail to keep potential victims away from hazards, alarms and warnings alert them to the imminent danger and direct the system controllers (or engineered safety features) to restore the system to a safe state. Should this not be achieved, physical barriers stand between potential losses and the hazards. Other defenses act to contain and eliminate the hazards. Should all of these prior defenses fail, then escape and rescue measures are brought into play. It is this multiplicity of overlapping and mutually supporting defenses that make complex technological systems, such as nuclear power plants and modern commercial aircraft, largely proof against single failures, either human or technical. The presence of sophisticated defenses in depth, more than any other factor, has changed the character of industrial accidents. In earlier technologies, there were, and to the extent that they continue to operate, still are relatively large numbers of individual accidents. In modern technologies, such as nuclear power generation and air transportation, there are very few individual accidents. Their greatest danger comes from rare, but often disastrous, organizational accidents involving causal contributions from many different people distributed widely both throughout the system and over time. The defensive functions outlined above are usually achieved through a mixture of 'hard' and 'soft' applications. 'Hard' defenses include such technical devices as automated engineered safety features, physical barriers, alarms and annunciators, interlocks, keys, personal protective equipment, non-destructive testing, designed-in structural weaknesses (for example, fuse pins on aircraft engine pylons), and improved system design. 'Soft' defenses, as the term implies, rely heavily upon a combination of paper and people: legislation, regulatory surveillance, rules and procedures, training, drills and briefings, administrative controls (for example, permit-to-work systems and shift handovers), licensing, certification, supervisory oversight and-most critically-front-line operators, particularly in highly automated control systems. In earlier technologies, human activities were primarily productive: people made or did things that led directly to commercial profit. However, the widespread availability of cheap computing power has brought about a dramatic change in the nature of human involvement in modern technologies. These changes are seen most starkly in nuclear power plants and' glass cockpit' commercial aircraft. Instead of being physically and directly involved in the business of production (and hence in immediate contact with the local hazards), power plant operators and pilots act as the planners, managers, maintainers, and supervisory controllers of largely automated systems.5 A crucial part of this latter role involves the defensive function of restoring the system to a safe state in the event of an emergency. Defenses-in-depth is a mixed blessing. One of their more unfortunate consequences is that they make systems more complex, and hence more opaque, to the people who manage and operate them. Human controllers have, in many such systems, become increasingly remote, both physically and intellectually, from the productive systems that they nominally control. This allows for the insidious build-up of latent conditions, which will be discussed later. The 'Swiss Cheese' Model of Defenses In an ideal world, all the defensive layers would be intact, allowing no penetration by possible accident trajectories, as shown on the left-hand side of Figure 1.4. In the real world, however, each layer has weaknesses and gaps of the kind revealed on the right-hand side of the figure. The precise nature of these 'holes' will be discussed in the next section; here, it is necessary to convey something of the dynamic nature of these various defenses in depth. Although Figure 1.4 shows the defensive layers and their associated 'holes' as being fixed and static, in reality, they are in constant flux. The 'Swiss cheese' metaphor is best represented by a moving picture, with each defensive layer coming in and out of the frame according to local conditions. Particular defenses can be removed deliberately during calibration, maintenance and testing, or as the result of errors and violations. Similarly, the holes within each layer could be seen as shifting around, coming and going, shrinking and expanding in response to operator actions and local demands. How are the 'holes' created? To answer this, we need to consider the distinction between active failures and latent conditions. Active Failures and Latent Conditions Since people design, manufacture, operate, maintain, and manage complex technological systems, it is hardly surprising that human decisions and actions are implicated in all organizational accidents. Human beings contribute to the breakdown of such systems in two ways. Most obviously, it is by errors and violations committed at the 'sharp end' of the system-by pilots, air traffic controllers, police officers, insurance brokers, financial traders, ships' crews, control room operators, maintenance personnel and the like. Such unsafe acts are likely to have a direct impact on the safety of the system and, because of the immediacy of their adverse effects, these acts are termed active failures. If these were individual accidents, the discovery of unsafe acts immediately prior to the bad outcome would probably be the end of the story. Indeed, it is only within the last 20 years or so that the identification of proximal active failures would not have closed the book on the investigation of a major accident. Limiting responsibility to erring front-line individuals suited both the investigators and the organizations concerned-to say nothing of the lawyers who continue to have problems with establishing the causal links between top-level decisions and specific events. Today, neither investigators nor responsible organizations are likely to end their search for the causes of an organizational accident with the mere identification of 'sharp-end' human failures. Such unsafe acts are now seen more as consequences than as principal causes? These developments and the events that shaped them will be discussed in Chapter 4. Although fallibility is an inescapable part of the human condition, it is now recognized that people working in complex systems make errors or violate procedures for reasons that generally go beyond the scope of individual psychology. These reasons are latent conditions. Latent conditions are to technological organizations what resident pathogens are to the human body. Like pathogens, latent conditions-such as poor design, gaps in supervision, undetected manufacturing defects or maintenance failures, unworkable procedures, clumsy automation, shortfalls in training, less than adequate tools and equipment-may be present for many years before they combine with local circumstances and active failures to penetrate the system's many layers of defenses. They arise from strategic and other top-level decisions made by governments, regulators, manufac- turers, designers and organizational managers. The impact of these decisions spreads throughout the organization, shaping a distinctive corporate culture (see Chapter 9) and creating error-producing fac- tors within the individual workplaces. Latent conditions are present in all systems. They are an inevitable part of organizational life. Nor are they necessarily the products of bad decisions, although they may well be. Resources, for example, are rarely distributed equally between an organization's various departments. The original decision on how to allocate them may have been based on sound commercial arguments, but all such inequities create quality, reliability or safety problems for someone somewhere in the system at some later point. No single group of senior man- agers can foresee all the future ramifications of their current decisions. A very important distinction between active failures and latent conditions thus rests on two largely organizational factors. The first has to do with the time taken to have an adverse impact. Active failures usually have immediate and relatively short lived effects whereas latent conditions can lie dormant for a time doing no particular harm until they interact with local circumstances to defeat the system's defenses. The second difference between them relates to the location within the organization of their human instigators. Active failures are committed by those at the human-system interface-the front-line or 'sharp-end' personnel. Latent conditions, on the other hand, are spawned in the upper echelons of the organization and within related manufacturing, contracting, regulatory and govern- mental agencies. Whereas particular active failures tend to be unique to a specific event, the same latent conditions-if undiscovered and uncorrected- can contribute to a number of different accidents. Latent conditions can increase the likelihood of active failures through the creation of local factors promoting errors and violations. They can also aggravate the consequences of unsafe acts by their effects upon the system's defenses, barriers and safeguards. The Accident Trajectory The necessary condition for an organizational accident is the rare conjunction of a set of holes in successive defenses, allowing hazards to come into damaging contact with people and assets. These 'win- dows of opportunity' are rare because of the multiplicity of defenses and the mobility of the holes. Such an accident trajectory is shown in Figure 1.5. Active failures can create gaps in the defenses in at least two ways. First, front-line personnel may deliberately disable certain defenses in order to achieve local operational objectives. The most tragic instance of this was the decision by the control room operators to remove successive layers of defence from the Chernobyl RBMK nuclear reactor in order to complete their task of testing a new volt- age generator. Second, front-line operators may unwittingly fail in their role as one of the system's most important lines of defence. A common example would be the wrong diagnosis of an off-normal system state, leading to an inappropriate course of recovery actions. Such 'sharp-end' mistakes played a significant role in the Three Mile Island nuclear power plant accident, the Bhopal methocyanate disas- ter, the Heysel and Sheffield football stadium crushes8 and numerous other organizational accidents. Since no one can foresee all the possible scenarios of disaster, it is therefore inevitable that some defensive weaknesses will be present from the very beginnings of a system's productive life, or will develop unnoticed-or at least uncorrected-during its subsequent operations. Such latent conditions can take a variety of forms: examples are the capsize-prone design of the Herald of Free Enterprise, the defective O-rings in Challenger's booster rockets, the inadequate fire containment and corroded sprinklers on the Piper Alpha gas plat- form, the failure to appreciate the risk of fire in London Underground stations, and the gradual erosion of supervisory checks that led both to the Clapham Junction rail disaster and the collapse of Baring's Bank. Chapter 2 will present three case studies that illustrate some of the different ways that active failures and latent conditions can bring about the partial or total penetration of a system's defenses. In the meantime, we need to consider the stages involved in the history of an organizational accident. From the Deadwood Stage to Jumbo Jet, We can think about the history of organizational accidents over two different timespans: the one covering the history of a particular- organization and the other covering the history of a particular organizational accident. Although our main interest is in the latter, it is worth giving some attention to the former, if only to illustrate further the distinction between individual and organizational acci- dents. Let us consider an imaginary organization that began its life more than 100 years ago in the American West, delivering passengers and mail from one frontier town to another by stagecoach. Let us also assume that the same company is today successfully operating a worldwide air freight business, using contemporary cargo jets and all the hi-tech paraphernalia that now accompanies such a venture. (Any resemblance to an actual organization is entirely coincidental.) In the beginning, the founding fathers would have raised the money to buy a stagecoach and a few teams of horses. They would probably have done the driving themselves-at least at the outset. Their business would expose them to many hazards: hostile Native Americans, bandits, deserts, inadequate tracks, ravines, and a variety of extreme weather conditions. Some of these dangers they could anticipate; others they would discover through bitter experience. Over time, their protection would gradually increase-from issuing the drivers with a hand gun and a sheepskin coat, to hiring additional people to ride shotgun and providing them with better weapons, to getting cavalry escorts through dangerous territory, and so on. If disaster struck in these pioneering days, the fault-if such it could be called-lay almost entirely with the people on the spot. A driver could ignore local warnings and select a dangerous route, or overturn his coach by taking a bend too fast; the guard's gritty Winchester rifle could jam, or run out of ammunition, or he could simply fail to shoot straight. At this stage in the organization's history, all such mishaps would be individual accidents, having very localized causes and consequences. Now consider a contemporary scenario. Suppose one of the company's jumbo jets takes off from a large city airport and then crashes into an apartment building, killing both the crew and many residents. The first possibility raised after such an accident is that of 'pilot error'. In this case, however, the air accident investigators soon establish that a fuse pin in one of the engine pylons failed just after takeoff, causing the engine to fall off, rendering the plane uncontrollable. Subsequent checks in the company's maintenance facility reveal that the aircraft had just undergone a major overhaul entailing the non-destructive testing of the engine fuse pins. It is also found that the fuse pin retainers on this particular engine had not been replaced following the service and their absence had not been spotted by the inspector. Access to the fuse pin area of the engine pylon was reached by an unstable underwing work platform and the area was poorly illuminated. The disassembly and replacement of the fastenings was the responsibility of a technician who was poorly trained and did not appreciate the need for red tag warnings to show the non-routine removal of parts-and so on. The investigators might then go on to ask why the technicians received no formal classroom training, why the Director of Training's post was currently vacant, why the workplace was inadequate, and why the manufacturers of the aircraft felt it necessary to install fuse pins in the first place since, although they were intended as a safety device (allowing the ready separation of the engine from the wing in the event of an explosion or collision), their failure has been implicated in many incidents and accidents. In short, this was an event for which no one person's failure was a sufficient cause. It was an organizational accident whose origins could be traced far back into many parts of the air cargo system, from the operator to the manufacturer, and-by implication-to the regulator. Should anyone think that such a combination of unhappy events is too unlikely to be credible, it must be pointed out that all of the individual failures catalogued in this story have actually happened, though not in a single aircraft accident. The message of this hypothetical case study is that, while it may have been appropriate for those concerned with individual accidents in low-tech systems to focus upon the unsafe acts of those involved, such an approach would entirely fail to find the remediable causes of an organizational accident. In a stagecoach mishap, virtually all the causes would be active failures. Finding the means to correct those failings-better weapons, better suspension, better maps, better weather protection-and the safety margins increase, or would do if these defensive improvements did not encourage drivers to take short-cuts through territory hitherto regarded as too dangerous (and that is a very big 'if). It should be apparent by now that a similar person-centred investigation in the case of the jumbo jet crash would have little or no chance of improving the system's safety. So long as people continue to be employed in modern technological systems, there will always be active failures-but very few of them will have bad consequences because most will be caught by the defenses. We cannot change the human condition, but we can change the conditions under which people work. However, radical improvements of this kind can only be achieved through a better understanding of the nature of organizational accidents. Some preliminary steps towards this goal are presented below. Stages in the Development of an Organizational Accident The story of the cargo jet crash, just presented, gives some clue as to the difficulties facing those who seek to track down the root causes of an organizational accident. Where do you draw the line? At the organizational boundaries? At the manufacturer? At the regulator? With the societal factors that shaped these various contributions? As we shall see later, all of these increasingly remote influences are being considered by contemporary accident inquiries. In theory, one could trace the various causal chains back to the Big Bang. What are the stop rules for the analysis of organizational accidents? Since time and causality are seamless, they have no natural break- points, only artificially imposed ones. Accident analysts, just like historians, are limited by their resources and by the availability of reliable evidence. Leaving aside legal concerns with responsibility, accident investigations are carried out for two main reasons: to establish what occurred and to stop something like it from happening in the future. Both of these ends are best satisfied by limiting the scope of the analysis to those things over which the people involved-and most particularly the system managers-might reasonably be expected to exercise some control. A sad little story will help to make this point clearer. Academician Valeri Legasov was the principal investigator of the Chernobyl reactor disaster that occurred on 26 April 1986. He was also the Soviet Union's chief spokesman at the international confer- ence on this accident, held in Vienna in September of the same year. At that meeting Legasov put the blame for the disaster squarely on the errors and violations of the operators. Later, he confided to friends, 'I told the truth in Vienna, but not the whole truth.' In April 1988, two years to the day after the disaster, he hanged himself from the balustrade of his apartment. Prior to his suicide, he confided his innermost feelings about the accident to a tape recorder. After being at Chernobyl, I drew the unequivocal conclusion that the accident was ... the summit of all the incorrect running of the economy which had been going on in our country for many years.9 Legasov may well have been right in his evaluation. But how does this information help us? We can hardly go back to 1917 and replay the years following the Russian Revolution. Although Legasov's verdict was correct, it was unlikely to lead to achievable improvements. Models of accident causation can only be judged by the extent to which their applications enhance system safety. The economic and societal shortcomings, identified by Legasov, are beyond the reach of system managers. From their perspective such problems are given and immutable. But our main interest must be in the changeable and the controllable. For these reasons, and because the quantity and the reliability of the relevant information will deteriorate rapidly with increasing distance from the event itself, the accident causation model presented in Figure 1.6 must, of necessity, be confined largely to the manageable boundaries of the organization concerned. It should also be appreciated, however, that all technological organizations have close ties with other organizations. The wider aviation system, for example, includes carriers, airport authorities, maintainers, manufacturers, air traffic controllers, regulators, unions, civil service agencies, government ministers, and international bodies such as the International Air Transport Association and the International Civil Aviation Organization-all of whom may have some part to play in an organizational accident. The principal stages involved in the development of an organizational accident are shown in Figure 1.6. This model seeks to link the various contributing elements into a coherent sequence that runs bottom-up in causation, and top-down in investigation. Accepting the time frame discussed earlier, the causal story starts with the organizational factors: strategic decisions, generic organizational processes-forecasting, budgeting, allocating resources, planning, scheduling, communicating, managing, auditing, and the like. These processes will be coloured and shaped by the corporate culture, or the unspoken attitudes and unwritten rules concerning the wayan organization carries out its business (see Chapter 9 for a further discussion of organizational culture). The consequences of these activities are then communicated throughout the organization to individual workplaces-control rooms, flight decks, air traffic control centres, maintenance facilities and so on-where they reveal themselves as factors likely to promote un- safe acts. These include undue time pressure, inadequate tools and equipment, poor human-machine interfaces, insufficient training, under-manning, poor supervisor-worker ratios, low pay, low status, macho culture, unworkable or ambiguous procedures, poor communications and the like. Within the workplace, these local factors combine with natural human tendencies to produce errors and violations-collectively termed 'unsafe acts' ---committed by individuals and teams at the 'sharp end', or the direct human-system interface. Large numbers of these unsafe acts will be made, but only very few of them will create holes in the defenses. The nature of these unsafe acts will be considered in detail in Chapter 4. Although unsafe acts are implicated in most organizational accidents, they are not a necessary condition. On some occasions, the defenses fail simply as the result of latent conditions, such as in the Challenger and King's Cross Underground fire disasters. This possibility is indicated in Figure 1.6 by the latent condition pathways, connecting workplace and organizational factors directly to failed defenses. So far, we have considered the causal sequence, from organizational factors, to local workplace conditions, to individual (or team) unsafe acts, to failed defenses and bad outcomes. In the analysis or investigation of accidents, the direction is reversed (i.e., along the white arrows in Figure 1.6). The inquiry begins with the bad out- come (what happened) and then considers how and when the defenses failed. For each breached or bypassed defence, it is necessary to establish what active failures and latent conditions were involved. And for each individual unsafe act that is identified, we must con- sider what local conditions could have shaped or provoked it. For each of these local conditions, we then go on to ask what upstream organizational factors could have contributed to it. Anyone local condition could be the product of a number of different organizational factors, since there is likely to be a many-to-many mapping between the organizational and workplace elements of the model. Pulling the Threads Together This chapter began by distinguishing between individual and organizational accidents. Although the frequency of individual accidents in the workplace has decreased dramatically over the years, their basic nature-unprotected slips, lapses, trips, and fumbles-has re- remained more or less unchanged in that the individual (or workgroup) is likely to be both the agent and victim of the accident. By contrast, organizational accidents-the topic of this book-are a relatively new phenomenon stemming from technological developments that have both greatly increased the system's defenses and changed the role of people from distributed makers and doers to centralized thinkers and controllers. In contrast to earlier times, the human elements of modern technologies are often distanced from the day-to-day hazards of their respective operations. Organizational accidents in hi-tech systems occur very rarely, but when they do happen, the outcomes are likely to be disastrous, affecting not only those immediately involved but also people and assets that are far removed from the event in both time and distance. The Chernobyl fallout, for example, continues to remain a threat to unborn generations over large areas of Europe. Understanding and limiting the occurrence of organizational acci- dents are the major challenges to be faced in the new millennium. They are unacceptable in terms of their human, environmental and commercial costs. But how do we develop a set of concepts that are equally applicable to all of these highly individual and infrequent events, and-most importantly-lead to improved prevention? Two sets of terms have been proposed. The first provides a framework for understanding the details of an individual event-hazards, defenses and losses. The second-the tension between production and protec- tion-offers a means of understanding the processes that lead to defensive failures. Two such processes were the trading off of protective gains for productive advantage and the gradual deterioration of defenses during periods in which the absence of bad events creates the impression that the system is operating safely-the case of the 'unrocked boat'. Defenses-in-depth have made modem technological systems largely immune to isolated failures. As such, they are the single feature most responsible for the emergence of organizational accidents. We have categorized these barriers and safeguards in two ways: by their func- tions (awareness, understanding, warning, guidance, restoration, interposition, containment, escape and rescue) and by their modes of application ('hard' and 'soft' defenses). However, no one defensive layer is ever entirely intact. Each one possesses gaps and holes cre- ated by combinations of active failures (errors and violations committed by front-line personnel) and latent conditions (the consequences of top-level decisions having a delayed-action effect upon the integrity of various defensive layers). The chapter concluded with two perspectives on the development of organizational accidents. The first outlined the development of a hypothetical organization from its early pioneering days, when all mishaps were likely to be individual accidents, until modem times, when the greatest risk was from rare but catastrophic organizational accidents. The second perspective covered the history of a particular organizational accident and traced its development from upstream systemic factors, through local factors promoting errors and violations, to the level of the individual who committed these unsafe acts. Some of these active failures are likely to have an immediate adverse effect upon local defenses, many others will be inconsequentiaL When the gaps produced by active failures 'line up' with those created by latent conditions, a 'window of opportunity' exists for an accident trajectory to bring hazards into damaging contact with people and assets. The main purpose of this chapter has been to introduce the basic ideas and to set the stage for a more thorough consideration of these issues in subsequent chapters. Chapter 2 begins this finer-grained analysis by examining the various' cracks in the system' revealed by case studies of three organizational accidents. Our purpose here is to put some flesh on these rather abstract bones. It must be stressed, however, that this is not yet another book of accident case studies. Their sole reason for inclusion here is to achieve a better understand- ing of organizational accidents in general rather than in the particular instance. Chapter 2 Defeating the Defenses Things are not Always What They Seem The human mind is prone to match like with like1• It is therefore natural for us to believe that disastrous accidents must be due to equally monstrous blunders. But the close investigation of organizational catastrophes has a way of turning conventional wisdom on its head. Defenses can be dangerous. The best people can make the worst mistakes. The greatest calamities can happen to conscientious and well run organizations. Most accident sequences, like the road to hell, are paved with good intentions-or with what seemed like good ideas at the time. In the previous chapter, we introduced the gaps created in the system's defenses by active failures and latent conditions. When they lead to a particularly bad outcome, we naturally assume that it could only have been the result of gaping holes due to gross negligence and habitual bad practice. Bad practices may be present, but the reality is often much more commonplace. Organizational accident trajectories can just as easily slip through the small and apparently insignificant cracks in the system as they can through the yawning gaps. This chapter examines three recent case studies drawn from widely differing domains. The first near-accident slipped through the minor cracks in the system; the second disaster fell through some yawning gaps in the defenses; the third, unusually, had its origins in a single longstanding latent condition. Many other well documented case studies would have served as well, but these-for the most part- have already been extensively analysed elsewhere. Our purpose is to try to identify recurring patterns in the way defenses fail. Are there sufficient common features, even in these disparate events, for us to abstract some general principles regarding the development of organizational accidents? The American social scientist, Karl Weick, stated the nub of the problem very well: We know that single causes are rare, but we don't know how small events can become chained together so that they result in a disastrous outcome. In the absence of this understanding, people must wait until some crisis actually occurs before they can diagnose a problem, rather than be in a position to detect a potential problem before it emerges. To anticipate and forestall disasters is to understand regularities in the ways small events can combine to have disproportionately large ef- fects.2 Slipping Through the Cracks of an Aircraft Maintenance System Our first case study is a prime example of an aircraft accident that nearly found its way through the small chinks in the system's extensive armour. Fortunately, there was no actual catastrophe, but it was a close-run thing, only avoided by the prompt action of the last-ditch human defenses.3 Shortly after departing from East Midlands Airport en route for Lanzarote in the Canary Islands, the pilots of a Boeing 737-400 detected the loss of oil quantity and oil pressure on both engines. They declared a 'Mayday' and diverted to Luton Airport, where both engines were shut down during the landing roll. There were no casualties. It was later discovered that the high-pressure rotor drive covers on both engines were missing, resulting in the almost total loss of the oil from both engines during flight. A scheduled borescope inspection (required every 750 hours) had been carried out on both engines during the previous night by the airline's maintenance engi- neers. The borescope inspections were to be performed by the line maintenance night shift. On the night before this work was to be done, the line engineer in charge of this shift had expressed his concern about the manpower available to carry out the next night's predicted work- load, which he knew would include the borescope inspections. However, on arriving for work on the night in question, he discovered that no extra maintenance personnel had been assigned to his shift. Instead of a nominal complement of six, there were only four on duty that night and two of them-including the line engineer- were working extra nights to cover shortfalls. The line engineer realized that he would have to carry out the borescope inspections himself since he was the only engineer on the line shift possessing the necessary authorization. As the inspection was to be carried out in a hangar at some distance from where the bulk of the line maintenance work was done, but close to the base maintenance hangar, he decided to put the inspection at the top of his list of jobs for that night. After organizing the night's work for his shift, he collected the inspection paperwork from the line office and went to the aircraft where he started to prepare one of the engines for inspection. Having done this, he went across to the nearby base maintenance hangar where the borescope equipment was stored. There he met the base maintenance controller (in charge of the base night shift) and asked for the inspection equipment and also for someone to help him, since the engine spool had to be turned by a second person as he carried out the inspection. The base night shift was also short-staffed. On arriving at his office that night, the base controller had received a request from Ramp Services to remove another aircraft from their area (a Boeing 737-500) because it was in the way. He could not do this immediately because of the shortage of personnel on his shift. However, when he met the line engineer, he saw a way of killing two birds with one stone. The company rules required staff to carry out two 7S0-hour borescope inspections4 within a 12-month period in order to maintain their borescope authorization. His own authorization was in danger of lapsing because this task very rarely continuing airworthines management exposition his way. He also knew that the line maintenance shift was lacking two people and that they had eight aircraft to deal with that night. So he offered to do a swap. He would carry out the borescope inspection, if the line engineer took over the job of moving the B737 -500 from the ramp to base. The line engineer agreed to this mutually convenient arrangement and told the base controller of his progress so far in the preparation of the engines. Since the line engineer had not visited the base hangar with any intention of handing over the inspection to someone else, there was no written statement or note of what had so far been done. Indeed, the line maintenance paperwork offered no suitable place to record these details. The base controller, however, was satisfied with the verbal briefing that he had received. His next step was to select a fitter to assist him and to check and prepare the borescope equipment. With these innocent and well intentioned preliminaries, the stage was set for a near-disaster. The fitter was sent to prepare the second engine for inspection and the base controller then spent a consider- able time organizing the duties for his short-staffed night shift. When he eventually joined the fitter at the aircraft, he brought with him his personal' copy of the borescope training manual on which he had written various details to help him in judging sizes through the probe. Although the work pack for the job was unfamiliar to him- being line rather than base maintenance paperwork-and although there were no Boeing task cards attached, he did not consider it necessary to draw any additional reference material. While the fitter was continuing to prepare the second engine for inspection, the base controller started his inspection of the first engine. Throughout the next few hours, the job continued with many interruptions brought on by the base controller's need to cope with pressing problems cropping up elsewhere on his patch. The upshot was that the rotor drive covers were not refitted, the ground idle engine runs (checks that could have revealed the oil leaks) were not carried out, and the · borescope inspections on both engines were signed off as being complete in the Aircraft's Technical Log. During the night, the line engineer returned with a line maintenance crew to prepare the aircraft for the next day's flying. He and the base controller discussed whether or not to trip out the aircraft's engine ignition and hydraulic circuit breakers. The base controller said that he did not think it was necessary since he had no intention of working on the aircraft with the electrical and hydraulic power systems active, but the line engineer pulled the circuit breakers anyway, feeling confident that the base controller was aware of this. When the aircraft was later returned to the line, the pilots noted that these circuit breakers had not been reset and expressed some dissatisfaction about this to a line engineer on the morning shift. The engineer said that it was probably an oversight and reset them. When the engineer returned to the line office, he wondered aloud to a colleague how the engine run had been done with the ignition circuits disabled. They were still discussing this when they heard that the aircraft had made an emergency landing at Luton. We do not know exactly why the covers were not replaced on the engines, nor how the inspection continuing airworthines management exposition to be signed off as completed when such vital parts were missing and the engine run not done. We do know, however, that omissions during reassembly are the single most common form of maintenance lapse-both in aviation and in the nuclear power industry. Indeed, such maintenance omissions are probably the largest single type of human performance problem in most hazardous technologies (see Chapter 5). We know too that there had been at least nine previous instances in which high-power rotor covers had been left off engines following maintenance in other air- lines. The available evidence also indicates that many aircraft maintenance jobs are signed off as complete without a thorough inspection. Such routine short-cuts are most likely to occur in the face of a pressing deadline-such as returning an aircraft to the line before its scheduled departure time. Knowing precisely what went on in the minds of the people con- cerned at the time these errors were committed would contribute little to our understanding of how organizational accidents happen or how their occurrence could be thwarted in the future. Such hu- man failures happen frequently, and will continue to do so while people are still engaged in removing and replacing the millions of detachable parts that make up a modern aircraft. Our primary concern, both here and in other organizational accidents, is with the systemic factors that promoted their occurrence and then allowed them to go undetected. Human fallibility, like gravity, weather and terrain, is just another foreseeable hazard in aviation. The issue is not why an error occurred, but how it failed to be corrected. To repeat a constant theme of this book: We cannot change the human condition, but we can change the conditions under which people work. What, then, were the small cracks in the system that permitted these commonplace deviations to come so close to causing a major air disaster? The first and most obvious factor is that both the line and base maintenance crews were short-staffed, though not so depleted that they had to abandon their assigned tasks. Such temporary shortfalls in maintenance personnel through sickness and leave are not uncommon. A similar problem in a separate incident contributed to the blow-out of a flight deck windscreen that had been secured by the wrong-sized bolts5• There, as in this case, a temporary shortage of personnel led supervisory staff to take on jobs for which they lacked the current skills and experience. And, in both events, their ability to perform these tasks reliably was impaired by the continuing need to carry out their managerial duties. To compound these problems fur- ther, the shift managers involved in both accidents were also in a position to sign off their own incomplete work, thus removing an important opportunity for detecting the error. In the case of the missing engine covers, however, there was also another more subtle crack in the system--"-:the difference in working practices between line and base maintenance. The division of labour between these two arms of an aircraft maintenance facility is normally clear-cut. Line engineers are accustomed to performing isolated and often unscheduled tasks (defect repairs) either individually or in small groups. These jobs are usually completed within a single shift. Base maintenance, on the other hand, deals with scheduled over- hauls. When an aircraft enters a base maintenance hangar, it is likely to be there for a relatively long time and a large number of different tasks will be carried out by rotating shifts, with incomplete work being handed on from one shift to the next. The line and base environments thus require different kinds of planning and supportive work packs. In the case of line maintenance, the paperwork is generated just before the work is due to be done and is subject to change according to operational needs. The job cards therefore give only a brief description of the work to be per- formed. In sharp contrast, planning for base maintenance work starts several weeks before the event and is delivered as a massive work pack that includes considerable supporting and explanatory docu- mentation. Life on the line is full of surprises, while that in base maintenance is far more regular and predictable. The 7S0-hour borescope inspection was designated as a line maintenance task, and the paperwork included none of the step-by-step task cards and maintenance manual sections normally supplied with a base maintenance work pack. It only contained references to the detailed documentation that was available elsewhere. The line engineer was familiar with the job and did not feel it necessary to draw this additional information, so the work pack that he handed over to the base controller was relatively sparse in comparison with the usual base maintenance documentation. In particular, the line-generated work pack contained no mention of the restorative work -replacing fasteners and covers-nor did it require step-by-step signatures confirming the completion of these tasks, as would be expected in a base-generated work pack. Nor were there any of the customary warnings-highlighted in the maintenance manual-that the safety of the aircraft would be seriously jeopardized if the reassembly work was not completed as specified. Given the short-staffed nature of his own shift and the additional demands that this would inevitably make upon his time, the base controller was probably unwise to offer to carry out the borescope inspection. But it could hardly be classed as a major blunder, and it was certainly allowable within company procedures. In view of his lack of current experience at the task, he was clearly mistaken in assuming that he could perform the task without reference to de- tailed task cards and using only his memory and the unofficial training notes as a guide. Until he actually received the work pack from the line engineer, he probably would not have known that the detailed supporting material was missing. At this point, he could have gone across to the line maintenance office or to base maintenance to fetch it, but it was a cold wimer's night and he was confident that he knew the procedure. And even the accident investigator acknowledged that had he retrieved this material, it would not have been easy to use. All manuals are continually being amended and updated, often making it hard for those unfamiliar with their layout to follow a continuous thread of task-descriptive text. Another small crack in the system was the absence of any formal procedure for handing over jobs from line to base maintenance. As a result, no preprepared stage sheets (indicating the prior work per- formed) were available and the hand over briefing was carried out verbally rather than in writing. This was not so much a case of a flawed defence as one that was entirely absent. Yet such handovers are comparatively rare events and, while it is easy to appreciate the need for handover procedures with hindsight, it is difficult to imagine why their creation should be high on anyone's agenda before this event occurred. These various cracks in the defenses are summarized in Table 2.1. Table 2.1 Summary of the active failures and latent conditions that undermined or breached the aircraft maintenance system's defenses Active Failures: The line maintenance engineer who released the aircraft to the flight crew missed an opportunity to discover the missing drive covers when he reset the ignition circuit breakers and queried how an engine run could have been performed. The flight crew accepted the pulled circuit breakers on the flight deck as 'normal' and did not pursue the matter further. The aircraft's Technical Log was signed off without adequate ispection of the work on the engines. The base controller was in a position to sign off his own incomplete work without further inspection. The base controller failed to supervise the work of the relatively inexperienced fitter sufficiently closely. This was partly due to the many interruptions arising from his administrative duties. No idle engine run was per-formed. The base controller carried out the work without reference to task cards or the maintenance manual. Instead, he relied on his memory and an unapproved reference-his training notes. Latent Conditions: The airline lacked an effective way of monitoring and adjusting the available manpower to the work-load, particularly on night shifts known to be subject to adverse time-of-day effects. The regulatory surveillance of this manpower problem was inadequate. Both the line and the maintenance night shifts were short-staffed. This was a fairly common occurence. An internal inquiry revealed that borescope inspections were often carried out in a non-procedural manner. This was a failure of the company's Quality Assurance system. No procedures existed for the transfer of part-completed work from line to base maintenance. As a result, no stage paperwork was available, and the line engineer only gave a verbal handover brief-ing to the base controller. . The line-oriented paperwork given to the base controller lacked the usual (for him) reference documentation. In particular, it lacked any reference to restorative work and provided no means of signing off such work. In July 1996 the airline involved in this accident was fined £150 000 plus £25 000 costs for 'recklessly or negligently' endangering an air- craft and its passengers.6 This was the first time in the UK that the Civil Aviation Authority had prosecuted a carrier under Articles 50 and 51 of the 1989 Air Navigation Order. The two maintenance engineers directly involved in the accident were dismissed. After the verdict, the deputy chairman of the airline told the press: 'This has been a difficult day for us, but we have learnt from the experience. We have completely changed our procedures.' We will consider whether or not this was the most appropriate response in Chapter 3. The captain of the aircraft, whose prompt actions averted a catastrophe, stated that it was a pilot's job to cope with the unexpected: 'We have to anticipate the worst case scenario. We are not just up there to press a button and trust in the wonders of modern technology. We have to be ready for this kind of eventuality.'7 This states the defensive role of flight crew very clearly. In Chapter 5 we pose the question: Is the primary task of pilots in contemporary aircraft to cope with emergencies produced by undiscovered maintenance errors? The Millions that Gushed Away: the Barings Collapse To those who work in mining, construction, transportation, nuclear power generation, chemical process plants and the like, the term 'hazard' usually means some inanimate danger associated with the uncontrolled release of mass, energy or toxic substances. But it is worth reminding ourselves that' defenses-in-depth' is a military term, relating to situations in which the hazards to be guarded against are other people. People constitute a hazard in many ways. For the military, it is 'the enemy'. In a football crowd, it is the crush of other bodies. For commercial aviation, it takes the form of terrorists and hijackers. In big cities, it is muggers, thieves and murderers. For banks, it can be rogue traders. The second case study concerns the collapse of the Barings banking group-or at least that part of the story relating to the failed defenses. In keeping with the approach adopted earlier, we will not delve into the competence, motives or the morals of the principal hazard in this case study-Nick Leeson, the self-styled rogue trader.8 We will take it as a given that anyone trading in millions on a daily basis is exposed to a variety of temptations and is capable of making some very costly mistakes. Our interest, as before, is in the manner in which the banking system's various barriers, safeguards and de- fences were absent, breached or bypassed. Inevitably, this account will be much abridged and will limit itself to the period of the actual collapse, although its origins may probably be traced back for 100 years or so. On 26 February 1995, the High Court in London appointed joint administrators to manage the affairs of Barings pIc, the Barings Group parent company. This followed the discovery of the loss of £869 million by Barings Futures (Singapore) Pte Limited (BFS) incurred on the Singaporean and Japanese exchanges. Barings Brothers & Co., Limited (BB & Co.) was the longest established merchant banking business in the City of London. Since the original foundation of the partnership in 1792, it had remained independent and privately con- trolled. Barings Securities had been operating in Singapore since 1987, but it was only in 1992 that it acquired seats on the Singapore Inter- national Monetary Exchange (SIMEX) with the intention of trading in the rapidly growing futures-and-options market. The new company established to conduct this business was BFS. Leeson was appointed to head the settlement operations. Most unusually, he was also asked to be Barings Futures' floor manager on SIMEX. This breached one of the basic principles of the securities business which was to keep the settlements and trading activities strictly separate. The job of the trader was to make money. That of the settlements clerk was to ensure that no errors occurred in the accounting and to fix them when they did. Giving Leeson this dual role was subsequently de- scribed by the Singaporean investigators as an 'ill-judged decision'.10 Between July 1992 and February 1995, the Barings Group were carrying out a radical restructuring of their hitherto autonomous businesses, each with different cultures and operations. Conflict had arisen between the more conventional banking operations at BB & Co and its subsidiary, Barings Securities Limited (BSL). BSL had been formed from the acquisition in 1984 of a UK-based stockbroker with strong connections in Asia. BSL had flourished in the heady days of the late 1980s, but by 1992 it was no longer so healthy. Its chief executive, Christopher Heath (once Britain's highest-paid worker), was faced with falling markets and a large staff, hungry for fat bonuses. The situation continuing airworthines management exposition to a head on Sunday, 26 September 1992, a day that Heath later described as 'the stabbath'.l1 Heath relinquished his post as CEO and Peter Norris was appointed as chief operating officer. In March 1993, Heath left Barings and Norris was made chief executive officer. At the end of 1993, BB & Co and BSL were merged to form the Investment Banking Group, later to become Baring's Investment Bank (BIB). BIB was subdivided into four groups. The two relevant to the collapse were the Equity Broking and Trading Group, headed by Norris, and the Banking Group, led by George Maclean. Maclean was later to tell the Bank of England investigators: I believe the seeds of this [collapse] were sown when we went into BSL to bring the two companies together and made the assumption that the quality controls that we [BB & Co] had could quickly get installed there [BSL]. As it turned out that appears not to be true.12 After passing the local exchange examinations, Leeson began trading on the floor of SIMEX. Soon he was appointed as General Manager and Head Trader of BSF, although there later seems to have been much confusion in London as to what this title entailed. Heath, who had originally appointed him, believed that he was just a clerk trans- mitting orders received by phone or fax to the trader on the SIMEX floor. Leeson, however, had other ideas. Being an execution clerk gave him access to the trading floor. In his book, Rogue Trader, he gives us a clear idea of the impression this made upon him. When I first stepped out on to the trading floor, I could smell and see the money. Throughout my time at Barings I had been inching closer and closer to it, and in Singapore I was suddenly there. I'd been working in various back offices for almost six years, pushing paper money around, sorting out other people's problems. Now, out on the trading floor, I could work with instant money-it was hanging in the air right in front of me, invisible but highly charged, just waiting to be earthed.13 The confusion surrounding Leeson's dual role as settlements clerk and trader lies at the heart of the Barings collapse. Just as a lighted cigarette fell through the cracks in the wooden escalator to cause the King's Cross Underground fire, so Leeson fell through the cracks in the Barings' matrix management structure. In this, each of the business ventures-banking, equity broking, trading and so on-formed the verticals of the matrix, while the various offices and regional structures scattered around the world formed the horizontals. The theory was that anyone individual was connected both to local management and to the London office. The reality in this case was that Leeson's activities were not closely monitored by anyone. Leeson's immediate bosses in Singapore not only failed to grasp the extent of his job specification, they were also reluctant to supervise him. He was always regarded as someone else's responsibility. His reporting lines were either blurred or non-existent. This problem was not helped by the top management in Singapore being located on the twenty-fourth floor of the new Ocean Tower block, while Leeson worked on the fourteenth floor. In July 1992 BFS opened an error account. This was a standard procedure for holding disputed trades until the disagreement is settled, usually within 24 hours. It was given the number 88888 (the five-eights account). Five days later, Leeson instructed an independent computer consultant in Singapore to change the software at BFS. He wanted the five-eights account to be excluded from daily trading, position and price reports to London, and only the daily margin file-relating to cash or securities deposited with the ex- change as collateral and as a means of settling profit and loss-to be passed on. This meant that when the daily margin file arrived in London, the automatic sorting system would not recognize the ac- count number. As a result, no information would be transferred into Barings' internal reporting system, called First Futures. The information held in London was presented on two computer screens. One showed the margin balances on the SIMEX computer system and included the margin file from the five-eights account. The second showed the balances transferred to the First Futures system. This was the only screen regularly scrutinized by the Lon- don office, and it had no record of the five-eights account. London only became aware of this secret account three days before the col- lapse. It was through this hole that hundreds of millions of pounds vanished. Leeson then embarked on a frenzy of unauthorized trading in futures and options. By December 1992, his losses amounted to £208 million. Throughout these and the succeeding months the London office regarded Leeson as a star performer. He was given a £130 000 bonus in 1993 and one of £450 000 the following year. Barings assumed that his declared profits continuing airworthines management exposition from arbitrage-trading on the price differences between SIMEX and the Japanese exchanges. Because the London office believed that these trades were fully matched-offsetting each other-there appeared to be no real risk to the bank. By February 1995 the accumulated losses amounted to £830 million. Leeson funded his losses in three ways. First, he used money lent by Barings Securities subsidiaries in Tokyo and London in the belief that it was being traded on their own accounts. Second, money was lent by BSL in London for margin payments to the exchanges-funds to cover unrealized losses. But no adequate steps were taken to verify the accounts or to check them with the trading records of clients. Third, as the losses escalated in January and February 1995, Leeson created artificial trades on the SIMEX computer system to cut the level of margin payments required by the exchange. He also covered his tracks by submitting false reports to London inflating BFS's prof- its and making false trading transactions and accounting entries. In July and August 1994, a BSL internal audit team visited Singa- pore to review the operations of Barings' offices in the region. The report, delivered. in December 1994, expressed some concern at the lack of segregation between BFS's front (trading) and back (settlement) offices. The team member who audited the Singapore office wrote that while there was no evidence that Leeson was abusing his dual role, '... the potential for doing so needs examining' .14 The report recommended that Leeson should no longer be solely responsible for supervising the back office, but this still left him with the powers necessary for his deceptions. One of the main reasons for the audit was to probe Leeson's large profits. The report concluded that the profits were due to Leeson's exceptional abilities and expressed the worry that should Leeson be poached by a competitor it would , ... spread the erosion of BFS's profitability greatly'. The auditors, along with the London management, continued to be mesmerized by Leeson's declared profits. By this time, however, the London office was aware that tens of millions of pounds remained unaccountable due to unreconciled trades-the failure to match what should have been in an account with what actually was there. Tony Hawes, the Group Treasurer of BIB, took some comfort from the audit report: 'We thought there couldn't have been anything too wrong, or they'd have spotted it.' Later, he was to say that there was no excuse for not making reconciliation the highest priority. 'But there always seemed to be something else more pressing'. This sentiment crops up frequently in the history of organizational accidents. By early February 1995 there were rumours in the markets about dealings in Japan and about possible client problems. The London office even received a concerned phone call from the Bank for International Settlements in Basle. But this did not worry the Barings' management because they believed-wrongly-that their positions on the Japanese markets were covered by equal and opposite positions on SIMEX. Nonetheless, the pace of discovery quickened through February 1995. On 6 February, Tony Hawes and a colleague, Tony Railton, visited Singapore in the hope of clarifying a number of issues. Gradually, they uncovered enough to cause them great anxiety. On 23 February, Leeson did not return to the office. On the same day, Hawes looked at a computer printout and noticed 'an account called an error account with goodness knows how many transactions on it, all of them seemingly standing at tremendous 10sses'.ls The next day, a Friday, Peter Baring, Chairman of Barings pIc, met the Deputy Gov- ernor of the Bank of England and informed him that Barings had been the victim of a massive fraud. Over the weekend, the Bank of England tried unsuccessfully to save Barings. On the Sunday evening, the administrators were called in. The 203 year-old merchant bank had collapsed. So far, we have focused on the failure of the internal controls. But what of the failure of the Bank of Enrland's regulatory role? The Bank of England's investigative reportl of the collapse criticized the supervision provided by one of its senior managers in charge of merchant banks, stating that this manager made an error of judgement in 1993 when he gave Barings the informal concession of allowing its dealings on the Osaka Stock Exchange to exceed 25 per cent of its capital. As a result, the manager in question resigned, as did Peter Baring and Andrew Tuckey, the Chairman and Deputy Chairman of Barings pic. Nick Leeson is currently serving a six-and- a-half-year sentence in a Singaporean prison for fraud. The assets and liabilities of the Barings Group were purchased by lNG, a large Dutch banking and insurance group, for a token £1-but the real cost was £660 million. In June 1996, the Singaporean Business Times re- ported that Singapore's largest lawsuit, involving more than S$2 billion, had been served on BFS's external auditors, Coopers & Lybrand Singapore and Deloitte & Touche, for giving an 'unqualified clearance on the Baring group's reporting package of BFS for the year ended 31 December 1994'. The fallout will no doubt continue. By using the five-eights account both to conceal his losses and to declare false profits to the London office-Leeson created a gaping hole in the system's defenses. Clearly, Leeson himself was the principal architect of the active failures. Some idea of the extent of these destructive trades can be gained from looking at the period immediately following the Kobe earthquake on 17 January 1995-the event that triggered his final downfall. Leeson gambled that Japanese stocks would rise rather than fall, and on 20 January, he built up a long position (one that appreciated in value if the market price increased) of 10014 Nikkei 225 futures contracts. On Monday 23 January, the Nikkei 225-an index based on 225 Japanese stocks traded on the Tokyo Stock Market-fell by 1175 points, but Leeson continued to buy. By 27 January, his holding had increased to 27158 contracts. On 23 January alone, Leeson's Nikkei futures position lost £34 million, while his options portfolio had lost £69 million since the earthquake. Leeson's response was to use cross-trades (a means of transferring buy-and-sell orders through the exchange between two clients belonging to the same firm) with the five-eights account to declare large dividends for Barings. Between 23 and 27 January, when he claimed to have made £5 million from arbitrage, he had actually lost £47 million. Fraud is extremely difficult to counter, and incompetent fraud of these proportions is even more difficult to predict or withstand. In April 1994 the international banking world was shaken by the news that a Wall Street trader had been discovered by his employer to have declared a wholly false profit of $350 million. The reaction of Barings' top management was to conduct a review of its own risk controls. They discovered that these controls needed radical improvement. The board was told that such a tougher risk management system was being developed, but nothing more was heard of this plan until January 1995 when it was too late to avert disaster. Risk controllers were appointed in Hong Kong and Tokyo during 1994, but the Singapore management said they were not necessary. ' The Bank of England report identified a number of warning signs that should have alerted Barings management to BFS's unauthorized activities. The report concedes that no single one of these indicators would have served as a sufficient warning, but collectively they should have raised the alarm in both London and Singapore. These indicators included the following: • The lack of segregation between the front and back offices identified by the internal audit carried out in July and August of 1994. • The high level of funding required to finance BFS's trading activities. • The unreconciled balance of funds transferred from London to Singapore for margins. • The very high apparent profits relative to the low level of risk as perceived by Baring's management in London. • The discovery of an apparent receivable of approximately £50 million from one of BFS's customers, Spear, Leeds & Kellogg, that had been faked by Leeson in December 1994. • A letter sent by SIMEX to BFS on 11 January 1995 that included specific references to the five-eights account. This was not passed on to London at the time. • And a further letter from SIMEX to BFS, dated 27 January 1995, seeking reassurance that BFS was able to fund its margin calls in the event of an adverse market move. This was communicated to London, but was not acted upon for the reasons given earlier. The Nakina Derailment: A 76-Year-old Latent Failure If there is one defensive feature that dominates all others in railway operations, it is the stability of the roadbed-the ground upon which the rails are laid. Should it fail, the alignment and the holding power of the rails are lost and any train passing over this region is likely to be derailed. This is what happened in July 1992 at a bend near Nakina Ontario P What distinguishes this accident from other derailments is that the root cause dated back to 1916 when the track was first laid. Just after coming round a bend at the regulation 35 mph, the crew of a Canadian National freight train saw that the rails immediately ahead of them were suspended in mid-air and that the roadbed beneath them was completely washed out. Unable to stop in time, the train derailed, toppling into an adjacent pond. Two crew members were killed and the train driver was seriously injured. The subsequent investigation, carried out by the Canadian Trans- port Safety Board, established from the original plans that the roadbed had been located on the corner of an existing beaver pond. The rails had been laid across a beaver dam-an unstable mix of silt and peat. Although the upper portions of the roadbed had been maintained and upgraded over the ensuing decades, the basic instability of their foundations had not been rectified. Organizational accidents involve the interaction of latent conditions with local triggering factors. Two such local events precipitated the accident. First, the railway company implemented a policy of killing beavers in the vicinity of rail tracks-to minimize the problems of flooding and washouts of railway infrastructure associated with beaver activity. This meant that the longstanding dam was no longer maintained by the beavers. Second, it had been an unusually wet summer and water had built up behind the dam supersaturating the nearby roadbed. The unattended beaver dam, gradually weak- ened under the increasing pressure of the accumulated water, collapsed when a critical mass of the subgrade sludge was washed into the bed of the pond. The investigators built a computer model of the beaver dam and tested its stability under various scenarios. This exercise established that a temporary loading on the dam created by a freight train passing over it would not have been sufficient to cause the collapse. The catastrophic destabilization was due entirely to a rapid two-metre fall in the water level on one side of the railway embankment. There were no visual indications that the roadbed had been undermined, nor could this have been predicted without a detailed geotechnical analysis. Since the track appeared to be stable to those carrying out routine inspections of its exterior features, no such analysis was performed nor were measures taken to strengthen the underlying bed. Unusually, therefore, no active failures (errors and violations) on the part of either the train's crew or the track's maintainers were implicated in this accident. The root cause was a latent construction failure-permitted by the standards existing in 1916-that had lain dormant for 76 years until a set of local triggering conditions caused its intrinsic weaknesses to be fatally revealed. Common Features The purpose of presenting these three case studies was twofold. First, to demonstrate in detail the variety of ways that a system's defenses can be degraded and defeated to cause organizational accidents. Second, to see what-if any-common principles could legitimately be applied to these three very diverse events. It is the latter issue that will occupy us in this concluding section. In this regard, it is worth remembering that, in safety science, the test of any general principle is a very practical one. Does it identify workable remedial applications? Theoretical abstractions are of little interest unless they lead to improved safety. The first and most obvious fact about all three of these accidents is that pre-existing-and often longstanding-latent conditions contrib- uted to the breakdown of the system's defenses. Errors and violations committed by those at the sharp end are common enough in organizational accidents, but they are neither necessary nor sufficient causes, as the Nakina derailment clearly showed. Latent conditions, how- ever, are always present in complex systems. And they are present now. We may not know which particular conditions will be impli- cated in any future event, but we do know that some latent conditions will be involved. From the now extensive analyses of organizational accidents, we also have a fairly good idea of the kinds of latent conditions that are most likely to constitute a threat to the safety of the system. There is no mystery to this. They relate to basic organizational processes: designing, constructing, operating, maintaining, communicating, selecting, training, supervising and managing. This list is incomplete, but it will serve to show that they are the generic essentials of any productive process-the everyday business of management, in fact. This brings us to two important generalizations. First, the quality of both production and protection is dependent upon the same underlying organizational processes. Safety is not a separate issue. Second, we cannot prevent latent conditions from being seeded into the system since they are an inevitable product of strategic decisions. All we can usefully do is to make them visible to those who manage and operate the organization so that the worst of them, at anyone time, can be corrected. We cannot hope to solve all of our problems in one go. Resources are always limited. But we can target and then address the latent conditions most in need of urgent attention within a given period. Of course, while these are being fixed, other things will be going wrong. Risk management, like life, is 'one damn thing after another'. Some of the proactive diagnostic tools by which this long-term 'fitness programme' can be achieved will be described in Chapter 7. In all three events, the essential process of checking and reviewing the defenses broke down. In the train derailment, the underlying problem was virtually undiscoverable before the event. It is thus a very special case. More usually, latent weaknesses in defenses are potentially-or even actually-evident prior to a bad outcome. The first two case studies revealed one very common reason why defensive weaknesses are not detected and repaired: the people involved had forgotten to be afraid. It is very easy for those with production or profit goals uppermost in their minds to lose sight of the dangers. Bad events are mercifully rare in well defended systems. Very few people have direct experience of them. Production demands, on the other hand, are immediate, continuous and ever-present. They are also variable and attention- grabbing, whereas safe operations generate a constant-and hence relatively uninteresting-non-event outcome. The mechanisms by which this reliability is achieved can be opaque to those who operate and manage the system. Once again, Karl Weick has given us an elegant summary of this problem. 8 Reliability is invisible in the sense that reliable outcomes are constant, which means there is nothing to pay attention to. Operators see nothing and seeing nothing presume that nothing is happening. If nothing is happening and they continue to act the way they have been, nothing will continue to happen. This diagnosis is deceptive and misleading because dynamic inputs create stable outcomes. Weick's point is that safety is a dynamic non-event-what produces the stable outcome is constant change rather than continuous repetition. To achieve this stability, a change in one system parameter must be compensated for by changes in other parameters. If eternal vigilance is the price of liberty, then chronic unease is the price of safety. Studies of high-reliability organizations-systems having fewer than their 'fair share' of accidents-indicate that the people who operate and manage them tend to assume that each day will be a bad day and act accordingly. But this is not an easy state to sustain, particularly when the thing about which one is uneasy has either not happened, or has happened a long time ago, and perhaps to another organization. Nor is this Cassandra-like attitude likely to be well received within certain organizational cultures, as will be shown below. Ron Westrum, a leading American industrial sociologist, has distinguished organizational cultures according to the way they deal with safety-related information.19 He identified three types of culture-pathological, bureaucratic and generative-and their principal characteristics are summarized in Table 2.2. Table 2.2 How different organizational cultures handle safety information Pathological culture: Don't want to know. Messengers (whistle-blowers) are 'shot'. Responsibility is shirked. Failure is punished or concealed. New ideas are actively discouraged. Bureaucratic culture: May not find out. • Actively seek it. • Messengers are listened to if they arrive. • Responsibility is compartmentalized. • Failures lead to local repairs. • New ideas often present problems. • Generative culture: Actively seek it. Messengers are trained and rewarded Responsibility is shared. • Failures lead to far-reaching reforms. • New ideas are welcomed. Westrum argues that organizations conducting potentially hazardous operations need requisite imagination-a diversity of thinking and imagining that matches the variety of possible failure scenarios. The possession of this requisite imagination not only characterizes high- reliability (or generative) organizations, its absence-as we saw in the first two case studies-features prominently in the developmental stages of an organizational accident. With hindsight, it is nearly always possible to identify, prior to a disaster, the presence of warning signs which, if heeded and acted upon, could have thwarted the accident sequence. The question that often arises after the event is: How could these warnings have been missed or ignored at the time? There are a number of possible rea- sons why this happens, but most of them have to do with the fact that after-the-fact observers armed with '20/20' hindsight view events quite differently from the active participants who possessed only limited foresight. Knowing how events turned out-what psychologists have called outcome knowledge-profoundly biases our judgement of the actions of those on the spot. Several studies20 have shown that: • people greatly overestimate what they would have known in foresight, • they also overestimate what others knew in foresight, • they misremember what they themselves knew in foresight. One of the facts we most readily overlook when we review the causal history of an accident is that some prior indication of disaster is only truly a warning if you know what kind of disaster you will suffer. But this is rarely the case, as the Dutch psychologists, Willem Albert Wagenaar and JoP Groeneweg, have explained: Accidents appear to be the result of highly complex coincidences which could rarely be foreseen by the people involved. The unpredictability is caused by the large number of causes and by the spread of information over the participants .... Accidents do not occur because people gamble and lose, they occur because people do not believe that the accident that is about to occur is at all possible.21 In short, many accidents are impossible accidents-at least from the perspective of those involved.22 This was probably the case for the aircraft maintenance accident, but perhaps it was not so true of the Barings Bank collapse. In that case, the Barings' management had been sufficiently alarmed by the fraud perpetrated by the Wall Street trader in April 1994 to review their own risk controls. They noted the deficiencies and started to correct them, but not quickly enough to avert the disaster. Perhaps, as their Group Treasurer observed, 'There was always something else that seemed more pressing'. That is an appropriate epitaph for most organizational accidents. Chapter 3 Dangerous Defenses Killed by their Armour On a damp late October morning in 1415, a considerable force of heavily armoured French cavalry advanced towards a small and sickly English army made up of 5 000 lightly clad archers and around 300 men-at-arms (the French army was at least five times this number). The mounted French knights rode across ground bounded on either side by thick woods. Although they had intended to attack the flanks of the English army to avoid the well understood threat of their longbows, the terrain caused the two wings of the attack to bunch together in the centre of the field. When they continuing airworthines management exposition within range, the English archers loosed a storm of yard-long steel-tipped arrows. Some of the French were killed outright; but many were thrown from their disabled horses. By the early fifteenth century, the plate armour worn by knights had almost reached its zenith of weight and sophistication. It was proof against most penetrating and edged weapons, but it had a fatal flaw. The armour was so heavy that its unhorsed occupant found it difficult to get to his feet, particularly in the confined and muddy conditions that prevailed on the battlefield of Agincourt.1 Once on the ground, they lay helpless and were slaughtered by the unencumbered English foot soldiers armed with mallets, spikes and daggers. (Some were taken prisoner and then killed.) While the English army lost around 100 men and boys in the battle, the French dead, mostly nobles, ran into many thousands. This bloody episode in a long war-that the English eventually lost-serves to introduce the basic theme of this chapter, namely that defenses designed to protect against one kind of hazard can render their users prey to other kinds of danger, usually not foreseen by those who created them, or even appreciated by those who use them. In short, defenses can be dangerous. This is no less true now-in the age of high-technology systems-than it was at the time of Agincourt. Some Paradoxes The history of defenses, barriers and safeguards abounds with para- doxes, often painful ones. Protective measures can cause harm. Conversely, small doses of a harmful entity can provide long-lasting protection, as in vaccination or inoculation. A parallel to inoculation in the world of hazardous technologies is the opportunity to reform the system afforded by incidents, near- misses and other free lessons---events that could have been disastrous, but which were not on that occasion. To some degree, systems can be 'vaccinated' against organizational accidents by learning more about the strengths and weaknesses of their defenses from these close shaves (see Chapter 6). This chapter considers some of these defence-related ironies and paradoxes as they apply to systems that are vulnerable to organizational accidents. At no point will it be argued that defenses are intrinsically bad. There is no doubt that, in absolute terms, the pro- vision of redundant and diverse defenses has greatly reduced the numbers of adverse events. They have, however, radically changed both the nature of the accidents that do happen and the character of the systems they protect-most particularly, they have transformed the relationship between the system's human and technical components. It is essential, therefore, that those who manage and operate complex technologies should appreciate both the advantages and the dangers of their multi-layered defenses. Automation: Ironies, Traps and Surprises One of the ways in which system designers have sought to reduce what has come to be known as the 80:20 problem-the common finding that around 80 per cent of accident causes are due to human failures and only 20 per cent to technical failures-has been to pro- vide ever more automation at the human-system interface. Of course, the wish to distance fallible human beings from the control loop is not the only reason for this rapid increase in automation. A powerful incentive has been the availability of cheap computing power over the last 20 years, and the use of such leading edge technology can offer considerable commercial advantages. When the European consortium, Airbus Industrie, was formed in the early 1970s, Europe's share of the world commercial jet transport . market was close to zero. In order to compete with the dominant US manufacturers, Airbus resolved to use the latest available technology in the design of automated cockpits. Although other manufacturers such as Boeing and McDonnell Douglas followed suit, Airbus Industrie continue to pioneer the division of labour between pilots and computerized flight management systems. Beating the competion required an aggressive policy of being different-particularly in the lengths to which they were prepared to go in order to make aircraft less reliant upon human control. Their radical philosophy of flight-deck automation gained them a substantial place in the jet transport market, but it also created a fresh crop of human factors difficulties. These problems are by no means unique to Airbus air- craft, or indeed to aviaHon.2 Before looking in detail at these aviation problems, it is worth considering in general terms what the British engineering psychologist, Lisanne Bainbridge, has aptly called 'the ironies of automaHon'.3 • By taking away the easy parts of a human operator's task, automation can make the difficult parts of the job even more difficult. • Many systems designers regard human beings as unreliable and inefficient, yet they still leave people to cope with those tasks that the designer could not think how to automate-most especially, the job of restoring the system to a safe state after some unforeseen failure. • In highly automated systems, the task of the human operator is to monitor the system to ensure that the 'automatics' are work- ing as they should. But it is well known that even the best motivated people have trouble maintaining vigilance for long periods of time. They are thus ill-suited to watch out for these very rare abnormal conditions. • Skills need to be practiced continuously in order to preserve them. Yet an automatic system that fails only very occasionally denies the human operator the opportunity to practise the skills that will be called upon in an emergency. Thus, operators can become deskilled in just those abilities that justify their marginalized existence. • And, as Bainbridge pointed out, 'Perhaps the final irony is that it is the most successful automated systems with rare need for manual intervention which may need the greatest investment in operator training'. Although automatic control systems and their problems are not unique to aviation, it is the most extensively studied domain since it yields the best accounts of automation-related incidents and accidents-thanks to on-board flight data recording. In addition, flight management systems represent some of the most advanced forms of automation currently available. For these reasons-and because many of the problems found in aircraft are common to other technologies-we will focus mainly upon aircraft automation in the remain- designated engineering representative of this section. Flight deck automation has proved to be a mixed blessing. On the one hand, it has greatly simplified the task of horizontal navigation- knowing where you are in relation to the ground-through an electronic map that not only shows the aircraft's position relative to the geography beneath, but displays en route weather as welL Pilots regard this as one of the most useful features of 'glass-cockpit' aircraft. On the other hand, there have been many problems with height changes or vertical navigation, some of them leading to fatal accidents.4 Vertical navigation is complex because it is controlled by a combi- nation of elevator and engine thrust inputs. A Flight Management System (FMS) offers at least five ways for changing altitude, each with different levels of automation. Some of these modes will be selected by the pilot, others will come into play automatically. In one mode, for example, the flight management system will control the aircraft's vertical speed; in another, it will maintain a given flight path angle. A further com- plication is that the flight management system will automatically switch modes according to the local situations For example, if the pilot had selected a target height of 5000 feet, the climb mode would remain active only until this altitude is reached. At that point, the aircraft would level off and the flight management system would automatically switch to the 'altitude hold' mode. In addition, some flight management systems are fitted with 'hard' protection envelopes. These are designed to prevent the aircraft's speed from going outside certain critical values, and from exceeding the airframe's stress tolerances. The term 'hard' means that the flight management system "Yill impose these barriers automatically without the need for pilot input. The idea is that should the pilot erroneously take the aircraft beyond these 'hard' protection limits, the flight management system will switch itself into a 'safe' mode setting. If, for example, an aircraft's speed dropped below a certain critical value on the approach, the flight management system could initiate the 'go around' mode by automatically advancing the throttles. This defensive feature has proved dangerous on a number of occasions when pilots, unaware of a mode transition, have fought with the flight management system for control of the aircraft-sometimes unsuccessfully. The nub of the problem-from a human factors standpoint-is that the flight management system can change modes either as the result of a pilot intervention or according to its own internal logic. It is not surprising, therefore, that pilots can become confused about which mode is active, or what will happen next. In order to keep 'ahead' of the aircraft, flight crews must not only know which mode they are in, but also how the flight management system selects new modes. This is sometimes a tall orqer, particularly when these demands fall, as they often do, during periods of high mental workload. Here then is another irony of automation: flight management systems designed to ease the pilot's mental burden tend to be most enigmatic and attention-demanding during periods of maximum workload-a feature that Earl Wiener, an aviation psychologist at the University of Miami, has termed 'clumsy automation'.6 A recent study at the Massachusetts Institute of Technology examined 184 cases of mode confusion-some resulting in fatal accidents-occurring between 1990 and 1994.7 Of these, 74 per cent were associated with vertical navigation maneuvers. Only 26 per cent were related to horizontal navigation. Mode errors were classified into a number of categories. The largest class (45 per cent of the total) was due to pilots entering data incorrectly or into the wrong mode, which resulted in an unexpected flight management system mode or a surprising mode change. The second largest category (20 per cent) was associated with mode transition problems in which pilot confusion (or an unplanned aircraft deviation) occurred because the flight management system executed an unexpected mode transition or failed to perform an expected one. A slightly smaller group of mode errors (18 per cent) arose because the flight crew did not fully understand the automation and consequently either made an inappropriate input-or failed to make a necessary one-creating confusion on their part or an unexpected aircraft manoeuvre. Of these confusions, 14 per cent were due to a breakdown in crew coordination-a sequence of events initiated by one crew member caused others on the flight deck to make wrong assumptions about a mode, a mode transition or a deviation. And, in 12 per cent of cases, a failure on the part of the flight management system created confusion or an unwanted aircraft deviation. These findings reveal yet another irony of automation. Computer- ized control systems intended to remove the possibility of localized slips, lapses and fumbles on the flight deck can increase the probability of higher-level mistakes with the capacity to cause the destruction of the entire aircraft and its occupants. While the mental burdens of horizontal navigation have been greatly reduced, those relating to the more complex and safety-critical task of vertical navigation can, under certain circumstances, be markedly increased. Furthermore, these additional problems involving height changes will occur during takeoffs and landings-the periods of greatest pilot workload. In their detailed study of mode confusions (also known as 'mode errors') in a variety of automated control systems, including aviation, anaesthesia, nuclear power plants and space flight} David Woods and Nadine Sarter have identified a number of human factors problems that apply across all of these domains. Automated systems can: • increase demands on users' memory, • cause users to be uncertain as to where and when they should focus their attention, • make it difficult for users working in teams to share the same situational awareness, • impair mental models of the system, • increase workload during high-demand periods, • limit the users' ability to develop effective strategies for coping with task demands, • increase stress and anxiety, • increase the potential for confusion through enhanced flexibility (i.e., many possible levels and types of automation). Woods and Sarter go on to identify the properties of automated systems that promote these problems: • they can hide interesting events, changes and anomalies, • they possess multiple modes, • they force serial access to highly related information, • they offer a 'keyhole' view of a limitless virtual space, • they contain complex and arbitrary sequences of operations and modes, • they suppress important information about the activities of other team members. In summary, mode confusions can occur for one of two main reasons: either the user makes a wrong assessment of the active mode at a particular time or the user fails to notice transitions in mode status. The first is a failure of perception (or interpretation), the second is a failure of attention. In both cases, the causes can be traced to 'clumsy' automation. In their efforts to compensate for the unreliability of human performance, the designers of automated control systems have unwittingly created opportunities for new error types that can be even more serious than those they were seeking to avoid. Quality Control Versus Quality Assurance Has the shift from quality control to quality assurance opened a gap in the protection of hazardous technologies? To understand this question, we need to review very briefly the milestones in the history of achieving and measuring quality.9 The ancients-Babylonians, Egyptians, Greeks and Romans-initiated the quality process by establishing the units of weight and dimension. Throughout the Middle Ages, trade and craft guilds set standards for quality as well as for working conditions and wages. With the arrival of the Industrial Revolution in Britain, however, and with the subsequent spread of mass production methods in the United States, making things was no longer the exclusive province of experienced craftsmen. Jigs, templates and molds combined with powered machinery meant that complex manufacturing work could be carried out by relatively untrained people. During this period the main responsibility for monitoring quality fell to the supervisors. Later, following the impact of Taylorism (scientific management) and a substantial increase in worker-to-supervisor ratios, supervisors concentrated more on production issues and left the control of quality to a new breed of overseer, the inspector. The term used to describe this specialist inspection function at the end of the production line was quality control. Working at Bell Laboratories in New York in the 1920s, Walter Shewhart and his team invented Statistical Process Control (SPC). A crucial feature of this technique was that it required quality measurements to be made at the point of manufacture rather than at the end of the line. SPC was later adopted by W. Edwards Deming in the 1950s and taken to Japan. The following passage describes the situation, as Deming found it, in post-war Japan:l0 In management, Japan also lagged behind, using the so-called Taylor method in certain quarters .... Quality control was totally dependent on inspection, and not every product was sufficiently inspected. In those days, Japan was still competing with cost and price, but not with quality. It was literally still the age of 'cheap and poor' products. Deming developed the Plan-Do-Check Action Cycle to guide continuous quality improvement. The general tendency in an action-oriented society is to focus on the' doing' part of the cycle and to skimp on the planning and checking phases. In the 1970s, Deming re-imported these ideas to the United States. Other quality 'gurus', like Joseph Juran, Armand Feigenbaum and Kaoru Ishikawa, were also having a profound effect upon the beliefs and attitudes of American managers. The outcome was the Total Quality Management (TQM) movement that has subsequently spread throughout the industrialized world. A key feature of TQM is that everyone in a company shares the responsibility for quality. Quality was not something to be 'controlled' at the end of the line by inspectors, but something that had to be 'assured' throughout the entire work process, from top management decisions to the individual actions on the shopfloor. Hence, we now speak of quality assurance rather than quality control. Quality, in other words, can only be 'engineered' into a task, not 'inspected' into it. So much for the background. Now let us see what effect this development has had upon one particular activity-aircraft maintenance -where quality and safety are inextricably linked. It must be stressed, however, that the discussion below is relevant to all work domains in which modem quality assurance techniques are being used, and where the quality of work has a direct impact upon system safety. In Chapter 2, we examined the case of the missing engine rotor drive covers that led to a near-disaster. In his report, the accident investigator noted a number of similarities between the precursors of this event and those of other British maintenance-related accidents. Two common features in these incidents were, first, that a senior engineer 'signed off' his own work as being satisfactory when it was not; and, second, no member of the Quality Assurance department was available during night-time hours to assess the adequacy of working practices. In the absence of a separate inspection, a suitably authorized individual must sign or place his personal system-theoretic accident model and processes against a declaration that the work has been performed to a high standard of quality. This represents the only 'proof' that the job has been completed, and a full set of such stamps or signatures is required to show that an aircraft is fit to fly following maintenance work. Are such quality assurance (QA) measures a sufficient guarantee of the airworthiness of an aircraft? These incidents suggest that they are not. The lack of a separate inspection plus the absence of QA staff allowed unfinished or wrongly executed jobs to be signed off as fit for service. One important lesson to be drawn from these events is that we need to be cautious about defensive developments designed to im- prove-or at least ensure-quality and safety when they can also serve other goals, unrelated or even inimical to quality and safety. Let us consider, for a moment, what else QA offers to both the organization and the individual. From a company perspective, the abolition of an independent inspection function (for most jobs, at least) confers at least two commercial advantages. First, it removes the often lengthy delays associated with separate inspections. Second, it allows more of the workforce to be engaged in obviously productive 'doing' work as opposed to 'checking' work. At an individual level, QA practices offer the hard-pressed engineer a path of least effort: namely, to sign off on task steps-either before or after the event-without actually monitoring the quality of the work. One of the enduring findings of work psychology is that people will be tempted to take short-cuts whenever such opportunities present themselves. It is no accident, therefore, that 'signing off without checking' is one of the more common procedural violations to be found in aircraft maintenance. It is not being suggested that either the organization or the individual is deliberately cheating on quality or safety. Indeed, they are both likely to declare their genuine commitment to such goals, and point to their adoption of the latest quality assurance techniques as clear evidence of this. In· a continually evolving discipline like air- craft engineering it is hard to appreciate that not all changes are for the better. While the emperor's new clothes may not have left him entirely naked, he could well have been stripped of an important garment. Working life is very complicated. We are subject to many forces, not all of which are pushing in the same direction at the same time. As we have seen before, conflicts between production and protection pressures tend to be resolved in favour of the former-at least until a bad accident occurs. Writing Another Procedure All organizations suffer a tension between the natural variability of human behaviour and the system's needs for a high degree of regularity in the activities of its members. The managers of hazardous systems must try to restrict human actions to pathways that are not only efficient and productive, but also safe. The most widely used means to achieve both goals are written procedures. But there are a number of important differences between the procedures for production and those for protection. Although by no means immutable, the procedures designed to ensure efficient working tend to arise fairly naturally from the nature of the productive equipment and the task to which it is put. Safe operating procedures, on the other hand, are continually being amended to prohibit actions that have been implicated in some recent accident or incident. Over time, these additions to the 'rule book' becoming increasingly restrictive, often reducing the range of permitted actions to far less than those necessary to get the job done under anything but optimal conditions. Figure 3.1 illustrates this shrinkage of allowable action as it occurs over the history of a given system. This could be a chemical process plant, a railway, an aircraft operating company--or any hazardous technology at risk to organizational accidents. The space between the shaded areas represents the scope of prescribed action. As time passes, the organization inevitably suffers accidents and incidents in which human actions are identified as contributing factors. After each event, the procedures are modified so as to proscribe these implicated actions. As a consequence, the scope of allowable actions gradually shrinks to a range that is less than that required to perform all the necessary tasks. The only way to do these jobs is to violate the procedures. For example, the British Rail Rule Book prohibits shunters (the people who join up the wagons and carriages that go to form a train) from remaining between wagons during easing up-that is, when a set of wagons are being propelled by a pilot engine towards some stationary wagons to which they need to be attached.12 Only when the wagons are stopped can the shunter go between them to make the necessary coupling. Sometimes, however, the shackle for connecting the wagons is too short to be coupled when the buffers are at their full extension. The job can only be done when they are momentarily compressed as the wagons first come into contact. The best way of making this connection is by remaining between the wagons during the easing up process. Shunting is a dangerous job. Many shunters die as the result of being caught between the buffers or by falling under the wheels. Just the act of violating the Rule Book does not kill them. There is adequate clearance between both the buffers and the wagons. What sometimes proves fatal is the subsequent error that is committed while the shunter is violating the Rule Book by being between the wagons. He could be momentarily distracted and unwittingly place himself between a set of buffers, or could slip and fall under the wagon wheel. A violation plus an error is frequently a formula for disaster in hazardous work However, the mere repetition of these accident-implicated actions in isolation does not usually bring about a bad outcome. Their contribution to an earlier event was merely one cause-necessary, perhaps, but hardly ever sufficient-among a complex interaction of latent conditions, local triggers and other active failures. Because this combination of factors rarely, if ever, recurs in precisely the same form, the workforce quickly learns that these isolated violations generally carry no penalty. Indeed, they often discover that they lead to an easier and more efficient way of working. Ironically, then, one of the effects of continually tightening up safe working practices is to increase the likelihood of violations being committed. The scope of permitted action shrinks to such an extent that procedures are either violated routinely, or on those occasions when operational necessity demands it. Whereas errors arise from the underspecification of various mental operations, many violations are created by procedural overspecification.14 But that is not the end of the story. Although the commission of these isolated forbidden acts may be mostly inconsequential, the fact that they were implicated in some past event indicates that they can increase the risks for their perpetrators, even though they do not usually have a bad outcome. One of the ways in which violations increase risk is by making a subsequent error more likely to happen. Driving a car at 110 mph, for example, is not in itself sufficient to cause an accident. However, since this speed will almost certainly be higher than the driver's norm, there is a greater chance of misjudging the braking distance or the vehicle's handling characteristics. Since the margins for tolerating such errors are now considerably reduced, the error is less likely to be forgiven. Violations can thus have two consequences. They can increase the probability of a later error, and they can also increase the likelihood that it will have a bad outcome. The important issue in many hazardous technologies is not whether to violate, but when to violate-or perhaps, more importantly, when to comply. When proscribed actions are necessary in order to get the job done, then the rules will be violated. Nearly all hazardous operations involve making actions that lie outside the prescribed boundaries yet remain within the limits of what would be judged as acceptable practice by people sharing comparable skills. Most experienced work- ers know approximately where the I edge' between safety and disaster lies and do not exceed it, except in extreme circumstances. What they do not always appreciate, however, is where they currently are in relation to that edge. There are times when it is prudent to retreat within the narrow boundaries of permitted action or even to stop operations altogether. Deciding when to do this, though, is a matter of delicate and sometimes fallible judgement. Causing the Next Accident by Trying to Prevent the Last One Accident investigators are required not only to establish the causes of an event but also to recommend measures that will help to prevent its recurrence. Often, these preventative measures take the form of engineering or regulatory 'fixes' designed to overcome a particular problem or systemic weakness that featured conspicuously among the causal factors. Unfortunately, these very same 'fixes' sometimes playa major part in causing some subsequent accident. Two exam- ples will illustrate this point. Following a number of air accidents in the 1950s in which aircraft crashed on takeoff from slushy or otherwise contaminated runways, a new flight deck instrument was introduced-the takeoff monitor.IS This directed the pilot to adjust the aircraft's angle of climb to match the position of a target marker shown on the instrument. The pilot manipulated the throttle and elevator controls until a cursor-indi- cating the aircraft's current climb angle-coincided on a vertical scale with the target marker. It was a relatively novel 'command instrument' that calculated the appropriate climb angle for the current conditions and then 'ordered' the flight crew to match this target angle by tracking the aircraft cursor up the scale. Shortly after the introduction of the takeoff monitor, an aircraft adopted an unusually steep angle of climb on takeoff.16 Unable to sustain it, the aircraft stalled and crashed back on to the runway. The accident investigators subsequently found that a small retaining screw had failed within the takeoff monitor, causing it to 'command' a wholly inappropriate angle of climb. Despite the fact that their remaining flight instruments were intact, the pilots obeyed the faulty indicator. As discovered elsewhere, instruments that do some of the pilots' thinking for them have a way of capturing attention-the problem was termed 'fascination' at the time -even though alternative, but less easily processed, sources of information are available on the flight deck. The second example begins with a 'sentinel event' in the history of nuclear power generation. At 0400 on 28 March 1979 a turbine in one of Metropolitan Edison's two pressurized water reactors (PWRs) on Three Mile Island-near Harrisburg, Pennsylvania-stopped (tripped) automatically. IS The cause was the leak of a cupful of water through a faulty seal during maintenance work. The moisture interrupted the air pressure applied to two valves connecting to the feedwater pumps and caused them to shut down automatically. This, in turn, cut the water flow to the steam generator and tripped the turbine. Without the pumps working, the heat of the primary cooling system-circulating through the reactor core-could not be transferred to the cooler water in the secondary circuit. At this point, hundreds of alarms and annunciators continuing airworthines management exposition on in the control room and the emergency feedwater pumps started up, again automatically. These were de- signed to pull water from an emergency storage tank and run it through the secondary cooling system to compensate for the water that boils off once it is not circulating. The emergency lasted for 16 hours and resulted in the release of small quantities of radioactive material into the surrounding atmosphere. The operators on duty at the start of the event made a number of well documented errors, but the one that most concerns us here was the cutting back of the high-pressure injection of water into the reactor coolant system. This reduced the net flow rate from around 1000 gallons per minute to about 25 gallons per minute. This 'throttling' caused serious damage to the reactor's core. It is not necessary for us to go into the reasons why this action was taken-they have been discussed at length elsewhere. What concerns us are the measures taken by the United States Nuclear Regulatory Commission to prevent the recurrence of this step in any future nuclear power plant event. Following Three Mile Island (TMI), the US nuclear regulators made it a requirement that operators should not cut back on the high- pressure injection during the recovery stages of an off-normal event. Three years later, a control room crew at another PWR, this time at Ginna,19 were faced with an emergency in which reducing the high- pressure injection would have been an appropriate step to take. Well aware of the post-TMI regulatory restriction on this action, they did not take it. As a result, the emergency was prolonged by several hours. These examples illustrate some of the potential dangers that can ensue from what, on the face it, appear to be perfectly sensible at- tempts to 'fix' the causes of previous accidents. There are two main problems. As the case of the takeoff monitor showed, introducing new engineered defensive features adds complexity to the system. In particular, it adds components which themselves can fail. Moreover, in this case as in the more modern varieties of automation, attempts to distance pilots from the direct control loop can create unforeseen types of human error. The second problem, illustrated by the Ginna event, is that regu- lators-just as much as system designers-cannot foresee all the possible scenarios of failure in complex, tightly-coupled and highly interactive systems such as nuclear power plants, and so cannot universally proscribe particular types of human response. What proved to be an error in the TMI event turned out to be a vital step at Ginna. As we shall see in Chapter 4, regulations and procedures share with other feedforward control devices the problem of being insensitive to local conditions. Defenses-in-Depth: Protection or Dangerous Concealment? The philosophy of defenses-in-depth is summarized in Figure 3.2. Here, safety depends upon the causal independence between errors and technical faults affecting the process equipment, the normally inactive engineered protective systems, the physical barriers against unplanned release of dangerous substances and the proximity of the likely victims to the bad event. Major accidents occur when this assumption of mutual independence between the various layers of defence is violated. During the Bhopal disaster, for example, three supposedly independent defenses failed simultaneously: a flare tower to burn off the deadly methocyanate gas, a scrubber to clean air emissions and a water sprinkler system to neutralize the remaining fumes. Defenses-in-depth are thus built upon redundancy (many layers of protection) and diversity (many different varieties of protection). However, it is these very features-which are highly desirable from an engineering standpoint-that also create a variety of problems in complex sociotechnical systems-to such an extent that Jens Rasmussen, engineer and leading philosopher of technology, has coined the phrase 'the fallacy of defenses-in-depth',21 One of the main problems that defenses-in-depth pose for a sys- tem's managers and controllers is that they can conceal both the occurrence of their errors and their longer-term consequences. A characteristic of such defenses is that they do not always respond to individual failures. These can either be countered or concealed, and in neither case need the individuals directly concerned be aware of their existence. This feature allows for the insidious build-up of the latent conditions-resident pathogens-that may subsequently com- bine with local conditions and 'sharp-end' errors to breach or bypass the defensive layers. A system with few defenses-driving a car, for example-does not normally remain so impassive in the face of accumulated failures. Mistakes are immediately detectable and the subsequent learning will limit their recurrence. This does not necessarily happen with multi-layered defenses. A dangerous penalty of this concealment is that neither individuals nor organizations can easily profit from their errors. This penalty is even more likely to be exacted when those who design or manage such a system are more inclined to attribute its occasional catastrophic failures to individual fallibility than to intrinsic system weaknesses. In 1984 Charles Perrow, an organizational theorist, published a highly influential book, Normal Accidents: Living with High-Risk Technologies, in which he advanced the bleak proposition that accidents are inevitable in complex, tightly-coupled systems like nuclear power plants-regardless of the skills of their operators and managers.22 Hence the title: accidents in such systems are 'normal'. According to Perrow, the redundancies that go to make up defenses-in-depth have three dangerous features. • Redundant defensive back-ups increase the interactive complexity of high-technology organizations and thus increase the likelihood of unforeseeable common-mode failures. While the assumption of independence may be appropriate for purely technical breakdowns, human errors at the 'sharp end', in the maintenance sector and in the managerial domains are uniquely capable of creating failures that can affect a number of defensive layers simultaneously. • Adding redundancy makes the system more opaque to the people who nominally control and manage it. Undiscovered errors and other latent problems accumulate over time and thus increase the likelihood of the 'holes' in the defensive layers lining up to permit the passage of an accident trajectory (see Figure 1.5). This alignment of the gaps can be created either by interactive common-mode failures or by the simultaneous disabling of supposedly independent defenses, as at Chernobyl. • As a consequence of this dangerous concealment, and because of their obvious engineering sophistication, redundant defenses can cause system operators and managers to forget to be afraid. This false sense of security prompts them to strive for even higher levels of production (see Chapter 1). As Perrow put it: 'Fixes, including safety devices, often merely allow those in charge to run the system faster, or in worse weather, or with bigger explosives.' False Alarms To those who have been enraged by the repeated wailing of a neigh- bour's car alarm while the vehicle stands undisturbed, the idea that alarms can lie will come as no surprise. In everyday life these false alarms can create intense irritation. In hazardous technologies they can cause disaster. It is the 'cry wolf' situation. Frequent false alarms cause people to lose trust, and to ignore or disbelieve warnings when they signal genuine emergencies. At just after 4.00pm on Sunday, 18 June 1972, a British European Airways Trident passenger aircraft (Papa India) took off from Lon- don Heathrow for Brussels and crashed 120 seconds later in a field on the outskirts of Staines.23 There were no survivors. This was the worst air disaster to occur within the British Isles and the first fatal accident involving a Trident in normal airline operations. It also remains one of the most mysterious. Although an intact Flight Data Recorder (FDR) revealed exactly what happened to the aircraft, we have no certain knowledge of how it happened or why it happened, since there was no Cockpit Voice Recorder (CVR) aboard at that time. Unusually, there were four pilots on the flight deck. A senior, 51- year-old BEA captain was in the left-hand seat, a 22-year-old junior pilot was in the right-hand seat, and behind and between them was a somewhat more experienced 24-year-old pilot whose task was to monitor the instruments and the performance of the other two. Fur- ther back, in the jump seat, was another BEA captain. The action that initiated the disaster is not in dispute. At around one-and-a-half minutes into the flight, the Trident's droops, mounted on the leading edges of the wings, were retracted prematurely at too Iowan air- speed and the aircraft entered an incipient stall. We do not know who moved the control or why it was done. The Trident was fitted with a number of stall warning systems. In addition to audio-alarms, there was also a 'stick-shaker' warning system and a pneumatically-powered 'stick pusher' stall recovery system. The latter automatically disengages the autopilot and lowers the nose of the aircraft in order to recover from the incipient stall. There was also a lever on the left side of the central control pedestal that could disable the stick-shaker warning system. The flight data recorder evidence revealed that both the stick-shaker and the stick-pusher mechanisms continuing airworthines management exposition into playas soon as the aircraft approached the stalling speed. The aircraft's nose pitched down, but with the elevator trim unadjusted, the aircraft had become tail heavy. As a result the nose pitched up again and eight seconds after the first stick push, the stall recovery system operated a second time. Three seconds later, the same pattern repeated itself, and once again the stick-pusher forced the nose down. At this stage, someone on the flight deck made a fatal error. The stick warning mechanism was turned off. The nose of the aircraft pitched up in excess of 30 degrees and entered a true stall. It then descended vertically and hit the ground 22 seconds later. Why was the stick-shaker mechanism disabled on its third activa- tion? We will never know for sure, but it is widely believed among the British commercial pilot community that the crucial reasons were, first, that the flight crew did not believe they were in a stalled state (probably because they were unaware of the droop retraction), and, second, they profoundly mistrusted the stick-shaker warning system. In short, they thought it was indicating falsely. There had not only been several false alarms in the development stages of the recovery system some years earlier but also a number of documented incidents involving the false activation of this system during line operations. Deliberate Weak Links One way of limiting the damage caused by an accident or a technical breakdown is to design deliberate points of weakness into a system so that it will fail in safer and more predictable ways. The fuses in an electrical circuit are a commonplace example of this principle. Should a circuit become overloaded, it will 'blow' at the fuse wire rather than within an appliance, thus minimizing the risk of a fire breaking out in some unattended location. A similar principle is used in the construction of large commercial aircraft whose engines are suspended beneath the wings by pylons. In a typical Boeing 747, for example, each engine pylon contains two kinds of fuse pin (or strut): an upper link fuse pin and an aft diagonal brace fuse pin. If an undue force is applied to the engine and its pylon, the assembly is designed to break away cleanly at the fuse pins, thereby avoiding the possibility of tearing off large pieces of the wing structure. An aircraft will survive a lost engine a good deal better than losing a substantial portion of its lift-producing structure, the wing. While fuse pins may have served their intended function in the early days of suspended jet engines, their unwanted failure has recently been implicated in a number of serious accidents and incidents, most notably at Schiphol in 1992.24 Quite often, these fuse pin failures have been due to faulty installation or improper fastening during maintenance work. On 1 March 1994, a Northwest Airlines (NWA) Boeing 747-251B dropped and then dragged its Number 1 engine during the landing rollout at New Tokyo International Airport, Narita.25 The immediate cause of this accident was the fracture of the upper link fuse pin within the engine pylon. This, in turn, was due to the failure(s) to secure the fastenings of the aft diagonal brace fuse pin, causing it to come free some time after a major overhaul check at NWA's Minneapolis/St Paul maintenance facility. The engine had been opened at that time to permit the non-destructive testing of the vari- ous fuse pins. The appropriate fastenings were later found in the hangar in an unmarked cloth bag concealed by a piece of wooden board on an underwing workstand. This accident illustrates how the measures taken to avoid one safety problem can contribute to another. The migration of fuse pins from B-747 engine pylons had been reported on five occasions prior to the NWA Narita accident. One had resulted in an accident similar to that at Narita. All were attributed to improper installation-though design defects must surely have played their part. These incidents led Boeing to require the addition of secondary fuse pin retainers. At the time of the accident, seven of NWA's fleet of 41 B-747s had these secondary retainers installed on the aft diagonal braces. It is likely that this variety of fastenings contributed to the misunderstandings and procedural confusions that subsequently allowed the installation errors to go undetected. Safety devices, like the addition of any other component, increase the complexity of a system, particularly during maintenance. As such, they create additional opportunities for human failure of an especially safety-critical kind. Once again, then, we have an instance of a safety feature becoming a point of vulnerability. The main hazard facing an engine in a modern aircraft is not so much excessively high loading as an excessive amount of contact with the human hand. Summary In this chapter we have identified at least six ways in which defenses might be dangerous: • Defensive measures designed to reduce the opportunities for a particular kind of human error can relocate the error opportunities to some other part of the system, and these errors may be even more costly. • Gains in defenses are often converted into productive, rather than protective, advantage, thus rendering the system less safe than it was before. • Defenses-in-depth, based upon redundancy and diversity, make the system more opaque to its operators, and hence allow the insidious build-up of latent conditions. • Warnings and alarms that acquire a reputation for indicating dangers where none exist are less likely to be acted upon in the event of a true emergency. • Measures designed to eliminate a conspicuous cause of some previous accident can contribute to the next one. • Defenses, barriers and safeguards add additional components and linkages. These not only make the system more complex, they can also fail catastrophically in their own right.