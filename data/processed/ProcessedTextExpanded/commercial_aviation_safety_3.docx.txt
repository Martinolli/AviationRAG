CHAPTER THREE • Describe what the investigation of human error entails. • Describe what the management of human error entails. • Describe the difficulties associated with ensuring that flight crew are both physi­ologically and psychologically healthy. INTRODUCTION Is commercial aviation special? Yes, but only in the same way that other high-con­sequence businesses are special. By that we mean that there are some professions where relatively small mistakes can have very large negative consequences. For example, an otherwise innocent distraction of a commercial airline flight crew right before landing, such as from a laser or bird, could cause the pilots to undershoot the landing and touch down just prior to the runway. Implications from this could include the death of hundreds of passengers and paralysis of operations at a major airport for hours. Professionals working in commercial aviation, like those who operate nuclear power plants, aircraft carriers, or high-speed trains, cannot afford to make small mistakes because of the potential for huge negative consequences. When such mistakes make it through the defenses we have created, terrible disas­ters can occur. So in this sense, yes, commercial aviation is special. As indicated, commercial aviation is considered a high consequence environment because even small mistakes can be disastrous. Some refer to those types of businesses as being conducted by high-reliability organizations, high-risk indus­tries, or high-consequence operations, where all terms are expressions of the same sentiment. When considering how small errors trigger large problems, it becomes clear why commercial aviation operations require very special safeguards to protect against the potentially terrible outcomes from minor mistakes. It also becomes evident why an entire chapter of this book is dedicated to the challenge of dealing with human fallibility. It is an ever-present condition that follows us as a shadow through life, rearing its ugly head all too often to remind us that as humans we are very prone to error. This chapter explores humans as a major challenge in commercial aviation. All the technical knowledge in the world does not guarantee a successful outcome to a complex, technical situation. Whenever people are part of the equation, we rely on procedures, communication skills, teamwork, experience, and extensive training to properly use any technical knowledge to produce an acceptable outcome. Sometimes it all goes wrong-terribly wrong. Philosophy Human Error It was a curious sight to behold at the beach. A young osprey, which is a type of bird, hovered approximately 30 feet above and just offshore. Ospreys are among the supreme hunters of the avian world. Scanning the water below for fish, its head tracking back and forth, back and forth like a radar dish; the bird resembled noth­ing so much as a miniature jet fighter scanning the skies for the enemy. Suddenly its gaze locked on a target. Furling its wings, it swooped down toward a fish like a fighter pilot diving into combat. Usually, an osprey's dive terminates in a stunningly graceful display of airmanship as the plunge is broken and the prey captured with a fierce piercing of talons. This time, however, the attack did not go as planned. As the descending bird neared the water, a sudden flutter of wings altered the flight path radically just prior to the point where the talons should have sunk into the prey. Perhaps the abrupt maneuver was due to the osprey seeing a more desirable fish or a menacing shark; perhaps it was a reaction to a last-second change in the prey's position. Regardless of the reason, instead of ending with an elegant talon-grab, the modified approach resulted in a spectacular semi-controlled crash into the water. After impact, the stunned hunter paused momentarily to gather its senses, then slowly flapped its wings, struggled free of the water, and took flight while letting out some pained squawks of avian profanity, reflecting no doubt its confused mental state. With a bit of anthropomorphic imagination we can hear the osprey flying away and thinking, "Well, that approach was terrible! What happened? Everything was looking good and then suddenly, crash. Lucky I could fly away from it. I really need to stick to stabilized approaches or go around! “ We doubt the osprey safety community convened an accident investigation board, submitted an anony­mous pilot error report, or held a safety standdown; but it is likely that the osprey learned from the incident. If not, the future bodes ill for survival because the mortality rate of young raptors is very high, and only the fittest and best adapted survive. Consider now the story of a human rather than that of the avian hunter. Near the beach where the osprey crashed into the sea there is a drawbridge-popular with fishermen-that spans the inland waterway. Above the bridge, an electrical line runs parallel from the mainland power generating station, across the water­way, and to the beachside barrier island. For most of this stretch, the power line is 40 feet or more above the low-lying bridge. Where the bridge rises toward its draw above the waterway channel, however, the distance between the bridge walkway and the power line diminishes to about 10-12 feet. This distance provides a sig­nificant inconvenience for fisherman casting lines from the rise of the draw into the waters of the channel. And, of course, fishermen prefer to fish from the draw rather from the lower bridge because the channel is where the big fish lie in wait. As shown in Figure 3-1, the power line in the area of the bridge's draw is cov­ered with bobbers, hooks, lures, sinkers, bits, and pieces of line-the remains from countless miscasts. In one sense it is a small, yet notable, monument to human fal­libility and to the repetitive nature of error. Imagine the next angler who challenges the power line. Prepping the gear to catch a trophy fish from atop the draw, the angler says to his fishing buddy, "Look at all those lures up there. I wish I could get up there and cut me some down for my tackle box." Moments later, the angler casts his line and yells out, "Oh no, I just got caught up in the power line! I can't believe it. What bad luck!" The angler's problem with the power lines was not a matter of bad luck. He knew-or should have known-that a threat existed. The story of the frustrated angler reminds us that every time our personal performance does not meet our expectations, we are presented with an opportunity for improvement. Seizing such opportunities constitutes what has been called embracing our blunders. Similarly, when we avoid blunders by learning from the missteps of others who have preceded us, we are embracing the blunders of others. Some people simply dismiss missteps as examples of human fallibility. Others may take the cow­ardly position of covering up their actions or redirecting blame instead of learning from their embarrassing missteps. It takes courage to foster improvement by actively seeking out and embracing shortfalls. Nonetheless, we must not fail to do so. In commercial aviation safety we must constantly strive to recognize and analyze our errors and those of others. In doing so, we prepare ourselves to recognize devel­oping accident chains and the opportunity to intervene before they result in tragedy. Who Is The Ace? What does an aviation professional look like today? What does it take to add to the safety value chain that over the past quarter century has produced the lowest accident rates seen in commercial aviation history? Let us first focus on the flight crew for a moment, while realizing that similar changes have occurred for other professions within commercial aviation. Ponder the following question. How would you define an excellent pilot? Is it someone with a type-A personality, muscular physique, and a former high school football captain, who can pull on the yoke to recover from a steep dive while barking orders at his crew? There are still a few remaining pockets around the world where such an image holds true. For the most part, however, the past few generations have witnessed a significant evolution in the desired traits sought in pilots. We have witnessed a subtle, but persistent, transformation in the description of an expert pilot. While pilot actions and decision making used to suffer from a lack of informa­tion, today pilots have to sift through a multitude of resources and information when they make decisions. The very nature of the profession has been redefined. Intuition and luck have given way to knowledge and procedural discipline. How pilots manage the information avalanche determines the outcome of a flight, and a result defines their worth as aviators. Today, the new vision of an excellent pilot is someone who can coordinate the actions of numerous highly trained team players while mastering the technical intricacies of a complex system, knowing which resource to tap from an extensive collection, all with grace and a level head in a time-sensitive environment. During the early days of aviation, airmail pilots had to create a mental model of what was happening based on miniscule amounts of data, many of which were unreliable. Some would use hand-scribbled descriptions of airfields and surrounding terrain from previous flights, determine wind direction by noticing the direction that cows were facing, or estimate how much fuel was remaining in the tanks by trying to remember what time it was when they departed on their flight. The professionals on the flight decks of commercial aircraft today increasingly work in an all-glass, computer-enhanced cockpit. Often through the use of electronic tablets, they can summon weather observations and forecasts for numerous locations simultaneously, weight-and-balance calculations, notices about taxiway closures, navigation transmitter statuses, and computerized flight plans from highly competent dispatchers, reference updated procedural guidance and use flight deck instruments to receive data-linked air traffic clearances and weather radar depictions. All this information helps create a mental model of the operating context, but only if properly presented and interpreted. Can too much information be just as detrimental to the safety of the flight as insufficient information? The definition of an expert pilot today is not one who can make accurate decisions without proper information. Rather it is one who knows how to seek out the right type of accurate information and apply it at key moments, all while operating the aircraft at peak efficiency through automated flight control systems while fostering outstanding teamwork among the crew. Figure 3-2 depicts the appearance and features of a modern flight deck. The access to information is unrivaled and so is the need to know where to go and what to do with the information as a pilot. Similar technological advances have required other professions that impact the safety value chain, such as dispatchers and aviation maintenance technician, evolve in order to make decisions in an infor­mation-rich environment. Technology may be changing, but the need for so-called soft skills remains imperative. It is widely recognized that pilots must possess equal doses of technical expertise and polished human interaction skills. The increasing complexity of technology, airspace, regulations, and procedures mean that pilots must have complete knowledge of their realm. Automation produces gain in efficiency and situational awareness, but only if the pilots who use it are proficient in how it operates during both normal and emergency situations. Additionally, pilots must possess human interaction skills, including effec­tive communication, leadership, and followership. Such soft skills sound intuitive but are actually very complex, requiring professionals to combine an open mind for learning new concepts with the commitment to putting crew communication and coordination into practice, as will be the focus of the next chapter in this book. Some of the challenges faced by aviation professionals today remain the same as those a century ago, such as fatigue, dehydration, and schedule pressures. Yet there are new challenges that must also be addressed, such as information overload, interfacing with complex automation, and high density of air traffic. The next section provides an introduction to such factors. HUMAN FACTORS The people who operate and support the U.S. aviation system are crucial to its safety; the resourcefulness and skills of crewmembers, air traffic controllers, and aviation maintenance technicians help prevent countless accidents each day. However, despite the excellent safety record, many studies attribute human error as a significant factor in most commercial aviation accidents. The leading human-factors theorists and modern researchers believe that between 70% and 80% of all aviation accidents are attributable to human error somewhere in the chain of causation. Modern aviation safety theory at present is heavily focused on trying to understand the human decision-making process. It aims to understand how humans react to operational situations and interact with new technology and improvements in aviation safety systems. The way in which human beings are managed affects their attitudes, which affects their performance of critical tasks. Their performance affects efficiency and, therefore, the eco­nomic results and safety of the operation. It is important to understand how people can be managed to yield the highest levels of error-free judgment and performance in critical situations, while at the same time providing them with a satisfactory work environment. A review of cockpit voice recordings from accidents clearly indicates that distractions must be minimized, and strict compliance with the sterile cockpit rule must be maintained during the critical phases of flight (taxi, takeoff, approach, and landing). The sterile cockpit rule, in its different forms, is a ban on no pertinent conversation among members of a flight crew during critical phases of flight to minimize distraction. While emphasis often focuses on the pilots, they are by no means the only ones who should be discussed in the sterile cockpit rule philosophy. The same holds for all human factors-related initiatives. Pilots are, however, the last link in the safety value chain and are usually in a position to identify and correct errors that could otherwise result in accidents and incidents. One problem is poor human decision making. Essentially, three reasons explain why people make poor decisions: • They have incomplete information. • They use inaccurate or irrelevant information. • They process the information poorly. Psychologists have traditionally explained the limited information processing capabilities of humans with Miller's law. George Miller was a cognitive psycholo­gist at Princeton University when in 1956 he published what would be known as his law. Miller's "Magic number" concept states that one can make 7 (plus or minus 2) absolute judgments, or differentiations, along a single dimension. For example, we can differentiate seven sound intensities and seven shades of the color red, plus or minus two in both examples. This idea has been applied to the number of mental objects an average human can hold in working memory. Such an ability is improved when a pilot uses both his visual and auditory channels because the information is processed differently in the brain. Although the reality is much more complicated than that that has been summarized here with regards to Miller's law, the sentiment still holds. Modern research has shown that accidents are more likely to occur during high workload, task satura­tion periods, and when there is an overload of one or more of the pilot's processing channels. To reduce the workload during critical task saturation situations, new pilots are taught a task-shedding strategy to focus on the most important task on the flight deck: flying the plane. In other words, in an emergency, fly the plane first, then if circumstances per­mit, navigate, and finally, if the other two tasks are in hand, communicate with air traffic control. Too many accidents have occurred when pilots prioritized naviga­tion or communication instead of actually controlling the aircraft. There is a com­mon expression in the flight instructing world that emphasizes this hierarchy of priorities: "don't drop the airplane to fly the microphone. Post-accident investigations usually uncover the details of what happened. With mechanical failures, accident data analysis often leads logically to why the accident occurred. Determining the precise reason for human errors is much more difficult. Without an understanding of human behavior factors in the operation of a system, preventive or corrective actions are impossible. Understanding human factors is especially important to systems where humans interact regularly with sophisticated machinery and in industries where human error-induced accidents can have catastrophic consequences. Human factors principles are increasingly folded into design standards in the design of aviation systems, such as the seating and display control interfaces on a flight deck. However, such principles are sometimes not treated as a technology in commercial aviation where technical decisions in aircraft designs, regulations, and operations are primarily based on "hard" sci­ences, such as aerodynamics, propulsion, and structures. Human capabilities do not often lend themselves readily to consistent, precise measurements. Human factors research requires much more time and cooperation than most other aeronautics research. Data on human performance and reliabil­ity are regarded by many technical experts as "soft" and receive little attention in some aviation system designs, testing, and certification. Data used in designs are often after the fact. Aviation human factors is a multidisciplinary science that attempts to optimize the interaction between people, machines, methods, and procedures that interface with one another within an environment in a defined system to achieve a set of systems goals. Aviation human factors encompass fields of study that include, but are not limited to, engineering, psychology, physiology, anthropometry, biomechan­ics, biology, and certain fields of medicine. Human factors science concentrates on studying the capabilities and limitations of the human in a system with the intent of using this knowledge to design systems that reduce the mismatch between what is required of the human and what the human is capable of doing. If this mismatch is minimized, errors that could lead to accidents will be minimized, and human performance will be maximized. By looking at Figure 3-4 we are prompted to contemplate numerous human factors, such as the comfort of the seats on the flight deck, the mental prepared­ness of the pilots to fly, the visibility from the flight deck of the ramp area when looking through the windshield, the ability to find specific knobs on the panels by touch alone, the ease of interpreting the written information in front of the pilot in the right seat, the color-coding of annunciator lights shining in the display panel, the verbal and nonverbal communication occurring between both pilots, and dozens of other considerations. ERROR CHAINS In the first-century AD, the Roman stoic philosopher Seneca often attributed the Latin musing, "errare humanum eastern standard time perseverare diabolicum," meaning that it is human to make mistakes, but it is downright devilish not to do something about that tendency. Throughout the ages, philosophers and safety advocates have wrestled with the notion of human error and have proposed different means for preventing error from occurring and ways to perform damage control once an error is made. Some of the human error initiatives have resulted in prerequisite knowledge, skills, abilities, and psychological standards that are used for selecting and training aviation professionals. Other ideas have been directed at employees who are already exercising their profession. Crew Resource Management (Crew Resource Management ) and Threat and Error Management (TEM) are two of the most recent and widespread expressions for managing error in flight operations and both rely heavily on using the positive synergy of teamwork. From a philosophical perspective, one of the greatest achievements in the aviation safety movement over the last century has been the recognition that accidents seldom occur due to a single cause. According to the theory of accident multicausality, every causal mechanism involves the joint action of a multitude of component causes. Recognition of multi-causality by accident investigators has allowed error intervention methodologies to target a plethora of human breakdowns that typically lead to accidents. As Chapter 2 pointed out, accident investigators dug up an error chain by "reverse engineering" the accident's causes, so to speak. Whenever something of relevance is uncovered, an investigator must probe deeper by asking, "Yes, but why did that occur? "By asking, "Why? Why? Why?" over and over, an investigator can usually uproot most of the causal tree instead of just seeing the obvious branches. Only then can all the shortcomings of training programs, coordinating agencies, and quality-control measures be addressed to prevent future problems. An experienced investigator picks apart each of these causes and asks all the "whys" of each, until a comprehensive picture of the failures is seen and correc­tive actions can be recommended. Each error by itself will not bring down an aircraft, but the likelihood of an accident increases with each error "link" added to the chain. Is "Pilot Error" A Myth? It is almost impossible to open a book on flight safety or Crew Resource Management that does not promptly refer to the high percentage of accidents that are caused due to pilot error. Although the numbers vary depending on which source is quoted, experts and records typically refer to large percentages of aircraft accidents as being caused by pilot error. In 1 999 the Data Acquisition and Analysis Working Group of the Flight Safety Foundation analyzed 287 fatal approach-and-landing accidents involving jet and turboprop aircraft between 1980 and 1996. The study noticed that Crew Resource Management breakdowns were present as circumstantial factors in nearly half of the accidents. The same study looked at 76 approach-and-landing accidents between 1984 and 1997 and found that Crew Resource Management failure was the third most frequent causal factor (63 % ) and the second most frequent circumstantial factor ( 5 8 % ) (Khatwa & Helmreich, 1 999). In yet another example, in 2001 a group of researchers from the Department of Emergency Medicine at Johns Hopkins University School of Medicine ana­lyzed 329 major airline crashes, 1,627 commuter/air taxi crashes, and 27,935 general aviation crashes for the years 1983 through 1996. They found that pilot error was a probable cause for 38 % of the major airline crashes, 74% of the commuter air taxi crashes, and 85 % of the general aviation crashes (Johns Hopkins, 2001). Although some of the depicted statistics refer to human error, versus pilot error, it makes no mistake that the average public consumer of information com­pletely interprets both terms to be synonymous when discussing it in the aviation context. Of course, experienced investigators are quick to point out that using the term human opens up the possibility of analyzing root errors that took place outside of the purview of the flight crew. Such root errors may not have been perceived by the accident flight crew, but helped induce the active errors of the flight crew that immediately preceded the accident. Society often neglects to consider that humans have also had a hand in all the automation that surrounds us. It is incorrect to ask, "Is it a human problem or an automation design problem" since a human had to design the automation. Similarly, it is incorrect to ask, "Was it human error or mechanical failure" if the mechanical failure was due to an inspector failing to catch an existing fracture in a component. Both the pilot and inspector are human, and human error makes no distinction as to the profession being performed by a person. There are times, however, when pilot error truly is a critical factor in an acci­dent. When a series of errors take place that leads to an accident, and all of those errors are created by the flight crew, then it is tempting to assign cause to them and not look at contributing factors such as training or procedures. This is especially the case when the flight crew was properly trained and procedures existed, but mistakes were still made. Stunning statistics exist about the very high percentage of accidents attributable to pilot error. What is truly shocking about such statistics is that the statistics are shocking in the first place. Who continuing airworthines management exposition up with the notion that pilots should not make mistakes? Are pilots somehow supposed to be exempt from human nature? Historically, accidents were often conveniently written off by deeming the cause to be "pilot error" and taking scant further action. The public was relieved to know that a single "bad apple" had created the problem, and after a period of shock and grieving, the aviation world would return to business as usual. In a sense, we treated the symptoms of a disease while permitting the underlying cause to grow unchecked. Such a process may possibly have originated because many of the pilots involved in accidents did not survive to defend their reputation, and some com­panies found it convenient to excuse any possible mismanagement leading to the accident by assigning cause exclusively to the dead pilots. Also, the science of acci­dent investigation only recently developed to the point where root causes form part of the investigation process. Lastly, the general public is usually not educated in the complexities of aviation and seeks simple answers instead of complicated acci­dent causes. One is reminded of the saying, "You want it bad? You'll get it bad!" Aircraft accidents are complex multicausal puzzles that demand time and attention to detail to solve. The first few days following an aircraft accident can be summarized as an amorphous and emotional mass of conflicting eyewitness reports, mis­directed reporters' questions, a tendency to believe initial causal theories because they sound good, and the temptation to oversimplify what happened. To wade through all that chaos, accident investigators must exercise patience and focus on minutiae yet strive to see the big picture. Is calling "pilot error" a myth meant to remove individual accountability from pilots in terms of error management? Absolutely not, it is merely to point out that such characterization is a "cop out" or an excuse for not solving the real root cause problem. Over the past decades several high-profile accidents, such as the loss of the Challenger and Columbia space shuttles, called dramatic attention to the effects of organizational root errors. Some accident scientists fear that the increased emphasis on organizational errors as part of an accident sequence has detracted attention from the phenomena of pilot error. In reality, both root and active errors must continue to be addressed as part of a coordinated attack to reduce accident rates. COGNITIVE ERROR In the past, human factors' analyses of aviation accidents and incidents relied on "loss of situational awareness (SA) " or similar generalized descriptions as a simplistic label and propagated the use of such a catch-all term with great alacrity throughout accident reports. "Loss of SA" and "breakdown in SA" are commonly used umbrella terms that mask in the multitude of cognitive breakdowns that caused the ultimate loss of SA. The use of "loss of SA" as a causal finding for an accident, although technically correct, proves insufficient for exposing where the breakdown in SA occurred. Therefore, aviation training specialists have been unable to develop effective measures for teaching how to prevent the erosion of SA past a certain superficial level. Critics argue that investigators need, " . . . access to the psychological life beneath an error . . . more guidance beyond a cursory analysis . . . if one is to truly understand the genesis of human error" (Wiegmann & Shappell, 2003, p. 1 50). This section of the chapter examines some of the underlying cognitive processes that can lead to loss of SA. "Cognition" is a term that refers to the mental processes generated in the brain and related to thinking. Cognition is defined by the APA Dictionary of Psychology as "all forms of knowing and awareness, such as perceiving, conceiving, remem­bering, reasoning, judging, imagining, and problem-solving" (Vandenbos, 2007, p. 1 87). As such, human cognition lies at the heart of every action taken prior to and during an aircraft's flight. It proves daunting to consider that every aspect of a flight is controlled by a series of loosely networked, complex, error-prone supercomputers that are subject to a single point of failure-our brains! No user manual has ever been written to teach humans how to effectively use our brains. Our reaction to such a depiction may be shocking and outrage, but that is the reality of the situation. We would not dare fly an aircraft or operate other complex equipment without first studying an operator's manual or receiving training, but that is essentially how we approach the use of our brains every day. Accident chains can start forming way back during the design of an aircraft or system while still in the brain of an engineer due to errors in thinking or knowledge gaps. New errors or more errors can then be introduced in the brain of an assembly line worker who manufactures the aircraft and those mistakes, like those of an engineer, may not surface until later, during the life of the aircraft. The brains of numerous support personnel serve as external crew members for any given flight, and their errors directly impact the success of the flight. The brains of training department instructors and chief pilots create procedural guidance and teach pilots how to follow such guidance. The brains of government regulators provide oversight of operations. At the end of the long safety value chain are the brains of the pilots. We must recognize that causal errors in commercial aviation accidents almost always have some link to cognitive processes. Scholars of human factors are quick to point out that benefits reaped from automatic cognitive functions gener­ally come at the expense of control over the processes. Automatic mental modeling and SA construction often rely on deep-seated biases that taint our actions and decisions without us even knowing about it but that occur in order to speed up our cognitive processes. We can conceive of cognitive biases as "thinking shortcuts" that generally work very well in that they help speed up our thought processes. In the case of verbal and nonverbal communication, our brain has been trained since childhood to "fill in the gaps" when information is missing. Additional information on this important area can be found in the online supplement to this chapter. Situational Awareness (Sa) As previously mentioned, statistics show that the majority of aviation accidents are caused by human error. In fact, we would argue that if you expand the investiga­tion of accidents to include contributing factors, as any good accident investigator would, then virtually all accidents are associated with human failure of some sort. Such failures often contribute to a loss of situational awareness, which precedes the accident. The loss often acts as a catalyst for the accident to occur because such a loss deprives people of seeing the mounting errors that together usually lead to the accident, and therefore, they are unaware that any action is required to prevent the event. Situational awareness has long been considered the bedrock of pilot action and decision making. The effectiveness of flight operations and the decision processes that bolster such effectiveness are highly dependent on situational awareness. In order to understand how situational awareness is lost, we must first come to grips with what exactly this mysterious entity is known as situational awareness. Because situational awareness is an intangible concept and not necessarily intui­tive to understand, many definitions have surfaced. Perhaps the simplest definition of situational awareness is "a mental model of one's environment. " A more detailed definition of situational awareness is "the ability to perceive factors that affect us and to understand how those factors impact us now and in the future." But such a definition is incomplete since situational awareness is more than perception. Others have defined situational awareness in flight operations as "the realistic understanding of all factors which affect the safety and effectiveness of a flight"; except that such a definition implies that situational awareness is realistic, whereas in reality we know there can be good situational awareness and poor situational awareness. A person also can have a continuously varying level of situational awareness during a period of time. Still others have defined situational awareness as "the understanding operators have of the system and its environment at any one time." The problem with that definition is that situational awareness is more than just understanding. As we can see, it is a complex topic that defines simple answers. Interest in situational awareness research increased significantly in the 1 9 8 0s and acceler­ated through the 1990s. The surge in research was due to, in great measure, the increased use of automation and technology and the new type of mistakes that such innovations engendered. A significant moment occurred in 1995 when what would later be termed the Situation Awareness Error Taxonomy was created by a human factor’s researcher, Dr. Mica Endsley. She researched situational awareness exhaustively and realized that situational awareness is developed, sustained, and destroyed along three distinct levels: • Level 1 situational awareness: perceiving critical factors in the environment. • Level 2 situational awareness: understanding what those factors mean, particularly when integrated together with your goals • Level 3 situational awareness: projection-understanding what will happen with the system in the near future So, in a nutshell, we can think of the three levels of situational awareness as perception, understanding, and projection. As such, the best definition of situational awareness for this book is, "The perception of the elements in the environment within a volume of time and space, the comprehension of their meaning, and the projection of their status in the near future" (Endsley, 1995, p. 36). The three levels form a hierarchy of situational awareness where people must attain a lower level prior to moving onto higher levels of situational awareness in order to retain good situational awareness. In other words, the accuracy of situational awareness at higher levels is intimately linked to the accuracy of situational awareness at the lower levels. For example, a flight attendant directing passengers through an emergency evacuation on a taxiway who does not perceive the presence of fire out­side of an emergency exit (poor Level 1 situational awareness) cannot be expected to understand the risk of opening the exit (poor Level 2 situational awareness), and therefore may inappropriately pro­ceed to open the exit and direct passengers to exit into the fire (poor Level 3 situational awareness). This would be considered poor overall situational awareness, leading to death or injury of passengers. By contrast to the previous example, the same flight attendant who correctly perceives the presence of fire outside of an emergency exit (good Level 1 situational awareness) will be more likely to then understand the risk of opening the exit (good Level 2 situational awareness), more likely to direct passengers to an alternate exit away from the fire (good Level 3 situational awareness), and therefore be considered to have good overall situational awareness, leading to a safe evacuation. Based on the previous explanation, we can see that situational awareness can be challenging to build and easy to lose. Imagine an aviation maintenance technician who is taking over the task of replacing a leaky oil pump from a colleague during a shift change. The handoff has to be carefully performed in order for the incoming technician to correctly perceive what tasks have already been accomplished (Level 1 situational awareness), to then understand how the completed tasks fit into the overall process for replacing the pump (Level 2 situational awareness), and lastly determine how she will proceed to complete the task (Level 3 situational awareness). An error in the first level makes it hard to correctly establish Level 2 situational awareness. Or, the technician may correctly build Level 1 and incorrectly build Level 2 situational awareness, making it difficult to determine the proper remaining actions (Level 3 situational awareness). Even if the first two situational awareness levels are correctly built, the technician may incorrectly determine the remaining actions and therefore end up with poor situational awareness and an improperly replaced oil pump. Why would it be difficult to establish situational awareness at each level? Human factors scientists have stressed the need to investigate the psychological roots of each level of situational awareness ver­sus simply stating superficial descriptions of how poor situational awareness contributed to an event. For too many years, and still in too many reports today, investigators simply explain the causes of an event as "loss of situational awareness" without probing deeper. To say that an accident happened due to loss of situational awareness is not very helpful. Why? It is not helpful because it does not allow us to prevent future accidents. After a bad event, we cannot simply tell aviation professionals, "don't lose situational awareness in the future. “ That would be like demanding of a pilot, "don't crash again. " That is not very helpful! Hence the reader has been exposed to the previous discussion in this chapter about the roots of cognitive error. Such errors and biases can work against creating situational awareness at each level of the hierarchy. Thinking about these errors can help us explain how situational awareness breaks down. Continuing with our previous example of the technician replacing the oil pump, insufficient knowledge as to how to use a maintenance manual may lead to the technicians not knowing to consult a checklist and then not perceive a key check­list item in the oil pump change procedure (Level 1 situational awareness). Or perhaps an instructor in a training course improperly taught the incoming technician that several clamps were acceptable for connecting the pump fittings to the engine gearbox, whereas only one special type of clamp can be used for the oil scavenge line (Level 2 situational awareness). Or, perhaps a false expectancy that a more seasoned colleague was going to check the work caused complacency leading to a rushed job to complete the replacement and get the aircraft back in service (Level 3 situational awareness). It thus proves necessary to study the underlying factors that make loss of situation awareness so prevalent and so detrimental to flight safety. One key fac­tor that can both build and destroy situational awareness, as previously mentioned, is expectations. In the context of human psychology, expectancy is an attitude or mental model that guides operators to relevant situation cues, thus determining how a person approaches a situation in order to increase the efficiency of his or her situation assessment (Strauch, 2004 ). A false cognitive expectancy is the unreasonable anticipation of an event or condition that does not occur as envisioned. The determination of reason is a sub­jective assessment by the researcher based on experience and training. The incor­rect anticipatory mindset may be the result of conscious or unconscious cognitive processes (Cortes, 2011). The reason why expectations function as a crucial aspect of cognition is quite clearly to increase the efficiency of actions. Expectations about information can impact the speed and accuracy of the perception of information. Continued experi­ence in an environment helps people to develop expectations about future events that predispose them to perceive the information accordingly. In fact, the constant fusion of cognitive expectancies to actual perceived situations in order to build situational awareness, at the risk of suffering expectancy violations, has been shown as one of the most critical processes responsible for situation awareness creation and diminution. In other words, incorrect expectancies, together with a very long list of other men­tal biases, can both help create and destroy situational awareness (Cortes, 2011). There are numerous other factors that affect situational awareness, such as poor communication. Bad communication can occur when a controller would like for a pilot to perform a certain action while the pilot does not know what the controller is asking of them. It also may occur between a captain and first officer, resulting in confusion among them that causes a loss of tracking of what the airplane is doing. Other important factors impacting situational awareness are fatigue or stress. If an air traffic controller is fatigued or stressed, he would be less likely to perform a visual sweep of a runway ahead of an early morning takeoff and may not pay as close of attention to the situation as he should. Let us look at a flight deck example of the same concept of how workload impacts situational awareness. Imagine a pilot who leaves the flight deck to use the lavatory during cruise flight but shortly before commencing the descent for landing. The longer the pilot is gone from the flight deck, the less sensory inputs are received with informa­tion and workload requirements for the flight, and therefore the lower the pilot's situational awareness becomes. On the other side, once the pilot comes back from a lavatory break prior to descent for landing and notices that her colleague has already started the descent, the returning pilot may end up playing catch-up due to all the changes that have happened since she left the flight deck. In addition to strapping back into the seat, adjusting the seat position, and donning a headset, the returning pilot will have to work hard to quickly assess the situation to build situational awareness. She may find herself having to rush to get through checklists, change frequencies, call the company to announce their imminent arrival time estimate, and quickly glancing at the instruments to determine what exactly the aircraft is doing. As a result, in spite of her best efforts to catch up with all the changes since she left the flight deck, the large amount of catch-up workload may result in the returning pilot not noticing as the other pilot improperly sets a vertical speed mode instead of an airspeed mode in the flight guidance panel, resulting in the crew bust­ing a speed limit at a certain waypoint during the descent, all which could result in a violation being issued by ATC. However, if the returning pilot has sufficient time to catch up with what is happening before her colleague starts making the erro­neous mode input into the flight guidance panel, her situational awareness may be sufficiently high that she catches the error and saves the day. This scenario illustrates how there is a goldilocks zone, so to speak, that is described as the optimal amount of work where there is not too little and not too much workload. In conclusion, there are many actions that need to happen to have and keep good situation awareness. Underlying factors can cause a loss of situational awareness, and such a loss is often associ­ated with negative outcomes, such as accidents. Researchers continue studying the ways in which situational awareness breaks down and how aviation professionals can maximize the accuracy with which they build each of the three levels of situational awareness. Professionals in com­mercial aviation must take full advantage of all the available sources around them, which include other team members, air traffic controllers, dispatchers, and flight attendants, in order to stay in the loop as to developments and thus keep their situational awareness high, while postponing noncritical tasks or delegating tasks in order to prevent workload from getting too demanding. HUMAN PERFORMANCE When we think of performance, it is tempting to consider a race car and how we assess its performance by asking how fast it can negotiate a turn while not losing its grip on pavement, or by learning how long it takes to accelerate from zero to 60 miles per hour. What about us? Human performance can be defined as the man­ner, speed, and accuracy with which individuals accomplish tasks. It is a measure of human activity that expresses how well a human has carried out an assigned, well-defined task, or a portion of a task (task element). Human performance is a function of speed and accuracy. Here we focus mostly on the accuracy component of human performance. If a task is not performed "accurately" in accordance with its requirements, an error has occurred. Accidents are generally caused by situations in which a person's capabilities are inadequate or are overwhelmed in an adverse situation. Humans are subject to such a wide range of varying situations and circumstances that not all can be easily foreseen. Careful attention should therefore be given to all the factors that may have influenced the person involved. In other words, consideration must be given not only to the human error (failure to perform as required) but also to why the error occurred. Variables that affect human performance can be grouped into seven categories: physical factors, physiological factors, psychological factors, psychosocial factors, hardware factors, task factors, and environmental factors. These factors are now briefly reviewed. 1. Physical factors include body dimensions and size (anthropometric measurements), age, strength, aerobic capacity, motor skills, and body senses such as visual, auditory, olfactory, and vestibular. 2. Physiological factors include general health, mental blood flow and oxygenation, and medical conditions such as low blood sugar, irregular heart rates, incapacitation, illusions, and history of injury, disability, or disease. Also included in this category are human conditions brought on by lifestyle such as the use of drugs, alcohol, or medica­tion; nutrition; exercise; sports; leisure activities; hobbies; physical stress; and fatigue. 3. Psychological factors include mental and emotional states, mental capacity to process information, and personality types (introverts and extroverts. Some human personality traits include the following: • Motivation is a desire of an individual to complete the task at hand. Motivation affects one's ability to focus on all the necessary faculties to carry out the task. • Memory allows us to benefit from experience. It is the mental faculty that allows us to prepare and act upon plans. Memory can be improved through the processes of association, visualization, rehearsal, priming, mnemonics, heuristics, and chaining. Memory management organizes remembering skills in a structured procedure while considering time and criticality. It is a step-by-step process to increase the accuracy and completeness of remembering. • Complacency can lead to a reduced awareness of danger. The high degree of automation and reliability present in today's aircraft and the routines involved in their operation are all factors that may cause complacency. • Attention (or its deficit) determines what part of the world exists for you at the moment. Conscious control of attention is needed to balance the environment's pull-on attention. An intrapersonal accident prevention approach would describe the hazardous states of attention as distraction, preoccupation, absorption, and attention mismanagement-the inability to cope with tasks requiring flexible attention and focused tracking and steering. The inability to concentrate can lead to lack of (situational) awareness, which has been identified as a contributing factor in many accidents and incidents. • Attitude strongly influences the functioning of attention and memory. Attitudes are built from thought patterns. An intrapersonal approach to the attitudes of team members attempts to identify the desirable ranges between such hazardous thought patterns as macho-wimp, impulsive-indecisive, invulnerable-paranoid, resigned-compulsive, and antiauthority-brainwashed. • Perceptions can be faulty. What we perceive is not always what we see or hear. Initial perceptions based solely on intended actions are espe­cially susceptible to error. An intrapersonal approach prescribes ways to make self-checking more efficient and reliable. • Self-discipline is an important element of organized activities. Lack of self-discipline encourages negligence and poor performance. • Risk taking is considered by some to be a fundamental trait of human behavior. It is present in all of us to varying extents since an element of risk is present in most normal daily activities. Risk will be present as long as aircraft fly and penalties for failure are high. Accordingly, the taking of risks needs to be carefully weighed against the perceived benefits. • Judgment and decision making are unique capabilities of humans. They enable us to evaluate data from a number of sources in the light of education or past experience and to come to a conclusion. Good judgment is vital for safe aircraft operations. Before a person can respond to a stimulus, he or she must make a judgment. Usually, good judgment and sound decision making are the results of training, experience, and correct perceptions. Judgment, however, may be seri­ously affected by psychological pressures (or stress) or by other human traits, such as personality, emotion, ego, and temperament. Good judgment is vitally important to making correct decisions. • Aeronautical decision making (ADM): The Federal Aviation Administration describes ADM as a systematic approach to the mental process used by aircraft pilots to consistently determine the best course of action in response to a given set of circumstances (Federal Aviation Administration Advisory Circular 60-22). The Federal Aviation Administration Pilot's Handbook of Aeronautical The DECIDE Model of Aeronautical Decision-Making Knowledge recommends the DECIDE model to provide the pilot with a logical way of making decisions. The DECIDE acronym means to Detect, Estimate, Choose a course of action, Identify solutions, Do the necessary actions, and Evaluate the effects of the actions. Detect. The decision maker detects the fact that change has occurred. Estimate. The decision maker estimates the need to counter or react to the change. Choose. The decision maker chooses a desirable outcome (in terms of success) for the flight. Identify. The decision maker identifies actions which could successfully control the change. Do. The decision maker takes the necessary action. Evaluate. The decision maker evaluates the effect(s) of his/her action countering the change. 4. Psychosocial factors include mental and emotional states due to death in the family or personal finances, mood swings, and stresses due to relations with family, friends, coworkers, and the work environment. Some of the factors that cause stress are inadequate rest, too much cognitive activity, noise, vibration and glare in the cockpit, anxiety over weather and traffic conditions, anger, frustration, and other emotions. Stress causes fatigue and degrades performance and decision making, and the overall effect of multiple stresses is cumulative. Interactions with coworkers are influenced by two important variables, namely, peer pressure and ego. • Peer pressure can build to dangerous levels in competitive environments with high standards, such as aviation, in which a person's self-image is based on a high standard of performance relative to his or her peers. Such pressure can be beneficial in someone with the necessary competence and self-discipline, but it may be dangerous in a person with inferior skill, knowledge, or judgment. For example, a young, inexperienced pilot may feel the need to prove himself or herself and may, therefore, attempt tasks beyond his or her capability. Humans have many conflicting "needs," and the need to prove oneself is not limited to the young or inexperienced. Some people, because of training or background, have a fear that others may consider them lacking in courage or ability. For such people, the safe course of action may be perceived as involving an unacceptable "loss of face." • Ego relates to a person's sense of individuality or self-esteem. In moderate doses, it has a positive effect on motivation and performance. A strong ego is usually associated with a domineering personality. For pilots in command, this trait may produce good leadership qualities in emergency situations, but it may also result in poor crew or resource management. The domineering personality may discourage advice from others or may disregard established procedures, previous training, or good airmanship. Piloting an aircraft is one situation in which an overriding ego or sense of pride is hazardous. Although usually not specifically identified as such in accident reports, these traits may often be hidden behind such statements as "descended below minima," "failed to divert to an alternate," "attempted operation beyond experience/ability level," "continued flight into known adverse weather," and so forth. 5. Hardware factors include the design of equipment, displays, controls, software, and the interface with humans in the system. 6. Task factors include the nature of the task being performed (vigilance and inspection tasks versus assembly operations), workload (work intensity, multitasking, and/or time constraints), and level of training. 7. Environmental factors include noise, temperature, humidity, partial pressure of oxygen, vibration, and motion/acceleration. It is important to note that the factors discussed above can act alone or in combination with two or more other factors to further degrade human performance in the occupational setting. These factors can produce synergistic effects on human performance. Some examples include an air traffic controller monitoring air traffic during extremely low air traffic volume while on allergy medication or a quality control inspector monitoring low defect rate products while on cold medication. Fitness For Duty Human performance is so important that the Federal Aviation Administration has crafted a regulation dealing solely with the topic of fitness for duty. For many people, if they wake up not feeling well one morning, there is still a good chance they can survive the work day without putting anyone in danger. However, for aviation professionals, even the smallest disruption of balance in one's body could severely compromise the safety of hundreds of people. Recent accidents have highlighted the need for future awareness campaigns and areas of research to improve fitness for duty. The Federal Aviation Administration explains fitness for duty in FAR 117.5 as being "physiologically and mentally prepared and capable of performing assigned duties at the highest degree of safety. " Unfortunately, there are gray lines in the meaning of the Federal Aviation Administration definition as it is hard to standardize and quantify what physiological and mental fitness is for each professional. Although the presence of stress in one's life has clear effects on performance, and alcohol can obviously severely impact performance, the fatigue element is less obvious and requires elaboration. Although it can be hard to pinpoint, fatigue is classified by the following: 1. Weariness from mental or bodily exertion 2. Decreased capacity or complete inability of an organism, an organ, or a part to function normally because of excessive stimulation or prolonged exertion Fatigue can decrease short-term memory capacity, impairs neurobehavioral performance, leads to more errors of commission and omission, and increases attentional failures. A study of medical residents looked at the impact of fatigue and found that the risk of making a fatigue-related mistake that harmed a patient soared by an incredible 700% when medical residents reported working five mara­thon shifts in a single month (Hallinan, 2009). Think about how these same findings could cause breakdowns in safety for aviation professionals during long shifts. Some studies have shown sleep deprivation in pilots causing performance reduc­tions equivalent to having a blood-alcohol level of 0.08 % (Paul & Miller, 2007). In other words, being fatigued can deteriorate our performance as much as if we were drunk. There are three types of fatigue: transient, cumulative, and circadian. Transient fatigue is brought on by sleep deprivation or extended hours awake. The second type, cumulative fatigue, is repeated mild sleep restriction or tiredness due to being awake for extended hours across a series of days. Last, circadian fatigue is the reduced performance during nighttime hours, particularly between 2 am and 6 am for those who are used to being awake during daytime and asleep at night. Contributing factors to all types of fatigue include personal sleep needs, sleeping opportunities, physical conditioning, diet, age, alcohol, stress, smoking, sleep disor­ders, mental distress, sleep apnea, and even heart disease. It is imperative to recognize the symptoms of fatigue because it is associated with accidents, reduced safety margins, and reduced operational efficiencies. In1 993 the NTBS started including fatigue as a probable cause of some accidents, starting with the report on the uncontrolled collision with terrain by an American International Airways Flight 808, a Douglas DC-8 carrying cargo to Guantanamo Bay, Cuba. Fatigue is a significant problem in aviation because of long shifts, circadian disruptions, and the sometimes-unpredictable work hours. Attempts to regulate rest are limited by conditions that are still not adequately addressed, such as the following: 1. Reporting for duty while being fatigued because stress from family or personal sources does not allow adequate sleep. 2. Commuting from home to the location where a trip will commence may require significant time awake and induce stress prior to actually commencing professional duties. 3. A crewmember may be provided an adequate rest period prior to a flight but may be unable to obtain quality rest due to ambient noise, such as when there is a party or a loud television in an adjoining hotel room, construction noise, or if the crewmember misuses allotted rest time for personal activities that are not conducive to rest, such as sightseeing or working a second job. 4. Traveling between time zones and not being able to be on a consistent sleep schedule. Taking the above reasons into consideration, it is fortunate that there have not been more aviation accidents and incidents as a result of fatigue, particularly when contemplating that fatigue does pose a threat not only to pilots but also to air traf­fic controllers, aviation maintenance technicians, and everyone who can affect the safety value chain in commercial aviation. The professional life of many people in aviation can best be described as shift work, which is a schedule that falls outside the traditional 9 am to 5 pm workday and which can include evening, night, morning, or rotating periods. Negative side effects of shift work include acute/chronic fatigue, sleep disturbance, disruption of circadian rhythm, impaired performance, cardiovascular problems, and family/social disruption. When operating while fatigued, controllers may commit an error, such as providing less than required separation between two or more aircraft and assigning closed runways to aircraft. Potential errors in maintenance may include incorrect assembly of parts, not properly installing equipment, and overlooking items that need attention. To combat fatigue in the future, airlines have developed Fatigue Risk Management Plans (FRMPs). In 2010, the President of the United States signed Public Law 111-216, the Airline Safety and Federal Aviation Administration Extension Act of 010, which focuses on improving aviation safety. Section 212(b) of the Act requires each air carrier conducting operations under Title 14 of the Code of Federal Regulations part 121 to develop, implement, and maintain an FRMP. Such plans consist of an air carrier's management strategy, including policies and procedures that reduce the risks of flight crewmember fatigue and improve flight crewmember alertness. Further information on FRMP is contained in Federal Aviation Administration Advisory Circular 120-103A, entitled "Fatigue Risk Management Systems for Aviation Safety." These guidelines provide a source of reference for managers directly respon­sible for mitigating fatigue, detailed step-by-step guidelines on how to build, imple­ment, and maintain an FRMP, and describe the elements of an FRMP that comply with industry good practice. A typical FRMP structure consists of nine elements, all of which the carriers must address in their own plan: 1. Senior Level Management Commitment to Reducing Fatigue and Improving Flight Crew Member Alertness 2. Scope and Fatigue Management Policies and Procedures 3. Current Flight Time and Duty Period Limitations 4. Rest Scheme Consistent with Limitations 5. Reporting Policy 6. Education and Awareness Training Program 7. Fatigue Incident Reporting Process 8. System for Monitoring Flight Crew Fatigue 9. FRMP Evaluation Program By using this system, fatigue becomes a part of the safety management system in the same way other aspects of health, environment, productivity, and safety are managed. Additionally, the Federal Aviation Administration has a few recommendations to mitigate fatigue, which are as follows: 1. Shift rotation time should be no less than 10 hours. 2. Utilize modeling and scheduling tools to assist in mitigating fatigue-promoting schedules. 3. Educate schedulers and workforce about issues regarding shiftwork and fatigue. 4. Promote application of personal and operational counter-fatigue strategies. Communication Issues The term communication usually includes all facets of information transfer. It is an essential part of teamwork, and language clarity is central to the communica­tion process. Different types of communication, and verbal communication in par­ticular, remain one of the weakest links in the modern aviation system. More than 70% of the reports to the Aviation Safety Reporting System involve some type of oral communication problem related to the operation of an aircraft. Technologies, such as airport surface lights or data link communication, have been available for years to circumvent some of the problems inherent in air traffic control asso­ciated with verbal information transfer. Sometimes, however, solutions bring unin­tended negative consequences. For example, one potential problem with air traffic control data link communication is the loss of the "party line" effect (hearing the instructions to other pilots), which removes an important source of information for building pilot situational awareness about the air traffic control environment. That being said, the so-called party line is also a source of errors for pilots who act on instructions provided to other aircraft or who misunderstand instructions that differ from what they anticipated by listening to the party line. Switching air traffic control communication from hearing to visual, as is the case with reading data link communiques, also can increase pilot workload under some conditions. Further human factors studies are necessary to define the opti­mum uses of visual and voice communications. Miscommunication between aircrews and air traffic controllers has been long recognized as a leading type of human error. It has also been an area rich in potential for interventions. Examples are the restricted or contrived lexicon (e.g., the phrase say again hails from military communications, where it was mandated to avoid confusing the words repeat and retreat); the phonetic alphabet ("alpha," "bravo," etc.); and stylized pronunciations (e.g., "niner" to prevent confusion of the spoken words nine and five) . Adequate communication requires that the recipient receive, understand, and can act on the information gained. For example, radio communication is one of the few areas of aviation in which complete redundancy is not incorporated. Consequently, particular care is required to ensure that the recipient receives and fully understands radio communication. There is more to communication than the use of clear, simple, and concise language. For instance, intelligent compliance with directions and instructions requires knowledge of why these are necessary in the first place. Trust and confidence are essential ingredients of good communication. For instance, experience has shown that the discovery of hazards through incident or hazard reporting is only effective if the person communicating the information is confident that no retribution will follow her or his reporting of a mistake. The horrific ground collision between two Boeing 747 aircraft in Tenerife in 1977 resulted in the greatest loss of life in an aviation accident and featured a key communication error as part of the accident sequence. Today, controllers restrict the word cleared to two circumstances-cleared to take off and cleared to land although other uses of the word are not prohibited. In the past, a pilot might have been cleared to start engines, cleared to push back, or cleared to cross a runway. The recommendation would have the controller say, " Cross runway 2 7," and "Pushback approved," reserving the word cleared for its most flight-critical use. The need for linguistic intervention never ends, as trouble can appear in unlikely places. For example, pilots reading back altimeter settings often abbreviate by omitting the first digit from the number of inches of barometric pressure. For example, 29.97 (inches of mercury) is read back "niner niner seven." Since barometric settings are given in millibars in many parts of the world, varying above and below the sea level pressure standard value of 1013, the readback "niner niner seven" might be interpreted reasonably but inaccurately as 997 millibars. The obvious corrective-action strategy would be to require full readback of all four digits. A long-range intervention and contribution to safety would be to accept the more common (in aviation) English system of measurement, eliminating meters, kilometers, and millibars once and for all. Whether English or metric forms should both be used in aviation, of course, is arguable and raises sensitive cultural issues. At this time, the English system clearly prevails, since English is the instructions for continued airworthiness mandated international language of aviation, as stressed in a decree by international civil aviation organization regarding language proficiency on January 1, 2008. HUMANS AND AUTOMATION It is exciting when we get a new cell phone, isn't it? Sure, it requires somewhat of a learning curve to master the new features, but once we determine how to use the new capabilities, we feel so much more powerful with our new technology. That learning curve, however, can mean that you inadvertently delete your song list. That is a very minor mistake that is recoverable. Wrestling with the automated features on a rental car makes the situation more serious, often requiring renters to sit in the parking lot of the rental car agency after receiving the car, with the engine running, while the driver figures out just how to figure out the fancy gadgetry. Or worse, the driver may try to figure out how to operate the technology in mid drive, creating distraction that could lead to an accident. Now imagine strapping into your brand-new Airbus 350 and undertaking the same learning curve with the flight deck automation. A simple mistake is no lon­ger a laughing matter. After all, you would be in a high consequence environment where a small error may not delete a song list but result in the aircraft running off a taxiway or flying into terrain killing hundreds of passengers. It is not that funny anymore, is it? On a much more serious note, the confusion caused by automation has sur­faced time and time again in accidents. In 1992 an Airbus 320 operating as Air Inter Flight 148 crashed into terrain near Strasbourg, France, when pilots mis­understood the descent mode that was selected in the automatic flight system. Two years later, an Airbus 300 operating as China Airlines 140 stalled on approach to Nagoya, Japan, when the automation confused the pilots, and an unintentional go-around was initiated during approach to landing. More recently in 2013, a Boeing 777 operating as Asiana Flight 214 crashed on approach to San Francisco, partly because the crew did not understand how the aircraft's automation would behave in certain flight modes. According to Michael Feary, a research psychologist for Situational awareness, part of the problem was that the information provided in the flight computer displays was written in "engineer-speak" versus “pilot-speak." How automation combines with human error is a fascinating subject. You could write an entire book on the topic. In fact, many very good books have already been written. As a minimum, we should become familiar with three terms: automation surprise, mode confusion, and GIGO (garbage in, garbage out). Automation surprise is when a person has a mental model of the expected performance of technology and then encounters something different. In some cases, we anticipate a response to the last selected mode but do not notice that the automation has reverted to a sub-mode, and thus, behaves differently than expected. Designers strive to make automation intuitive; however, operators may encounter functioning that they have not been taught and which is not announced by the automation control heads or associated displays. A study that surveyed 1 ,268 pilots revealed that 61 % of them still were surprised by the flight deck automation from time to time (BASI, 1999). For an example of automation surprise from everyday life, imagine that your laptop has a factory preset to check for updates every week and to restart if there are any. While writing a critical document that is due within the hour, a dialogue pops up detailing that the computer is going to start the scan and restart. You were away for a moment getting coffee and returned to find the laptop rebooting, much to your surprise. If you are lucky, you did not lose much content on the document. Within automation surprise, one specific aspect is mode confusion, which describes how selecting a certain mode for automation operation may actually result in a different mode being engaged, resulting in a very confusing situation. Mode confusion happens when we do not completely grasp the inner workings of automation or if we fail to monitor the automation as it operates, much as we would monitor a colleague to stay in the loop as to what is happening. One such situation is called an indirect mode change, when the automation changes mode without an explicit command by the operator, which is a byproduct of reaping the benefits of having automation perform what was previously manual functions. This happens more frequently than one would expect. A research study of 1,268 pilots revealed that 73 % of them reported having inadvertently selected a wrong automa­tion mode (BASI, 1999). Another study found that most mode confusion events occurred during unusual uses of the automation, such as during aborted takeoffs or disengagement of automatic modes during approach to landing (Sarter & Woods, 1995). In 1995 the Federal Aviation Administration Aircraft Certification Directorate conducted a study about how modern glass cockpit automation correlates to aviation accidents stemming from automation confusion. The study demonstrated concern about pilot understanding, use, and modal awareness of automation, sug­gesting that when a pilot is not sufficiently engaged, awareness decreases. Another potential hazard that can result from mode confusion is altitude deviation. Using the aviation safety report system database, national aeronautics and space administration looked at 500 reports and found this to be the most reported automation problem. To correct this problem, the researchers recommend better and redundant feedback loops to detect anomalies, as well as new system designs altogether. When mode confusion occurs, it can have significant consequences. How often are errors committed, though, and of what type? A University of Texas research team conducted Line Operations Safety Audits on 1 84 crews across three airlines between 1 997 and 1998. They found that 68% of crews committed at least one error, the most frequent error being associated with automation. Sixty-five percent of these automation errors were a failure to verify settings. Incorrect switch or mode selection accounted for 21 %. To understand the types of mode confusion that occurs, a national aeronautics and space administration study used 30 flights aboard Boeing 757/767 aircraft to help categorize the types. The researchers divided mode confusion into two areas. The first was the misidentification of an automation behavior when the actual behavior was different from what was expected. The second was when a pilot acted on the assumption that the machine was in one mode, when it was actually in another. An example of mode confusion previously mentioned, Asiana 2 14, occurred in part because the flight officer controlling the Boeing 777 was in manual control of the aircraft yoke but mistakenly believed that the auto-throttle mode he had selected would control the airspeed. When the pilot noticed the dangerously slow speed on approach and realized that he was confused about the auto-throttle mode, he intervened by trying to take manual control of the aircraft instead of using the automation, but it was too late to prevent the accident. Another source of human error when using automation is GIGO, an acronym for garbage in, garbage out. It is a phrase that refers to the fact that comput­ers, operating through logical processes, will unquestionably process erroneous, even senseless, data that are input into the system and still produce an output. This output, though, is usually undesired and often nonsensical. GIGO highlights just how important the human remains in automated operations. Until we are able to develop automation that is intelligent and which can therefore sense that a mistaken input is probably being made and warn the operator of the suspected mistake, the GIGO phenomena will continue to occur. We can see GIGO in the 2014 story of U.S. Airways Flight 1 702, an Airbus 320 departing from Philadelphia to Fort Lauderdale. According to national transportation safety board docu­ments, prior to departing the gate for takeoff, one of the pilots entered the wrong runway into the flight computer. The captain noticed, prompting the other pilot to change the runway information but not the parameters needed for the new runway. At takeoff, as the aircraft accelerated through 80 knots of airspeed, the audio alert in the cockpit announced "retard, retard, retard." This was an automated audio signal prompting the pilots to reduce thrust setting. Unfortunately, neither of the pilots knew what the warning meant during take­off because it was associated with landings. Initially the captain decided to continue with the takeoff and address the discrepancy once airborne, but once in the air realized that something was seriously amiss and decided to touch down again and stop the aircraft. This action resulted in the plane's tail striking the runway and causing the nose gear to collapse as it hit the ground and skidded to a halt 2,000 feet later, stopping on the runway's left edge. Later, Airbus said that the "retard" audio annunciation had sounded because the automated system reverted to landing mode since the parameters needed for the runway were not changed. As we can see, a mistake in the input of data caused the automation to revert to an unexpected mode and produce a surprising response. It should be noted that, while this section has focused on the interaction of pilots with automation, everyone else in the safety value chain is also affected. Flight attendants must learn how to operate the increasingly complex systems used for entertainment, cooking, and communication. Aviation maintenance technicians increasingly rely on software systems for reference guidance and tracking task accomplishment. Dispatchers often rely on several software packages that often have changing features due to software upgrades, thus presenting numerous oppor­tunities for confusion and impeding dispatcher situational awareness of numerous flights under their purv1ew. Lastly, the element of distraction created by smartphones can be incessant and affects every single person on the safety value chain. This is a new source of dis­traction that has emerged over the past 10 years and which, some claim, is an epi­demic. One of the authors of this chapter remembers flying as a pilot of an airliner and being very distracted, at the critical moment of rotation on takeoff, when the other pilot's cell phone rang loudly on the flight deck. How many times have you checked your smartphone while reading this chapter? How many times have you answered a text or checked social media while reading this chapter? How have those actions impacted your processing of the concepts in this chapter . . . the same concepts that could one day possibly save your life? It gives you something to think about, doesn't it? HUMAN FACTORS ANALYSIS AND CLASSIFICATION SYSTEM (HFACS) Since World War II, the study of aviation human factors has developed, promising new tools and a classification system based upon previous research in this critical area. The father of the "Swiss Cheese" model described in the previous chapter, Dr. James Reason, used an interesting analogy when discussing the importance of this topic when he stated " [Human errors] are like mosquitoes. They can be swatted one by one, but they still keep corning. The best remedies are to create more effec­tive defenses and to drain the swamps in which they breed" (Reason, 2005, p. 769). Starting with Reason's " Swiss Cheese" model and building upon its frame­work of latent conditions and active failures, Dr. Douglas Wiegmann and Dr. Scott Shappell developed Human Factors Analysis and Classification System (HFACS) for the U.S. Navy in response to increasing human errors and a high accident rate. human factor analysis and classification sytems is a theoretically based tool for investigating, analyzing, and clas­sifying human error associated with aviation accidents and incidents. The human factor analysis and classification sytems model has been validated through comprehensive research of 1,020 national transportation safety board acci­dent investigations that occurred over a 13-year period. human factor analysis and classification sytems uses a systems approach in which human error is seen within the context of a larger problem in the organization. A review of the " Swiss Cheese" model from the previous chapter (Figures 2-9 and 2-10) is helpful to illustrate the defenses (screens or barriers) that should be set up by an organization to stop the human error chain of causation. The human factor analysis and classification sytems model uses the same four barrier levels used by Dr. Reason to prevent the accident by controlling: • Organizational Influences • Unsafe Supervision • Preconditions for Unsafe Acts • The Unsafe Act Itself (the Active Failure) The human factor analysis and classification sytems Framework Chart, from Wiegmann and Shappell (2003) In the next section, we will discuss the active failure level (unsafe act itself) to determine what control strategies may be effective in combating the inevitable human error dilemma. Unsafe Acts-Errors or Violations? Since human error continues to be the largest cause of commercial aviation accidents, the next question becomes, what are these errors and how do we prevent them? The Reason "Swiss Cheese" model and human factor analysis and classification sytems both describe two general categories of these errors or unsafe acts. As pointed out by Shappell and Wiegmann in an human factor analysis and classification sytems article entitled, "A Methodology for Assessing Safety Programs Targeting Human Error in Aviation" (2009), an error is either an honest mistake or a violation, that is, the willful disre­gard for the rules and regulations of safety. Human Factors Analysis and Classification System further describes three types of honest mistakes (decision, skill-based, and perceptual errors) and two types of violations (routine and exceptional). Of course, the primary difference between an error as an honest mistake and a violation is the intent necessary to create a violation. Violations can be routine, habitual, and condoned by the orga­nization, or they can be exceptional or extreme, such as an intentional violation of the U.S. Federal Aviation Regulations (FARs). Once an accident or incident has occurred, proper utilization of human factor analysis and classification sytems allows a safety analyst to identify the specific types of human error that occurred at various levels of the organization. Notice once again that the focus of the accident investigation is on "what" happened and specifically "why" it happened in the organization. The next step in this complex process is to review the multi-level framework of factors potentially contributing to the accident. In the 2009 article, Shappell and Wiegmann proposed five intervention strategies to control human error in these categories: • Organizational/ Administrative • Human/Crew • Technology/Engineering • Task/Mission • Operational/Physical Environment The human factor analysis and classification sytems team has also developed a post-accident tool to investigate the human error problem called the Human Factors Intervention Matrix (HFIX). When plotted against the unsafe acts previously discussed, the five intervention strategies become the HFIX. The human factor analysis and classification sytems classification method and the HFIX intervention matrix are modern human factors tools which were designed for sophisticated organizations with high-quality data and knowledgeable human factors personnel. It provides a very useful framework for accident aviation investigators to use to study the organization and its role in accident causation due to latent conditions. The aviation safety report system EXAMPLES Following are some examples from NASA's Aviation Safety Reporting System (ASRS), in the original text submitted, including grammatical errors. These reports demonstrate the complexities inherent in the modern aviation system that make studying human error very challenging. FLIGHT CREW DISTRACTION Title: Distraction in the cockpit. As I was approaching gate, I shut down the #2 engine (per our Ops Manual). I was momentarily distracted inside the cockpit. There was enough room to make a turn . . . to the gate. I added power on the #1 engine. During the left turn, the jet blast from the engine blew a mechanic off a maintenance stand. It also blew part of an engine cowling off the stand. In future situations, I will . . . shut down and use a tug to reposition if there is any doubt about jet blast. Question for the reader: Speculate on what other types of human perfor­mance issues may have also contributed to this event but are not mentioned in the narrative. FLIGHT CREW FATIGUE Title: Fatigue resulting from personal life. Nav #1 failed en route to airport. VOR/DME A approach in use landing runway 19. With Nav #1 inoperative, Captain and First Officer decided to have First Officer fly the approach and return aircraft control to Captain when approach completed overhead the airport. In hindsight, this was poor decision making and was likely agreed to due to both pilots being tired. The first Officer was nearing the end of a 1 4-hour duty period. The first Officer was briefed 1 0 hours prior to show time, and between late notice and a new baby, had been awake nearly 20 hours, not eaten in 7 hours. In any case, the Captain was handling radios and forgot to contact Tower, and the First Officer relinquished control to Captain overhead airport, thinking they were in a position to turn downwind. First Officer was unable to see the airport. The checklist was still being performed and crew turned downwind overhead [airport], completing the checklist. The first Officer was now PNF (Pilot Not Flying) and asked Captain if he called Tower. No reply. The first Officer called Tower, who was already asking us what was going on, and at this time, Captain had begun a right turn to correct the course. It was too late by now. We turned final, overshot and returned to course, landed. The next morning, after calling in fatigued [and], with adequate rest, it was clear to us that we created our own mess out of a simple instrument error. This goes to show how two tired pilots over-complicate what should have been a routine flight with a simple vhf ominidirectional range out. Recommendations: (1) Call in fatigued before you deteriorate, even if coming in from a day off (2) When the company knows an early show with a time change is scheduled, [they should] not wait until the last minute to notify a crew member. Question for the reader: What actions could the first officer have taken to mitigate the fatigue resulting from having a baby at home? FLIGHT CREW INCAPACITATION Title: Pilot incapacitation during flight. We had started the final descent to the ILS. The Captain was flying the autopilot. air traffic control gave us a heading change. I acknowledged but noticed that the Captain was not turning the heading knob. I repeated the heading change to him, and he reached for the airspeed knob. I asked him if he was OK. He sud­denly started shaking all over and . . . pushing on the rudder and leaning on the yoke. I quickly started to counter his input as the autopilot disconnected. When the flight attendant continuing airworthines management exposition in, I was still wrestling with the controls. The Captain suddenly went limp, but with his leg still pushing on the rudder. A doctor sitting in First Class continuing airworthines management exposition up to help move the Captain out of his seat. In the meantime, I had declared an emergency and requested a turn to final. By then, the Captain had woken up and was fighting the doctor and the Flight Attendant to get back up. [Eventually], they secured the Captain. He required further medical treatment. Question for the reader: What possible medical conditions could create the behavior exhibited by the captain? FLIGHT CREW SUBTLE INCAPACITATION Title: Subtle incapacitation from an infection. All preflight duties and initial takeoff normally. During the enroute climb, I had to remind the Captain to reset his altimeter, as well as insist that he participate in altitude awareness procedures. Small portions of the Captain's speech became unrecognizable. I took control of the aircraft, and advised the Captain that I would fly the remainder of the flight. The Captain agreed, however, his actions indicated that he wanted to participate. Not wanting to create a confrontational atmosphere, I asked the Captain to get the automatic terminal information system and the approach plates. These tasks became too difficult for the Captain to accomplish. An uneventful landing was accomplished. The incapacitation was very subtle, with the Captain going into and out of a completely normal state periodically. He wanted to "help " with the flying when he was not lucid. I wish that it had been a sudden and complete incapacitation, as this would have been easier to recognize and deal with. Question for the reader: How would you determine if a colleague was subtly incapacitated in the middle of a flight operation? MAINTENANCE MISCOMMUNICATION Title: Miscommunication from confusing maintenance directions. A landing gear bushing was significantly over-torqued when three Aviation Maintenance Technicians (AMTs), a Lead Technician, and a Shift Supervisor all misinterpreted a torque setting. I was assigned to work on securing an A320 right main landing gear Side Stay Bushing. I was directed by my Lead Mechanic to work with [two other AMTs] . . . . We briefly went over the paperwork for this phase and Lead showed us the torque was 500 foot-pounds . . . . I set the tooling in place, put the nut and locking tab washer in place, spun it down by hand, and then engaged the tooling to begin the final torquing of the retaining nut. [The other AMTs] read that the final torque setting was 500 foot-pounds and that the initial torque setting was 440 foot-pounds. The torque wrench was set to 440 foot-pounds, shown to our inspector, and then attached to the tooling. Once the initial torque was reached, we (myself and our Inspector) checked the tab lock positions, and it was necessary to advance the position of the retaining nut by close to 1/4 inch to align the lock tab. Once we reached 500 foot-pounds, the tab lock was still not aligned. The Inspector instructed us to back the collar off and then reapply the minimum torque of 440 foot-pounds and recheck the tab lock position. We continued this through four break/reset sequences with no better luck. We went to the incoming midnight Supervisor and explained the dilemma. He took the paperwork and briefly perused it and then said that we should turn the issue over to the incoming crew. We turned the paperwork over to [the midnight shift Lead] and explained the problem we were having. He left with the paperwork and returned approximately 15 minutes later to show me that he read that the torque was to be no more than 500 INCH pounds. The paperwork had "500 lbf.in" in the text. Because of this misinter­pretation, the applied torque was 12 times greater than was intended in the operation. There is a difference between the way Boeing and Airbus present this information. Boeing uses "lb-ft" for foot-pounds and "lb-in" for inch-pounds. Airbus references foot-pounds with "LBF.FT" and inch-pounds with "LBF.IN". I believe that "LBF.IN" is very confusing and led to our mistake in applying the improper torque for the job. Perhaps "LB.IN” or spelling out "foot-pounds " or "inch-pounds " would be clearer. Question for the reader: If you were a safety manager with responsibility over this maintenance operation what actions would you take based on this report? AIR TRAFFIC CONTROL DEVIATION Title: Deviating from standardized phraseology . . . . we finally contacted Departure passing through approximately 6,500 feet climbing. The Controller's response was a hurried, 'Roger, maintain 2-3-0.' The Captain responded, 'Roger, 2-3-0.' At this point, flight level 230 was selected on the aircraft's MCP (Mode Control Panel) . . . . It was at this point that the Controller said that we had been assigned 8,000 feet. The Captain replied that we had been assigned flight level 230. The Controller's response was, 'I said two-hundred thirty knots, sir.' . . . Those numbers can imply heading, altitude or airspeed. Question for the reader: How did both the air traffic controller and the flight crew contribute to this grave miscommunication? CONCLUSION Aviation professionals are key members of the safety value chain that keeps acci­dent rates low in commercial flight operations. Although humans are largely responsible for commercial aviation's excellent safety record, human errors none­theless cause or contribute to most accidents. Moreover, the rate of pilot-error accidents shows no sign of decreasing, while weather-related crashes are declin­ing, and aircraft component failures are rarely the sole factor in serious accidents. Furthermore, accident and incident data analyses indicate that if only a portion of human-error problems can be solved, substantial reductions in accident risk can be attained. Another concern among human-performance experts is that the increased level of automation in professional settings may create a generation of workers whose basic flying " Stick and Rudder" skills deteriorate from lack of practice. If such manual abilities ever become needed because of automation failure/degradation or unusual aircraft attitudes and conditions that automation cannot handle, the pilot may not be up to the challenge. Manual piloting skills may have degraded because of the use or overuse of automatic flight systems in lieu of hand flying or because of the lack of training and practice on certain maneuvers and skills. The increasing complexity of technology, airspace, and legislation means that pilots must be complete knowledge masters of their realm. Automation does produce the gains in efficiency and situational awareness that prompted its development in the first place, but only if the operators are proficient with the automation during both normal and emergency situations. Additionally, pilots must possess the soft skills of effec­tive communication, leadership, and followership which will be discussed in the next chapter. These are very involved skills that require an open-minded approach, a willingness to learn, and lots of practice. What we consider to be an expert pilot has changed in the past century from someone who can make accurate decisions without proper information to someone who knows how to seek out accurate informa­tion and apply it via automation in a team environment. Human error on the flight deck can never be totally eliminated. The same holds for human error by dispatchers, flight attendants, ramp personnel, aviation maintenance technicians, air traffic controllers, and anyone else who forms part of the commercial aviation safety value chain. However, through judicious design, constant monitoring of accidents, incidents, and internal reports, and the aggressive use of reporting systems such as NASA's ASRS, different means of preventive safety measures can be created. Air transportation enjoys an excellent safety record today largely because no part of the system is ever allowed to rest. Lack of situational awareness has been identified as a contributing factor in many accidents and incidents, but the reasons for its breakdown, such as cognitive errors or fatigue, must be probed and understood through human factor analysis and classification sytems and other investigative means to obtain continued safety gains. Modern airline training tools can greatly improve pilot performance through enhanced ADM and a clear understanding of human factors principles to identify and control human error. Numerous errors grow out of our natural cognitive processes. The tendency to take "thinking shortcuts" makes our mind very susceptible to a host of subtle influences and biases, such as false expectations of what is about to happen. We are left to wonder how the human race has made it as far as it has, particularly when we contemplate that these cognitive errors or biases can work in concert with each other. Our success has been in spite of our failings because the brain is usually a great match for the tasks we face. However, that answer is not good enough when dealing with high-reliability operations such as those of aviation. The next chapter in this book will expose the flipside of this chapter by addressing the reasons humans, despite our fallibility, add greatly to the safety value chain and are not just the problem, but also the solution. KEY TERMS Automation, Automation Surprise Aviate, Navigate, Communicate Cognitive Error Crew Resource Management (Crew Resource Management ) DECIDE Model Embracing Our Blunders Error Chain Fatigue Fitness for Duty Garbage In, Garbage Out Goldilocks Zone High-Consequence Operation High-Reliability Organization High-Risk Industry Human Factors Human Factors Analysis and Classification System (HFACS) Human Factors Intervention Matrix (HFIX) Human Performance Information Overload Miller's Law Mode Confusion Multi-causality Pilot Error Safety Value Chain Situational Awareness Soft Skills Sterile Cockpit Rule Threat and Error Management (TEM) Unsafe Acts REVIEW QUESTIONS 1. Do you think that aviation can be or ever will be devoid of human error? 2. What are the three reasons why people make poor decisions? How does this relate to commercial aviation safety? 3. Describe the difference between human factors and human performance. 4. Do you think error chains always have a clear starting point? Why or why not? 5. How do you think the "pilot error" myth affects the attitude of pilots in the skies today? 6. Describe two different types of cognitive errors. 7. Explain what is meant by the so-called "goldilocks" zone of workload with regards to optimal situational awareness. 8. Give a detailed explanation of fatigue and how it affects a flight crew. 9. What is situation awareness? 10. In your opinion, which human-automation interaction outcomes pose the largest threat to commercial aviation safety: automation surprise, mode confusion, or garbage in, garbage out? Defend your answer. 11. If you were an air safety investigator, how would you use human factor analysis and classification sytems and HFIX to address a miscommunication between a flight attendant and an